{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "877bf339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from dbn.tensorflow import SupervisedDBNRegression,UnsupervisedDBN\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras import Sequential, layers, callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f125bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('yol_53.csv').set_index('year_month')\n",
    "scalerx = StandardScaler()\n",
    "scalery = StandardScaler()\n",
    "borehole = 'D4N0053'\n",
    "figname = \"p53x.jpeg\"\n",
    "bbh = \"BH0053\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f65af4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MMS = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7af7e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Discharge_log</th>\n",
       "      <th>Temperature_log</th>\n",
       "      <th>precip_trend_log</th>\n",
       "      <th>Abstraction_log</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>D4N0053</th>\n",
       "      <th>dex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year_month</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1974-02-01</th>\n",
       "      <td>-1.471598</td>\n",
       "      <td>-7.476635</td>\n",
       "      <td>1.396056</td>\n",
       "      <td>-3.304228</td>\n",
       "      <td>4.476516</td>\n",
       "      <td>4.357043</td>\n",
       "      <td>0.506091</td>\n",
       "      <td>1974-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974-03-01</th>\n",
       "      <td>-0.892100</td>\n",
       "      <td>-7.476635</td>\n",
       "      <td>1.282178</td>\n",
       "      <td>-3.304228</td>\n",
       "      <td>0.483592</td>\n",
       "      <td>4.357043</td>\n",
       "      <td>0.651097</td>\n",
       "      <td>1974-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974-04-01</th>\n",
       "      <td>-0.167495</td>\n",
       "      <td>-7.476635</td>\n",
       "      <td>1.383397</td>\n",
       "      <td>-3.304228</td>\n",
       "      <td>0.624827</td>\n",
       "      <td>0.462552</td>\n",
       "      <td>0.691078</td>\n",
       "      <td>1974-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974-05-01</th>\n",
       "      <td>0.249022</td>\n",
       "      <td>-7.476635</td>\n",
       "      <td>1.380273</td>\n",
       "      <td>-3.304228</td>\n",
       "      <td>0.663768</td>\n",
       "      <td>0.600305</td>\n",
       "      <td>0.667209</td>\n",
       "      <td>1974-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974-06-01</th>\n",
       "      <td>0.275379</td>\n",
       "      <td>-7.476635</td>\n",
       "      <td>1.014413</td>\n",
       "      <td>-3.304228</td>\n",
       "      <td>0.640520</td>\n",
       "      <td>0.638286</td>\n",
       "      <td>0.638566</td>\n",
       "      <td>1974-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-01</th>\n",
       "      <td>-0.283679</td>\n",
       "      <td>0.286993</td>\n",
       "      <td>0.556564</td>\n",
       "      <td>1.251855</td>\n",
       "      <td>1.007845</td>\n",
       "      <td>0.987487</td>\n",
       "      <td>1.049116</td>\n",
       "      <td>2004-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-09-01</th>\n",
       "      <td>-0.327951</td>\n",
       "      <td>-0.007973</td>\n",
       "      <td>0.278455</td>\n",
       "      <td>1.251855</td>\n",
       "      <td>1.012495</td>\n",
       "      <td>0.973881</td>\n",
       "      <td>1.044343</td>\n",
       "      <td>2004-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-10-01</th>\n",
       "      <td>-0.350234</td>\n",
       "      <td>-0.337653</td>\n",
       "      <td>0.224374</td>\n",
       "      <td>1.251855</td>\n",
       "      <td>1.007845</td>\n",
       "      <td>0.978416</td>\n",
       "      <td>1.039569</td>\n",
       "      <td>2004-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-11-01</th>\n",
       "      <td>-0.368879</td>\n",
       "      <td>-0.932084</td>\n",
       "      <td>0.442389</td>\n",
       "      <td>1.251855</td>\n",
       "      <td>1.003196</td>\n",
       "      <td>0.973881</td>\n",
       "      <td>1.044421</td>\n",
       "      <td>2004-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-01</th>\n",
       "      <td>-0.380100</td>\n",
       "      <td>-1.229243</td>\n",
       "      <td>0.460406</td>\n",
       "      <td>1.251855</td>\n",
       "      <td>1.007921</td>\n",
       "      <td>0.969346</td>\n",
       "      <td>1.049116</td>\n",
       "      <td>2004-12-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>371 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Discharge_log  Temperature_log  precip_trend_log  Abstraction_log  \\\n",
       "year_month                                                                      \n",
       "1974-02-01      -1.471598        -7.476635          1.396056        -3.304228   \n",
       "1974-03-01      -0.892100        -7.476635          1.282178        -3.304228   \n",
       "1974-04-01      -0.167495        -7.476635          1.383397        -3.304228   \n",
       "1974-05-01       0.249022        -7.476635          1.380273        -3.304228   \n",
       "1974-06-01       0.275379        -7.476635          1.014413        -3.304228   \n",
       "...                   ...              ...               ...              ...   \n",
       "2004-08-01      -0.283679         0.286993          0.556564         1.251855   \n",
       "2004-09-01      -0.327951        -0.007973          0.278455         1.251855   \n",
       "2004-10-01      -0.350234        -0.337653          0.224374         1.251855   \n",
       "2004-11-01      -0.368879        -0.932084          0.442389         1.251855   \n",
       "2004-12-01      -0.380100        -1.229243          0.460406         1.251855   \n",
       "\n",
       "                 t_1       t_2   D4N0053         dex  \n",
       "year_month                                            \n",
       "1974-02-01  4.476516  4.357043  0.506091  1974-02-01  \n",
       "1974-03-01  0.483592  4.357043  0.651097  1974-03-01  \n",
       "1974-04-01  0.624827  0.462552  0.691078  1974-04-01  \n",
       "1974-05-01  0.663768  0.600305  0.667209  1974-05-01  \n",
       "1974-06-01  0.640520  0.638286  0.638566  1974-06-01  \n",
       "...              ...       ...       ...         ...  \n",
       "2004-08-01  1.007845  0.987487  1.049116  2004-08-01  \n",
       "2004-09-01  1.012495  0.973881  1.044343  2004-09-01  \n",
       "2004-10-01  1.007845  0.978416  1.039569  2004-10-01  \n",
       "2004-11-01  1.003196  0.973881  1.044421  2004-11-01  \n",
       "2004-12-01  1.007921  0.969346  1.049116  2004-12-01  \n",
       "\n",
       "[371 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract(lst):\n",
    "    return [[el] for el in lst]\n",
    "ydf = df[borehole]\n",
    "ydf = extract(ydf)\n",
    "df = pd.DataFrame(scalerx.fit_transform(df), columns=df.columns, index=df.index)\n",
    "df = df.reset_index()\n",
    "df['dex'] = df['year_month']\n",
    "df = df.set_index('year_month')\n",
    "ydf = scalery.fit_transform(ydf)\n",
    "# df = df.drop([\"Abstraction_log\"], axis = 1)\n",
    "X = np.array(df.drop([borehole], axis = 1).reset_index(drop=True))\n",
    "Y = np.array(df[borehole])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5af652fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ave(lis):\n",
    "    return sum(lis)/ len(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c37dc8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 10\n",
    "dbn_ave_r2 = []\n",
    "gru_ave_r2 = []\n",
    "lstm_ave_r2 = []\n",
    "lstm_gru_ave_r2 = []\n",
    "ffnn_ave_r2 = []\n",
    "dbn_lstm_ave_r2 = []\n",
    "dbn_lstm_blind_r2 = []\n",
    "\n",
    "dbn_ave_mse = []\n",
    "gru_ave_mse = []\n",
    "lstm_ave_mse = []\n",
    "lstm_gru_ave_mse = []\n",
    "dbn_lstm_ave_mse = []\n",
    "ffnn_ave_mse = []\n",
    "dbn_lstm_blind_mse = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fbbe06",
   "metadata": {},
   "source": [
    "## DBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d67bc5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310, 6) (310,)\n",
      "(61, 6) (61,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "tss = TimeSeriesSplit(n_splits = 5)\n",
    "for train_index, test_index in tss.split(X):\n",
    "    X_train, X_test = X[train_index, :], X[test_index,:]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "dummyXtest = pd.DataFrame(X_test)\n",
    "dummyXtest.columns = ['a','b','c','d','e','f', 'g']\n",
    "# dummyXtest.columns = ['a','b','d','e','f', 'g']\n",
    "X_test = np.array(dummyXtest.drop(['g'], axis = 1).reset_index(drop=True))\n",
    "\n",
    "dummyXtrain = pd.DataFrame(X_train)\n",
    "dummyXtrain.columns = ['a','b','c','d','e','f', 'g']\n",
    "# dummyXtrain.columns = ['a','b','d','e','f', 'g']\n",
    "\n",
    "X_train = np.array(dummyXtrain.drop(['g'], axis = 1).reset_index(drop=True))\n",
    "\n",
    "\n",
    "print (X_train.shape, Y_train.shape)\n",
    "print (X_test.shape, Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b75816",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL:  1\n",
      "\n",
      "\n",
      "[START] Pre-training step:\n",
      "WARNING:tensorflow:From C:\\Users\\b14ck\\Desktop\\Research Report\\53\\dbn\\tensorflow\\models.py:151: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 5.472568\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 4.381129\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 4.518039\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 5.011850\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 5.869471\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 7.273640\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 9.392431\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 13.366038\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 18.659871\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 25.717840\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 59.585434\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 363.296722\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 752.454773\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 1035.211548\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 1191.103149\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 1346.150391\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 1385.429688\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 1493.677002\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 1365.841675\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 1575.817749\n",
      "[END] Pre-training step\n",
      "WARNING:tensorflow:From C:\\Users\\b14ck\\Desktop\\Research Report\\53\\dbn\\tensorflow\\models.py:339: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.479649\n",
      ">> Epoch 1 finished \tANN training loss 1.267526\n",
      ">> Epoch 2 finished \tANN training loss 1.145206\n",
      ">> Epoch 3 finished \tANN training loss 1.094647\n",
      ">> Epoch 4 finished \tANN training loss 1.065218\n",
      ">> Epoch 5 finished \tANN training loss 1.066819\n",
      ">> Epoch 6 finished \tANN training loss 1.057718\n",
      ">> Epoch 7 finished \tANN training loss 1.041689\n",
      ">> Epoch 8 finished \tANN training loss 1.034888\n",
      ">> Epoch 9 finished \tANN training loss 1.034546\n",
      ">> Epoch 10 finished \tANN training loss 1.029571\n",
      ">> Epoch 11 finished \tANN training loss 1.023658\n",
      ">> Epoch 12 finished \tANN training loss 1.018058\n",
      ">> Epoch 13 finished \tANN training loss 1.008457\n",
      ">> Epoch 14 finished \tANN training loss 0.998083\n",
      ">> Epoch 15 finished \tANN training loss 0.984916\n",
      ">> Epoch 16 finished \tANN training loss 0.965621\n",
      ">> Epoch 17 finished \tANN training loss 0.942683\n",
      ">> Epoch 18 finished \tANN training loss 0.912522\n",
      ">> Epoch 19 finished \tANN training loss 0.868615\n",
      ">> Epoch 20 finished \tANN training loss 0.824782\n",
      ">> Epoch 21 finished \tANN training loss 0.778242\n",
      ">> Epoch 22 finished \tANN training loss 0.712206\n",
      ">> Epoch 23 finished \tANN training loss 0.694632\n",
      ">> Epoch 24 finished \tANN training loss 0.617709\n",
      ">> Epoch 25 finished \tANN training loss 0.563716\n",
      ">> Epoch 26 finished \tANN training loss 0.516549\n",
      ">> Epoch 27 finished \tANN training loss 0.463630\n",
      ">> Epoch 28 finished \tANN training loss 0.428851\n",
      ">> Epoch 29 finished \tANN training loss 0.408023\n",
      ">> Epoch 30 finished \tANN training loss 0.373900\n",
      ">> Epoch 31 finished \tANN training loss 0.346471\n",
      ">> Epoch 32 finished \tANN training loss 0.329298\n",
      ">> Epoch 33 finished \tANN training loss 0.307550\n",
      ">> Epoch 34 finished \tANN training loss 0.295592\n",
      ">> Epoch 35 finished \tANN training loss 0.286590\n",
      ">> Epoch 36 finished \tANN training loss 0.274152\n",
      ">> Epoch 37 finished \tANN training loss 0.264423\n",
      ">> Epoch 38 finished \tANN training loss 0.249295\n",
      ">> Epoch 39 finished \tANN training loss 0.246175\n",
      ">> Epoch 40 finished \tANN training loss 0.238934\n",
      ">> Epoch 41 finished \tANN training loss 0.229835\n",
      ">> Epoch 42 finished \tANN training loss 0.232251\n",
      ">> Epoch 43 finished \tANN training loss 0.223480\n",
      ">> Epoch 44 finished \tANN training loss 0.218834\n",
      ">> Epoch 45 finished \tANN training loss 0.214926\n",
      ">> Epoch 46 finished \tANN training loss 0.210739\n",
      ">> Epoch 47 finished \tANN training loss 0.209364\n",
      ">> Epoch 48 finished \tANN training loss 0.200140\n",
      ">> Epoch 49 finished \tANN training loss 0.196944\n",
      ">> Epoch 50 finished \tANN training loss 0.191963\n",
      ">> Epoch 51 finished \tANN training loss 0.183647\n",
      ">> Epoch 52 finished \tANN training loss 0.186651\n",
      ">> Epoch 53 finished \tANN training loss 0.181687\n",
      ">> Epoch 54 finished \tANN training loss 0.177657\n",
      ">> Epoch 55 finished \tANN training loss 0.173701\n",
      ">> Epoch 56 finished \tANN training loss 0.169221\n",
      ">> Epoch 57 finished \tANN training loss 0.167119\n",
      ">> Epoch 58 finished \tANN training loss 0.166423\n",
      ">> Epoch 59 finished \tANN training loss 0.163138\n",
      ">> Epoch 60 finished \tANN training loss 0.165566\n",
      ">> Epoch 61 finished \tANN training loss 0.163517\n",
      ">> Epoch 62 finished \tANN training loss 0.164090\n",
      ">> Epoch 63 finished \tANN training loss 0.161842\n",
      ">> Epoch 64 finished \tANN training loss 0.158506\n",
      ">> Epoch 65 finished \tANN training loss 0.152858\n",
      ">> Epoch 66 finished \tANN training loss 0.153147\n",
      ">> Epoch 67 finished \tANN training loss 0.149856\n",
      ">> Epoch 68 finished \tANN training loss 0.149612\n",
      ">> Epoch 69 finished \tANN training loss 0.147688\n",
      ">> Epoch 70 finished \tANN training loss 0.147534\n",
      ">> Epoch 71 finished \tANN training loss 0.150968\n",
      ">> Epoch 72 finished \tANN training loss 0.154065\n",
      ">> Epoch 73 finished \tANN training loss 0.149790\n",
      ">> Epoch 74 finished \tANN training loss 0.149400\n",
      ">> Epoch 75 finished \tANN training loss 0.146246\n",
      ">> Epoch 76 finished \tANN training loss 0.146863\n",
      ">> Epoch 77 finished \tANN training loss 0.146904\n",
      ">> Epoch 78 finished \tANN training loss 0.139961\n",
      ">> Epoch 79 finished \tANN training loss 0.142094\n",
      ">> Epoch 80 finished \tANN training loss 0.136794\n",
      ">> Epoch 81 finished \tANN training loss 0.134416\n",
      ">> Epoch 82 finished \tANN training loss 0.138195\n",
      ">> Epoch 83 finished \tANN training loss 0.137299\n",
      ">> Epoch 84 finished \tANN training loss 0.134536\n",
      ">> Epoch 85 finished \tANN training loss 0.130441\n",
      ">> Epoch 86 finished \tANN training loss 0.131894\n",
      ">> Epoch 87 finished \tANN training loss 0.131589\n",
      ">> Epoch 88 finished \tANN training loss 0.131021\n",
      ">> Epoch 89 finished \tANN training loss 0.131427\n",
      ">> Epoch 90 finished \tANN training loss 0.132288\n",
      ">> Epoch 91 finished \tANN training loss 0.133701\n",
      ">> Epoch 92 finished \tANN training loss 0.130942\n",
      ">> Epoch 93 finished \tANN training loss 0.131688\n",
      ">> Epoch 94 finished \tANN training loss 0.132981\n",
      ">> Epoch 95 finished \tANN training loss 0.130390\n",
      ">> Epoch 96 finished \tANN training loss 0.130582\n",
      ">> Epoch 97 finished \tANN training loss 0.125201\n",
      ">> Epoch 98 finished \tANN training loss 0.123178\n",
      ">> Epoch 99 finished \tANN training loss 0.122612\n",
      ">> Epoch 100 finished \tANN training loss 0.122917\n",
      ">> Epoch 101 finished \tANN training loss 0.122138\n",
      ">> Epoch 102 finished \tANN training loss 0.121821\n",
      ">> Epoch 103 finished \tANN training loss 0.127155\n",
      ">> Epoch 104 finished \tANN training loss 0.122221\n",
      ">> Epoch 105 finished \tANN training loss 0.125338\n",
      ">> Epoch 106 finished \tANN training loss 0.122191\n",
      ">> Epoch 107 finished \tANN training loss 0.121829\n",
      ">> Epoch 108 finished \tANN training loss 0.126451\n",
      ">> Epoch 109 finished \tANN training loss 0.123573\n",
      ">> Epoch 110 finished \tANN training loss 0.128262\n",
      ">> Epoch 111 finished \tANN training loss 0.134551\n",
      ">> Epoch 112 finished \tANN training loss 0.124012\n",
      ">> Epoch 113 finished \tANN training loss 0.125519\n",
      ">> Epoch 114 finished \tANN training loss 0.127607\n",
      ">> Epoch 115 finished \tANN training loss 0.126712\n",
      ">> Epoch 116 finished \tANN training loss 0.121218\n",
      ">> Epoch 117 finished \tANN training loss 0.120339\n",
      ">> Epoch 118 finished \tANN training loss 0.115006\n",
      ">> Epoch 119 finished \tANN training loss 0.118370\n",
      ">> Epoch 120 finished \tANN training loss 0.117025\n",
      ">> Epoch 121 finished \tANN training loss 0.119284\n",
      ">> Epoch 122 finished \tANN training loss 0.117561\n",
      ">> Epoch 123 finished \tANN training loss 0.116621\n",
      ">> Epoch 124 finished \tANN training loss 0.118407\n",
      ">> Epoch 125 finished \tANN training loss 0.120870\n",
      ">> Epoch 126 finished \tANN training loss 0.124130\n",
      ">> Epoch 127 finished \tANN training loss 0.116239\n",
      ">> Epoch 128 finished \tANN training loss 0.113295\n",
      ">> Epoch 129 finished \tANN training loss 0.114874\n",
      ">> Epoch 130 finished \tANN training loss 0.118424\n",
      ">> Epoch 131 finished \tANN training loss 0.117718\n",
      ">> Epoch 132 finished \tANN training loss 0.127047\n",
      ">> Epoch 133 finished \tANN training loss 0.119454\n",
      ">> Epoch 134 finished \tANN training loss 0.123826\n",
      ">> Epoch 135 finished \tANN training loss 0.122278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 136 finished \tANN training loss 0.119139\n",
      ">> Epoch 137 finished \tANN training loss 0.121674\n",
      ">> Epoch 138 finished \tANN training loss 0.119610\n",
      ">> Epoch 139 finished \tANN training loss 0.116677\n",
      ">> Epoch 140 finished \tANN training loss 0.114915\n",
      ">> Epoch 141 finished \tANN training loss 0.117218\n",
      ">> Epoch 142 finished \tANN training loss 0.110158\n",
      ">> Epoch 143 finished \tANN training loss 0.113975\n",
      ">> Epoch 144 finished \tANN training loss 0.110897\n",
      ">> Epoch 145 finished \tANN training loss 0.111694\n",
      ">> Epoch 146 finished \tANN training loss 0.108677\n",
      ">> Epoch 147 finished \tANN training loss 0.109294\n",
      ">> Epoch 148 finished \tANN training loss 0.111377\n",
      ">> Epoch 149 finished \tANN training loss 0.112138\n",
      ">> Epoch 150 finished \tANN training loss 0.121332\n",
      ">> Epoch 151 finished \tANN training loss 0.117031\n",
      ">> Epoch 152 finished \tANN training loss 0.117916\n",
      ">> Epoch 153 finished \tANN training loss 0.113095\n",
      ">> Epoch 154 finished \tANN training loss 0.114021\n",
      ">> Epoch 155 finished \tANN training loss 0.116987\n",
      ">> Epoch 156 finished \tANN training loss 0.114945\n",
      ">> Epoch 157 finished \tANN training loss 0.115384\n",
      ">> Epoch 158 finished \tANN training loss 0.113694\n",
      ">> Epoch 159 finished \tANN training loss 0.113947\n",
      ">> Epoch 160 finished \tANN training loss 0.114912\n",
      ">> Epoch 161 finished \tANN training loss 0.112013\n",
      ">> Epoch 162 finished \tANN training loss 0.113696\n",
      ">> Epoch 163 finished \tANN training loss 0.117056\n",
      ">> Epoch 164 finished \tANN training loss 0.114693\n",
      ">> Epoch 165 finished \tANN training loss 0.122447\n",
      ">> Epoch 166 finished \tANN training loss 0.113848\n",
      ">> Epoch 167 finished \tANN training loss 0.115905\n",
      ">> Epoch 168 finished \tANN training loss 0.113478\n",
      ">> Epoch 169 finished \tANN training loss 0.112883\n",
      ">> Epoch 170 finished \tANN training loss 0.113162\n",
      ">> Epoch 171 finished \tANN training loss 0.112542\n",
      ">> Epoch 172 finished \tANN training loss 0.113034\n",
      ">> Epoch 173 finished \tANN training loss 0.111967\n",
      ">> Epoch 174 finished \tANN training loss 0.108322\n",
      ">> Epoch 175 finished \tANN training loss 0.116233\n",
      ">> Epoch 176 finished \tANN training loss 0.108193\n",
      ">> Epoch 177 finished \tANN training loss 0.110595\n",
      ">> Epoch 178 finished \tANN training loss 0.111137\n",
      ">> Epoch 179 finished \tANN training loss 0.110791\n",
      ">> Epoch 180 finished \tANN training loss 0.111743\n",
      ">> Epoch 181 finished \tANN training loss 0.109507\n",
      ">> Epoch 182 finished \tANN training loss 0.106942\n",
      ">> Epoch 183 finished \tANN training loss 0.111260\n",
      ">> Epoch 184 finished \tANN training loss 0.107867\n",
      ">> Epoch 185 finished \tANN training loss 0.108836\n",
      ">> Epoch 186 finished \tANN training loss 0.110305\n",
      ">> Epoch 187 finished \tANN training loss 0.109378\n",
      ">> Epoch 188 finished \tANN training loss 0.108508\n",
      ">> Epoch 189 finished \tANN training loss 0.109507\n",
      ">> Epoch 190 finished \tANN training loss 0.109199\n",
      ">> Epoch 191 finished \tANN training loss 0.107542\n",
      ">> Epoch 192 finished \tANN training loss 0.106225\n",
      ">> Epoch 193 finished \tANN training loss 0.112785\n",
      ">> Epoch 194 finished \tANN training loss 0.106060\n",
      ">> Epoch 195 finished \tANN training loss 0.107963\n",
      ">> Epoch 196 finished \tANN training loss 0.112711\n",
      ">> Epoch 197 finished \tANN training loss 0.105497\n",
      ">> Epoch 198 finished \tANN training loss 0.104809\n",
      ">> Epoch 199 finished \tANN training loss 0.106768\n",
      ">> Epoch 200 finished \tANN training loss 0.106558\n",
      ">> Epoch 201 finished \tANN training loss 0.104668\n",
      ">> Epoch 202 finished \tANN training loss 0.105414\n",
      ">> Epoch 203 finished \tANN training loss 0.114034\n",
      ">> Epoch 204 finished \tANN training loss 0.105819\n",
      ">> Epoch 205 finished \tANN training loss 0.106542\n",
      ">> Epoch 206 finished \tANN training loss 0.106701\n",
      ">> Epoch 207 finished \tANN training loss 0.106104\n",
      ">> Epoch 208 finished \tANN training loss 0.108156\n",
      ">> Epoch 209 finished \tANN training loss 0.110205\n",
      ">> Epoch 210 finished \tANN training loss 0.105918\n",
      ">> Epoch 211 finished \tANN training loss 0.106366\n",
      ">> Epoch 212 finished \tANN training loss 0.106363\n",
      ">> Epoch 213 finished \tANN training loss 0.114766\n",
      ">> Epoch 214 finished \tANN training loss 0.111419\n",
      ">> Epoch 215 finished \tANN training loss 0.111396\n",
      ">> Epoch 216 finished \tANN training loss 0.109403\n",
      ">> Epoch 217 finished \tANN training loss 0.109652\n",
      ">> Epoch 218 finished \tANN training loss 0.106687\n",
      ">> Epoch 219 finished \tANN training loss 0.105037\n",
      ">> Epoch 220 finished \tANN training loss 0.106961\n",
      ">> Epoch 221 finished \tANN training loss 0.104202\n",
      ">> Epoch 222 finished \tANN training loss 0.102579\n",
      ">> Epoch 223 finished \tANN training loss 0.108895\n",
      ">> Epoch 224 finished \tANN training loss 0.108598\n",
      ">> Epoch 225 finished \tANN training loss 0.109250\n",
      ">> Epoch 226 finished \tANN training loss 0.112924\n",
      ">> Epoch 227 finished \tANN training loss 0.106958\n",
      ">> Epoch 228 finished \tANN training loss 0.105693\n",
      ">> Epoch 229 finished \tANN training loss 0.107023\n",
      ">> Epoch 230 finished \tANN training loss 0.115237\n",
      ">> Epoch 231 finished \tANN training loss 0.109918\n",
      ">> Epoch 232 finished \tANN training loss 0.103696\n",
      ">> Epoch 233 finished \tANN training loss 0.101376\n",
      ">> Epoch 234 finished \tANN training loss 0.107520\n",
      ">> Epoch 235 finished \tANN training loss 0.111020\n",
      ">> Epoch 236 finished \tANN training loss 0.105624\n",
      ">> Epoch 237 finished \tANN training loss 0.105966\n",
      ">> Epoch 238 finished \tANN training loss 0.110617\n",
      ">> Epoch 239 finished \tANN training loss 0.105260\n",
      ">> Epoch 240 finished \tANN training loss 0.104835\n",
      ">> Epoch 241 finished \tANN training loss 0.105622\n",
      ">> Epoch 242 finished \tANN training loss 0.105437\n",
      ">> Epoch 243 finished \tANN training loss 0.107224\n",
      ">> Epoch 244 finished \tANN training loss 0.106825\n",
      ">> Epoch 245 finished \tANN training loss 0.106455\n",
      ">> Epoch 246 finished \tANN training loss 0.109351\n",
      ">> Epoch 247 finished \tANN training loss 0.103326\n",
      ">> Epoch 248 finished \tANN training loss 0.104314\n",
      ">> Epoch 249 finished \tANN training loss 0.102711\n",
      ">> Epoch 250 finished \tANN training loss 0.104990\n",
      ">> Epoch 251 finished \tANN training loss 0.104087\n",
      ">> Epoch 252 finished \tANN training loss 0.106741\n",
      ">> Epoch 253 finished \tANN training loss 0.110340\n",
      ">> Epoch 254 finished \tANN training loss 0.111369\n",
      ">> Epoch 255 finished \tANN training loss 0.108125\n",
      ">> Epoch 256 finished \tANN training loss 0.116867\n",
      ">> Epoch 257 finished \tANN training loss 0.106255\n",
      ">> Epoch 258 finished \tANN training loss 0.106451\n",
      ">> Epoch 259 finished \tANN training loss 0.113438\n",
      ">> Epoch 260 finished \tANN training loss 0.108532\n",
      ">> Epoch 261 finished \tANN training loss 0.113316\n",
      ">> Epoch 262 finished \tANN training loss 0.110255\n",
      ">> Epoch 263 finished \tANN training loss 0.103181\n",
      ">> Epoch 264 finished \tANN training loss 0.111235\n",
      ">> Epoch 265 finished \tANN training loss 0.107008\n",
      ">> Epoch 266 finished \tANN training loss 0.108932\n",
      ">> Epoch 267 finished \tANN training loss 0.103619\n",
      ">> Epoch 268 finished \tANN training loss 0.099161\n",
      ">> Epoch 269 finished \tANN training loss 0.101191\n",
      ">> Epoch 270 finished \tANN training loss 0.097931\n",
      ">> Epoch 271 finished \tANN training loss 0.100910\n",
      ">> Epoch 272 finished \tANN training loss 0.110666\n",
      ">> Epoch 273 finished \tANN training loss 0.109802\n",
      ">> Epoch 274 finished \tANN training loss 0.112959\n",
      ">> Epoch 275 finished \tANN training loss 0.109946\n",
      ">> Epoch 276 finished \tANN training loss 0.107670\n",
      ">> Epoch 277 finished \tANN training loss 0.108517\n",
      ">> Epoch 278 finished \tANN training loss 0.101949\n",
      ">> Epoch 279 finished \tANN training loss 0.099963\n",
      ">> Epoch 280 finished \tANN training loss 0.111461\n",
      ">> Epoch 281 finished \tANN training loss 0.101697\n",
      ">> Epoch 282 finished \tANN training loss 0.102816\n",
      ">> Epoch 283 finished \tANN training loss 0.101794\n",
      ">> Epoch 284 finished \tANN training loss 0.105176\n",
      ">> Epoch 285 finished \tANN training loss 0.102873\n",
      ">> Epoch 286 finished \tANN training loss 0.102786\n",
      ">> Epoch 287 finished \tANN training loss 0.105750\n",
      ">> Epoch 288 finished \tANN training loss 0.104457\n",
      ">> Epoch 289 finished \tANN training loss 0.111411\n",
      ">> Epoch 290 finished \tANN training loss 0.106188\n",
      ">> Epoch 291 finished \tANN training loss 0.103194\n",
      ">> Epoch 292 finished \tANN training loss 0.110331\n",
      ">> Epoch 293 finished \tANN training loss 0.103175\n",
      ">> Epoch 294 finished \tANN training loss 0.106651\n",
      ">> Epoch 295 finished \tANN training loss 0.106057\n",
      ">> Epoch 296 finished \tANN training loss 0.114194\n",
      ">> Epoch 297 finished \tANN training loss 0.108570\n",
      ">> Epoch 298 finished \tANN training loss 0.113064\n",
      ">> Epoch 299 finished \tANN training loss 0.104969\n",
      ">> Epoch 300 finished \tANN training loss 0.106914\n",
      ">> Epoch 301 finished \tANN training loss 0.100963\n",
      ">> Epoch 302 finished \tANN training loss 0.106514\n",
      ">> Epoch 303 finished \tANN training loss 0.106275\n",
      ">> Epoch 304 finished \tANN training loss 0.106672\n",
      ">> Epoch 305 finished \tANN training loss 0.103556\n",
      ">> Epoch 306 finished \tANN training loss 0.103674\n",
      ">> Epoch 307 finished \tANN training loss 0.105174\n",
      ">> Epoch 308 finished \tANN training loss 0.106919\n",
      ">> Epoch 309 finished \tANN training loss 0.101371\n",
      ">> Epoch 310 finished \tANN training loss 0.102405\n",
      ">> Epoch 311 finished \tANN training loss 0.104766\n",
      ">> Epoch 312 finished \tANN training loss 0.102285\n",
      ">> Epoch 313 finished \tANN training loss 0.102913\n",
      ">> Epoch 314 finished \tANN training loss 0.105704\n",
      ">> Epoch 315 finished \tANN training loss 0.111254\n",
      ">> Epoch 316 finished \tANN training loss 0.105841\n",
      ">> Epoch 317 finished \tANN training loss 0.105198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 318 finished \tANN training loss 0.103811\n",
      ">> Epoch 319 finished \tANN training loss 0.101421\n",
      ">> Epoch 320 finished \tANN training loss 0.115001\n",
      ">> Epoch 321 finished \tANN training loss 0.110721\n",
      ">> Epoch 322 finished \tANN training loss 0.103665\n",
      ">> Epoch 323 finished \tANN training loss 0.110279\n",
      ">> Epoch 324 finished \tANN training loss 0.105261\n",
      ">> Epoch 325 finished \tANN training loss 0.102316\n",
      ">> Epoch 326 finished \tANN training loss 0.101411\n",
      ">> Epoch 327 finished \tANN training loss 0.101788\n",
      ">> Epoch 328 finished \tANN training loss 0.099778\n",
      ">> Epoch 329 finished \tANN training loss 0.102296\n",
      ">> Epoch 330 finished \tANN training loss 0.103968\n",
      ">> Epoch 331 finished \tANN training loss 0.103284\n",
      ">> Epoch 332 finished \tANN training loss 0.100778\n",
      ">> Epoch 333 finished \tANN training loss 0.102331\n",
      ">> Epoch 334 finished \tANN training loss 0.099200\n",
      ">> Epoch 335 finished \tANN training loss 0.101823\n",
      ">> Epoch 336 finished \tANN training loss 0.103747\n",
      ">> Epoch 337 finished \tANN training loss 0.102855\n",
      ">> Epoch 338 finished \tANN training loss 0.105882\n",
      ">> Epoch 339 finished \tANN training loss 0.109038\n",
      ">> Epoch 340 finished \tANN training loss 0.111623\n",
      ">> Epoch 341 finished \tANN training loss 0.104794\n",
      ">> Epoch 342 finished \tANN training loss 0.103229\n",
      ">> Epoch 343 finished \tANN training loss 0.109733\n",
      ">> Epoch 344 finished \tANN training loss 0.104430\n",
      ">> Epoch 345 finished \tANN training loss 0.109375\n",
      ">> Epoch 346 finished \tANN training loss 0.109042\n",
      ">> Epoch 347 finished \tANN training loss 0.105475\n",
      ">> Epoch 348 finished \tANN training loss 0.107755\n",
      ">> Epoch 349 finished \tANN training loss 0.101293\n",
      ">> Epoch 350 finished \tANN training loss 0.102805\n",
      ">> Epoch 351 finished \tANN training loss 0.100568\n",
      ">> Epoch 352 finished \tANN training loss 0.099679\n",
      ">> Epoch 353 finished \tANN training loss 0.101195\n",
      ">> Epoch 354 finished \tANN training loss 0.106161\n",
      ">> Epoch 355 finished \tANN training loss 0.100893\n",
      ">> Epoch 356 finished \tANN training loss 0.099304\n",
      ">> Epoch 357 finished \tANN training loss 0.106003\n",
      ">> Epoch 358 finished \tANN training loss 0.097443\n",
      ">> Epoch 359 finished \tANN training loss 0.099574\n",
      ">> Epoch 360 finished \tANN training loss 0.095356\n",
      ">> Epoch 361 finished \tANN training loss 0.097699\n",
      ">> Epoch 362 finished \tANN training loss 0.098134\n",
      ">> Epoch 363 finished \tANN training loss 0.100042\n",
      ">> Epoch 364 finished \tANN training loss 0.098634\n",
      ">> Epoch 365 finished \tANN training loss 0.105287\n",
      ">> Epoch 366 finished \tANN training loss 0.102547\n",
      ">> Epoch 367 finished \tANN training loss 0.103837\n",
      ">> Epoch 368 finished \tANN training loss 0.102373\n",
      ">> Epoch 369 finished \tANN training loss 0.099331\n",
      ">> Epoch 370 finished \tANN training loss 0.108318\n",
      ">> Epoch 371 finished \tANN training loss 0.100060\n",
      ">> Epoch 372 finished \tANN training loss 0.104261\n",
      ">> Epoch 373 finished \tANN training loss 0.101740\n",
      ">> Epoch 374 finished \tANN training loss 0.103881\n",
      ">> Epoch 375 finished \tANN training loss 0.105899\n",
      ">> Epoch 376 finished \tANN training loss 0.104452\n",
      ">> Epoch 377 finished \tANN training loss 0.106808\n",
      ">> Epoch 378 finished \tANN training loss 0.103212\n",
      ">> Epoch 379 finished \tANN training loss 0.099740\n",
      ">> Epoch 380 finished \tANN training loss 0.106826\n",
      ">> Epoch 381 finished \tANN training loss 0.102768\n",
      ">> Epoch 382 finished \tANN training loss 0.107410\n",
      ">> Epoch 383 finished \tANN training loss 0.103915\n",
      ">> Epoch 384 finished \tANN training loss 0.105814\n",
      ">> Epoch 385 finished \tANN training loss 0.108038\n",
      ">> Epoch 386 finished \tANN training loss 0.101170\n",
      ">> Epoch 387 finished \tANN training loss 0.099677\n",
      ">> Epoch 388 finished \tANN training loss 0.101277\n",
      ">> Epoch 389 finished \tANN training loss 0.104130\n",
      ">> Epoch 390 finished \tANN training loss 0.099908\n",
      ">> Epoch 391 finished \tANN training loss 0.101774\n",
      ">> Epoch 392 finished \tANN training loss 0.102394\n",
      ">> Epoch 393 finished \tANN training loss 0.110755\n",
      ">> Epoch 394 finished \tANN training loss 0.107937\n",
      ">> Epoch 395 finished \tANN training loss 0.101803\n",
      ">> Epoch 396 finished \tANN training loss 0.101746\n",
      ">> Epoch 397 finished \tANN training loss 0.100870\n",
      ">> Epoch 398 finished \tANN training loss 0.103474\n",
      ">> Epoch 399 finished \tANN training loss 0.102567\n",
      ">> Epoch 400 finished \tANN training loss 0.100491\n",
      ">> Epoch 401 finished \tANN training loss 0.102653\n",
      ">> Epoch 402 finished \tANN training loss 0.099963\n",
      ">> Epoch 403 finished \tANN training loss 0.102634\n",
      ">> Epoch 404 finished \tANN training loss 0.105356\n",
      ">> Epoch 405 finished \tANN training loss 0.107358\n",
      ">> Epoch 406 finished \tANN training loss 0.109698\n",
      ">> Epoch 407 finished \tANN training loss 0.104862\n",
      ">> Epoch 408 finished \tANN training loss 0.103079\n",
      ">> Epoch 409 finished \tANN training loss 0.101357\n",
      ">> Epoch 410 finished \tANN training loss 0.098889\n",
      ">> Epoch 411 finished \tANN training loss 0.102781\n",
      ">> Epoch 412 finished \tANN training loss 0.104370\n",
      ">> Epoch 413 finished \tANN training loss 0.105528\n",
      ">> Epoch 414 finished \tANN training loss 0.105873\n",
      ">> Epoch 415 finished \tANN training loss 0.110192\n",
      ">> Epoch 416 finished \tANN training loss 0.111916\n",
      ">> Epoch 417 finished \tANN training loss 0.116354\n",
      ">> Epoch 418 finished \tANN training loss 0.110506\n",
      ">> Epoch 419 finished \tANN training loss 0.109411\n",
      ">> Epoch 420 finished \tANN training loss 0.105472\n",
      ">> Epoch 421 finished \tANN training loss 0.108277\n",
      ">> Epoch 422 finished \tANN training loss 0.101947\n",
      ">> Epoch 423 finished \tANN training loss 0.102547\n",
      ">> Epoch 424 finished \tANN training loss 0.103195\n",
      ">> Epoch 425 finished \tANN training loss 0.101972\n",
      ">> Epoch 426 finished \tANN training loss 0.100475\n",
      ">> Epoch 427 finished \tANN training loss 0.102769\n",
      ">> Epoch 428 finished \tANN training loss 0.104148\n",
      ">> Epoch 429 finished \tANN training loss 0.104122\n",
      ">> Epoch 430 finished \tANN training loss 0.098851\n",
      ">> Epoch 431 finished \tANN training loss 0.100679\n",
      ">> Epoch 432 finished \tANN training loss 0.100197\n",
      ">> Epoch 433 finished \tANN training loss 0.102744\n",
      ">> Epoch 434 finished \tANN training loss 0.098973\n",
      ">> Epoch 435 finished \tANN training loss 0.103389\n",
      ">> Epoch 436 finished \tANN training loss 0.106605\n",
      ">> Epoch 437 finished \tANN training loss 0.106045\n",
      ">> Epoch 438 finished \tANN training loss 0.103386\n",
      ">> Epoch 439 finished \tANN training loss 0.104475\n",
      ">> Epoch 440 finished \tANN training loss 0.100836\n",
      ">> Epoch 441 finished \tANN training loss 0.100239\n",
      ">> Epoch 442 finished \tANN training loss 0.100043\n",
      ">> Epoch 443 finished \tANN training loss 0.102265\n",
      ">> Epoch 444 finished \tANN training loss 0.100889\n",
      ">> Epoch 445 finished \tANN training loss 0.097906\n",
      ">> Epoch 446 finished \tANN training loss 0.110759\n",
      ">> Epoch 447 finished \tANN training loss 0.098660\n",
      ">> Epoch 448 finished \tANN training loss 0.098457\n",
      ">> Epoch 449 finished \tANN training loss 0.104819\n",
      ">> Epoch 450 finished \tANN training loss 0.100869\n",
      ">> Epoch 451 finished \tANN training loss 0.098493\n",
      ">> Epoch 452 finished \tANN training loss 0.103832\n",
      ">> Epoch 453 finished \tANN training loss 0.097520\n",
      ">> Epoch 454 finished \tANN training loss 0.101989\n",
      ">> Epoch 455 finished \tANN training loss 0.099783\n",
      ">> Epoch 456 finished \tANN training loss 0.099076\n",
      ">> Epoch 457 finished \tANN training loss 0.101868\n",
      ">> Epoch 458 finished \tANN training loss 0.101709\n",
      ">> Epoch 459 finished \tANN training loss 0.100760\n",
      ">> Epoch 460 finished \tANN training loss 0.106043\n",
      ">> Epoch 461 finished \tANN training loss 0.101086\n",
      ">> Epoch 462 finished \tANN training loss 0.109918\n",
      ">> Epoch 463 finished \tANN training loss 0.103077\n",
      ">> Epoch 464 finished \tANN training loss 0.103483\n",
      ">> Epoch 465 finished \tANN training loss 0.117489\n",
      ">> Epoch 466 finished \tANN training loss 0.100424\n",
      ">> Epoch 467 finished \tANN training loss 0.099980\n",
      ">> Epoch 468 finished \tANN training loss 0.100691\n",
      ">> Epoch 469 finished \tANN training loss 0.100388\n",
      ">> Epoch 470 finished \tANN training loss 0.100048\n",
      ">> Epoch 471 finished \tANN training loss 0.103221\n",
      ">> Epoch 472 finished \tANN training loss 0.101385\n",
      ">> Epoch 473 finished \tANN training loss 0.102000\n",
      ">> Epoch 474 finished \tANN training loss 0.112062\n",
      ">> Epoch 475 finished \tANN training loss 0.111040\n",
      ">> Epoch 476 finished \tANN training loss 0.110367\n",
      ">> Epoch 477 finished \tANN training loss 0.112108\n",
      ">> Epoch 478 finished \tANN training loss 0.108468\n",
      ">> Epoch 479 finished \tANN training loss 0.102208\n",
      ">> Epoch 480 finished \tANN training loss 0.105742\n",
      ">> Epoch 481 finished \tANN training loss 0.107512\n",
      ">> Epoch 482 finished \tANN training loss 0.102474\n",
      ">> Epoch 483 finished \tANN training loss 0.102165\n",
      ">> Epoch 484 finished \tANN training loss 0.098675\n",
      ">> Epoch 485 finished \tANN training loss 0.098484\n",
      ">> Epoch 486 finished \tANN training loss 0.100156\n",
      ">> Epoch 487 finished \tANN training loss 0.098113\n",
      ">> Epoch 488 finished \tANN training loss 0.096617\n",
      ">> Epoch 489 finished \tANN training loss 0.102910\n",
      ">> Epoch 490 finished \tANN training loss 0.107377\n",
      ">> Epoch 491 finished \tANN training loss 0.099341\n",
      ">> Epoch 492 finished \tANN training loss 0.107359\n",
      ">> Epoch 493 finished \tANN training loss 0.105597\n",
      ">> Epoch 494 finished \tANN training loss 0.104783\n",
      ">> Epoch 495 finished \tANN training loss 0.101230\n",
      ">> Epoch 496 finished \tANN training loss 0.097641\n",
      ">> Epoch 497 finished \tANN training loss 0.101375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 498 finished \tANN training loss 0.098273\n",
      ">> Epoch 499 finished \tANN training loss 0.109200\n",
      ">> Epoch 500 finished \tANN training loss 0.109074\n",
      ">> Epoch 501 finished \tANN training loss 0.110450\n",
      ">> Epoch 502 finished \tANN training loss 0.099943\n",
      ">> Epoch 503 finished \tANN training loss 0.103569\n",
      ">> Epoch 504 finished \tANN training loss 0.101212\n",
      ">> Epoch 505 finished \tANN training loss 0.104167\n",
      ">> Epoch 506 finished \tANN training loss 0.109491\n",
      ">> Epoch 507 finished \tANN training loss 0.098226\n",
      ">> Epoch 508 finished \tANN training loss 0.100089\n",
      ">> Epoch 509 finished \tANN training loss 0.096862\n",
      ">> Epoch 510 finished \tANN training loss 0.103333\n",
      ">> Epoch 511 finished \tANN training loss 0.099294\n",
      ">> Epoch 512 finished \tANN training loss 0.099136\n",
      ">> Epoch 513 finished \tANN training loss 0.103127\n",
      ">> Epoch 514 finished \tANN training loss 0.104024\n",
      ">> Epoch 515 finished \tANN training loss 0.100152\n",
      ">> Epoch 516 finished \tANN training loss 0.098710\n",
      ">> Epoch 517 finished \tANN training loss 0.100430\n",
      ">> Epoch 518 finished \tANN training loss 0.094396\n",
      ">> Epoch 519 finished \tANN training loss 0.094652\n",
      ">> Epoch 520 finished \tANN training loss 0.093411\n",
      ">> Epoch 521 finished \tANN training loss 0.101358\n",
      ">> Epoch 522 finished \tANN training loss 0.092792\n",
      ">> Epoch 523 finished \tANN training loss 0.092598\n",
      ">> Epoch 524 finished \tANN training loss 0.098487\n",
      ">> Epoch 525 finished \tANN training loss 0.099485\n",
      ">> Epoch 526 finished \tANN training loss 0.097511\n",
      ">> Epoch 527 finished \tANN training loss 0.095981\n",
      ">> Epoch 528 finished \tANN training loss 0.095648\n",
      ">> Epoch 529 finished \tANN training loss 0.098572\n",
      ">> Epoch 530 finished \tANN training loss 0.105333\n",
      ">> Epoch 531 finished \tANN training loss 0.099275\n",
      ">> Epoch 532 finished \tANN training loss 0.108231\n",
      ">> Epoch 533 finished \tANN training loss 0.105070\n",
      ">> Epoch 534 finished \tANN training loss 0.097288\n",
      ">> Epoch 535 finished \tANN training loss 0.097925\n",
      ">> Epoch 536 finished \tANN training loss 0.100573\n",
      ">> Epoch 537 finished \tANN training loss 0.100775\n",
      ">> Epoch 538 finished \tANN training loss 0.098672\n",
      ">> Epoch 539 finished \tANN training loss 0.101160\n",
      ">> Epoch 540 finished \tANN training loss 0.109456\n",
      ">> Epoch 541 finished \tANN training loss 0.100970\n",
      ">> Epoch 542 finished \tANN training loss 0.100522\n",
      ">> Epoch 543 finished \tANN training loss 0.100127\n",
      ">> Epoch 544 finished \tANN training loss 0.102261\n",
      ">> Epoch 545 finished \tANN training loss 0.101889\n",
      ">> Epoch 546 finished \tANN training loss 0.100382\n",
      ">> Epoch 547 finished \tANN training loss 0.100436\n",
      ">> Epoch 548 finished \tANN training loss 0.101398\n",
      ">> Epoch 549 finished \tANN training loss 0.099543\n",
      ">> Epoch 550 finished \tANN training loss 0.101990\n",
      ">> Epoch 551 finished \tANN training loss 0.099935\n",
      ">> Epoch 552 finished \tANN training loss 0.106106\n",
      ">> Epoch 553 finished \tANN training loss 0.106705\n",
      ">> Epoch 554 finished \tANN training loss 0.105221\n",
      ">> Epoch 555 finished \tANN training loss 0.105103\n",
      ">> Epoch 556 finished \tANN training loss 0.108238\n",
      ">> Epoch 557 finished \tANN training loss 0.099688\n",
      ">> Epoch 558 finished \tANN training loss 0.102210\n",
      ">> Epoch 559 finished \tANN training loss 0.100004\n",
      ">> Epoch 560 finished \tANN training loss 0.100196\n",
      ">> Epoch 561 finished \tANN training loss 0.099514\n",
      ">> Epoch 562 finished \tANN training loss 0.103500\n",
      ">> Epoch 563 finished \tANN training loss 0.099675\n",
      ">> Epoch 564 finished \tANN training loss 0.101790\n",
      ">> Epoch 565 finished \tANN training loss 0.103128\n",
      ">> Epoch 566 finished \tANN training loss 0.107169\n",
      ">> Epoch 567 finished \tANN training loss 0.102145\n",
      ">> Epoch 568 finished \tANN training loss 0.100523\n",
      ">> Epoch 569 finished \tANN training loss 0.102260\n",
      ">> Epoch 570 finished \tANN training loss 0.104951\n",
      ">> Epoch 571 finished \tANN training loss 0.109861\n",
      ">> Epoch 572 finished \tANN training loss 0.108661\n",
      ">> Epoch 573 finished \tANN training loss 0.099158\n",
      ">> Epoch 574 finished \tANN training loss 0.100024\n",
      ">> Epoch 575 finished \tANN training loss 0.101569\n",
      ">> Epoch 576 finished \tANN training loss 0.108094\n",
      ">> Epoch 577 finished \tANN training loss 0.100279\n",
      ">> Epoch 578 finished \tANN training loss 0.099940\n",
      ">> Epoch 579 finished \tANN training loss 0.104844\n",
      ">> Epoch 580 finished \tANN training loss 0.099931\n",
      ">> Epoch 581 finished \tANN training loss 0.107226\n",
      ">> Epoch 582 finished \tANN training loss 0.104880\n",
      ">> Epoch 583 finished \tANN training loss 0.098645\n",
      ">> Epoch 584 finished \tANN training loss 0.098178\n",
      ">> Epoch 585 finished \tANN training loss 0.100237\n",
      ">> Epoch 586 finished \tANN training loss 0.104227\n",
      ">> Epoch 587 finished \tANN training loss 0.103121\n",
      ">> Epoch 588 finished \tANN training loss 0.105561\n",
      ">> Epoch 589 finished \tANN training loss 0.109886\n",
      ">> Epoch 590 finished \tANN training loss 0.108349\n",
      ">> Epoch 591 finished \tANN training loss 0.109734\n",
      ">> Epoch 592 finished \tANN training loss 0.114155\n",
      ">> Epoch 593 finished \tANN training loss 0.109397\n",
      ">> Epoch 594 finished \tANN training loss 0.109533\n",
      ">> Epoch 595 finished \tANN training loss 0.109883\n",
      ">> Epoch 596 finished \tANN training loss 0.107296\n",
      ">> Epoch 597 finished \tANN training loss 0.109910\n",
      ">> Epoch 598 finished \tANN training loss 0.104926\n",
      ">> Epoch 599 finished \tANN training loss 0.103410\n",
      ">> Epoch 600 finished \tANN training loss 0.103966\n",
      ">> Epoch 601 finished \tANN training loss 0.104819\n",
      ">> Epoch 602 finished \tANN training loss 0.101011\n",
      ">> Epoch 603 finished \tANN training loss 0.098672\n",
      ">> Epoch 604 finished \tANN training loss 0.095102\n",
      ">> Epoch 605 finished \tANN training loss 0.098913\n",
      ">> Epoch 606 finished \tANN training loss 0.100446\n",
      ">> Epoch 607 finished \tANN training loss 0.097629\n",
      ">> Epoch 608 finished \tANN training loss 0.097038\n",
      ">> Epoch 609 finished \tANN training loss 0.097578\n",
      ">> Epoch 610 finished \tANN training loss 0.094845\n",
      ">> Epoch 611 finished \tANN training loss 0.098062\n",
      ">> Epoch 612 finished \tANN training loss 0.099728\n",
      ">> Epoch 613 finished \tANN training loss 0.095016\n",
      ">> Epoch 614 finished \tANN training loss 0.108379\n",
      ">> Epoch 615 finished \tANN training loss 0.096301\n",
      ">> Epoch 616 finished \tANN training loss 0.104618\n",
      ">> Epoch 617 finished \tANN training loss 0.100454\n",
      ">> Epoch 618 finished \tANN training loss 0.097238\n",
      ">> Epoch 619 finished \tANN training loss 0.099694\n",
      ">> Epoch 620 finished \tANN training loss 0.102458\n",
      ">> Epoch 621 finished \tANN training loss 0.103511\n",
      ">> Epoch 622 finished \tANN training loss 0.100895\n",
      ">> Epoch 623 finished \tANN training loss 0.105772\n",
      ">> Epoch 624 finished \tANN training loss 0.103522\n",
      ">> Epoch 625 finished \tANN training loss 0.102301\n",
      ">> Epoch 626 finished \tANN training loss 0.103161\n",
      ">> Epoch 627 finished \tANN training loss 0.101721\n",
      ">> Epoch 628 finished \tANN training loss 0.098213\n",
      ">> Epoch 629 finished \tANN training loss 0.098749\n",
      ">> Epoch 630 finished \tANN training loss 0.099598\n",
      ">> Epoch 631 finished \tANN training loss 0.106193\n",
      ">> Epoch 632 finished \tANN training loss 0.105444\n",
      ">> Epoch 633 finished \tANN training loss 0.107972\n",
      ">> Epoch 634 finished \tANN training loss 0.100409\n",
      ">> Epoch 635 finished \tANN training loss 0.099129\n",
      ">> Epoch 636 finished \tANN training loss 0.097102\n",
      ">> Epoch 637 finished \tANN training loss 0.097211\n",
      ">> Epoch 638 finished \tANN training loss 0.094052\n",
      ">> Epoch 639 finished \tANN training loss 0.094518\n",
      ">> Epoch 640 finished \tANN training loss 0.093617\n",
      ">> Epoch 641 finished \tANN training loss 0.092982\n",
      ">> Epoch 642 finished \tANN training loss 0.095996\n",
      ">> Epoch 643 finished \tANN training loss 0.091817\n",
      ">> Epoch 644 finished \tANN training loss 0.094871\n",
      ">> Epoch 645 finished \tANN training loss 0.093343\n",
      ">> Epoch 646 finished \tANN training loss 0.103646\n",
      ">> Epoch 647 finished \tANN training loss 0.097024\n",
      ">> Epoch 648 finished \tANN training loss 0.093438\n",
      ">> Epoch 649 finished \tANN training loss 0.104707\n",
      ">> Epoch 650 finished \tANN training loss 0.102350\n",
      ">> Epoch 651 finished \tANN training loss 0.105688\n",
      ">> Epoch 652 finished \tANN training loss 0.100273\n",
      ">> Epoch 653 finished \tANN training loss 0.099556\n",
      ">> Epoch 654 finished \tANN training loss 0.103954\n",
      ">> Epoch 655 finished \tANN training loss 0.112405\n",
      ">> Epoch 656 finished \tANN training loss 0.105617\n",
      ">> Epoch 657 finished \tANN training loss 0.103482\n",
      ">> Epoch 658 finished \tANN training loss 0.100393\n",
      ">> Epoch 659 finished \tANN training loss 0.104216\n",
      ">> Epoch 660 finished \tANN training loss 0.105523\n",
      ">> Epoch 661 finished \tANN training loss 0.109201\n",
      ">> Epoch 662 finished \tANN training loss 0.104065\n",
      ">> Epoch 663 finished \tANN training loss 0.103295\n",
      ">> Epoch 664 finished \tANN training loss 0.099070\n",
      ">> Epoch 665 finished \tANN training loss 0.105763\n",
      ">> Epoch 666 finished \tANN training loss 0.102106\n",
      ">> Epoch 667 finished \tANN training loss 0.100386\n",
      ">> Epoch 668 finished \tANN training loss 0.104649\n",
      ">> Epoch 669 finished \tANN training loss 0.100512\n",
      ">> Epoch 670 finished \tANN training loss 0.096826\n",
      ">> Epoch 671 finished \tANN training loss 0.099812\n",
      ">> Epoch 672 finished \tANN training loss 0.101335\n",
      ">> Epoch 673 finished \tANN training loss 0.094084\n",
      ">> Epoch 674 finished \tANN training loss 0.095139\n",
      ">> Epoch 675 finished \tANN training loss 0.096867\n",
      ">> Epoch 676 finished \tANN training loss 0.100145\n",
      ">> Epoch 677 finished \tANN training loss 0.099320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 678 finished \tANN training loss 0.100848\n",
      ">> Epoch 679 finished \tANN training loss 0.100283\n",
      ">> Epoch 680 finished \tANN training loss 0.095198\n",
      ">> Epoch 681 finished \tANN training loss 0.099582\n",
      ">> Epoch 682 finished \tANN training loss 0.093087\n",
      ">> Epoch 683 finished \tANN training loss 0.093585\n",
      ">> Epoch 684 finished \tANN training loss 0.098187\n",
      ">> Epoch 685 finished \tANN training loss 0.105177\n",
      ">> Epoch 686 finished \tANN training loss 0.098301\n",
      ">> Epoch 687 finished \tANN training loss 0.097143\n",
      ">> Epoch 688 finished \tANN training loss 0.099620\n",
      ">> Epoch 689 finished \tANN training loss 0.097796\n",
      ">> Epoch 690 finished \tANN training loss 0.096333\n",
      ">> Epoch 691 finished \tANN training loss 0.101371\n",
      ">> Epoch 692 finished \tANN training loss 0.105441\n",
      ">> Epoch 693 finished \tANN training loss 0.096867\n",
      ">> Epoch 694 finished \tANN training loss 0.105817\n",
      ">> Epoch 695 finished \tANN training loss 0.091705\n",
      ">> Epoch 696 finished \tANN training loss 0.098488\n",
      ">> Epoch 697 finished \tANN training loss 0.096964\n",
      ">> Epoch 698 finished \tANN training loss 0.097068\n",
      ">> Epoch 699 finished \tANN training loss 0.098511\n",
      ">> Epoch 700 finished \tANN training loss 0.098247\n",
      ">> Epoch 701 finished \tANN training loss 0.096014\n",
      ">> Epoch 702 finished \tANN training loss 0.097353\n",
      ">> Epoch 703 finished \tANN training loss 0.096387\n",
      ">> Epoch 704 finished \tANN training loss 0.097154\n",
      ">> Epoch 705 finished \tANN training loss 0.100036\n",
      ">> Epoch 706 finished \tANN training loss 0.100083\n",
      ">> Epoch 707 finished \tANN training loss 0.103257\n",
      ">> Epoch 708 finished \tANN training loss 0.099458\n",
      ">> Epoch 709 finished \tANN training loss 0.102795\n",
      ">> Epoch 710 finished \tANN training loss 0.101265\n",
      ">> Epoch 711 finished \tANN training loss 0.096769\n",
      ">> Epoch 712 finished \tANN training loss 0.092772\n",
      ">> Epoch 713 finished \tANN training loss 0.093810\n",
      ">> Epoch 714 finished \tANN training loss 0.096377\n",
      ">> Epoch 715 finished \tANN training loss 0.095394\n",
      ">> Epoch 716 finished \tANN training loss 0.096978\n",
      ">> Epoch 717 finished \tANN training loss 0.095802\n",
      ">> Epoch 718 finished \tANN training loss 0.113041\n",
      ">> Epoch 719 finished \tANN training loss 0.103890\n",
      ">> Epoch 720 finished \tANN training loss 0.097939\n",
      ">> Epoch 721 finished \tANN training loss 0.100416\n",
      ">> Epoch 722 finished \tANN training loss 0.103943\n",
      ">> Epoch 723 finished \tANN training loss 0.098357\n",
      ">> Epoch 724 finished \tANN training loss 0.100862\n",
      ">> Epoch 725 finished \tANN training loss 0.104124\n",
      ">> Epoch 726 finished \tANN training loss 0.101681\n",
      ">> Epoch 727 finished \tANN training loss 0.099646\n",
      ">> Epoch 728 finished \tANN training loss 0.098324\n",
      ">> Epoch 729 finished \tANN training loss 0.093524\n",
      ">> Epoch 730 finished \tANN training loss 0.093836\n",
      ">> Epoch 731 finished \tANN training loss 0.096973\n",
      ">> Epoch 732 finished \tANN training loss 0.097645\n",
      ">> Epoch 733 finished \tANN training loss 0.100030\n",
      ">> Epoch 734 finished \tANN training loss 0.103130\n",
      ">> Epoch 735 finished \tANN training loss 0.098949\n",
      ">> Epoch 736 finished \tANN training loss 0.100372\n",
      ">> Epoch 737 finished \tANN training loss 0.104822\n",
      ">> Epoch 738 finished \tANN training loss 0.099580\n",
      ">> Epoch 739 finished \tANN training loss 0.103197\n",
      ">> Epoch 740 finished \tANN training loss 0.096491\n",
      ">> Epoch 741 finished \tANN training loss 0.096389\n",
      ">> Epoch 742 finished \tANN training loss 0.094142\n",
      ">> Epoch 743 finished \tANN training loss 0.100247\n",
      ">> Epoch 744 finished \tANN training loss 0.096680\n",
      ">> Epoch 745 finished \tANN training loss 0.095515\n",
      ">> Epoch 746 finished \tANN training loss 0.102463\n",
      ">> Epoch 747 finished \tANN training loss 0.103159\n",
      ">> Epoch 748 finished \tANN training loss 0.103627\n",
      ">> Epoch 749 finished \tANN training loss 0.100248\n",
      ">> Epoch 750 finished \tANN training loss 0.101055\n",
      ">> Epoch 751 finished \tANN training loss 0.095626\n",
      ">> Epoch 752 finished \tANN training loss 0.098328\n",
      ">> Epoch 753 finished \tANN training loss 0.101144\n",
      ">> Epoch 754 finished \tANN training loss 0.104173\n",
      ">> Epoch 755 finished \tANN training loss 0.105557\n",
      ">> Epoch 756 finished \tANN training loss 0.108270\n",
      ">> Epoch 757 finished \tANN training loss 0.102945\n",
      ">> Epoch 758 finished \tANN training loss 0.100654\n",
      ">> Epoch 759 finished \tANN training loss 0.103347\n",
      ">> Epoch 760 finished \tANN training loss 0.104343\n",
      ">> Epoch 761 finished \tANN training loss 0.098349\n",
      ">> Epoch 762 finished \tANN training loss 0.097749\n",
      ">> Epoch 763 finished \tANN training loss 0.101349\n",
      ">> Epoch 764 finished \tANN training loss 0.100274\n",
      ">> Epoch 765 finished \tANN training loss 0.098326\n",
      ">> Epoch 766 finished \tANN training loss 0.099527\n",
      ">> Epoch 767 finished \tANN training loss 0.095365\n",
      ">> Epoch 768 finished \tANN training loss 0.101267\n",
      ">> Epoch 769 finished \tANN training loss 0.099823\n",
      ">> Epoch 770 finished \tANN training loss 0.100077\n",
      ">> Epoch 771 finished \tANN training loss 0.099546\n",
      ">> Epoch 772 finished \tANN training loss 0.103639\n",
      ">> Epoch 773 finished \tANN training loss 0.099879\n",
      ">> Epoch 774 finished \tANN training loss 0.096551\n",
      ">> Epoch 775 finished \tANN training loss 0.094852\n",
      ">> Epoch 776 finished \tANN training loss 0.104055\n",
      ">> Epoch 777 finished \tANN training loss 0.100381\n",
      ">> Epoch 778 finished \tANN training loss 0.103283\n",
      ">> Epoch 779 finished \tANN training loss 0.101358\n",
      ">> Epoch 780 finished \tANN training loss 0.110896\n",
      ">> Epoch 781 finished \tANN training loss 0.101223\n",
      ">> Epoch 782 finished \tANN training loss 0.100827\n",
      ">> Epoch 783 finished \tANN training loss 0.098909\n",
      ">> Epoch 784 finished \tANN training loss 0.100502\n",
      ">> Epoch 785 finished \tANN training loss 0.095463\n",
      ">> Epoch 786 finished \tANN training loss 0.100518\n",
      ">> Epoch 787 finished \tANN training loss 0.100014\n",
      ">> Epoch 788 finished \tANN training loss 0.099937\n",
      ">> Epoch 789 finished \tANN training loss 0.106252\n",
      ">> Epoch 790 finished \tANN training loss 0.096005\n",
      ">> Epoch 791 finished \tANN training loss 0.102787\n",
      ">> Epoch 792 finished \tANN training loss 0.103668\n",
      ">> Epoch 793 finished \tANN training loss 0.095239\n",
      ">> Epoch 794 finished \tANN training loss 0.096212\n",
      ">> Epoch 795 finished \tANN training loss 0.098598\n",
      ">> Epoch 796 finished \tANN training loss 0.097714\n",
      ">> Epoch 797 finished \tANN training loss 0.098070\n",
      ">> Epoch 798 finished \tANN training loss 0.099588\n",
      ">> Epoch 799 finished \tANN training loss 0.095235\n",
      ">> Epoch 800 finished \tANN training loss 0.093417\n",
      ">> Epoch 801 finished \tANN training loss 0.104521\n",
      ">> Epoch 802 finished \tANN training loss 0.098035\n",
      ">> Epoch 803 finished \tANN training loss 0.094715\n",
      ">> Epoch 804 finished \tANN training loss 0.100945\n",
      ">> Epoch 805 finished \tANN training loss 0.096795\n",
      ">> Epoch 806 finished \tANN training loss 0.095350\n",
      ">> Epoch 807 finished \tANN training loss 0.101160\n",
      ">> Epoch 808 finished \tANN training loss 0.098912\n",
      ">> Epoch 809 finished \tANN training loss 0.091184\n",
      ">> Epoch 810 finished \tANN training loss 0.097319\n",
      ">> Epoch 811 finished \tANN training loss 0.093111\n",
      ">> Epoch 812 finished \tANN training loss 0.095508\n",
      ">> Epoch 813 finished \tANN training loss 0.090512\n",
      ">> Epoch 814 finished \tANN training loss 0.092838\n",
      ">> Epoch 815 finished \tANN training loss 0.093596\n",
      ">> Epoch 816 finished \tANN training loss 0.092032\n",
      ">> Epoch 817 finished \tANN training loss 0.097705\n",
      ">> Epoch 818 finished \tANN training loss 0.104921\n",
      ">> Epoch 819 finished \tANN training loss 0.100858\n",
      ">> Epoch 820 finished \tANN training loss 0.101475\n",
      ">> Epoch 821 finished \tANN training loss 0.097979\n",
      ">> Epoch 822 finished \tANN training loss 0.100041\n",
      ">> Epoch 823 finished \tANN training loss 0.100809\n",
      ">> Epoch 824 finished \tANN training loss 0.097354\n",
      ">> Epoch 825 finished \tANN training loss 0.096103\n",
      ">> Epoch 826 finished \tANN training loss 0.099696\n",
      ">> Epoch 827 finished \tANN training loss 0.098718\n",
      ">> Epoch 828 finished \tANN training loss 0.100368\n",
      ">> Epoch 829 finished \tANN training loss 0.101520\n",
      ">> Epoch 830 finished \tANN training loss 0.099399\n",
      ">> Epoch 831 finished \tANN training loss 0.101211\n",
      ">> Epoch 832 finished \tANN training loss 0.101274\n",
      ">> Epoch 833 finished \tANN training loss 0.094063\n",
      ">> Epoch 834 finished \tANN training loss 0.100460\n",
      ">> Epoch 835 finished \tANN training loss 0.101514\n",
      ">> Epoch 836 finished \tANN training loss 0.099918\n",
      ">> Epoch 837 finished \tANN training loss 0.110028\n",
      ">> Epoch 838 finished \tANN training loss 0.100515\n",
      ">> Epoch 839 finished \tANN training loss 0.107077\n",
      ">> Epoch 840 finished \tANN training loss 0.098147\n",
      ">> Epoch 841 finished \tANN training loss 0.097466\n",
      ">> Epoch 842 finished \tANN training loss 0.098607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 843 finished \tANN training loss 0.102470\n",
      ">> Epoch 844 finished \tANN training loss 0.103562\n",
      ">> Epoch 845 finished \tANN training loss 0.101178\n",
      ">> Epoch 846 finished \tANN training loss 0.104233\n",
      ">> Epoch 847 finished \tANN training loss 0.097622\n",
      ">> Epoch 848 finished \tANN training loss 0.104820\n",
      ">> Epoch 849 finished \tANN training loss 0.096727\n",
      ">> Epoch 850 finished \tANN training loss 0.104691\n",
      ">> Epoch 851 finished \tANN training loss 0.099231\n",
      ">> Epoch 852 finished \tANN training loss 0.103684\n",
      ">> Epoch 853 finished \tANN training loss 0.100514\n",
      ">> Epoch 854 finished \tANN training loss 0.096536\n",
      ">> Epoch 855 finished \tANN training loss 0.104789\n",
      ">> Epoch 856 finished \tANN training loss 0.095009\n",
      ">> Epoch 857 finished \tANN training loss 0.099196\n",
      ">> Epoch 858 finished \tANN training loss 0.095879\n",
      ">> Epoch 859 finished \tANN training loss 0.096770\n",
      ">> Epoch 860 finished \tANN training loss 0.092886\n",
      ">> Epoch 861 finished \tANN training loss 0.091555\n",
      ">> Epoch 862 finished \tANN training loss 0.096740\n",
      ">> Epoch 863 finished \tANN training loss 0.095267\n",
      ">> Epoch 864 finished \tANN training loss 0.094776\n",
      ">> Epoch 865 finished \tANN training loss 0.096689\n",
      ">> Epoch 866 finished \tANN training loss 0.096657\n",
      ">> Epoch 867 finished \tANN training loss 0.093149\n",
      ">> Epoch 868 finished \tANN training loss 0.095753\n",
      ">> Epoch 869 finished \tANN training loss 0.103338\n",
      ">> Epoch 870 finished \tANN training loss 0.095970\n",
      ">> Epoch 871 finished \tANN training loss 0.095409\n",
      ">> Epoch 872 finished \tANN training loss 0.097121\n",
      ">> Epoch 873 finished \tANN training loss 0.097726\n",
      ">> Epoch 874 finished \tANN training loss 0.094174\n",
      ">> Epoch 875 finished \tANN training loss 0.100434\n",
      ">> Epoch 876 finished \tANN training loss 0.095007\n",
      ">> Epoch 877 finished \tANN training loss 0.098630\n",
      ">> Epoch 878 finished \tANN training loss 0.095864\n",
      ">> Epoch 879 finished \tANN training loss 0.099558\n",
      ">> Epoch 880 finished \tANN training loss 0.092179\n",
      ">> Epoch 881 finished \tANN training loss 0.101143\n",
      ">> Epoch 882 finished \tANN training loss 0.101879\n",
      ">> Epoch 883 finished \tANN training loss 0.096676\n",
      ">> Epoch 884 finished \tANN training loss 0.094344\n",
      ">> Epoch 885 finished \tANN training loss 0.097145\n",
      ">> Epoch 886 finished \tANN training loss 0.097577\n",
      ">> Epoch 887 finished \tANN training loss 0.099226\n",
      ">> Epoch 888 finished \tANN training loss 0.096971\n",
      ">> Epoch 889 finished \tANN training loss 0.095156\n",
      ">> Epoch 890 finished \tANN training loss 0.093795\n",
      ">> Epoch 891 finished \tANN training loss 0.092367\n",
      ">> Epoch 892 finished \tANN training loss 0.092246\n",
      ">> Epoch 893 finished \tANN training loss 0.093670\n",
      ">> Epoch 894 finished \tANN training loss 0.093578\n",
      ">> Epoch 895 finished \tANN training loss 0.095536\n",
      ">> Epoch 896 finished \tANN training loss 0.097373\n",
      ">> Epoch 897 finished \tANN training loss 0.099193\n",
      ">> Epoch 898 finished \tANN training loss 0.100279\n",
      ">> Epoch 899 finished \tANN training loss 0.093746\n",
      ">> Epoch 900 finished \tANN training loss 0.092536\n",
      ">> Epoch 901 finished \tANN training loss 0.094566\n",
      ">> Epoch 902 finished \tANN training loss 0.096460\n",
      ">> Epoch 903 finished \tANN training loss 0.099211\n",
      ">> Epoch 904 finished \tANN training loss 0.094731\n",
      ">> Epoch 905 finished \tANN training loss 0.096820\n",
      ">> Epoch 906 finished \tANN training loss 0.095146\n",
      ">> Epoch 907 finished \tANN training loss 0.092605\n",
      ">> Epoch 908 finished \tANN training loss 0.097167\n",
      ">> Epoch 909 finished \tANN training loss 0.095742\n",
      ">> Epoch 910 finished \tANN training loss 0.103764\n",
      ">> Epoch 911 finished \tANN training loss 0.095955\n",
      ">> Epoch 912 finished \tANN training loss 0.096263\n",
      ">> Epoch 913 finished \tANN training loss 0.095661\n",
      ">> Epoch 914 finished \tANN training loss 0.102870\n",
      ">> Epoch 915 finished \tANN training loss 0.099265\n",
      ">> Epoch 916 finished \tANN training loss 0.093284\n",
      ">> Epoch 917 finished \tANN training loss 0.093626\n",
      ">> Epoch 918 finished \tANN training loss 0.095494\n",
      ">> Epoch 919 finished \tANN training loss 0.096467\n",
      ">> Epoch 920 finished \tANN training loss 0.094125\n",
      ">> Epoch 921 finished \tANN training loss 0.094882\n",
      ">> Epoch 922 finished \tANN training loss 0.093525\n",
      ">> Epoch 923 finished \tANN training loss 0.095736\n",
      ">> Epoch 924 finished \tANN training loss 0.092271\n",
      ">> Epoch 925 finished \tANN training loss 0.095527\n",
      ">> Epoch 926 finished \tANN training loss 0.092064\n",
      ">> Epoch 927 finished \tANN training loss 0.094597\n",
      ">> Epoch 928 finished \tANN training loss 0.097098\n",
      ">> Epoch 929 finished \tANN training loss 0.095118\n",
      ">> Epoch 930 finished \tANN training loss 0.095526\n",
      ">> Epoch 931 finished \tANN training loss 0.094158\n",
      ">> Epoch 932 finished \tANN training loss 0.091336\n",
      ">> Epoch 933 finished \tANN training loss 0.091139\n",
      ">> Epoch 934 finished \tANN training loss 0.090849\n",
      ">> Epoch 935 finished \tANN training loss 0.093129\n",
      ">> Epoch 936 finished \tANN training loss 0.090848\n",
      ">> Epoch 937 finished \tANN training loss 0.088792\n",
      ">> Epoch 938 finished \tANN training loss 0.102919\n",
      ">> Epoch 939 finished \tANN training loss 0.091038\n",
      ">> Epoch 940 finished \tANN training loss 0.091326\n",
      ">> Epoch 941 finished \tANN training loss 0.094298\n",
      ">> Epoch 942 finished \tANN training loss 0.092817\n",
      ">> Epoch 943 finished \tANN training loss 0.100188\n",
      ">> Epoch 944 finished \tANN training loss 0.092035\n",
      ">> Epoch 945 finished \tANN training loss 0.093287\n",
      ">> Epoch 946 finished \tANN training loss 0.095818\n",
      ">> Epoch 947 finished \tANN training loss 0.090858\n",
      ">> Epoch 948 finished \tANN training loss 0.097686\n",
      ">> Epoch 949 finished \tANN training loss 0.098346\n",
      ">> Epoch 950 finished \tANN training loss 0.105530\n",
      ">> Epoch 951 finished \tANN training loss 0.096826\n",
      ">> Epoch 952 finished \tANN training loss 0.098504\n",
      ">> Epoch 953 finished \tANN training loss 0.100356\n",
      ">> Epoch 954 finished \tANN training loss 0.103675\n",
      ">> Epoch 955 finished \tANN training loss 0.107892\n",
      ">> Epoch 956 finished \tANN training loss 0.092601\n",
      ">> Epoch 957 finished \tANN training loss 0.094443\n",
      ">> Epoch 958 finished \tANN training loss 0.096050\n",
      ">> Epoch 959 finished \tANN training loss 0.095887\n",
      ">> Epoch 960 finished \tANN training loss 0.093430\n",
      ">> Epoch 961 finished \tANN training loss 0.094680\n",
      ">> Epoch 962 finished \tANN training loss 0.116606\n",
      ">> Epoch 963 finished \tANN training loss 0.098985\n",
      ">> Epoch 964 finished \tANN training loss 0.097465\n",
      ">> Epoch 965 finished \tANN training loss 0.106226\n",
      ">> Epoch 966 finished \tANN training loss 0.098345\n",
      ">> Epoch 967 finished \tANN training loss 0.099191\n",
      ">> Epoch 968 finished \tANN training loss 0.103682\n",
      ">> Epoch 969 finished \tANN training loss 0.097055\n",
      ">> Epoch 970 finished \tANN training loss 0.099329\n",
      ">> Epoch 971 finished \tANN training loss 0.107602\n",
      ">> Epoch 972 finished \tANN training loss 0.095346\n",
      ">> Epoch 973 finished \tANN training loss 0.101482\n",
      ">> Epoch 974 finished \tANN training loss 0.109361\n",
      ">> Epoch 975 finished \tANN training loss 0.101373\n",
      ">> Epoch 976 finished \tANN training loss 0.099590\n",
      ">> Epoch 977 finished \tANN training loss 0.094588\n",
      ">> Epoch 978 finished \tANN training loss 0.094369\n",
      ">> Epoch 979 finished \tANN training loss 0.091623\n",
      ">> Epoch 980 finished \tANN training loss 0.090798\n",
      ">> Epoch 981 finished \tANN training loss 0.094906\n",
      ">> Epoch 982 finished \tANN training loss 0.094487\n",
      ">> Epoch 983 finished \tANN training loss 0.096317\n",
      ">> Epoch 984 finished \tANN training loss 0.093803\n",
      ">> Epoch 985 finished \tANN training loss 0.097000\n",
      ">> Epoch 986 finished \tANN training loss 0.092063\n",
      ">> Epoch 987 finished \tANN training loss 0.096299\n",
      ">> Epoch 988 finished \tANN training loss 0.096873\n",
      ">> Epoch 989 finished \tANN training loss 0.097055\n",
      ">> Epoch 990 finished \tANN training loss 0.100407\n",
      ">> Epoch 991 finished \tANN training loss 0.096910\n",
      ">> Epoch 992 finished \tANN training loss 0.092534\n",
      ">> Epoch 993 finished \tANN training loss 0.104069\n",
      ">> Epoch 994 finished \tANN training loss 0.102608\n",
      ">> Epoch 995 finished \tANN training loss 0.092140\n",
      ">> Epoch 996 finished \tANN training loss 0.093440\n",
      ">> Epoch 997 finished \tANN training loss 0.090401\n",
      ">> Epoch 998 finished \tANN training loss 0.096545\n",
      ">> Epoch 999 finished \tANN training loss 0.094620\n",
      "[END] Fine tuning step\n",
      "\n",
      "\n",
      "TRIAL:  2\n",
      "\n",
      "\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 5.777055\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 4.696581\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 4.367716\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 5.023835\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 5.671390\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 6.765386\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 8.158913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 8 finished \tRBM Reconstruction error 12.042552\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 15.017810\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 23.289060\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 38.122875\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 333.308960\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 795.743958\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 922.939087\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 1171.109497\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 1186.535889\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 1270.746216\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 1279.270508\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 1319.979370\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 1427.183228\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.090288\n",
      ">> Epoch 1 finished \tANN training loss 1.062241\n",
      ">> Epoch 2 finished \tANN training loss 1.045817\n",
      ">> Epoch 3 finished \tANN training loss 1.041191\n",
      ">> Epoch 4 finished \tANN training loss 1.038844\n",
      ">> Epoch 5 finished \tANN training loss 1.037010\n",
      ">> Epoch 6 finished \tANN training loss 1.036889\n",
      ">> Epoch 7 finished \tANN training loss 1.037181\n",
      ">> Epoch 8 finished \tANN training loss 1.036657\n",
      ">> Epoch 9 finished \tANN training loss 1.036661\n",
      ">> Epoch 10 finished \tANN training loss 1.037109\n",
      ">> Epoch 11 finished \tANN training loss 1.036655\n",
      ">> Epoch 12 finished \tANN training loss 1.037812\n",
      ">> Epoch 13 finished \tANN training loss 1.036824\n",
      ">> Epoch 14 finished \tANN training loss 1.037716\n",
      ">> Epoch 15 finished \tANN training loss 1.038364\n",
      ">> Epoch 16 finished \tANN training loss 1.036661\n",
      ">> Epoch 17 finished \tANN training loss 1.036672\n",
      ">> Epoch 18 finished \tANN training loss 1.036996\n",
      ">> Epoch 19 finished \tANN training loss 1.036820\n",
      ">> Epoch 20 finished \tANN training loss 1.036844\n",
      ">> Epoch 21 finished \tANN training loss 1.036656\n",
      ">> Epoch 22 finished \tANN training loss 1.037118\n",
      ">> Epoch 23 finished \tANN training loss 1.036660\n",
      ">> Epoch 24 finished \tANN training loss 1.036664\n",
      ">> Epoch 25 finished \tANN training loss 1.036682\n",
      ">> Epoch 26 finished \tANN training loss 1.037860\n",
      ">> Epoch 27 finished \tANN training loss 1.036732\n",
      ">> Epoch 28 finished \tANN training loss 1.037248\n",
      ">> Epoch 29 finished \tANN training loss 1.037888\n",
      ">> Epoch 30 finished \tANN training loss 1.037161\n",
      ">> Epoch 31 finished \tANN training loss 1.037117\n",
      ">> Epoch 32 finished \tANN training loss 1.036697\n",
      ">> Epoch 33 finished \tANN training loss 1.038430\n",
      ">> Epoch 34 finished \tANN training loss 1.037525\n",
      ">> Epoch 35 finished \tANN training loss 1.036904\n",
      ">> Epoch 36 finished \tANN training loss 1.037703\n",
      ">> Epoch 37 finished \tANN training loss 1.037178\n",
      ">> Epoch 38 finished \tANN training loss 1.038045\n",
      ">> Epoch 39 finished \tANN training loss 1.041017\n",
      ">> Epoch 40 finished \tANN training loss 1.037826\n",
      ">> Epoch 41 finished \tANN training loss 1.038407\n",
      ">> Epoch 42 finished \tANN training loss 1.036656\n",
      ">> Epoch 43 finished \tANN training loss 1.036970\n",
      ">> Epoch 44 finished \tANN training loss 1.036844\n",
      ">> Epoch 45 finished \tANN training loss 1.036733\n",
      ">> Epoch 46 finished \tANN training loss 1.036871\n",
      ">> Epoch 47 finished \tANN training loss 1.037225\n",
      ">> Epoch 48 finished \tANN training loss 1.037150\n",
      ">> Epoch 49 finished \tANN training loss 1.036766\n",
      ">> Epoch 50 finished \tANN training loss 1.037118\n",
      ">> Epoch 51 finished \tANN training loss 1.039041\n",
      ">> Epoch 52 finished \tANN training loss 1.037394\n",
      ">> Epoch 53 finished \tANN training loss 1.037148\n",
      ">> Epoch 54 finished \tANN training loss 1.037897\n",
      ">> Epoch 55 finished \tANN training loss 1.036803\n",
      ">> Epoch 56 finished \tANN training loss 1.036708\n",
      ">> Epoch 57 finished \tANN training loss 1.036700\n",
      ">> Epoch 58 finished \tANN training loss 1.036890\n",
      ">> Epoch 59 finished \tANN training loss 1.037045\n",
      ">> Epoch 60 finished \tANN training loss 1.037081\n",
      ">> Epoch 61 finished \tANN training loss 1.037445\n",
      ">> Epoch 62 finished \tANN training loss 1.036677\n",
      ">> Epoch 63 finished \tANN training loss 1.037358\n",
      ">> Epoch 64 finished \tANN training loss 1.036655\n",
      ">> Epoch 65 finished \tANN training loss 1.036794\n",
      ">> Epoch 66 finished \tANN training loss 1.037174\n",
      ">> Epoch 67 finished \tANN training loss 1.036788\n",
      ">> Epoch 68 finished \tANN training loss 1.036671\n",
      ">> Epoch 69 finished \tANN training loss 1.037612\n",
      ">> Epoch 70 finished \tANN training loss 1.036753\n",
      ">> Epoch 71 finished \tANN training loss 1.036717\n",
      ">> Epoch 72 finished \tANN training loss 1.036778\n",
      ">> Epoch 73 finished \tANN training loss 1.036666\n",
      ">> Epoch 74 finished \tANN training loss 1.036694\n",
      ">> Epoch 75 finished \tANN training loss 1.036684\n",
      ">> Epoch 76 finished \tANN training loss 1.036695\n",
      ">> Epoch 77 finished \tANN training loss 1.036741\n",
      ">> Epoch 78 finished \tANN training loss 1.036847\n",
      ">> Epoch 79 finished \tANN training loss 1.036722\n",
      ">> Epoch 80 finished \tANN training loss 1.036949\n",
      ">> Epoch 81 finished \tANN training loss 1.036662\n",
      ">> Epoch 82 finished \tANN training loss 1.036655\n",
      ">> Epoch 83 finished \tANN training loss 1.036714\n",
      ">> Epoch 84 finished \tANN training loss 1.037166\n",
      ">> Epoch 85 finished \tANN training loss 1.036712\n",
      ">> Epoch 86 finished \tANN training loss 1.036656\n",
      ">> Epoch 87 finished \tANN training loss 1.036658\n",
      ">> Epoch 88 finished \tANN training loss 1.036676\n",
      ">> Epoch 89 finished \tANN training loss 1.037447\n",
      ">> Epoch 90 finished \tANN training loss 1.036754\n",
      ">> Epoch 91 finished \tANN training loss 1.037428\n",
      ">> Epoch 92 finished \tANN training loss 1.037002\n",
      ">> Epoch 93 finished \tANN training loss 1.038296\n",
      ">> Epoch 94 finished \tANN training loss 1.037490\n",
      ">> Epoch 95 finished \tANN training loss 1.037675\n",
      ">> Epoch 96 finished \tANN training loss 1.037833\n",
      ">> Epoch 97 finished \tANN training loss 1.037434\n",
      ">> Epoch 98 finished \tANN training loss 1.037400\n",
      ">> Epoch 99 finished \tANN training loss 1.037762\n",
      ">> Epoch 100 finished \tANN training loss 1.037916\n",
      ">> Epoch 101 finished \tANN training loss 1.036660\n",
      ">> Epoch 102 finished \tANN training loss 1.036678\n",
      ">> Epoch 103 finished \tANN training loss 1.036877\n",
      ">> Epoch 104 finished \tANN training loss 1.037623\n",
      ">> Epoch 105 finished \tANN training loss 1.036655\n",
      ">> Epoch 106 finished \tANN training loss 1.036754\n",
      ">> Epoch 107 finished \tANN training loss 1.036748\n",
      ">> Epoch 108 finished \tANN training loss 1.036741\n",
      ">> Epoch 109 finished \tANN training loss 1.036844\n",
      ">> Epoch 110 finished \tANN training loss 1.036711\n",
      ">> Epoch 111 finished \tANN training loss 1.038374\n",
      ">> Epoch 112 finished \tANN training loss 1.037783\n",
      ">> Epoch 113 finished \tANN training loss 1.036811\n",
      ">> Epoch 114 finished \tANN training loss 1.036676\n",
      ">> Epoch 115 finished \tANN training loss 1.036692\n",
      ">> Epoch 116 finished \tANN training loss 1.036674\n",
      ">> Epoch 117 finished \tANN training loss 1.037033\n",
      ">> Epoch 118 finished \tANN training loss 1.037200\n",
      ">> Epoch 119 finished \tANN training loss 1.037112\n",
      ">> Epoch 120 finished \tANN training loss 1.036766\n",
      ">> Epoch 121 finished \tANN training loss 1.036792\n",
      ">> Epoch 122 finished \tANN training loss 1.036660\n",
      ">> Epoch 123 finished \tANN training loss 1.036656\n",
      ">> Epoch 124 finished \tANN training loss 1.036659\n",
      ">> Epoch 125 finished \tANN training loss 1.036780\n",
      ">> Epoch 126 finished \tANN training loss 1.036868\n",
      ">> Epoch 127 finished \tANN training loss 1.037114\n",
      ">> Epoch 128 finished \tANN training loss 1.036655\n",
      ">> Epoch 129 finished \tANN training loss 1.036657\n",
      ">> Epoch 130 finished \tANN training loss 1.039196\n",
      ">> Epoch 131 finished \tANN training loss 1.037860\n",
      ">> Epoch 132 finished \tANN training loss 1.036717\n",
      ">> Epoch 133 finished \tANN training loss 1.037132\n",
      ">> Epoch 134 finished \tANN training loss 1.038213\n",
      ">> Epoch 135 finished \tANN training loss 1.038378\n",
      ">> Epoch 136 finished \tANN training loss 1.036718\n",
      ">> Epoch 137 finished \tANN training loss 1.036686\n",
      ">> Epoch 138 finished \tANN training loss 1.037881\n",
      ">> Epoch 139 finished \tANN training loss 1.036756\n",
      ">> Epoch 140 finished \tANN training loss 1.036857\n",
      ">> Epoch 141 finished \tANN training loss 1.037070\n",
      ">> Epoch 142 finished \tANN training loss 1.037237\n",
      ">> Epoch 143 finished \tANN training loss 1.036716\n",
      ">> Epoch 144 finished \tANN training loss 1.037359\n",
      ">> Epoch 145 finished \tANN training loss 1.036809\n",
      ">> Epoch 146 finished \tANN training loss 1.036776\n",
      ">> Epoch 147 finished \tANN training loss 1.036702\n",
      ">> Epoch 148 finished \tANN training loss 1.036891\n",
      ">> Epoch 149 finished \tANN training loss 1.036908\n",
      ">> Epoch 150 finished \tANN training loss 1.036930\n",
      ">> Epoch 151 finished \tANN training loss 1.037273\n",
      ">> Epoch 152 finished \tANN training loss 1.036655\n",
      ">> Epoch 153 finished \tANN training loss 1.036655\n",
      ">> Epoch 154 finished \tANN training loss 1.036836\n",
      ">> Epoch 155 finished \tANN training loss 1.036696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 156 finished \tANN training loss 1.036717\n",
      ">> Epoch 157 finished \tANN training loss 1.036668\n",
      ">> Epoch 158 finished \tANN training loss 1.037090\n",
      ">> Epoch 159 finished \tANN training loss 1.036684\n",
      ">> Epoch 160 finished \tANN training loss 1.036800\n",
      ">> Epoch 161 finished \tANN training loss 1.036668\n",
      ">> Epoch 162 finished \tANN training loss 1.036904\n",
      ">> Epoch 163 finished \tANN training loss 1.037743\n",
      ">> Epoch 164 finished \tANN training loss 1.036929\n",
      ">> Epoch 165 finished \tANN training loss 1.036666\n",
      ">> Epoch 166 finished \tANN training loss 1.036694\n",
      ">> Epoch 167 finished \tANN training loss 1.036868\n",
      ">> Epoch 168 finished \tANN training loss 1.037062\n",
      ">> Epoch 169 finished \tANN training loss 1.036765\n",
      ">> Epoch 170 finished \tANN training loss 1.036655\n",
      ">> Epoch 171 finished \tANN training loss 1.036659\n",
      ">> Epoch 172 finished \tANN training loss 1.036655\n",
      ">> Epoch 173 finished \tANN training loss 1.036685\n",
      ">> Epoch 174 finished \tANN training loss 1.036862\n",
      ">> Epoch 175 finished \tANN training loss 1.036675\n",
      ">> Epoch 176 finished \tANN training loss 1.036886\n",
      ">> Epoch 177 finished \tANN training loss 1.036928\n",
      ">> Epoch 178 finished \tANN training loss 1.036671\n",
      ">> Epoch 179 finished \tANN training loss 1.037031\n",
      ">> Epoch 180 finished \tANN training loss 1.036692\n",
      ">> Epoch 181 finished \tANN training loss 1.036655\n",
      ">> Epoch 182 finished \tANN training loss 1.036775\n",
      ">> Epoch 183 finished \tANN training loss 1.036767\n",
      ">> Epoch 184 finished \tANN training loss 1.036747\n",
      ">> Epoch 185 finished \tANN training loss 1.036804\n",
      ">> Epoch 186 finished \tANN training loss 1.037137\n",
      ">> Epoch 187 finished \tANN training loss 1.036690\n",
      ">> Epoch 188 finished \tANN training loss 1.036762\n",
      ">> Epoch 189 finished \tANN training loss 1.038683\n",
      ">> Epoch 190 finished \tANN training loss 1.036893\n",
      ">> Epoch 191 finished \tANN training loss 1.037778\n",
      ">> Epoch 192 finished \tANN training loss 1.039900\n",
      ">> Epoch 193 finished \tANN training loss 1.037977\n",
      ">> Epoch 194 finished \tANN training loss 1.036956\n",
      ">> Epoch 195 finished \tANN training loss 1.036659\n",
      ">> Epoch 196 finished \tANN training loss 1.036765\n",
      ">> Epoch 197 finished \tANN training loss 1.037385\n",
      ">> Epoch 198 finished \tANN training loss 1.036675\n",
      ">> Epoch 199 finished \tANN training loss 1.036662\n",
      ">> Epoch 200 finished \tANN training loss 1.036787\n",
      ">> Epoch 201 finished \tANN training loss 1.036664\n",
      ">> Epoch 202 finished \tANN training loss 1.037282\n",
      ">> Epoch 203 finished \tANN training loss 1.036815\n",
      ">> Epoch 204 finished \tANN training loss 1.036958\n",
      ">> Epoch 205 finished \tANN training loss 1.037068\n",
      ">> Epoch 206 finished \tANN training loss 1.037381\n",
      ">> Epoch 207 finished \tANN training loss 1.037126\n",
      ">> Epoch 208 finished \tANN training loss 1.037184\n",
      ">> Epoch 209 finished \tANN training loss 1.037201\n",
      ">> Epoch 210 finished \tANN training loss 1.036662\n",
      ">> Epoch 211 finished \tANN training loss 1.036854\n",
      ">> Epoch 212 finished \tANN training loss 1.036662\n",
      ">> Epoch 213 finished \tANN training loss 1.036947\n",
      ">> Epoch 214 finished \tANN training loss 1.036699\n",
      ">> Epoch 215 finished \tANN training loss 1.036734\n",
      ">> Epoch 216 finished \tANN training loss 1.036663\n",
      ">> Epoch 217 finished \tANN training loss 1.036663\n",
      ">> Epoch 218 finished \tANN training loss 1.037158\n",
      ">> Epoch 219 finished \tANN training loss 1.036693\n",
      ">> Epoch 220 finished \tANN training loss 1.036692\n",
      ">> Epoch 221 finished \tANN training loss 1.036724\n",
      ">> Epoch 222 finished \tANN training loss 1.036784\n",
      ">> Epoch 223 finished \tANN training loss 1.037632\n",
      ">> Epoch 224 finished \tANN training loss 1.036715\n",
      ">> Epoch 225 finished \tANN training loss 1.036679\n",
      ">> Epoch 226 finished \tANN training loss 1.036665\n",
      ">> Epoch 227 finished \tANN training loss 1.036662\n",
      ">> Epoch 228 finished \tANN training loss 1.036659\n",
      ">> Epoch 229 finished \tANN training loss 1.036924\n",
      ">> Epoch 230 finished \tANN training loss 1.036881\n",
      ">> Epoch 231 finished \tANN training loss 1.036748\n",
      ">> Epoch 232 finished \tANN training loss 1.036782\n",
      ">> Epoch 233 finished \tANN training loss 1.037401\n",
      ">> Epoch 234 finished \tANN training loss 1.036740\n",
      ">> Epoch 235 finished \tANN training loss 1.036697\n",
      ">> Epoch 236 finished \tANN training loss 1.036664\n",
      ">> Epoch 237 finished \tANN training loss 1.037030\n",
      ">> Epoch 238 finished \tANN training loss 1.036764\n",
      ">> Epoch 239 finished \tANN training loss 1.036936\n",
      ">> Epoch 240 finished \tANN training loss 1.037025\n",
      ">> Epoch 241 finished \tANN training loss 1.036920\n",
      ">> Epoch 242 finished \tANN training loss 1.036819\n",
      ">> Epoch 243 finished \tANN training loss 1.036808\n",
      ">> Epoch 244 finished \tANN training loss 1.037153\n",
      ">> Epoch 245 finished \tANN training loss 1.037124\n",
      ">> Epoch 246 finished \tANN training loss 1.037983\n",
      ">> Epoch 247 finished \tANN training loss 1.037977\n",
      ">> Epoch 248 finished \tANN training loss 1.036712\n",
      ">> Epoch 249 finished \tANN training loss 1.036674\n",
      ">> Epoch 250 finished \tANN training loss 1.036672\n",
      ">> Epoch 251 finished \tANN training loss 1.036838\n",
      ">> Epoch 252 finished \tANN training loss 1.036701\n",
      ">> Epoch 253 finished \tANN training loss 1.037657\n",
      ">> Epoch 254 finished \tANN training loss 1.036707\n",
      ">> Epoch 255 finished \tANN training loss 1.036803\n",
      ">> Epoch 256 finished \tANN training loss 1.036895\n",
      ">> Epoch 257 finished \tANN training loss 1.036662\n",
      ">> Epoch 258 finished \tANN training loss 1.036725\n",
      ">> Epoch 259 finished \tANN training loss 1.036658\n",
      ">> Epoch 260 finished \tANN training loss 1.036676\n",
      ">> Epoch 261 finished \tANN training loss 1.036657\n",
      ">> Epoch 262 finished \tANN training loss 1.036966\n",
      ">> Epoch 263 finished \tANN training loss 1.037061\n",
      ">> Epoch 264 finished \tANN training loss 1.036677\n",
      ">> Epoch 265 finished \tANN training loss 1.036769\n",
      ">> Epoch 266 finished \tANN training loss 1.037971\n",
      ">> Epoch 267 finished \tANN training loss 1.037594\n",
      ">> Epoch 268 finished \tANN training loss 1.037150\n",
      ">> Epoch 269 finished \tANN training loss 1.037388\n",
      ">> Epoch 270 finished \tANN training loss 1.038478\n",
      ">> Epoch 271 finished \tANN training loss 1.036677\n",
      ">> Epoch 272 finished \tANN training loss 1.036658\n",
      ">> Epoch 273 finished \tANN training loss 1.037263\n",
      ">> Epoch 274 finished \tANN training loss 1.036877\n",
      ">> Epoch 275 finished \tANN training loss 1.036703\n",
      ">> Epoch 276 finished \tANN training loss 1.036661\n",
      ">> Epoch 277 finished \tANN training loss 1.037523\n",
      ">> Epoch 278 finished \tANN training loss 1.036734\n",
      ">> Epoch 279 finished \tANN training loss 1.036758\n",
      ">> Epoch 280 finished \tANN training loss 1.036888\n",
      ">> Epoch 281 finished \tANN training loss 1.039668\n",
      ">> Epoch 282 finished \tANN training loss 1.038478\n",
      ">> Epoch 283 finished \tANN training loss 1.038455\n",
      ">> Epoch 284 finished \tANN training loss 1.040158\n",
      ">> Epoch 285 finished \tANN training loss 1.036810\n",
      ">> Epoch 286 finished \tANN training loss 1.037060\n",
      ">> Epoch 287 finished \tANN training loss 1.037315\n",
      ">> Epoch 288 finished \tANN training loss 1.037445\n",
      ">> Epoch 289 finished \tANN training loss 1.036655\n",
      ">> Epoch 290 finished \tANN training loss 1.036940\n",
      ">> Epoch 291 finished \tANN training loss 1.036729\n",
      ">> Epoch 292 finished \tANN training loss 1.037253\n",
      ">> Epoch 293 finished \tANN training loss 1.036737\n",
      ">> Epoch 294 finished \tANN training loss 1.037662\n",
      ">> Epoch 295 finished \tANN training loss 1.036685\n",
      ">> Epoch 296 finished \tANN training loss 1.036675\n",
      ">> Epoch 297 finished \tANN training loss 1.037088\n",
      ">> Epoch 298 finished \tANN training loss 1.038213\n",
      ">> Epoch 299 finished \tANN training loss 1.037832\n",
      ">> Epoch 300 finished \tANN training loss 1.036740\n",
      ">> Epoch 301 finished \tANN training loss 1.036762\n",
      ">> Epoch 302 finished \tANN training loss 1.036656\n",
      ">> Epoch 303 finished \tANN training loss 1.036675\n",
      ">> Epoch 304 finished \tANN training loss 1.037773\n",
      ">> Epoch 305 finished \tANN training loss 1.037035\n",
      ">> Epoch 306 finished \tANN training loss 1.037221\n",
      ">> Epoch 307 finished \tANN training loss 1.037418\n",
      ">> Epoch 308 finished \tANN training loss 1.036660\n",
      ">> Epoch 309 finished \tANN training loss 1.036930\n",
      ">> Epoch 310 finished \tANN training loss 1.036780\n",
      ">> Epoch 311 finished \tANN training loss 1.037535\n",
      ">> Epoch 312 finished \tANN training loss 1.037184\n",
      ">> Epoch 313 finished \tANN training loss 1.036723\n",
      ">> Epoch 314 finished \tANN training loss 1.037746\n",
      ">> Epoch 315 finished \tANN training loss 1.036697\n",
      ">> Epoch 316 finished \tANN training loss 1.036658\n",
      ">> Epoch 317 finished \tANN training loss 1.036968\n",
      ">> Epoch 318 finished \tANN training loss 1.036914\n",
      ">> Epoch 319 finished \tANN training loss 1.036671\n",
      ">> Epoch 320 finished \tANN training loss 1.036685\n",
      ">> Epoch 321 finished \tANN training loss 1.036992\n",
      ">> Epoch 322 finished \tANN training loss 1.037049\n",
      ">> Epoch 323 finished \tANN training loss 1.037751\n",
      ">> Epoch 324 finished \tANN training loss 1.037525\n",
      ">> Epoch 325 finished \tANN training loss 1.037087\n",
      ">> Epoch 326 finished \tANN training loss 1.037525\n",
      ">> Epoch 327 finished \tANN training loss 1.038324\n",
      ">> Epoch 328 finished \tANN training loss 1.036935\n",
      ">> Epoch 329 finished \tANN training loss 1.036693\n",
      ">> Epoch 330 finished \tANN training loss 1.036657\n",
      ">> Epoch 331 finished \tANN training loss 1.036672\n",
      ">> Epoch 332 finished \tANN training loss 1.036772\n",
      ">> Epoch 333 finished \tANN training loss 1.036798\n",
      ">> Epoch 334 finished \tANN training loss 1.037073\n",
      ">> Epoch 335 finished \tANN training loss 1.036732\n",
      ">> Epoch 336 finished \tANN training loss 1.036737\n",
      ">> Epoch 337 finished \tANN training loss 1.036754\n",
      ">> Epoch 338 finished \tANN training loss 1.037026\n",
      ">> Epoch 339 finished \tANN training loss 1.037539\n",
      ">> Epoch 340 finished \tANN training loss 1.036655\n",
      ">> Epoch 341 finished \tANN training loss 1.036691\n",
      ">> Epoch 342 finished \tANN training loss 1.037092\n",
      ">> Epoch 343 finished \tANN training loss 1.036656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 344 finished \tANN training loss 1.037180\n",
      ">> Epoch 345 finished \tANN training loss 1.037070\n",
      ">> Epoch 346 finished \tANN training loss 1.038521\n",
      ">> Epoch 347 finished \tANN training loss 1.037419\n",
      ">> Epoch 348 finished \tANN training loss 1.036843\n",
      ">> Epoch 349 finished \tANN training loss 1.037755\n",
      ">> Epoch 350 finished \tANN training loss 1.037049\n",
      ">> Epoch 351 finished \tANN training loss 1.036731\n",
      ">> Epoch 352 finished \tANN training loss 1.036676\n",
      ">> Epoch 353 finished \tANN training loss 1.037256\n",
      ">> Epoch 354 finished \tANN training loss 1.036765\n",
      ">> Epoch 355 finished \tANN training loss 1.036905\n",
      ">> Epoch 356 finished \tANN training loss 1.036655\n",
      ">> Epoch 357 finished \tANN training loss 1.036676\n",
      ">> Epoch 358 finished \tANN training loss 1.036655\n",
      ">> Epoch 359 finished \tANN training loss 1.036886\n",
      ">> Epoch 360 finished \tANN training loss 1.037596\n",
      ">> Epoch 361 finished \tANN training loss 1.036679\n",
      ">> Epoch 362 finished \tANN training loss 1.036686\n",
      ">> Epoch 363 finished \tANN training loss 1.036655\n",
      ">> Epoch 364 finished \tANN training loss 1.036842\n",
      ">> Epoch 365 finished \tANN training loss 1.036740\n",
      ">> Epoch 366 finished \tANN training loss 1.036668\n",
      ">> Epoch 367 finished \tANN training loss 1.036946\n",
      ">> Epoch 368 finished \tANN training loss 1.036949\n",
      ">> Epoch 369 finished \tANN training loss 1.036905\n",
      ">> Epoch 370 finished \tANN training loss 1.036940\n",
      ">> Epoch 371 finished \tANN training loss 1.036765\n",
      ">> Epoch 372 finished \tANN training loss 1.036663\n",
      ">> Epoch 373 finished \tANN training loss 1.037931\n",
      ">> Epoch 374 finished \tANN training loss 1.037571\n",
      ">> Epoch 375 finished \tANN training loss 1.036740\n",
      ">> Epoch 376 finished \tANN training loss 1.036657\n",
      ">> Epoch 377 finished \tANN training loss 1.036660\n",
      ">> Epoch 378 finished \tANN training loss 1.036925\n",
      ">> Epoch 379 finished \tANN training loss 1.036903\n",
      ">> Epoch 380 finished \tANN training loss 1.036678\n",
      ">> Epoch 381 finished \tANN training loss 1.036655\n",
      ">> Epoch 382 finished \tANN training loss 1.036902\n",
      ">> Epoch 383 finished \tANN training loss 1.036688\n",
      ">> Epoch 384 finished \tANN training loss 1.036986\n",
      ">> Epoch 385 finished \tANN training loss 1.036659\n",
      ">> Epoch 386 finished \tANN training loss 1.036655\n",
      ">> Epoch 387 finished \tANN training loss 1.036696\n",
      ">> Epoch 388 finished \tANN training loss 1.036905\n",
      ">> Epoch 389 finished \tANN training loss 1.037356\n",
      ">> Epoch 390 finished \tANN training loss 1.037655\n",
      ">> Epoch 391 finished \tANN training loss 1.037393\n",
      ">> Epoch 392 finished \tANN training loss 1.037031\n",
      ">> Epoch 393 finished \tANN training loss 1.036655\n",
      ">> Epoch 394 finished \tANN training loss 1.037734\n",
      ">> Epoch 395 finished \tANN training loss 1.037436\n",
      ">> Epoch 396 finished \tANN training loss 1.036995\n",
      ">> Epoch 397 finished \tANN training loss 1.036655\n",
      ">> Epoch 398 finished \tANN training loss 1.036708\n",
      ">> Epoch 399 finished \tANN training loss 1.036766\n",
      ">> Epoch 400 finished \tANN training loss 1.036988\n",
      ">> Epoch 401 finished \tANN training loss 1.037366\n",
      ">> Epoch 402 finished \tANN training loss 1.037122\n",
      ">> Epoch 403 finished \tANN training loss 1.036691\n",
      ">> Epoch 404 finished \tANN training loss 1.036699\n",
      ">> Epoch 405 finished \tANN training loss 1.036664\n",
      ">> Epoch 406 finished \tANN training loss 1.037758\n",
      ">> Epoch 407 finished \tANN training loss 1.037439\n",
      ">> Epoch 408 finished \tANN training loss 1.036976\n",
      ">> Epoch 409 finished \tANN training loss 1.036858\n",
      ">> Epoch 410 finished \tANN training loss 1.038227\n",
      ">> Epoch 411 finished \tANN training loss 1.037012\n",
      ">> Epoch 412 finished \tANN training loss 1.036675\n",
      ">> Epoch 413 finished \tANN training loss 1.036860\n",
      ">> Epoch 414 finished \tANN training loss 1.038325\n",
      ">> Epoch 415 finished \tANN training loss 1.036677\n",
      ">> Epoch 416 finished \tANN training loss 1.037632\n",
      ">> Epoch 417 finished \tANN training loss 1.036659\n",
      ">> Epoch 418 finished \tANN training loss 1.036655\n",
      ">> Epoch 419 finished \tANN training loss 1.036723\n",
      ">> Epoch 420 finished \tANN training loss 1.036668\n",
      ">> Epoch 421 finished \tANN training loss 1.036764\n",
      ">> Epoch 422 finished \tANN training loss 1.036861\n",
      ">> Epoch 423 finished \tANN training loss 1.036675\n",
      ">> Epoch 424 finished \tANN training loss 1.036723\n",
      ">> Epoch 425 finished \tANN training loss 1.037264\n",
      ">> Epoch 426 finished \tANN training loss 1.037044\n",
      ">> Epoch 427 finished \tANN training loss 1.036873\n",
      ">> Epoch 428 finished \tANN training loss 1.037897\n",
      ">> Epoch 429 finished \tANN training loss 1.036657\n",
      ">> Epoch 430 finished \tANN training loss 1.036733\n",
      ">> Epoch 431 finished \tANN training loss 1.036759\n",
      ">> Epoch 432 finished \tANN training loss 1.036891\n",
      ">> Epoch 433 finished \tANN training loss 1.036662\n",
      ">> Epoch 434 finished \tANN training loss 1.036684\n",
      ">> Epoch 435 finished \tANN training loss 1.038204\n",
      ">> Epoch 436 finished \tANN training loss 1.037494\n",
      ">> Epoch 437 finished \tANN training loss 1.041424\n",
      ">> Epoch 438 finished \tANN training loss 1.038427\n",
      ">> Epoch 439 finished \tANN training loss 1.037084\n",
      ">> Epoch 440 finished \tANN training loss 1.038050\n",
      ">> Epoch 441 finished \tANN training loss 1.037489\n",
      ">> Epoch 442 finished \tANN training loss 1.036799\n",
      ">> Epoch 443 finished \tANN training loss 1.036718\n",
      ">> Epoch 444 finished \tANN training loss 1.037506\n",
      ">> Epoch 445 finished \tANN training loss 1.037249\n",
      ">> Epoch 446 finished \tANN training loss 1.036775\n",
      ">> Epoch 447 finished \tANN training loss 1.037083\n",
      ">> Epoch 448 finished \tANN training loss 1.038195\n",
      ">> Epoch 449 finished \tANN training loss 1.037294\n",
      ">> Epoch 450 finished \tANN training loss 1.039343\n",
      ">> Epoch 451 finished \tANN training loss 1.037869\n",
      ">> Epoch 452 finished \tANN training loss 1.036805\n",
      ">> Epoch 453 finished \tANN training loss 1.036657\n",
      ">> Epoch 454 finished \tANN training loss 1.036697\n",
      ">> Epoch 455 finished \tANN training loss 1.036770\n",
      ">> Epoch 456 finished \tANN training loss 1.036655\n",
      ">> Epoch 457 finished \tANN training loss 1.036748\n",
      ">> Epoch 458 finished \tANN training loss 1.036701\n",
      ">> Epoch 459 finished \tANN training loss 1.037122\n",
      ">> Epoch 460 finished \tANN training loss 1.038100\n",
      ">> Epoch 461 finished \tANN training loss 1.037251\n",
      ">> Epoch 462 finished \tANN training loss 1.037590\n",
      ">> Epoch 463 finished \tANN training loss 1.037485\n",
      ">> Epoch 464 finished \tANN training loss 1.037414\n",
      ">> Epoch 465 finished \tANN training loss 1.036728\n",
      ">> Epoch 466 finished \tANN training loss 1.036883\n",
      ">> Epoch 467 finished \tANN training loss 1.036830\n",
      ">> Epoch 468 finished \tANN training loss 1.036769\n",
      ">> Epoch 469 finished \tANN training loss 1.037474\n",
      ">> Epoch 470 finished \tANN training loss 1.037982\n",
      ">> Epoch 471 finished \tANN training loss 1.036678\n",
      ">> Epoch 472 finished \tANN training loss 1.037182\n",
      ">> Epoch 473 finished \tANN training loss 1.037154\n",
      ">> Epoch 474 finished \tANN training loss 1.038999\n",
      ">> Epoch 475 finished \tANN training loss 1.038446\n",
      ">> Epoch 476 finished \tANN training loss 1.039082\n",
      ">> Epoch 477 finished \tANN training loss 1.036660\n",
      ">> Epoch 478 finished \tANN training loss 1.036859\n",
      ">> Epoch 479 finished \tANN training loss 1.036734\n",
      ">> Epoch 480 finished \tANN training loss 1.037398\n",
      ">> Epoch 481 finished \tANN training loss 1.036780\n",
      ">> Epoch 482 finished \tANN training loss 1.036981\n",
      ">> Epoch 483 finished \tANN training loss 1.037679\n",
      ">> Epoch 484 finished \tANN training loss 1.036878\n",
      ">> Epoch 485 finished \tANN training loss 1.036742\n",
      ">> Epoch 486 finished \tANN training loss 1.037076\n",
      ">> Epoch 487 finished \tANN training loss 1.036746\n",
      ">> Epoch 488 finished \tANN training loss 1.036660\n",
      ">> Epoch 489 finished \tANN training loss 1.036659\n",
      ">> Epoch 490 finished \tANN training loss 1.036702\n",
      ">> Epoch 491 finished \tANN training loss 1.037062\n",
      ">> Epoch 492 finished \tANN training loss 1.037316\n",
      ">> Epoch 493 finished \tANN training loss 1.036838\n",
      ">> Epoch 494 finished \tANN training loss 1.037109\n",
      ">> Epoch 495 finished \tANN training loss 1.036758\n",
      ">> Epoch 496 finished \tANN training loss 1.036881\n",
      ">> Epoch 497 finished \tANN training loss 1.037115\n",
      ">> Epoch 498 finished \tANN training loss 1.037055\n",
      ">> Epoch 499 finished \tANN training loss 1.036717\n",
      ">> Epoch 500 finished \tANN training loss 1.036708\n",
      ">> Epoch 501 finished \tANN training loss 1.038141\n",
      ">> Epoch 502 finished \tANN training loss 1.038159\n",
      ">> Epoch 503 finished \tANN training loss 1.037271\n",
      ">> Epoch 504 finished \tANN training loss 1.036967\n",
      ">> Epoch 505 finished \tANN training loss 1.037027\n",
      ">> Epoch 506 finished \tANN training loss 1.041499\n",
      ">> Epoch 507 finished \tANN training loss 1.037956\n",
      ">> Epoch 508 finished \tANN training loss 1.036658\n",
      ">> Epoch 509 finished \tANN training loss 1.036801\n",
      ">> Epoch 510 finished \tANN training loss 1.038209\n",
      ">> Epoch 511 finished \tANN training loss 1.037550\n",
      ">> Epoch 512 finished \tANN training loss 1.038644\n",
      ">> Epoch 513 finished \tANN training loss 1.038432\n",
      ">> Epoch 514 finished \tANN training loss 1.037594\n",
      ">> Epoch 515 finished \tANN training loss 1.038631\n",
      ">> Epoch 516 finished \tANN training loss 1.037151\n",
      ">> Epoch 517 finished \tANN training loss 1.039522\n",
      ">> Epoch 518 finished \tANN training loss 1.038331\n",
      ">> Epoch 519 finished \tANN training loss 1.036660\n",
      ">> Epoch 520 finished \tANN training loss 1.036707\n",
      ">> Epoch 521 finished \tANN training loss 1.036725\n",
      ">> Epoch 522 finished \tANN training loss 1.036672\n",
      ">> Epoch 523 finished \tANN training loss 1.036689\n",
      ">> Epoch 524 finished \tANN training loss 1.036739\n",
      ">> Epoch 525 finished \tANN training loss 1.036671\n",
      ">> Epoch 526 finished \tANN training loss 1.036930\n",
      ">> Epoch 527 finished \tANN training loss 1.036658\n",
      ">> Epoch 528 finished \tANN training loss 1.036685\n",
      ">> Epoch 529 finished \tANN training loss 1.036898\n",
      ">> Epoch 530 finished \tANN training loss 1.036759\n",
      ">> Epoch 531 finished \tANN training loss 1.036660\n",
      ">> Epoch 532 finished \tANN training loss 1.036660\n",
      ">> Epoch 533 finished \tANN training loss 1.036797\n",
      ">> Epoch 534 finished \tANN training loss 1.036657\n",
      ">> Epoch 535 finished \tANN training loss 1.036675\n",
      ">> Epoch 536 finished \tANN training loss 1.036696\n",
      ">> Epoch 537 finished \tANN training loss 1.037617\n",
      ">> Epoch 538 finished \tANN training loss 1.036879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 539 finished \tANN training loss 1.036660\n",
      ">> Epoch 540 finished \tANN training loss 1.036755\n",
      ">> Epoch 541 finished \tANN training loss 1.037315\n",
      ">> Epoch 542 finished \tANN training loss 1.038404\n",
      ">> Epoch 543 finished \tANN training loss 1.036821\n",
      ">> Epoch 544 finished \tANN training loss 1.036660\n",
      ">> Epoch 545 finished \tANN training loss 1.036659\n",
      ">> Epoch 546 finished \tANN training loss 1.037602\n",
      ">> Epoch 547 finished \tANN training loss 1.036892\n",
      ">> Epoch 548 finished \tANN training loss 1.037037\n",
      ">> Epoch 549 finished \tANN training loss 1.037845\n",
      ">> Epoch 550 finished \tANN training loss 1.037665\n",
      ">> Epoch 551 finished \tANN training loss 1.036795\n",
      ">> Epoch 552 finished \tANN training loss 1.037864\n",
      ">> Epoch 553 finished \tANN training loss 1.036880\n",
      ">> Epoch 554 finished \tANN training loss 1.037065\n",
      ">> Epoch 555 finished \tANN training loss 1.037206\n",
      ">> Epoch 556 finished \tANN training loss 1.036710\n",
      ">> Epoch 557 finished \tANN training loss 1.037015\n",
      ">> Epoch 558 finished \tANN training loss 1.036798\n",
      ">> Epoch 559 finished \tANN training loss 1.036798\n",
      ">> Epoch 560 finished \tANN training loss 1.038346\n",
      ">> Epoch 561 finished \tANN training loss 1.038485\n",
      ">> Epoch 562 finished \tANN training loss 1.039207\n",
      ">> Epoch 563 finished \tANN training loss 1.037310\n",
      ">> Epoch 564 finished \tANN training loss 1.038734\n",
      ">> Epoch 565 finished \tANN training loss 1.039945\n",
      ">> Epoch 566 finished \tANN training loss 1.038586\n",
      ">> Epoch 567 finished \tANN training loss 1.040560\n",
      ">> Epoch 568 finished \tANN training loss 1.039627\n",
      ">> Epoch 569 finished \tANN training loss 1.037777\n",
      ">> Epoch 570 finished \tANN training loss 1.036907\n",
      ">> Epoch 571 finished \tANN training loss 1.037452\n",
      ">> Epoch 572 finished \tANN training loss 1.037942\n",
      ">> Epoch 573 finished \tANN training loss 1.037181\n",
      ">> Epoch 574 finished \tANN training loss 1.036811\n",
      ">> Epoch 575 finished \tANN training loss 1.036750\n",
      ">> Epoch 576 finished \tANN training loss 1.037231\n",
      ">> Epoch 577 finished \tANN training loss 1.036701\n",
      ">> Epoch 578 finished \tANN training loss 1.037359\n",
      ">> Epoch 579 finished \tANN training loss 1.036887\n",
      ">> Epoch 580 finished \tANN training loss 1.036696\n",
      ">> Epoch 581 finished \tANN training loss 1.037458\n",
      ">> Epoch 582 finished \tANN training loss 1.036815\n",
      ">> Epoch 583 finished \tANN training loss 1.037981\n",
      ">> Epoch 584 finished \tANN training loss 1.036656\n",
      ">> Epoch 585 finished \tANN training loss 1.036661\n",
      ">> Epoch 586 finished \tANN training loss 1.036670\n",
      ">> Epoch 587 finished \tANN training loss 1.036661\n",
      ">> Epoch 588 finished \tANN training loss 1.036863\n",
      ">> Epoch 589 finished \tANN training loss 1.036731\n",
      ">> Epoch 590 finished \tANN training loss 1.036963\n",
      ">> Epoch 591 finished \tANN training loss 1.036659\n",
      ">> Epoch 592 finished \tANN training loss 1.036665\n",
      ">> Epoch 593 finished \tANN training loss 1.036734\n",
      ">> Epoch 594 finished \tANN training loss 1.036911\n",
      ">> Epoch 595 finished \tANN training loss 1.036764\n",
      ">> Epoch 596 finished \tANN training loss 1.036660\n",
      ">> Epoch 597 finished \tANN training loss 1.036671\n",
      ">> Epoch 598 finished \tANN training loss 1.036655\n",
      ">> Epoch 599 finished \tANN training loss 1.037576\n",
      ">> Epoch 600 finished \tANN training loss 1.037201\n",
      ">> Epoch 601 finished \tANN training loss 1.036732\n",
      ">> Epoch 602 finished \tANN training loss 1.037152\n",
      ">> Epoch 603 finished \tANN training loss 1.036749\n",
      ">> Epoch 604 finished \tANN training loss 1.037431\n",
      ">> Epoch 605 finished \tANN training loss 1.036671\n",
      ">> Epoch 606 finished \tANN training loss 1.036664\n",
      ">> Epoch 607 finished \tANN training loss 1.036827\n",
      ">> Epoch 608 finished \tANN training loss 1.036831\n",
      ">> Epoch 609 finished \tANN training loss 1.037229\n",
      ">> Epoch 610 finished \tANN training loss 1.038315\n",
      ">> Epoch 611 finished \tANN training loss 1.037556\n",
      ">> Epoch 612 finished \tANN training loss 1.037971\n",
      ">> Epoch 613 finished \tANN training loss 1.037301\n",
      ">> Epoch 614 finished \tANN training loss 1.038557\n",
      ">> Epoch 615 finished \tANN training loss 1.037010\n",
      ">> Epoch 616 finished \tANN training loss 1.036657\n",
      ">> Epoch 617 finished \tANN training loss 1.036670\n",
      ">> Epoch 618 finished \tANN training loss 1.037106\n",
      ">> Epoch 619 finished \tANN training loss 1.037214\n",
      ">> Epoch 620 finished \tANN training loss 1.036656\n",
      ">> Epoch 621 finished \tANN training loss 1.036669\n",
      ">> Epoch 622 finished \tANN training loss 1.036758\n",
      ">> Epoch 623 finished \tANN training loss 1.037539\n",
      ">> Epoch 624 finished \tANN training loss 1.037801\n",
      ">> Epoch 625 finished \tANN training loss 1.037019\n",
      ">> Epoch 626 finished \tANN training loss 1.037072\n",
      ">> Epoch 627 finished \tANN training loss 1.036655\n",
      ">> Epoch 628 finished \tANN training loss 1.036697\n",
      ">> Epoch 629 finished \tANN training loss 1.036823\n",
      ">> Epoch 630 finished \tANN training loss 1.037082\n",
      ">> Epoch 631 finished \tANN training loss 1.036655\n",
      ">> Epoch 632 finished \tANN training loss 1.037131\n",
      ">> Epoch 633 finished \tANN training loss 1.036706\n",
      ">> Epoch 634 finished \tANN training loss 1.036696\n",
      ">> Epoch 635 finished \tANN training loss 1.036935\n",
      ">> Epoch 636 finished \tANN training loss 1.036873\n",
      ">> Epoch 637 finished \tANN training loss 1.037228\n",
      ">> Epoch 638 finished \tANN training loss 1.039603\n",
      ">> Epoch 639 finished \tANN training loss 1.036772\n",
      ">> Epoch 640 finished \tANN training loss 1.037078\n",
      ">> Epoch 641 finished \tANN training loss 1.036695\n",
      ">> Epoch 642 finished \tANN training loss 1.036759\n",
      ">> Epoch 643 finished \tANN training loss 1.036834\n",
      ">> Epoch 644 finished \tANN training loss 1.037423\n",
      ">> Epoch 645 finished \tANN training loss 1.039029\n",
      ">> Epoch 646 finished \tANN training loss 1.037260\n",
      ">> Epoch 647 finished \tANN training loss 1.038869\n",
      ">> Epoch 648 finished \tANN training loss 1.041424\n",
      ">> Epoch 649 finished \tANN training loss 1.041198\n",
      ">> Epoch 650 finished \tANN training loss 1.038113\n",
      ">> Epoch 651 finished \tANN training loss 1.037428\n",
      ">> Epoch 652 finished \tANN training loss 1.036802\n",
      ">> Epoch 653 finished \tANN training loss 1.036655\n",
      ">> Epoch 654 finished \tANN training loss 1.036655\n",
      ">> Epoch 655 finished \tANN training loss 1.036668\n",
      ">> Epoch 656 finished \tANN training loss 1.037014\n",
      ">> Epoch 657 finished \tANN training loss 1.036815\n",
      ">> Epoch 658 finished \tANN training loss 1.036942\n",
      ">> Epoch 659 finished \tANN training loss 1.036706\n",
      ">> Epoch 660 finished \tANN training loss 1.037080\n",
      ">> Epoch 661 finished \tANN training loss 1.037258\n",
      ">> Epoch 662 finished \tANN training loss 1.037416\n",
      ">> Epoch 663 finished \tANN training loss 1.036878\n",
      ">> Epoch 664 finished \tANN training loss 1.036700\n",
      ">> Epoch 665 finished \tANN training loss 1.037042\n",
      ">> Epoch 666 finished \tANN training loss 1.037571\n",
      ">> Epoch 667 finished \tANN training loss 1.036857\n",
      ">> Epoch 668 finished \tANN training loss 1.037396\n",
      ">> Epoch 669 finished \tANN training loss 1.037474\n",
      ">> Epoch 670 finished \tANN training loss 1.036943\n",
      ">> Epoch 671 finished \tANN training loss 1.037117\n",
      ">> Epoch 672 finished \tANN training loss 1.037259\n",
      ">> Epoch 673 finished \tANN training loss 1.040031\n",
      ">> Epoch 674 finished \tANN training loss 1.036661\n",
      ">> Epoch 675 finished \tANN training loss 1.036836\n",
      ">> Epoch 676 finished \tANN training loss 1.036658\n",
      ">> Epoch 677 finished \tANN training loss 1.037114\n",
      ">> Epoch 678 finished \tANN training loss 1.036743\n",
      ">> Epoch 679 finished \tANN training loss 1.036657\n",
      ">> Epoch 680 finished \tANN training loss 1.037508\n",
      ">> Epoch 681 finished \tANN training loss 1.036930\n",
      ">> Epoch 682 finished \tANN training loss 1.036659\n",
      ">> Epoch 683 finished \tANN training loss 1.036656\n",
      ">> Epoch 684 finished \tANN training loss 1.036928\n",
      ">> Epoch 685 finished \tANN training loss 1.036799\n",
      ">> Epoch 686 finished \tANN training loss 1.038257\n",
      ">> Epoch 687 finished \tANN training loss 1.036798\n",
      ">> Epoch 688 finished \tANN training loss 1.037161\n",
      ">> Epoch 689 finished \tANN training loss 1.037209\n",
      ">> Epoch 690 finished \tANN training loss 1.036684\n",
      ">> Epoch 691 finished \tANN training loss 1.036745\n",
      ">> Epoch 692 finished \tANN training loss 1.037003\n",
      ">> Epoch 693 finished \tANN training loss 1.036945\n",
      ">> Epoch 694 finished \tANN training loss 1.038429\n",
      ">> Epoch 695 finished \tANN training loss 1.036700\n",
      ">> Epoch 696 finished \tANN training loss 1.036927\n",
      ">> Epoch 697 finished \tANN training loss 1.037322\n",
      ">> Epoch 698 finished \tANN training loss 1.036729\n",
      ">> Epoch 699 finished \tANN training loss 1.036662\n",
      ">> Epoch 700 finished \tANN training loss 1.037192\n",
      ">> Epoch 701 finished \tANN training loss 1.036806\n",
      ">> Epoch 702 finished \tANN training loss 1.036699\n",
      ">> Epoch 703 finished \tANN training loss 1.036744\n",
      ">> Epoch 704 finished \tANN training loss 1.037113\n",
      ">> Epoch 705 finished \tANN training loss 1.037054\n",
      ">> Epoch 706 finished \tANN training loss 1.036660\n",
      ">> Epoch 707 finished \tANN training loss 1.037617\n",
      ">> Epoch 708 finished \tANN training loss 1.037355\n",
      ">> Epoch 709 finished \tANN training loss 1.036894\n",
      ">> Epoch 710 finished \tANN training loss 1.036983\n",
      ">> Epoch 711 finished \tANN training loss 1.036663\n",
      ">> Epoch 712 finished \tANN training loss 1.036704\n",
      ">> Epoch 713 finished \tANN training loss 1.036657\n",
      ">> Epoch 714 finished \tANN training loss 1.036667\n",
      ">> Epoch 715 finished \tANN training loss 1.036691\n",
      ">> Epoch 716 finished \tANN training loss 1.036655\n",
      ">> Epoch 717 finished \tANN training loss 1.036702\n",
      ">> Epoch 718 finished \tANN training loss 1.036737\n",
      ">> Epoch 719 finished \tANN training loss 1.036680\n",
      ">> Epoch 720 finished \tANN training loss 1.036660\n",
      ">> Epoch 721 finished \tANN training loss 1.036668\n",
      ">> Epoch 722 finished \tANN training loss 1.036927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 723 finished \tANN training loss 1.037249\n",
      ">> Epoch 724 finished \tANN training loss 1.036700\n",
      ">> Epoch 725 finished \tANN training loss 1.036822\n",
      ">> Epoch 726 finished \tANN training loss 1.036782\n",
      ">> Epoch 727 finished \tANN training loss 1.037142\n",
      ">> Epoch 728 finished \tANN training loss 1.036704\n",
      ">> Epoch 729 finished \tANN training loss 1.036695\n",
      ">> Epoch 730 finished \tANN training loss 1.036693\n",
      ">> Epoch 731 finished \tANN training loss 1.036830\n",
      ">> Epoch 732 finished \tANN training loss 1.036819\n",
      ">> Epoch 733 finished \tANN training loss 1.036944\n",
      ">> Epoch 734 finished \tANN training loss 1.037633\n",
      ">> Epoch 735 finished \tANN training loss 1.037111\n",
      ">> Epoch 736 finished \tANN training loss 1.036981\n",
      ">> Epoch 737 finished \tANN training loss 1.036809\n",
      ">> Epoch 738 finished \tANN training loss 1.036723\n",
      ">> Epoch 739 finished \tANN training loss 1.037267\n",
      ">> Epoch 740 finished \tANN training loss 1.037314\n",
      ">> Epoch 741 finished \tANN training loss 1.036682\n",
      ">> Epoch 742 finished \tANN training loss 1.036828\n",
      ">> Epoch 743 finished \tANN training loss 1.036660\n",
      ">> Epoch 744 finished \tANN training loss 1.036759\n",
      ">> Epoch 745 finished \tANN training loss 1.037487\n",
      ">> Epoch 746 finished \tANN training loss 1.036942\n",
      ">> Epoch 747 finished \tANN training loss 1.036889\n",
      ">> Epoch 748 finished \tANN training loss 1.037099\n",
      ">> Epoch 749 finished \tANN training loss 1.037434\n",
      ">> Epoch 750 finished \tANN training loss 1.038184\n",
      ">> Epoch 751 finished \tANN training loss 1.036697\n",
      ">> Epoch 752 finished \tANN training loss 1.036883\n",
      ">> Epoch 753 finished \tANN training loss 1.036864\n",
      ">> Epoch 754 finished \tANN training loss 1.036940\n",
      ">> Epoch 755 finished \tANN training loss 1.036783\n",
      ">> Epoch 756 finished \tANN training loss 1.036887\n",
      ">> Epoch 757 finished \tANN training loss 1.036949\n",
      ">> Epoch 758 finished \tANN training loss 1.037299\n",
      ">> Epoch 759 finished \tANN training loss 1.036725\n",
      ">> Epoch 760 finished \tANN training loss 1.036660\n",
      ">> Epoch 761 finished \tANN training loss 1.036754\n",
      ">> Epoch 762 finished \tANN training loss 1.036661\n",
      ">> Epoch 763 finished \tANN training loss 1.037061\n",
      ">> Epoch 764 finished \tANN training loss 1.036660\n",
      ">> Epoch 765 finished \tANN training loss 1.036658\n",
      ">> Epoch 766 finished \tANN training loss 1.036664\n",
      ">> Epoch 767 finished \tANN training loss 1.036690\n",
      ">> Epoch 768 finished \tANN training loss 1.036655\n",
      ">> Epoch 769 finished \tANN training loss 1.036677\n",
      ">> Epoch 770 finished \tANN training loss 1.036754\n",
      ">> Epoch 771 finished \tANN training loss 1.036807\n",
      ">> Epoch 772 finished \tANN training loss 1.037084\n",
      ">> Epoch 773 finished \tANN training loss 1.036655\n",
      ">> Epoch 774 finished \tANN training loss 1.037639\n",
      ">> Epoch 775 finished \tANN training loss 1.036695\n",
      ">> Epoch 776 finished \tANN training loss 1.037729\n",
      ">> Epoch 777 finished \tANN training loss 1.039378\n",
      ">> Epoch 778 finished \tANN training loss 1.036709\n",
      ">> Epoch 779 finished \tANN training loss 1.036886\n",
      ">> Epoch 780 finished \tANN training loss 1.037095\n",
      ">> Epoch 781 finished \tANN training loss 1.037084\n",
      ">> Epoch 782 finished \tANN training loss 1.036684\n",
      ">> Epoch 783 finished \tANN training loss 1.036657\n",
      ">> Epoch 784 finished \tANN training loss 1.036667\n",
      ">> Epoch 785 finished \tANN training loss 1.036800\n",
      ">> Epoch 786 finished \tANN training loss 1.036666\n",
      ">> Epoch 787 finished \tANN training loss 1.036886\n",
      ">> Epoch 788 finished \tANN training loss 1.036886\n",
      ">> Epoch 789 finished \tANN training loss 1.036970\n",
      ">> Epoch 790 finished \tANN training loss 1.036659\n",
      ">> Epoch 791 finished \tANN training loss 1.036979\n",
      ">> Epoch 792 finished \tANN training loss 1.036971\n",
      ">> Epoch 793 finished \tANN training loss 1.036942\n",
      ">> Epoch 794 finished \tANN training loss 1.036978\n",
      ">> Epoch 795 finished \tANN training loss 1.038647\n",
      ">> Epoch 796 finished \tANN training loss 1.037449\n",
      ">> Epoch 797 finished \tANN training loss 1.037902\n",
      ">> Epoch 798 finished \tANN training loss 1.037431\n",
      ">> Epoch 799 finished \tANN training loss 1.038315\n",
      ">> Epoch 800 finished \tANN training loss 1.036793\n",
      ">> Epoch 801 finished \tANN training loss 1.036720\n",
      ">> Epoch 802 finished \tANN training loss 1.036664\n",
      ">> Epoch 803 finished \tANN training loss 1.037170\n",
      ">> Epoch 804 finished \tANN training loss 1.037039\n",
      ">> Epoch 805 finished \tANN training loss 1.036769\n",
      ">> Epoch 806 finished \tANN training loss 1.037231\n",
      ">> Epoch 807 finished \tANN training loss 1.036675\n",
      ">> Epoch 808 finished \tANN training loss 1.036763\n",
      ">> Epoch 809 finished \tANN training loss 1.036936\n",
      ">> Epoch 810 finished \tANN training loss 1.037037\n",
      ">> Epoch 811 finished \tANN training loss 1.036922\n",
      ">> Epoch 812 finished \tANN training loss 1.037781\n",
      ">> Epoch 813 finished \tANN training loss 1.036833\n",
      ">> Epoch 814 finished \tANN training loss 1.037044\n",
      ">> Epoch 815 finished \tANN training loss 1.037567\n",
      ">> Epoch 816 finished \tANN training loss 1.036765\n",
      ">> Epoch 817 finished \tANN training loss 1.036777\n",
      ">> Epoch 818 finished \tANN training loss 1.036655\n",
      ">> Epoch 819 finished \tANN training loss 1.036734\n",
      ">> Epoch 820 finished \tANN training loss 1.036741\n",
      ">> Epoch 821 finished \tANN training loss 1.036656\n",
      ">> Epoch 822 finished \tANN training loss 1.037282\n",
      ">> Epoch 823 finished \tANN training loss 1.037730\n",
      ">> Epoch 824 finished \tANN training loss 1.036665\n",
      ">> Epoch 825 finished \tANN training loss 1.037093\n",
      ">> Epoch 826 finished \tANN training loss 1.036666\n",
      ">> Epoch 827 finished \tANN training loss 1.036655\n",
      ">> Epoch 828 finished \tANN training loss 1.036821\n",
      ">> Epoch 829 finished \tANN training loss 1.036887\n",
      ">> Epoch 830 finished \tANN training loss 1.036803\n",
      ">> Epoch 831 finished \tANN training loss 1.036721\n",
      ">> Epoch 832 finished \tANN training loss 1.036659\n",
      ">> Epoch 833 finished \tANN training loss 1.036656\n",
      ">> Epoch 834 finished \tANN training loss 1.037796\n",
      ">> Epoch 835 finished \tANN training loss 1.038893\n",
      ">> Epoch 836 finished \tANN training loss 1.037038\n",
      ">> Epoch 837 finished \tANN training loss 1.036662\n",
      ">> Epoch 838 finished \tANN training loss 1.036705\n",
      ">> Epoch 839 finished \tANN training loss 1.036931\n",
      ">> Epoch 840 finished \tANN training loss 1.036813\n",
      ">> Epoch 841 finished \tANN training loss 1.036670\n",
      ">> Epoch 842 finished \tANN training loss 1.036703\n",
      ">> Epoch 843 finished \tANN training loss 1.036896\n",
      ">> Epoch 844 finished \tANN training loss 1.039239\n",
      ">> Epoch 845 finished \tANN training loss 1.037071\n",
      ">> Epoch 846 finished \tANN training loss 1.037119\n",
      ">> Epoch 847 finished \tANN training loss 1.037036\n",
      ">> Epoch 848 finished \tANN training loss 1.036840\n",
      ">> Epoch 849 finished \tANN training loss 1.036903\n",
      ">> Epoch 850 finished \tANN training loss 1.036657\n",
      ">> Epoch 851 finished \tANN training loss 1.036842\n",
      ">> Epoch 852 finished \tANN training loss 1.036710\n",
      ">> Epoch 853 finished \tANN training loss 1.036770\n",
      ">> Epoch 854 finished \tANN training loss 1.036726\n",
      ">> Epoch 855 finished \tANN training loss 1.037187\n",
      ">> Epoch 856 finished \tANN training loss 1.036740\n",
      ">> Epoch 857 finished \tANN training loss 1.036982\n",
      ">> Epoch 858 finished \tANN training loss 1.037267\n",
      ">> Epoch 859 finished \tANN training loss 1.037673\n",
      ">> Epoch 860 finished \tANN training loss 1.036712\n",
      ">> Epoch 861 finished \tANN training loss 1.037550\n",
      ">> Epoch 862 finished \tANN training loss 1.038206\n",
      ">> Epoch 863 finished \tANN training loss 1.038566\n",
      ">> Epoch 864 finished \tANN training loss 1.037636\n",
      ">> Epoch 865 finished \tANN training loss 1.036666\n",
      ">> Epoch 866 finished \tANN training loss 1.036687\n",
      ">> Epoch 867 finished \tANN training loss 1.036828\n",
      ">> Epoch 868 finished \tANN training loss 1.036699\n",
      ">> Epoch 869 finished \tANN training loss 1.037424\n",
      ">> Epoch 870 finished \tANN training loss 1.036899\n",
      ">> Epoch 871 finished \tANN training loss 1.037019\n",
      ">> Epoch 872 finished \tANN training loss 1.037418\n",
      ">> Epoch 873 finished \tANN training loss 1.037017\n",
      ">> Epoch 874 finished \tANN training loss 1.037885\n",
      ">> Epoch 875 finished \tANN training loss 1.036960\n",
      ">> Epoch 876 finished \tANN training loss 1.036788\n",
      ">> Epoch 877 finished \tANN training loss 1.036655\n",
      ">> Epoch 878 finished \tANN training loss 1.036810\n",
      ">> Epoch 879 finished \tANN training loss 1.036806\n",
      ">> Epoch 880 finished \tANN training loss 1.036929\n",
      ">> Epoch 881 finished \tANN training loss 1.037070\n",
      ">> Epoch 882 finished \tANN training loss 1.036858\n",
      ">> Epoch 883 finished \tANN training loss 1.037280\n",
      ">> Epoch 884 finished \tANN training loss 1.036851\n",
      ">> Epoch 885 finished \tANN training loss 1.037518\n",
      ">> Epoch 886 finished \tANN training loss 1.036690\n",
      ">> Epoch 887 finished \tANN training loss 1.036791\n",
      ">> Epoch 888 finished \tANN training loss 1.037032\n",
      ">> Epoch 889 finished \tANN training loss 1.037828\n",
      ">> Epoch 890 finished \tANN training loss 1.037242\n",
      ">> Epoch 891 finished \tANN training loss 1.036918\n",
      ">> Epoch 892 finished \tANN training loss 1.036668\n",
      ">> Epoch 893 finished \tANN training loss 1.036680\n",
      ">> Epoch 894 finished \tANN training loss 1.036689\n",
      ">> Epoch 895 finished \tANN training loss 1.036750\n",
      ">> Epoch 896 finished \tANN training loss 1.037674\n",
      ">> Epoch 897 finished \tANN training loss 1.036675\n",
      ">> Epoch 898 finished \tANN training loss 1.036991\n",
      ">> Epoch 899 finished \tANN training loss 1.036706\n",
      ">> Epoch 900 finished \tANN training loss 1.038114\n",
      ">> Epoch 901 finished \tANN training loss 1.037491\n",
      ">> Epoch 902 finished \tANN training loss 1.038359\n",
      ">> Epoch 903 finished \tANN training loss 1.036908\n",
      ">> Epoch 904 finished \tANN training loss 1.036658\n",
      ">> Epoch 905 finished \tANN training loss 1.036947\n",
      ">> Epoch 906 finished \tANN training loss 1.036952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 907 finished \tANN training loss 1.037136\n",
      ">> Epoch 908 finished \tANN training loss 1.036860\n",
      ">> Epoch 909 finished \tANN training loss 1.036673\n",
      ">> Epoch 910 finished \tANN training loss 1.036663\n",
      ">> Epoch 911 finished \tANN training loss 1.036673\n",
      ">> Epoch 912 finished \tANN training loss 1.036728\n",
      ">> Epoch 913 finished \tANN training loss 1.037266\n",
      ">> Epoch 914 finished \tANN training loss 1.037423\n",
      ">> Epoch 915 finished \tANN training loss 1.038009\n",
      ">> Epoch 916 finished \tANN training loss 1.037395\n",
      ">> Epoch 917 finished \tANN training loss 1.037928\n",
      ">> Epoch 918 finished \tANN training loss 1.037238\n",
      ">> Epoch 919 finished \tANN training loss 1.037423\n",
      ">> Epoch 920 finished \tANN training loss 1.036858\n",
      ">> Epoch 921 finished \tANN training loss 1.036783\n",
      ">> Epoch 922 finished \tANN training loss 1.037388\n",
      ">> Epoch 923 finished \tANN training loss 1.036848\n",
      ">> Epoch 924 finished \tANN training loss 1.036878\n",
      ">> Epoch 925 finished \tANN training loss 1.037106\n",
      ">> Epoch 926 finished \tANN training loss 1.038502\n",
      ">> Epoch 927 finished \tANN training loss 1.037642\n",
      ">> Epoch 928 finished \tANN training loss 1.036656\n",
      ">> Epoch 929 finished \tANN training loss 1.036669\n",
      ">> Epoch 930 finished \tANN training loss 1.036908\n",
      ">> Epoch 931 finished \tANN training loss 1.036675\n",
      ">> Epoch 932 finished \tANN training loss 1.037262\n",
      ">> Epoch 933 finished \tANN training loss 1.037109\n",
      ">> Epoch 934 finished \tANN training loss 1.036982\n",
      ">> Epoch 935 finished \tANN training loss 1.038692\n",
      ">> Epoch 936 finished \tANN training loss 1.039441\n",
      ">> Epoch 937 finished \tANN training loss 1.037981\n",
      ">> Epoch 938 finished \tANN training loss 1.038698\n",
      ">> Epoch 939 finished \tANN training loss 1.038477\n",
      ">> Epoch 940 finished \tANN training loss 1.036871\n",
      ">> Epoch 941 finished \tANN training loss 1.036671\n",
      ">> Epoch 942 finished \tANN training loss 1.037005\n",
      ">> Epoch 943 finished \tANN training loss 1.036706\n",
      ">> Epoch 944 finished \tANN training loss 1.037542\n",
      ">> Epoch 945 finished \tANN training loss 1.037138\n",
      ">> Epoch 946 finished \tANN training loss 1.036718\n",
      ">> Epoch 947 finished \tANN training loss 1.037262\n",
      ">> Epoch 948 finished \tANN training loss 1.036741\n",
      ">> Epoch 949 finished \tANN training loss 1.036787\n",
      ">> Epoch 950 finished \tANN training loss 1.036685\n",
      ">> Epoch 951 finished \tANN training loss 1.036771\n",
      ">> Epoch 952 finished \tANN training loss 1.036667\n",
      ">> Epoch 953 finished \tANN training loss 1.036670\n",
      ">> Epoch 954 finished \tANN training loss 1.036658\n",
      ">> Epoch 955 finished \tANN training loss 1.036674\n",
      ">> Epoch 956 finished \tANN training loss 1.036721\n",
      ">> Epoch 957 finished \tANN training loss 1.036714\n",
      ">> Epoch 958 finished \tANN training loss 1.036663\n",
      ">> Epoch 959 finished \tANN training loss 1.036766\n",
      ">> Epoch 960 finished \tANN training loss 1.036743\n",
      ">> Epoch 961 finished \tANN training loss 1.036655\n",
      ">> Epoch 962 finished \tANN training loss 1.036793\n",
      ">> Epoch 963 finished \tANN training loss 1.036664\n",
      ">> Epoch 964 finished \tANN training loss 1.036825\n",
      ">> Epoch 965 finished \tANN training loss 1.036939\n",
      ">> Epoch 966 finished \tANN training loss 1.036657\n",
      ">> Epoch 967 finished \tANN training loss 1.036720\n",
      ">> Epoch 968 finished \tANN training loss 1.037513\n",
      ">> Epoch 969 finished \tANN training loss 1.037003\n",
      ">> Epoch 970 finished \tANN training loss 1.036659\n",
      ">> Epoch 971 finished \tANN training loss 1.036725\n",
      ">> Epoch 972 finished \tANN training loss 1.036686\n",
      ">> Epoch 973 finished \tANN training loss 1.036852\n",
      ">> Epoch 974 finished \tANN training loss 1.037887\n",
      ">> Epoch 975 finished \tANN training loss 1.037681\n",
      ">> Epoch 976 finished \tANN training loss 1.036701\n",
      ">> Epoch 977 finished \tANN training loss 1.036672\n",
      ">> Epoch 978 finished \tANN training loss 1.036945\n",
      ">> Epoch 979 finished \tANN training loss 1.036757\n",
      ">> Epoch 980 finished \tANN training loss 1.036785\n",
      ">> Epoch 981 finished \tANN training loss 1.037014\n",
      ">> Epoch 982 finished \tANN training loss 1.036910\n",
      ">> Epoch 983 finished \tANN training loss 1.037109\n",
      ">> Epoch 984 finished \tANN training loss 1.036838\n",
      ">> Epoch 985 finished \tANN training loss 1.036947\n",
      ">> Epoch 986 finished \tANN training loss 1.037159\n",
      ">> Epoch 987 finished \tANN training loss 1.037676\n",
      ">> Epoch 988 finished \tANN training loss 1.036854\n",
      ">> Epoch 989 finished \tANN training loss 1.037604\n",
      ">> Epoch 990 finished \tANN training loss 1.037853\n",
      ">> Epoch 991 finished \tANN training loss 1.036890\n",
      ">> Epoch 992 finished \tANN training loss 1.036695\n",
      ">> Epoch 993 finished \tANN training loss 1.036713\n",
      ">> Epoch 994 finished \tANN training loss 1.036676\n",
      ">> Epoch 995 finished \tANN training loss 1.037244\n",
      ">> Epoch 996 finished \tANN training loss 1.037742\n",
      ">> Epoch 997 finished \tANN training loss 1.037374\n",
      ">> Epoch 998 finished \tANN training loss 1.036668\n",
      ">> Epoch 999 finished \tANN training loss 1.037221\n",
      "[END] Fine tuning step\n",
      "\n",
      "\n",
      "TRIAL:  3\n",
      "\n",
      "\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 5.738715\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 4.669984\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 4.586661\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 5.181029\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 6.164362\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 7.831315\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 10.728307\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 13.271429\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 19.987584\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 25.286431\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 144.705032\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 770.755554\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 1456.501221\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 1945.348022\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 2555.214111\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 2693.716553\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 2690.434570\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 2814.746826\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 3177.925049\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 3177.278809\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 5.183673\n",
      ">> Epoch 1 finished \tANN training loss 2.757329\n",
      ">> Epoch 2 finished \tANN training loss 1.807149\n",
      ">> Epoch 3 finished \tANN training loss 1.337075\n",
      ">> Epoch 4 finished \tANN training loss 1.168275\n",
      ">> Epoch 5 finished \tANN training loss 1.079291\n",
      ">> Epoch 6 finished \tANN training loss 1.051840\n",
      ">> Epoch 7 finished \tANN training loss 1.040352\n",
      ">> Epoch 8 finished \tANN training loss 1.037358\n",
      ">> Epoch 9 finished \tANN training loss 1.033555\n",
      ">> Epoch 10 finished \tANN training loss 1.033514\n",
      ">> Epoch 11 finished \tANN training loss 1.030596\n",
      ">> Epoch 12 finished \tANN training loss 1.027663\n",
      ">> Epoch 13 finished \tANN training loss 1.024882\n",
      ">> Epoch 14 finished \tANN training loss 1.019693\n",
      ">> Epoch 15 finished \tANN training loss 1.012231\n",
      ">> Epoch 16 finished \tANN training loss 1.002396\n",
      ">> Epoch 17 finished \tANN training loss 0.990076\n",
      ">> Epoch 18 finished \tANN training loss 0.972844\n",
      ">> Epoch 19 finished \tANN training loss 0.951744\n",
      ">> Epoch 20 finished \tANN training loss 0.922428\n",
      ">> Epoch 21 finished \tANN training loss 0.889458\n",
      ">> Epoch 22 finished \tANN training loss 0.851072\n",
      ">> Epoch 23 finished \tANN training loss 0.806441\n",
      ">> Epoch 24 finished \tANN training loss 0.758740\n",
      ">> Epoch 25 finished \tANN training loss 0.703699\n",
      ">> Epoch 26 finished \tANN training loss 0.660117\n",
      ">> Epoch 27 finished \tANN training loss 0.605373\n",
      ">> Epoch 28 finished \tANN training loss 0.562896\n",
      ">> Epoch 29 finished \tANN training loss 0.525097\n",
      ">> Epoch 30 finished \tANN training loss 0.490283\n",
      ">> Epoch 31 finished \tANN training loss 0.466533\n",
      ">> Epoch 32 finished \tANN training loss 0.443017\n",
      ">> Epoch 33 finished \tANN training loss 0.420729\n",
      ">> Epoch 34 finished \tANN training loss 0.403622\n",
      ">> Epoch 35 finished \tANN training loss 0.388160\n",
      ">> Epoch 36 finished \tANN training loss 0.378831\n",
      ">> Epoch 37 finished \tANN training loss 0.368561\n",
      ">> Epoch 38 finished \tANN training loss 0.356710\n",
      ">> Epoch 39 finished \tANN training loss 0.357612\n",
      ">> Epoch 40 finished \tANN training loss 0.345587\n",
      ">> Epoch 41 finished \tANN training loss 0.344599\n",
      ">> Epoch 42 finished \tANN training loss 0.336119\n",
      ">> Epoch 43 finished \tANN training loss 0.327044\n",
      ">> Epoch 44 finished \tANN training loss 0.319557\n",
      ">> Epoch 45 finished \tANN training loss 0.322508\n",
      ">> Epoch 46 finished \tANN training loss 0.318184\n",
      ">> Epoch 47 finished \tANN training loss 0.316246\n",
      ">> Epoch 48 finished \tANN training loss 0.311141\n",
      ">> Epoch 49 finished \tANN training loss 0.312697\n",
      ">> Epoch 50 finished \tANN training loss 0.308545\n",
      ">> Epoch 51 finished \tANN training loss 0.308482\n",
      ">> Epoch 52 finished \tANN training loss 0.312116\n",
      ">> Epoch 53 finished \tANN training loss 0.303710\n",
      ">> Epoch 54 finished \tANN training loss 0.313363\n",
      ">> Epoch 55 finished \tANN training loss 0.306316\n",
      ">> Epoch 56 finished \tANN training loss 0.309691\n",
      ">> Epoch 57 finished \tANN training loss 0.308486\n",
      ">> Epoch 58 finished \tANN training loss 0.307187\n",
      ">> Epoch 59 finished \tANN training loss 0.301972\n",
      ">> Epoch 60 finished \tANN training loss 0.300307\n",
      ">> Epoch 61 finished \tANN training loss 0.305006\n",
      ">> Epoch 62 finished \tANN training loss 0.305155\n",
      ">> Epoch 63 finished \tANN training loss 0.300199\n",
      ">> Epoch 64 finished \tANN training loss 0.301961\n",
      ">> Epoch 65 finished \tANN training loss 0.303358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 66 finished \tANN training loss 0.304853\n",
      ">> Epoch 67 finished \tANN training loss 0.311211\n",
      ">> Epoch 68 finished \tANN training loss 0.310315\n",
      ">> Epoch 69 finished \tANN training loss 0.310087\n",
      ">> Epoch 70 finished \tANN training loss 0.302031\n",
      ">> Epoch 71 finished \tANN training loss 0.304981\n",
      ">> Epoch 72 finished \tANN training loss 0.303373\n",
      ">> Epoch 73 finished \tANN training loss 0.298609\n",
      ">> Epoch 74 finished \tANN training loss 0.297800\n",
      ">> Epoch 75 finished \tANN training loss 0.302775\n",
      ">> Epoch 76 finished \tANN training loss 0.303709\n",
      ">> Epoch 77 finished \tANN training loss 0.308873\n",
      ">> Epoch 78 finished \tANN training loss 0.316966\n",
      ">> Epoch 79 finished \tANN training loss 0.305197\n",
      ">> Epoch 80 finished \tANN training loss 0.294164\n",
      ">> Epoch 81 finished \tANN training loss 0.301900\n",
      ">> Epoch 82 finished \tANN training loss 0.299194\n",
      ">> Epoch 83 finished \tANN training loss 0.303468\n",
      ">> Epoch 84 finished \tANN training loss 0.304917\n",
      ">> Epoch 85 finished \tANN training loss 0.295414\n",
      ">> Epoch 86 finished \tANN training loss 0.294989\n",
      ">> Epoch 87 finished \tANN training loss 0.299954\n",
      ">> Epoch 88 finished \tANN training loss 0.298634\n",
      ">> Epoch 89 finished \tANN training loss 0.298001\n",
      ">> Epoch 90 finished \tANN training loss 0.300243\n",
      ">> Epoch 91 finished \tANN training loss 0.305438\n",
      ">> Epoch 92 finished \tANN training loss 0.313991\n",
      ">> Epoch 93 finished \tANN training loss 0.305001\n",
      ">> Epoch 94 finished \tANN training loss 0.305927\n",
      ">> Epoch 95 finished \tANN training loss 0.299977\n",
      ">> Epoch 96 finished \tANN training loss 0.307079\n",
      ">> Epoch 97 finished \tANN training loss 0.312097\n",
      ">> Epoch 98 finished \tANN training loss 0.305160\n",
      ">> Epoch 99 finished \tANN training loss 0.311980\n",
      ">> Epoch 100 finished \tANN training loss 0.307719\n",
      ">> Epoch 101 finished \tANN training loss 0.309710\n",
      ">> Epoch 102 finished \tANN training loss 0.318010\n",
      ">> Epoch 103 finished \tANN training loss 0.306938\n",
      ">> Epoch 104 finished \tANN training loss 0.310941\n",
      ">> Epoch 105 finished \tANN training loss 0.306413\n",
      ">> Epoch 106 finished \tANN training loss 0.307585\n",
      ">> Epoch 107 finished \tANN training loss 0.308290\n",
      ">> Epoch 108 finished \tANN training loss 0.314433\n",
      ">> Epoch 109 finished \tANN training loss 0.310062\n",
      ">> Epoch 110 finished \tANN training loss 0.314445\n",
      ">> Epoch 111 finished \tANN training loss 0.311148\n",
      ">> Epoch 112 finished \tANN training loss 0.307210\n",
      ">> Epoch 113 finished \tANN training loss 0.304772\n",
      ">> Epoch 114 finished \tANN training loss 0.307685\n",
      ">> Epoch 115 finished \tANN training loss 0.305162\n",
      ">> Epoch 116 finished \tANN training loss 0.307442\n",
      ">> Epoch 117 finished \tANN training loss 0.311219\n",
      ">> Epoch 118 finished \tANN training loss 0.314713\n",
      ">> Epoch 119 finished \tANN training loss 0.309934\n",
      ">> Epoch 120 finished \tANN training loss 0.305957\n",
      ">> Epoch 121 finished \tANN training loss 0.304771\n",
      ">> Epoch 122 finished \tANN training loss 0.305145\n",
      ">> Epoch 123 finished \tANN training loss 0.297648\n",
      ">> Epoch 124 finished \tANN training loss 0.304504\n",
      ">> Epoch 125 finished \tANN training loss 0.306055\n",
      ">> Epoch 126 finished \tANN training loss 0.295657\n",
      ">> Epoch 127 finished \tANN training loss 0.295115\n",
      ">> Epoch 128 finished \tANN training loss 0.306255\n",
      ">> Epoch 129 finished \tANN training loss 0.300858\n",
      ">> Epoch 130 finished \tANN training loss 0.301379\n",
      ">> Epoch 131 finished \tANN training loss 0.301900\n",
      ">> Epoch 132 finished \tANN training loss 0.304998\n",
      ">> Epoch 133 finished \tANN training loss 0.309341\n",
      ">> Epoch 134 finished \tANN training loss 0.311397\n",
      ">> Epoch 135 finished \tANN training loss 0.307335\n",
      ">> Epoch 136 finished \tANN training loss 0.302051\n",
      ">> Epoch 137 finished \tANN training loss 0.308559\n",
      ">> Epoch 138 finished \tANN training loss 0.305134\n",
      ">> Epoch 139 finished \tANN training loss 0.306170\n",
      ">> Epoch 140 finished \tANN training loss 0.317625\n",
      ">> Epoch 141 finished \tANN training loss 0.306124\n",
      ">> Epoch 142 finished \tANN training loss 0.301787\n",
      ">> Epoch 143 finished \tANN training loss 0.303470\n",
      ">> Epoch 144 finished \tANN training loss 0.306628\n",
      ">> Epoch 145 finished \tANN training loss 0.298583\n",
      ">> Epoch 146 finished \tANN training loss 0.307814\n",
      ">> Epoch 147 finished \tANN training loss 0.298752\n",
      ">> Epoch 148 finished \tANN training loss 0.295748\n",
      ">> Epoch 149 finished \tANN training loss 0.303474\n",
      ">> Epoch 150 finished \tANN training loss 0.302323\n",
      ">> Epoch 151 finished \tANN training loss 0.301512\n",
      ">> Epoch 152 finished \tANN training loss 0.302780\n",
      ">> Epoch 153 finished \tANN training loss 0.296898\n",
      ">> Epoch 154 finished \tANN training loss 0.298839\n",
      ">> Epoch 155 finished \tANN training loss 0.304916\n",
      ">> Epoch 156 finished \tANN training loss 0.309371\n",
      ">> Epoch 157 finished \tANN training loss 0.299433\n",
      ">> Epoch 158 finished \tANN training loss 0.299900\n",
      ">> Epoch 159 finished \tANN training loss 0.296736\n",
      ">> Epoch 160 finished \tANN training loss 0.299393\n",
      ">> Epoch 161 finished \tANN training loss 0.310523\n",
      ">> Epoch 162 finished \tANN training loss 0.301568\n",
      ">> Epoch 163 finished \tANN training loss 0.301859\n",
      ">> Epoch 164 finished \tANN training loss 0.301788\n",
      ">> Epoch 165 finished \tANN training loss 0.297660\n",
      ">> Epoch 166 finished \tANN training loss 0.299066\n",
      ">> Epoch 167 finished \tANN training loss 0.303387\n",
      ">> Epoch 168 finished \tANN training loss 0.305369\n",
      ">> Epoch 169 finished \tANN training loss 0.300394\n",
      ">> Epoch 170 finished \tANN training loss 0.300458\n",
      ">> Epoch 171 finished \tANN training loss 0.304641\n",
      ">> Epoch 172 finished \tANN training loss 0.308286\n",
      ">> Epoch 173 finished \tANN training loss 0.300014\n",
      ">> Epoch 174 finished \tANN training loss 0.300166\n",
      ">> Epoch 175 finished \tANN training loss 0.303757\n",
      ">> Epoch 176 finished \tANN training loss 0.305507\n",
      ">> Epoch 177 finished \tANN training loss 0.299171\n",
      ">> Epoch 178 finished \tANN training loss 0.300901\n",
      ">> Epoch 179 finished \tANN training loss 0.299023\n",
      ">> Epoch 180 finished \tANN training loss 0.308838\n",
      ">> Epoch 181 finished \tANN training loss 0.301744\n",
      ">> Epoch 182 finished \tANN training loss 0.301245\n",
      ">> Epoch 183 finished \tANN training loss 0.309445\n",
      ">> Epoch 184 finished \tANN training loss 0.304806\n",
      ">> Epoch 185 finished \tANN training loss 0.299345\n",
      ">> Epoch 186 finished \tANN training loss 0.297228\n",
      ">> Epoch 187 finished \tANN training loss 0.297038\n",
      ">> Epoch 188 finished \tANN training loss 0.303768\n",
      ">> Epoch 189 finished \tANN training loss 0.304770\n",
      ">> Epoch 190 finished \tANN training loss 0.306535\n",
      ">> Epoch 191 finished \tANN training loss 0.306798\n",
      ">> Epoch 192 finished \tANN training loss 0.309908\n",
      ">> Epoch 193 finished \tANN training loss 0.303292\n",
      ">> Epoch 194 finished \tANN training loss 0.298938\n",
      ">> Epoch 195 finished \tANN training loss 0.301488\n",
      ">> Epoch 196 finished \tANN training loss 0.293129\n",
      ">> Epoch 197 finished \tANN training loss 0.300845\n",
      ">> Epoch 198 finished \tANN training loss 0.306367\n",
      ">> Epoch 199 finished \tANN training loss 0.303945\n",
      ">> Epoch 200 finished \tANN training loss 0.309435\n",
      ">> Epoch 201 finished \tANN training loss 0.315469\n",
      ">> Epoch 202 finished \tANN training loss 0.304887\n",
      ">> Epoch 203 finished \tANN training loss 0.310275\n",
      ">> Epoch 204 finished \tANN training loss 0.304727\n",
      ">> Epoch 205 finished \tANN training loss 0.313469\n",
      ">> Epoch 206 finished \tANN training loss 0.300271\n",
      ">> Epoch 207 finished \tANN training loss 0.305210\n",
      ">> Epoch 208 finished \tANN training loss 0.304619\n",
      ">> Epoch 209 finished \tANN training loss 0.305986\n",
      ">> Epoch 210 finished \tANN training loss 0.305365\n",
      ">> Epoch 211 finished \tANN training loss 0.307999\n",
      ">> Epoch 212 finished \tANN training loss 0.306731\n",
      ">> Epoch 213 finished \tANN training loss 0.313111\n",
      ">> Epoch 214 finished \tANN training loss 0.301875\n",
      ">> Epoch 215 finished \tANN training loss 0.307293\n",
      ">> Epoch 216 finished \tANN training loss 0.311581\n",
      ">> Epoch 217 finished \tANN training loss 0.313201\n",
      ">> Epoch 218 finished \tANN training loss 0.312244\n",
      ">> Epoch 219 finished \tANN training loss 0.316990\n",
      ">> Epoch 220 finished \tANN training loss 0.306512\n",
      ">> Epoch 221 finished \tANN training loss 0.304291\n",
      ">> Epoch 222 finished \tANN training loss 0.303316\n",
      ">> Epoch 223 finished \tANN training loss 0.303334\n",
      ">> Epoch 224 finished \tANN training loss 0.304020\n",
      ">> Epoch 225 finished \tANN training loss 0.310010\n",
      ">> Epoch 226 finished \tANN training loss 0.318608\n",
      ">> Epoch 227 finished \tANN training loss 0.313975\n",
      ">> Epoch 228 finished \tANN training loss 0.319376\n",
      ">> Epoch 229 finished \tANN training loss 0.307674\n",
      ">> Epoch 230 finished \tANN training loss 0.309658\n",
      ">> Epoch 231 finished \tANN training loss 0.304795\n",
      ">> Epoch 232 finished \tANN training loss 0.300116\n",
      ">> Epoch 233 finished \tANN training loss 0.295562\n",
      ">> Epoch 234 finished \tANN training loss 0.296522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 235 finished \tANN training loss 0.302390\n",
      ">> Epoch 236 finished \tANN training loss 0.301779\n",
      ">> Epoch 237 finished \tANN training loss 0.304661\n",
      ">> Epoch 238 finished \tANN training loss 0.304723\n",
      ">> Epoch 239 finished \tANN training loss 0.306844\n",
      ">> Epoch 240 finished \tANN training loss 0.305343\n",
      ">> Epoch 241 finished \tANN training loss 0.311711\n",
      ">> Epoch 242 finished \tANN training loss 0.314745\n",
      ">> Epoch 243 finished \tANN training loss 0.303030\n",
      ">> Epoch 244 finished \tANN training loss 0.305278\n",
      ">> Epoch 245 finished \tANN training loss 0.306601\n",
      ">> Epoch 246 finished \tANN training loss 0.312418\n",
      ">> Epoch 247 finished \tANN training loss 0.311736\n",
      ">> Epoch 248 finished \tANN training loss 0.310522\n",
      ">> Epoch 249 finished \tANN training loss 0.307960\n",
      ">> Epoch 250 finished \tANN training loss 0.309610\n",
      ">> Epoch 251 finished \tANN training loss 0.305273\n",
      ">> Epoch 252 finished \tANN training loss 0.308013\n",
      ">> Epoch 253 finished \tANN training loss 0.303442\n",
      ">> Epoch 254 finished \tANN training loss 0.308846\n",
      ">> Epoch 255 finished \tANN training loss 0.313136\n",
      ">> Epoch 256 finished \tANN training loss 0.309956\n",
      ">> Epoch 257 finished \tANN training loss 0.311065\n",
      ">> Epoch 258 finished \tANN training loss 0.306291\n",
      ">> Epoch 259 finished \tANN training loss 0.313743\n",
      ">> Epoch 260 finished \tANN training loss 0.304793\n",
      ">> Epoch 261 finished \tANN training loss 0.299127\n",
      ">> Epoch 262 finished \tANN training loss 0.299888\n",
      ">> Epoch 263 finished \tANN training loss 0.302240\n",
      ">> Epoch 264 finished \tANN training loss 0.304123\n",
      ">> Epoch 265 finished \tANN training loss 0.309813\n",
      ">> Epoch 266 finished \tANN training loss 0.311452\n",
      ">> Epoch 267 finished \tANN training loss 0.309877\n",
      ">> Epoch 268 finished \tANN training loss 0.310962\n",
      ">> Epoch 269 finished \tANN training loss 0.313628\n",
      ">> Epoch 270 finished \tANN training loss 0.313670\n",
      ">> Epoch 271 finished \tANN training loss 0.313084\n",
      ">> Epoch 272 finished \tANN training loss 0.315703\n",
      ">> Epoch 273 finished \tANN training loss 0.316854\n",
      ">> Epoch 274 finished \tANN training loss 0.323670\n",
      ">> Epoch 275 finished \tANN training loss 0.310359\n",
      ">> Epoch 276 finished \tANN training loss 0.312273\n",
      ">> Epoch 277 finished \tANN training loss 0.312838\n",
      ">> Epoch 278 finished \tANN training loss 0.312450\n",
      ">> Epoch 279 finished \tANN training loss 0.307866\n",
      ">> Epoch 280 finished \tANN training loss 0.310731\n",
      ">> Epoch 281 finished \tANN training loss 0.310326\n",
      ">> Epoch 282 finished \tANN training loss 0.315307\n",
      ">> Epoch 283 finished \tANN training loss 0.312893\n",
      ">> Epoch 284 finished \tANN training loss 0.310801\n",
      ">> Epoch 285 finished \tANN training loss 0.307838\n",
      ">> Epoch 286 finished \tANN training loss 0.315933\n",
      ">> Epoch 287 finished \tANN training loss 0.311106\n",
      ">> Epoch 288 finished \tANN training loss 0.311799\n",
      ">> Epoch 289 finished \tANN training loss 0.305311\n",
      ">> Epoch 290 finished \tANN training loss 0.303645\n",
      ">> Epoch 291 finished \tANN training loss 0.312000\n",
      ">> Epoch 292 finished \tANN training loss 0.309015\n",
      ">> Epoch 293 finished \tANN training loss 0.313764\n",
      ">> Epoch 294 finished \tANN training loss 0.307939\n",
      ">> Epoch 295 finished \tANN training loss 0.304249\n",
      ">> Epoch 296 finished \tANN training loss 0.303835\n",
      ">> Epoch 297 finished \tANN training loss 0.305018\n",
      ">> Epoch 298 finished \tANN training loss 0.307004\n",
      ">> Epoch 299 finished \tANN training loss 0.308262\n",
      ">> Epoch 300 finished \tANN training loss 0.312778\n",
      ">> Epoch 301 finished \tANN training loss 0.316835\n",
      ">> Epoch 302 finished \tANN training loss 0.313574\n",
      ">> Epoch 303 finished \tANN training loss 0.313750\n",
      ">> Epoch 304 finished \tANN training loss 0.309693\n",
      ">> Epoch 305 finished \tANN training loss 0.309601\n",
      ">> Epoch 306 finished \tANN training loss 0.316618\n",
      ">> Epoch 307 finished \tANN training loss 0.315661\n",
      ">> Epoch 308 finished \tANN training loss 0.311741\n",
      ">> Epoch 309 finished \tANN training loss 0.316359\n",
      ">> Epoch 310 finished \tANN training loss 0.312768\n",
      ">> Epoch 311 finished \tANN training loss 0.306274\n",
      ">> Epoch 312 finished \tANN training loss 0.305101\n",
      ">> Epoch 313 finished \tANN training loss 0.311128\n",
      ">> Epoch 314 finished \tANN training loss 0.314501\n",
      ">> Epoch 315 finished \tANN training loss 0.319045\n",
      ">> Epoch 316 finished \tANN training loss 0.315864\n",
      ">> Epoch 317 finished \tANN training loss 0.312973\n",
      ">> Epoch 318 finished \tANN training loss 0.313281\n",
      ">> Epoch 319 finished \tANN training loss 0.312716\n",
      ">> Epoch 320 finished \tANN training loss 0.309244\n",
      ">> Epoch 321 finished \tANN training loss 0.308758\n",
      ">> Epoch 322 finished \tANN training loss 0.320141\n",
      ">> Epoch 323 finished \tANN training loss 0.315356\n",
      ">> Epoch 324 finished \tANN training loss 0.309743\n",
      ">> Epoch 325 finished \tANN training loss 0.307790\n",
      ">> Epoch 326 finished \tANN training loss 0.304272\n",
      ">> Epoch 327 finished \tANN training loss 0.306571\n",
      ">> Epoch 328 finished \tANN training loss 0.309785\n",
      ">> Epoch 329 finished \tANN training loss 0.307692\n",
      ">> Epoch 330 finished \tANN training loss 0.314296\n",
      ">> Epoch 331 finished \tANN training loss 0.319611\n",
      ">> Epoch 332 finished \tANN training loss 0.322821\n",
      ">> Epoch 333 finished \tANN training loss 0.317196\n",
      ">> Epoch 334 finished \tANN training loss 0.314961\n",
      ">> Epoch 335 finished \tANN training loss 0.314185\n",
      ">> Epoch 336 finished \tANN training loss 0.321839\n",
      ">> Epoch 337 finished \tANN training loss 0.311232\n",
      ">> Epoch 338 finished \tANN training loss 0.312315\n",
      ">> Epoch 339 finished \tANN training loss 0.310872\n",
      ">> Epoch 340 finished \tANN training loss 0.316995\n",
      ">> Epoch 341 finished \tANN training loss 0.309708\n",
      ">> Epoch 342 finished \tANN training loss 0.309109\n",
      ">> Epoch 343 finished \tANN training loss 0.309107\n",
      ">> Epoch 344 finished \tANN training loss 0.306430\n",
      ">> Epoch 345 finished \tANN training loss 0.308548\n",
      ">> Epoch 346 finished \tANN training loss 0.309316\n",
      ">> Epoch 347 finished \tANN training loss 0.308593\n",
      ">> Epoch 348 finished \tANN training loss 0.305923\n",
      ">> Epoch 349 finished \tANN training loss 0.306506\n",
      ">> Epoch 350 finished \tANN training loss 0.306209\n",
      ">> Epoch 351 finished \tANN training loss 0.306629\n",
      ">> Epoch 352 finished \tANN training loss 0.319702\n",
      ">> Epoch 353 finished \tANN training loss 0.305836\n",
      ">> Epoch 354 finished \tANN training loss 0.313350\n",
      ">> Epoch 355 finished \tANN training loss 0.309526\n",
      ">> Epoch 356 finished \tANN training loss 0.313717\n",
      ">> Epoch 357 finished \tANN training loss 0.329123\n",
      ">> Epoch 358 finished \tANN training loss 0.312237\n",
      ">> Epoch 359 finished \tANN training loss 0.310367\n",
      ">> Epoch 360 finished \tANN training loss 0.312422\n",
      ">> Epoch 361 finished \tANN training loss 0.305506\n",
      ">> Epoch 362 finished \tANN training loss 0.302618\n",
      ">> Epoch 363 finished \tANN training loss 0.301522\n",
      ">> Epoch 364 finished \tANN training loss 0.301248\n",
      ">> Epoch 365 finished \tANN training loss 0.305320\n",
      ">> Epoch 366 finished \tANN training loss 0.306399\n",
      ">> Epoch 367 finished \tANN training loss 0.307348\n",
      ">> Epoch 368 finished \tANN training loss 0.314087\n",
      ">> Epoch 369 finished \tANN training loss 0.303884\n",
      ">> Epoch 370 finished \tANN training loss 0.314783\n",
      ">> Epoch 371 finished \tANN training loss 0.311899\n",
      ">> Epoch 372 finished \tANN training loss 0.308013\n",
      ">> Epoch 373 finished \tANN training loss 0.303426\n",
      ">> Epoch 374 finished \tANN training loss 0.304177\n",
      ">> Epoch 375 finished \tANN training loss 0.313991\n",
      ">> Epoch 376 finished \tANN training loss 0.315583\n",
      ">> Epoch 377 finished \tANN training loss 0.327873\n",
      ">> Epoch 378 finished \tANN training loss 0.312847\n",
      ">> Epoch 379 finished \tANN training loss 0.310637\n",
      ">> Epoch 380 finished \tANN training loss 0.304509\n",
      ">> Epoch 381 finished \tANN training loss 0.303271\n",
      ">> Epoch 382 finished \tANN training loss 0.301034\n",
      ">> Epoch 383 finished \tANN training loss 0.306940\n",
      ">> Epoch 384 finished \tANN training loss 0.309185\n",
      ">> Epoch 385 finished \tANN training loss 0.310464\n",
      ">> Epoch 386 finished \tANN training loss 0.313638\n",
      ">> Epoch 387 finished \tANN training loss 0.317281\n",
      ">> Epoch 388 finished \tANN training loss 0.307132\n",
      ">> Epoch 389 finished \tANN training loss 0.308321\n",
      ">> Epoch 390 finished \tANN training loss 0.310805\n",
      ">> Epoch 391 finished \tANN training loss 0.305892\n",
      ">> Epoch 392 finished \tANN training loss 0.315585\n",
      ">> Epoch 393 finished \tANN training loss 0.312155\n",
      ">> Epoch 394 finished \tANN training loss 0.304503\n",
      ">> Epoch 395 finished \tANN training loss 0.306464\n",
      ">> Epoch 396 finished \tANN training loss 0.310225\n",
      ">> Epoch 397 finished \tANN training loss 0.312208\n",
      ">> Epoch 398 finished \tANN training loss 0.311945\n",
      ">> Epoch 399 finished \tANN training loss 0.316678\n",
      ">> Epoch 400 finished \tANN training loss 0.314082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 401 finished \tANN training loss 0.318734\n",
      ">> Epoch 402 finished \tANN training loss 0.315046\n",
      ">> Epoch 403 finished \tANN training loss 0.312753\n",
      ">> Epoch 404 finished \tANN training loss 0.309650\n",
      ">> Epoch 405 finished \tANN training loss 0.314457\n",
      ">> Epoch 406 finished \tANN training loss 0.321774\n",
      ">> Epoch 407 finished \tANN training loss 0.317735\n",
      ">> Epoch 408 finished \tANN training loss 0.313321\n",
      ">> Epoch 409 finished \tANN training loss 0.315393\n",
      ">> Epoch 410 finished \tANN training loss 0.313850\n",
      ">> Epoch 411 finished \tANN training loss 0.311015\n",
      ">> Epoch 412 finished \tANN training loss 0.316734\n",
      ">> Epoch 413 finished \tANN training loss 0.316977\n",
      ">> Epoch 414 finished \tANN training loss 0.313910\n",
      ">> Epoch 415 finished \tANN training loss 0.311830\n",
      ">> Epoch 416 finished \tANN training loss 0.308978\n",
      ">> Epoch 417 finished \tANN training loss 0.311407\n",
      ">> Epoch 418 finished \tANN training loss 0.317161\n",
      ">> Epoch 419 finished \tANN training loss 0.310381\n",
      ">> Epoch 420 finished \tANN training loss 0.310674\n",
      ">> Epoch 421 finished \tANN training loss 0.312292\n",
      ">> Epoch 422 finished \tANN training loss 0.311372\n",
      ">> Epoch 423 finished \tANN training loss 0.313654\n",
      ">> Epoch 424 finished \tANN training loss 0.312960\n",
      ">> Epoch 425 finished \tANN training loss 0.313518\n",
      ">> Epoch 426 finished \tANN training loss 0.313653\n",
      ">> Epoch 427 finished \tANN training loss 0.314148\n",
      ">> Epoch 428 finished \tANN training loss 0.308356\n",
      ">> Epoch 429 finished \tANN training loss 0.311672\n",
      ">> Epoch 430 finished \tANN training loss 0.314619\n",
      ">> Epoch 431 finished \tANN training loss 0.310468\n",
      ">> Epoch 432 finished \tANN training loss 0.308283\n",
      ">> Epoch 433 finished \tANN training loss 0.309790\n",
      ">> Epoch 434 finished \tANN training loss 0.312862\n",
      ">> Epoch 435 finished \tANN training loss 0.309462\n",
      ">> Epoch 436 finished \tANN training loss 0.305432\n",
      ">> Epoch 437 finished \tANN training loss 0.307140\n",
      ">> Epoch 438 finished \tANN training loss 0.306521\n",
      ">> Epoch 439 finished \tANN training loss 0.309638\n",
      ">> Epoch 440 finished \tANN training loss 0.318625\n",
      ">> Epoch 441 finished \tANN training loss 0.313760\n",
      ">> Epoch 442 finished \tANN training loss 0.319105\n",
      ">> Epoch 443 finished \tANN training loss 0.313693\n",
      ">> Epoch 444 finished \tANN training loss 0.313049\n",
      ">> Epoch 445 finished \tANN training loss 0.310875\n",
      ">> Epoch 446 finished \tANN training loss 0.315484\n",
      ">> Epoch 447 finished \tANN training loss 0.315101\n",
      ">> Epoch 448 finished \tANN training loss 0.311137\n",
      ">> Epoch 449 finished \tANN training loss 0.315025\n",
      ">> Epoch 450 finished \tANN training loss 0.312170\n",
      ">> Epoch 451 finished \tANN training loss 0.320390\n",
      ">> Epoch 452 finished \tANN training loss 0.309216\n",
      ">> Epoch 453 finished \tANN training loss 0.315572\n",
      ">> Epoch 454 finished \tANN training loss 0.311410\n",
      ">> Epoch 455 finished \tANN training loss 0.313978\n",
      ">> Epoch 456 finished \tANN training loss 0.313827\n",
      ">> Epoch 457 finished \tANN training loss 0.306617\n",
      ">> Epoch 458 finished \tANN training loss 0.319129\n",
      ">> Epoch 459 finished \tANN training loss 0.310487\n",
      ">> Epoch 460 finished \tANN training loss 0.311887\n",
      ">> Epoch 461 finished \tANN training loss 0.335282\n",
      ">> Epoch 462 finished \tANN training loss 0.322798\n",
      ">> Epoch 463 finished \tANN training loss 0.313165\n",
      ">> Epoch 464 finished \tANN training loss 0.314123\n",
      ">> Epoch 465 finished \tANN training loss 0.312454\n",
      ">> Epoch 466 finished \tANN training loss 0.321488\n",
      ">> Epoch 467 finished \tANN training loss 0.314057\n",
      ">> Epoch 468 finished \tANN training loss 0.312137\n",
      ">> Epoch 469 finished \tANN training loss 0.305753\n",
      ">> Epoch 470 finished \tANN training loss 0.304602\n",
      ">> Epoch 471 finished \tANN training loss 0.308620\n",
      ">> Epoch 472 finished \tANN training loss 0.304631\n",
      ">> Epoch 473 finished \tANN training loss 0.314966\n",
      ">> Epoch 474 finished \tANN training loss 0.311455\n",
      ">> Epoch 475 finished \tANN training loss 0.309304\n",
      ">> Epoch 476 finished \tANN training loss 0.310176\n",
      ">> Epoch 477 finished \tANN training loss 0.309835\n",
      ">> Epoch 478 finished \tANN training loss 0.314914\n",
      ">> Epoch 479 finished \tANN training loss 0.307881\n",
      ">> Epoch 480 finished \tANN training loss 0.308356\n",
      ">> Epoch 481 finished \tANN training loss 0.307090\n",
      ">> Epoch 482 finished \tANN training loss 0.308982\n",
      ">> Epoch 483 finished \tANN training loss 0.299762\n",
      ">> Epoch 484 finished \tANN training loss 0.295752\n",
      ">> Epoch 485 finished \tANN training loss 0.299331\n",
      ">> Epoch 486 finished \tANN training loss 0.299817\n",
      ">> Epoch 487 finished \tANN training loss 0.303648\n",
      ">> Epoch 488 finished \tANN training loss 0.305650\n",
      ">> Epoch 489 finished \tANN training loss 0.301980\n",
      ">> Epoch 490 finished \tANN training loss 0.302862\n",
      ">> Epoch 491 finished \tANN training loss 0.304319\n",
      ">> Epoch 492 finished \tANN training loss 0.296998\n",
      ">> Epoch 493 finished \tANN training loss 0.293619\n",
      ">> Epoch 494 finished \tANN training loss 0.293963\n",
      ">> Epoch 495 finished \tANN training loss 0.301202\n",
      ">> Epoch 496 finished \tANN training loss 0.305908\n",
      ">> Epoch 497 finished \tANN training loss 0.302066\n",
      ">> Epoch 498 finished \tANN training loss 0.298188\n",
      ">> Epoch 499 finished \tANN training loss 0.302435\n",
      ">> Epoch 500 finished \tANN training loss 0.306652\n",
      ">> Epoch 501 finished \tANN training loss 0.299201\n",
      ">> Epoch 502 finished \tANN training loss 0.304903\n",
      ">> Epoch 503 finished \tANN training loss 0.307121\n",
      ">> Epoch 504 finished \tANN training loss 0.308616\n",
      ">> Epoch 505 finished \tANN training loss 0.300887\n",
      ">> Epoch 506 finished \tANN training loss 0.305996\n",
      ">> Epoch 507 finished \tANN training loss 0.303978\n",
      ">> Epoch 508 finished \tANN training loss 0.304263\n",
      ">> Epoch 509 finished \tANN training loss 0.300463\n",
      ">> Epoch 510 finished \tANN training loss 0.297234\n",
      ">> Epoch 511 finished \tANN training loss 0.299572\n",
      ">> Epoch 512 finished \tANN training loss 0.305231\n",
      ">> Epoch 513 finished \tANN training loss 0.305852\n",
      ">> Epoch 514 finished \tANN training loss 0.303424\n",
      ">> Epoch 515 finished \tANN training loss 0.317915\n",
      ">> Epoch 516 finished \tANN training loss 0.309532\n",
      ">> Epoch 517 finished \tANN training loss 0.304432\n",
      ">> Epoch 518 finished \tANN training loss 0.298038\n",
      ">> Epoch 519 finished \tANN training loss 0.305170\n",
      ">> Epoch 520 finished \tANN training loss 0.298697\n",
      ">> Epoch 521 finished \tANN training loss 0.301720\n",
      ">> Epoch 522 finished \tANN training loss 0.297202\n",
      ">> Epoch 523 finished \tANN training loss 0.301310\n",
      ">> Epoch 524 finished \tANN training loss 0.299043\n",
      ">> Epoch 525 finished \tANN training loss 0.301947\n",
      ">> Epoch 526 finished \tANN training loss 0.306013\n",
      ">> Epoch 527 finished \tANN training loss 0.306967\n",
      ">> Epoch 528 finished \tANN training loss 0.309940\n",
      ">> Epoch 529 finished \tANN training loss 0.312462\n",
      ">> Epoch 530 finished \tANN training loss 0.308698\n",
      ">> Epoch 531 finished \tANN training loss 0.303946\n",
      ">> Epoch 532 finished \tANN training loss 0.302573\n",
      ">> Epoch 533 finished \tANN training loss 0.307841\n",
      ">> Epoch 534 finished \tANN training loss 0.307361\n",
      ">> Epoch 535 finished \tANN training loss 0.303393\n",
      ">> Epoch 536 finished \tANN training loss 0.308757\n",
      ">> Epoch 537 finished \tANN training loss 0.316069\n",
      ">> Epoch 538 finished \tANN training loss 0.309817\n",
      ">> Epoch 539 finished \tANN training loss 0.307783\n",
      ">> Epoch 540 finished \tANN training loss 0.305550\n",
      ">> Epoch 541 finished \tANN training loss 0.305849\n",
      ">> Epoch 542 finished \tANN training loss 0.304288\n",
      ">> Epoch 543 finished \tANN training loss 0.305123\n",
      ">> Epoch 544 finished \tANN training loss 0.307635\n",
      ">> Epoch 545 finished \tANN training loss 0.306671\n",
      ">> Epoch 546 finished \tANN training loss 0.308551\n",
      ">> Epoch 547 finished \tANN training loss 0.310717\n",
      ">> Epoch 548 finished \tANN training loss 0.316702\n",
      ">> Epoch 549 finished \tANN training loss 0.311780\n",
      ">> Epoch 550 finished \tANN training loss 0.308859\n",
      ">> Epoch 551 finished \tANN training loss 0.306136\n",
      ">> Epoch 552 finished \tANN training loss 0.308818\n",
      ">> Epoch 553 finished \tANN training loss 0.306326\n",
      ">> Epoch 554 finished \tANN training loss 0.307812\n",
      ">> Epoch 555 finished \tANN training loss 0.308300\n",
      ">> Epoch 556 finished \tANN training loss 0.301006\n",
      ">> Epoch 557 finished \tANN training loss 0.302080\n",
      ">> Epoch 558 finished \tANN training loss 0.300605\n",
      ">> Epoch 559 finished \tANN training loss 0.299766\n",
      ">> Epoch 560 finished \tANN training loss 0.303885\n",
      ">> Epoch 561 finished \tANN training loss 0.304951\n",
      ">> Epoch 562 finished \tANN training loss 0.302628\n",
      ">> Epoch 563 finished \tANN training loss 0.301866\n",
      ">> Epoch 564 finished \tANN training loss 0.308788\n",
      ">> Epoch 565 finished \tANN training loss 0.306775\n",
      ">> Epoch 566 finished \tANN training loss 0.305520\n",
      ">> Epoch 567 finished \tANN training loss 0.309966\n",
      ">> Epoch 568 finished \tANN training loss 0.306270\n",
      ">> Epoch 569 finished \tANN training loss 0.306151\n",
      ">> Epoch 570 finished \tANN training loss 0.303319\n",
      ">> Epoch 571 finished \tANN training loss 0.301534\n",
      ">> Epoch 572 finished \tANN training loss 0.310292\n",
      ">> Epoch 573 finished \tANN training loss 0.303549\n",
      ">> Epoch 574 finished \tANN training loss 0.307745\n",
      ">> Epoch 575 finished \tANN training loss 0.304755\n",
      ">> Epoch 576 finished \tANN training loss 0.304972\n",
      ">> Epoch 577 finished \tANN training loss 0.304267\n",
      ">> Epoch 578 finished \tANN training loss 0.302445\n",
      ">> Epoch 579 finished \tANN training loss 0.305201\n",
      ">> Epoch 580 finished \tANN training loss 0.310294\n",
      ">> Epoch 581 finished \tANN training loss 0.307770\n",
      ">> Epoch 582 finished \tANN training loss 0.305407\n",
      ">> Epoch 583 finished \tANN training loss 0.306824\n",
      ">> Epoch 584 finished \tANN training loss 0.312683\n",
      ">> Epoch 585 finished \tANN training loss 0.300565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 586 finished \tANN training loss 0.300794\n",
      ">> Epoch 587 finished \tANN training loss 0.303470\n",
      ">> Epoch 588 finished \tANN training loss 0.304154\n",
      ">> Epoch 589 finished \tANN training loss 0.302166\n",
      ">> Epoch 590 finished \tANN training loss 0.303204\n",
      ">> Epoch 591 finished \tANN training loss 0.306207\n",
      ">> Epoch 592 finished \tANN training loss 0.305113\n",
      ">> Epoch 593 finished \tANN training loss 0.301540\n",
      ">> Epoch 594 finished \tANN training loss 0.311708\n",
      ">> Epoch 595 finished \tANN training loss 0.306369\n",
      ">> Epoch 596 finished \tANN training loss 0.300615\n",
      ">> Epoch 597 finished \tANN training loss 0.301084\n",
      ">> Epoch 598 finished \tANN training loss 0.301013\n",
      ">> Epoch 599 finished \tANN training loss 0.298151\n",
      ">> Epoch 600 finished \tANN training loss 0.310249\n",
      ">> Epoch 601 finished \tANN training loss 0.303796\n",
      ">> Epoch 602 finished \tANN training loss 0.299487\n",
      ">> Epoch 603 finished \tANN training loss 0.293134\n",
      ">> Epoch 604 finished \tANN training loss 0.298590\n",
      ">> Epoch 605 finished \tANN training loss 0.303149\n",
      ">> Epoch 606 finished \tANN training loss 0.309319\n",
      ">> Epoch 607 finished \tANN training loss 0.307096\n",
      ">> Epoch 608 finished \tANN training loss 0.307870\n",
      ">> Epoch 609 finished \tANN training loss 0.303554\n",
      ">> Epoch 610 finished \tANN training loss 0.305581\n",
      ">> Epoch 611 finished \tANN training loss 0.309405\n",
      ">> Epoch 612 finished \tANN training loss 0.310599\n",
      ">> Epoch 613 finished \tANN training loss 0.311040\n",
      ">> Epoch 614 finished \tANN training loss 0.307675\n",
      ">> Epoch 615 finished \tANN training loss 0.309044\n",
      ">> Epoch 616 finished \tANN training loss 0.311724\n",
      ">> Epoch 617 finished \tANN training loss 0.311121\n",
      ">> Epoch 618 finished \tANN training loss 0.309351\n",
      ">> Epoch 619 finished \tANN training loss 0.305259\n",
      ">> Epoch 620 finished \tANN training loss 0.311767\n",
      ">> Epoch 621 finished \tANN training loss 0.310938\n",
      ">> Epoch 622 finished \tANN training loss 0.307612\n",
      ">> Epoch 623 finished \tANN training loss 0.316183\n",
      ">> Epoch 624 finished \tANN training loss 0.306631\n",
      ">> Epoch 625 finished \tANN training loss 0.304588\n",
      ">> Epoch 626 finished \tANN training loss 0.310722\n",
      ">> Epoch 627 finished \tANN training loss 0.313919\n",
      ">> Epoch 628 finished \tANN training loss 0.320975\n",
      ">> Epoch 629 finished \tANN training loss 0.310668\n",
      ">> Epoch 630 finished \tANN training loss 0.303099\n",
      ">> Epoch 631 finished \tANN training loss 0.307974\n",
      ">> Epoch 632 finished \tANN training loss 0.310449\n",
      ">> Epoch 633 finished \tANN training loss 0.306146\n",
      ">> Epoch 634 finished \tANN training loss 0.307435\n",
      ">> Epoch 635 finished \tANN training loss 0.311126\n",
      ">> Epoch 636 finished \tANN training loss 0.311373\n",
      ">> Epoch 637 finished \tANN training loss 0.314348\n",
      ">> Epoch 638 finished \tANN training loss 0.319193\n",
      ">> Epoch 639 finished \tANN training loss 0.307183\n",
      ">> Epoch 640 finished \tANN training loss 0.314847\n",
      ">> Epoch 641 finished \tANN training loss 0.306752\n",
      ">> Epoch 642 finished \tANN training loss 0.307528\n",
      ">> Epoch 643 finished \tANN training loss 0.305664\n",
      ">> Epoch 644 finished \tANN training loss 0.299947\n",
      ">> Epoch 645 finished \tANN training loss 0.303023\n",
      ">> Epoch 646 finished \tANN training loss 0.303895\n",
      ">> Epoch 647 finished \tANN training loss 0.305966\n",
      ">> Epoch 648 finished \tANN training loss 0.305915\n",
      ">> Epoch 649 finished \tANN training loss 0.304103\n",
      ">> Epoch 650 finished \tANN training loss 0.306682\n",
      ">> Epoch 651 finished \tANN training loss 0.310426\n",
      ">> Epoch 652 finished \tANN training loss 0.308985\n",
      ">> Epoch 653 finished \tANN training loss 0.308926\n",
      ">> Epoch 654 finished \tANN training loss 0.306652\n",
      ">> Epoch 655 finished \tANN training loss 0.304288\n",
      ">> Epoch 656 finished \tANN training loss 0.305918\n",
      ">> Epoch 657 finished \tANN training loss 0.308093\n",
      ">> Epoch 658 finished \tANN training loss 0.308616\n",
      ">> Epoch 659 finished \tANN training loss 0.309579\n",
      ">> Epoch 660 finished \tANN training loss 0.311307\n",
      ">> Epoch 661 finished \tANN training loss 0.314777\n",
      ">> Epoch 662 finished \tANN training loss 0.312135\n",
      ">> Epoch 663 finished \tANN training loss 0.314521\n",
      ">> Epoch 664 finished \tANN training loss 0.313864\n",
      ">> Epoch 665 finished \tANN training loss 0.309220\n",
      ">> Epoch 666 finished \tANN training loss 0.310703\n",
      ">> Epoch 667 finished \tANN training loss 0.303041\n",
      ">> Epoch 668 finished \tANN training loss 0.304951\n",
      ">> Epoch 669 finished \tANN training loss 0.306481\n",
      ">> Epoch 670 finished \tANN training loss 0.302037\n",
      ">> Epoch 671 finished \tANN training loss 0.300963\n",
      ">> Epoch 672 finished \tANN training loss 0.311063\n",
      ">> Epoch 673 finished \tANN training loss 0.309076\n",
      ">> Epoch 674 finished \tANN training loss 0.302877\n",
      ">> Epoch 675 finished \tANN training loss 0.305540\n",
      ">> Epoch 676 finished \tANN training loss 0.302568\n",
      ">> Epoch 677 finished \tANN training loss 0.302147\n",
      ">> Epoch 678 finished \tANN training loss 0.304604\n",
      ">> Epoch 679 finished \tANN training loss 0.303627\n",
      ">> Epoch 680 finished \tANN training loss 0.301054\n",
      ">> Epoch 681 finished \tANN training loss 0.300874\n",
      ">> Epoch 682 finished \tANN training loss 0.296773\n",
      ">> Epoch 683 finished \tANN training loss 0.302934\n",
      ">> Epoch 684 finished \tANN training loss 0.300915\n",
      ">> Epoch 685 finished \tANN training loss 0.305988\n",
      ">> Epoch 686 finished \tANN training loss 0.299456\n",
      ">> Epoch 687 finished \tANN training loss 0.307993\n",
      ">> Epoch 688 finished \tANN training loss 0.309736\n",
      ">> Epoch 689 finished \tANN training loss 0.311067\n",
      ">> Epoch 690 finished \tANN training loss 0.305615\n",
      ">> Epoch 691 finished \tANN training loss 0.306242\n",
      ">> Epoch 692 finished \tANN training loss 0.311900\n",
      ">> Epoch 693 finished \tANN training loss 0.314818\n",
      ">> Epoch 694 finished \tANN training loss 0.303269\n",
      ">> Epoch 695 finished \tANN training loss 0.302628\n",
      ">> Epoch 696 finished \tANN training loss 0.303540\n",
      ">> Epoch 697 finished \tANN training loss 0.298735\n",
      ">> Epoch 698 finished \tANN training loss 0.303775\n",
      ">> Epoch 699 finished \tANN training loss 0.290370\n",
      ">> Epoch 700 finished \tANN training loss 0.296440\n",
      ">> Epoch 701 finished \tANN training loss 0.293190\n",
      ">> Epoch 702 finished \tANN training loss 0.297501\n",
      ">> Epoch 703 finished \tANN training loss 0.296018\n",
      ">> Epoch 704 finished \tANN training loss 0.298813\n",
      ">> Epoch 705 finished \tANN training loss 0.302039\n",
      ">> Epoch 706 finished \tANN training loss 0.298579\n",
      ">> Epoch 707 finished \tANN training loss 0.299511\n",
      ">> Epoch 708 finished \tANN training loss 0.298800\n",
      ">> Epoch 709 finished \tANN training loss 0.295312\n",
      ">> Epoch 710 finished \tANN training loss 0.296886\n",
      ">> Epoch 711 finished \tANN training loss 0.300057\n",
      ">> Epoch 712 finished \tANN training loss 0.306801\n",
      ">> Epoch 713 finished \tANN training loss 0.300844\n",
      ">> Epoch 714 finished \tANN training loss 0.302662\n",
      ">> Epoch 715 finished \tANN training loss 0.302626\n",
      ">> Epoch 716 finished \tANN training loss 0.306605\n",
      ">> Epoch 717 finished \tANN training loss 0.301573\n",
      ">> Epoch 718 finished \tANN training loss 0.299958\n",
      ">> Epoch 719 finished \tANN training loss 0.296399\n",
      ">> Epoch 720 finished \tANN training loss 0.298005\n",
      ">> Epoch 721 finished \tANN training loss 0.300432\n",
      ">> Epoch 722 finished \tANN training loss 0.300463\n",
      ">> Epoch 723 finished \tANN training loss 0.306126\n",
      ">> Epoch 724 finished \tANN training loss 0.310854\n",
      ">> Epoch 725 finished \tANN training loss 0.305572\n",
      ">> Epoch 726 finished \tANN training loss 0.303852\n",
      ">> Epoch 727 finished \tANN training loss 0.304512\n",
      ">> Epoch 728 finished \tANN training loss 0.309989\n",
      ">> Epoch 729 finished \tANN training loss 0.305717\n",
      ">> Epoch 730 finished \tANN training loss 0.302573\n",
      ">> Epoch 731 finished \tANN training loss 0.305443\n",
      ">> Epoch 732 finished \tANN training loss 0.305637\n",
      ">> Epoch 733 finished \tANN training loss 0.302932\n",
      ">> Epoch 734 finished \tANN training loss 0.307037\n",
      ">> Epoch 735 finished \tANN training loss 0.310985\n",
      ">> Epoch 736 finished \tANN training loss 0.311635\n",
      ">> Epoch 737 finished \tANN training loss 0.310235\n",
      ">> Epoch 738 finished \tANN training loss 0.317559\n",
      ">> Epoch 739 finished \tANN training loss 0.314641\n",
      ">> Epoch 740 finished \tANN training loss 0.308069\n",
      ">> Epoch 741 finished \tANN training loss 0.306531\n",
      ">> Epoch 742 finished \tANN training loss 0.301470\n",
      ">> Epoch 743 finished \tANN training loss 0.302176\n",
      ">> Epoch 744 finished \tANN training loss 0.307201\n",
      ">> Epoch 745 finished \tANN training loss 0.306405\n",
      ">> Epoch 746 finished \tANN training loss 0.300277\n",
      ">> Epoch 747 finished \tANN training loss 0.305980\n",
      ">> Epoch 748 finished \tANN training loss 0.301591\n",
      ">> Epoch 749 finished \tANN training loss 0.297829\n",
      ">> Epoch 750 finished \tANN training loss 0.300591\n",
      ">> Epoch 751 finished \tANN training loss 0.297543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 752 finished \tANN training loss 0.300125\n",
      ">> Epoch 753 finished \tANN training loss 0.299921\n",
      ">> Epoch 754 finished \tANN training loss 0.299016\n",
      ">> Epoch 755 finished \tANN training loss 0.301305\n",
      ">> Epoch 756 finished \tANN training loss 0.296286\n",
      ">> Epoch 757 finished \tANN training loss 0.297416\n",
      ">> Epoch 758 finished \tANN training loss 0.299792\n",
      ">> Epoch 759 finished \tANN training loss 0.302891\n",
      ">> Epoch 760 finished \tANN training loss 0.299810\n",
      ">> Epoch 761 finished \tANN training loss 0.302883\n",
      ">> Epoch 762 finished \tANN training loss 0.297486\n",
      ">> Epoch 763 finished \tANN training loss 0.304871\n",
      ">> Epoch 764 finished \tANN training loss 0.306411\n",
      ">> Epoch 765 finished \tANN training loss 0.302500\n",
      ">> Epoch 766 finished \tANN training loss 0.300056\n",
      ">> Epoch 767 finished \tANN training loss 0.302992\n",
      ">> Epoch 768 finished \tANN training loss 0.302678\n",
      ">> Epoch 769 finished \tANN training loss 0.309999\n",
      ">> Epoch 770 finished \tANN training loss 0.298313\n",
      ">> Epoch 771 finished \tANN training loss 0.303465\n",
      ">> Epoch 772 finished \tANN training loss 0.299609\n",
      ">> Epoch 773 finished \tANN training loss 0.298066\n",
      ">> Epoch 774 finished \tANN training loss 0.300065\n",
      ">> Epoch 775 finished \tANN training loss 0.301365\n",
      ">> Epoch 776 finished \tANN training loss 0.307123\n",
      ">> Epoch 777 finished \tANN training loss 0.309900\n",
      ">> Epoch 778 finished \tANN training loss 0.305334\n",
      ">> Epoch 779 finished \tANN training loss 0.304489\n",
      ">> Epoch 780 finished \tANN training loss 0.305773\n",
      ">> Epoch 781 finished \tANN training loss 0.302209\n",
      ">> Epoch 782 finished \tANN training loss 0.308354\n",
      ">> Epoch 783 finished \tANN training loss 0.302388\n",
      ">> Epoch 784 finished \tANN training loss 0.301868\n",
      ">> Epoch 785 finished \tANN training loss 0.314369\n",
      ">> Epoch 786 finished \tANN training loss 0.317933\n",
      ">> Epoch 787 finished \tANN training loss 0.322625\n",
      ">> Epoch 788 finished \tANN training loss 0.309963\n",
      ">> Epoch 789 finished \tANN training loss 0.308980\n",
      ">> Epoch 790 finished \tANN training loss 0.322991\n",
      ">> Epoch 791 finished \tANN training loss 0.314277\n",
      ">> Epoch 792 finished \tANN training loss 0.316322\n",
      ">> Epoch 793 finished \tANN training loss 0.309103\n",
      ">> Epoch 794 finished \tANN training loss 0.312675\n",
      ">> Epoch 795 finished \tANN training loss 0.313071\n",
      ">> Epoch 796 finished \tANN training loss 0.309095\n",
      ">> Epoch 797 finished \tANN training loss 0.309680\n",
      ">> Epoch 798 finished \tANN training loss 0.312140\n",
      ">> Epoch 799 finished \tANN training loss 0.312048\n",
      ">> Epoch 800 finished \tANN training loss 0.310712\n",
      ">> Epoch 801 finished \tANN training loss 0.310182\n",
      ">> Epoch 802 finished \tANN training loss 0.312659\n",
      ">> Epoch 803 finished \tANN training loss 0.308535\n",
      ">> Epoch 804 finished \tANN training loss 0.310304\n",
      ">> Epoch 805 finished \tANN training loss 0.310026\n",
      ">> Epoch 806 finished \tANN training loss 0.317812\n",
      ">> Epoch 807 finished \tANN training loss 0.312750\n",
      ">> Epoch 808 finished \tANN training loss 0.311939\n",
      ">> Epoch 809 finished \tANN training loss 0.314030\n",
      ">> Epoch 810 finished \tANN training loss 0.309243\n",
      ">> Epoch 811 finished \tANN training loss 0.312009\n",
      ">> Epoch 812 finished \tANN training loss 0.311948\n",
      ">> Epoch 813 finished \tANN training loss 0.309430\n",
      ">> Epoch 814 finished \tANN training loss 0.311349\n",
      ">> Epoch 815 finished \tANN training loss 0.310756\n",
      ">> Epoch 816 finished \tANN training loss 0.311323\n",
      ">> Epoch 817 finished \tANN training loss 0.308001\n",
      ">> Epoch 818 finished \tANN training loss 0.310930\n",
      ">> Epoch 819 finished \tANN training loss 0.310451\n",
      ">> Epoch 820 finished \tANN training loss 0.304725\n",
      ">> Epoch 821 finished \tANN training loss 0.308400\n",
      ">> Epoch 822 finished \tANN training loss 0.305852\n",
      ">> Epoch 823 finished \tANN training loss 0.303798\n",
      ">> Epoch 824 finished \tANN training loss 0.302974\n",
      ">> Epoch 825 finished \tANN training loss 0.301760\n",
      ">> Epoch 826 finished \tANN training loss 0.318577\n",
      ">> Epoch 827 finished \tANN training loss 0.304582\n",
      ">> Epoch 828 finished \tANN training loss 0.306258\n",
      ">> Epoch 829 finished \tANN training loss 0.304389\n",
      ">> Epoch 830 finished \tANN training loss 0.305415\n",
      ">> Epoch 831 finished \tANN training loss 0.304900\n",
      ">> Epoch 832 finished \tANN training loss 0.309201\n",
      ">> Epoch 833 finished \tANN training loss 0.301652\n",
      ">> Epoch 834 finished \tANN training loss 0.309059\n",
      ">> Epoch 835 finished \tANN training loss 0.310918\n",
      ">> Epoch 836 finished \tANN training loss 0.310575\n",
      ">> Epoch 837 finished \tANN training loss 0.307206\n",
      ">> Epoch 838 finished \tANN training loss 0.316763\n",
      ">> Epoch 839 finished \tANN training loss 0.312627\n",
      ">> Epoch 840 finished \tANN training loss 0.322441\n",
      ">> Epoch 841 finished \tANN training loss 0.313393\n",
      ">> Epoch 842 finished \tANN training loss 0.317421\n",
      ">> Epoch 843 finished \tANN training loss 0.317014\n",
      ">> Epoch 844 finished \tANN training loss 0.313917\n",
      ">> Epoch 845 finished \tANN training loss 0.313575\n",
      ">> Epoch 846 finished \tANN training loss 0.307036\n",
      ">> Epoch 847 finished \tANN training loss 0.311181\n",
      ">> Epoch 848 finished \tANN training loss 0.313217\n",
      ">> Epoch 849 finished \tANN training loss 0.312254\n",
      ">> Epoch 850 finished \tANN training loss 0.312479\n",
      ">> Epoch 851 finished \tANN training loss 0.311757\n",
      ">> Epoch 852 finished \tANN training loss 0.309640\n",
      ">> Epoch 853 finished \tANN training loss 0.305373\n",
      ">> Epoch 854 finished \tANN training loss 0.308221\n",
      ">> Epoch 855 finished \tANN training loss 0.309444\n",
      ">> Epoch 856 finished \tANN training loss 0.310753\n",
      ">> Epoch 857 finished \tANN training loss 0.309650\n",
      ">> Epoch 858 finished \tANN training loss 0.306195\n",
      ">> Epoch 859 finished \tANN training loss 0.301401\n",
      ">> Epoch 860 finished \tANN training loss 0.302538\n",
      ">> Epoch 861 finished \tANN training loss 0.303585\n",
      ">> Epoch 862 finished \tANN training loss 0.304015\n",
      ">> Epoch 863 finished \tANN training loss 0.302466\n",
      ">> Epoch 864 finished \tANN training loss 0.307615\n",
      ">> Epoch 865 finished \tANN training loss 0.305047\n",
      ">> Epoch 866 finished \tANN training loss 0.300639\n",
      ">> Epoch 867 finished \tANN training loss 0.300281\n",
      ">> Epoch 868 finished \tANN training loss 0.297328\n",
      ">> Epoch 869 finished \tANN training loss 0.304157\n",
      ">> Epoch 870 finished \tANN training loss 0.305687\n",
      ">> Epoch 871 finished \tANN training loss 0.306134\n",
      ">> Epoch 872 finished \tANN training loss 0.302070\n",
      ">> Epoch 873 finished \tANN training loss 0.308276\n",
      ">> Epoch 874 finished \tANN training loss 0.300176\n",
      ">> Epoch 875 finished \tANN training loss 0.300239\n",
      ">> Epoch 876 finished \tANN training loss 0.296289\n",
      ">> Epoch 877 finished \tANN training loss 0.305814\n",
      ">> Epoch 878 finished \tANN training loss 0.298115\n",
      ">> Epoch 879 finished \tANN training loss 0.297323\n",
      ">> Epoch 880 finished \tANN training loss 0.297154\n",
      ">> Epoch 881 finished \tANN training loss 0.297114\n",
      ">> Epoch 882 finished \tANN training loss 0.298806\n",
      ">> Epoch 883 finished \tANN training loss 0.302060\n",
      ">> Epoch 884 finished \tANN training loss 0.299226\n",
      ">> Epoch 885 finished \tANN training loss 0.300401\n",
      ">> Epoch 886 finished \tANN training loss 0.304526\n",
      ">> Epoch 887 finished \tANN training loss 0.300425\n",
      ">> Epoch 888 finished \tANN training loss 0.306037\n",
      ">> Epoch 889 finished \tANN training loss 0.305261\n",
      ">> Epoch 890 finished \tANN training loss 0.300676\n",
      ">> Epoch 891 finished \tANN training loss 0.306629\n",
      ">> Epoch 892 finished \tANN training loss 0.311375\n",
      ">> Epoch 893 finished \tANN training loss 0.310894\n",
      ">> Epoch 894 finished \tANN training loss 0.296810\n",
      ">> Epoch 895 finished \tANN training loss 0.301674\n",
      ">> Epoch 896 finished \tANN training loss 0.297679\n",
      ">> Epoch 897 finished \tANN training loss 0.301491\n",
      ">> Epoch 898 finished \tANN training loss 0.300404\n",
      ">> Epoch 899 finished \tANN training loss 0.304543\n",
      ">> Epoch 900 finished \tANN training loss 0.308781\n",
      ">> Epoch 901 finished \tANN training loss 0.308157\n",
      ">> Epoch 902 finished \tANN training loss 0.305174\n",
      ">> Epoch 903 finished \tANN training loss 0.299233\n",
      ">> Epoch 904 finished \tANN training loss 0.301394\n",
      ">> Epoch 905 finished \tANN training loss 0.297390\n",
      ">> Epoch 906 finished \tANN training loss 0.293436\n",
      ">> Epoch 907 finished \tANN training loss 0.295806\n",
      ">> Epoch 908 finished \tANN training loss 0.296294\n",
      ">> Epoch 909 finished \tANN training loss 0.291511\n",
      ">> Epoch 910 finished \tANN training loss 0.304806\n",
      ">> Epoch 911 finished \tANN training loss 0.304256\n",
      ">> Epoch 912 finished \tANN training loss 0.304373\n",
      ">> Epoch 913 finished \tANN training loss 0.294910\n",
      ">> Epoch 914 finished \tANN training loss 0.298482\n",
      ">> Epoch 915 finished \tANN training loss 0.296152\n",
      ">> Epoch 916 finished \tANN training loss 0.302532\n",
      ">> Epoch 917 finished \tANN training loss 0.305372\n",
      ">> Epoch 918 finished \tANN training loss 0.307539\n",
      ">> Epoch 919 finished \tANN training loss 0.306036\n",
      ">> Epoch 920 finished \tANN training loss 0.311659\n",
      ">> Epoch 921 finished \tANN training loss 0.304386\n",
      ">> Epoch 922 finished \tANN training loss 0.304978\n",
      ">> Epoch 923 finished \tANN training loss 0.308528\n",
      ">> Epoch 924 finished \tANN training loss 0.309129\n",
      ">> Epoch 925 finished \tANN training loss 0.302741\n",
      ">> Epoch 926 finished \tANN training loss 0.306739\n",
      ">> Epoch 927 finished \tANN training loss 0.297516\n",
      ">> Epoch 928 finished \tANN training loss 0.293956\n",
      ">> Epoch 929 finished \tANN training loss 0.291713\n",
      ">> Epoch 930 finished \tANN training loss 0.290831\n",
      ">> Epoch 931 finished \tANN training loss 0.292195\n",
      ">> Epoch 932 finished \tANN training loss 0.291570\n",
      ">> Epoch 933 finished \tANN training loss 0.296764\n",
      ">> Epoch 934 finished \tANN training loss 0.296740\n",
      ">> Epoch 935 finished \tANN training loss 0.295996\n",
      ">> Epoch 936 finished \tANN training loss 0.296138\n",
      ">> Epoch 937 finished \tANN training loss 0.295663\n",
      ">> Epoch 938 finished \tANN training loss 0.304319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 939 finished \tANN training loss 0.307367\n",
      ">> Epoch 940 finished \tANN training loss 0.303504\n",
      ">> Epoch 941 finished \tANN training loss 0.307456\n",
      ">> Epoch 942 finished \tANN training loss 0.312344\n",
      ">> Epoch 943 finished \tANN training loss 0.312090\n",
      ">> Epoch 944 finished \tANN training loss 0.310717\n",
      ">> Epoch 945 finished \tANN training loss 0.308769\n",
      ">> Epoch 946 finished \tANN training loss 0.312551\n",
      ">> Epoch 947 finished \tANN training loss 0.311987\n",
      ">> Epoch 948 finished \tANN training loss 0.311314\n",
      ">> Epoch 949 finished \tANN training loss 0.310894\n",
      ">> Epoch 950 finished \tANN training loss 0.309225\n",
      ">> Epoch 951 finished \tANN training loss 0.304231\n",
      ">> Epoch 952 finished \tANN training loss 0.315171\n",
      ">> Epoch 953 finished \tANN training loss 0.301516\n",
      ">> Epoch 954 finished \tANN training loss 0.309126\n",
      ">> Epoch 955 finished \tANN training loss 0.302457\n",
      ">> Epoch 956 finished \tANN training loss 0.300816\n",
      ">> Epoch 957 finished \tANN training loss 0.302790\n",
      ">> Epoch 958 finished \tANN training loss 0.297218\n",
      ">> Epoch 959 finished \tANN training loss 0.296367\n",
      ">> Epoch 960 finished \tANN training loss 0.294980\n",
      ">> Epoch 961 finished \tANN training loss 0.297026\n",
      ">> Epoch 962 finished \tANN training loss 0.311044\n",
      ">> Epoch 963 finished \tANN training loss 0.313806\n",
      ">> Epoch 964 finished \tANN training loss 0.309569\n",
      ">> Epoch 965 finished \tANN training loss 0.306807\n",
      ">> Epoch 966 finished \tANN training loss 0.314505\n",
      ">> Epoch 967 finished \tANN training loss 0.319593\n",
      ">> Epoch 968 finished \tANN training loss 0.305114\n",
      ">> Epoch 969 finished \tANN training loss 0.301188\n",
      ">> Epoch 970 finished \tANN training loss 0.302704\n",
      ">> Epoch 971 finished \tANN training loss 0.299159\n",
      ">> Epoch 972 finished \tANN training loss 0.295884\n",
      ">> Epoch 973 finished \tANN training loss 0.297585\n",
      ">> Epoch 974 finished \tANN training loss 0.298060\n",
      ">> Epoch 975 finished \tANN training loss 0.299218\n",
      ">> Epoch 976 finished \tANN training loss 0.295585\n",
      ">> Epoch 977 finished \tANN training loss 0.293134\n",
      ">> Epoch 978 finished \tANN training loss 0.297689\n",
      ">> Epoch 979 finished \tANN training loss 0.298347\n",
      ">> Epoch 980 finished \tANN training loss 0.296710\n",
      ">> Epoch 981 finished \tANN training loss 0.302299\n",
      ">> Epoch 982 finished \tANN training loss 0.303955\n",
      ">> Epoch 983 finished \tANN training loss 0.299894\n",
      ">> Epoch 984 finished \tANN training loss 0.301764\n",
      ">> Epoch 985 finished \tANN training loss 0.305453\n",
      ">> Epoch 986 finished \tANN training loss 0.305283\n",
      ">> Epoch 987 finished \tANN training loss 0.309615\n",
      ">> Epoch 988 finished \tANN training loss 0.305995\n",
      ">> Epoch 989 finished \tANN training loss 0.306043\n",
      ">> Epoch 990 finished \tANN training loss 0.306857\n",
      ">> Epoch 991 finished \tANN training loss 0.304691\n",
      ">> Epoch 992 finished \tANN training loss 0.305764\n",
      ">> Epoch 993 finished \tANN training loss 0.300192\n",
      ">> Epoch 994 finished \tANN training loss 0.306591\n",
      ">> Epoch 995 finished \tANN training loss 0.307289\n",
      ">> Epoch 996 finished \tANN training loss 0.305804\n",
      ">> Epoch 997 finished \tANN training loss 0.301232\n",
      ">> Epoch 998 finished \tANN training loss 0.296455\n",
      ">> Epoch 999 finished \tANN training loss 0.306111\n",
      "[END] Fine tuning step\n",
      "\n",
      "\n",
      "TRIAL:  4\n",
      "\n",
      "\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 5.822144\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 4.899340\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 4.306309\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 4.650754\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 5.246866\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 6.264917\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 7.992496\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 10.358418\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 13.588826\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 20.946904\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 69.607262\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 810.319031\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 1638.048340\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 1986.101196\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 2223.496338\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 2353.130127\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 2495.108398\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 2299.166992\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 2335.366699\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 2408.639160\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.228893\n",
      ">> Epoch 1 finished \tANN training loss 1.733026\n",
      ">> Epoch 2 finished \tANN training loss 1.019412\n",
      ">> Epoch 3 finished \tANN training loss 0.674304\n",
      ">> Epoch 4 finished \tANN training loss 0.475304\n",
      ">> Epoch 5 finished \tANN training loss 0.428364\n",
      ">> Epoch 6 finished \tANN training loss 0.359364\n",
      ">> Epoch 7 finished \tANN training loss 0.507593\n",
      ">> Epoch 8 finished \tANN training loss 0.338496\n",
      ">> Epoch 9 finished \tANN training loss 0.413354\n",
      ">> Epoch 10 finished \tANN training loss 0.344051\n",
      ">> Epoch 11 finished \tANN training loss 0.582761\n",
      ">> Epoch 12 finished \tANN training loss 0.306558\n",
      ">> Epoch 13 finished \tANN training loss 0.212054\n",
      ">> Epoch 14 finished \tANN training loss 0.109093\n",
      ">> Epoch 15 finished \tANN training loss 0.097454\n",
      ">> Epoch 16 finished \tANN training loss 0.216821\n",
      ">> Epoch 17 finished \tANN training loss 0.276659\n",
      ">> Epoch 18 finished \tANN training loss 0.808464\n",
      ">> Epoch 19 finished \tANN training loss 0.590766\n",
      ">> Epoch 20 finished \tANN training loss 0.792313\n",
      ">> Epoch 21 finished \tANN training loss 0.452168\n",
      ">> Epoch 22 finished \tANN training loss 0.646270\n",
      ">> Epoch 23 finished \tANN training loss 1.140803\n",
      ">> Epoch 24 finished \tANN training loss 1.136724\n",
      ">> Epoch 25 finished \tANN training loss 1.072672\n",
      ">> Epoch 26 finished \tANN training loss 1.046717\n",
      ">> Epoch 27 finished \tANN training loss 1.039145\n",
      ">> Epoch 28 finished \tANN training loss 1.037039\n",
      ">> Epoch 29 finished \tANN training loss 1.036781\n",
      ">> Epoch 30 finished \tANN training loss 1.036799\n",
      ">> Epoch 31 finished \tANN training loss 1.036853\n",
      ">> Epoch 32 finished \tANN training loss 1.038839\n",
      ">> Epoch 33 finished \tANN training loss 1.037276\n",
      ">> Epoch 34 finished \tANN training loss 1.036793\n",
      ">> Epoch 35 finished \tANN training loss 1.036863\n",
      ">> Epoch 36 finished \tANN training loss 1.036740\n",
      ">> Epoch 37 finished \tANN training loss 1.036996\n",
      ">> Epoch 38 finished \tANN training loss 1.036770\n",
      ">> Epoch 39 finished \tANN training loss 1.036898\n",
      ">> Epoch 40 finished \tANN training loss 1.036697\n",
      ">> Epoch 41 finished \tANN training loss 1.037620\n",
      ">> Epoch 42 finished \tANN training loss 1.037476\n",
      ">> Epoch 43 finished \tANN training loss 1.036934\n",
      ">> Epoch 44 finished \tANN training loss 1.036589\n",
      ">> Epoch 45 finished \tANN training loss 1.036697\n",
      ">> Epoch 46 finished \tANN training loss 1.036647\n",
      ">> Epoch 47 finished \tANN training loss 1.036613\n",
      ">> Epoch 48 finished \tANN training loss 1.036225\n",
      ">> Epoch 49 finished \tANN training loss 1.036205\n",
      ">> Epoch 50 finished \tANN training loss 1.048007\n",
      ">> Epoch 51 finished \tANN training loss 1.042684\n",
      ">> Epoch 52 finished \tANN training loss 0.758969\n",
      ">> Epoch 53 finished \tANN training loss 0.634977\n",
      ">> Epoch 54 finished \tANN training loss 0.532893\n",
      ">> Epoch 55 finished \tANN training loss 0.772320\n",
      ">> Epoch 56 finished \tANN training loss 0.563193\n",
      ">> Epoch 57 finished \tANN training loss 0.676690\n",
      ">> Epoch 58 finished \tANN training loss 1.161642\n",
      ">> Epoch 59 finished \tANN training loss 0.401070\n",
      ">> Epoch 60 finished \tANN training loss 1.253583\n",
      ">> Epoch 61 finished \tANN training loss 1.127428\n",
      ">> Epoch 62 finished \tANN training loss 1.081833\n",
      ">> Epoch 63 finished \tANN training loss 1.054322\n",
      ">> Epoch 64 finished \tANN training loss 1.043909\n",
      ">> Epoch 65 finished \tANN training loss 1.041755\n",
      ">> Epoch 66 finished \tANN training loss 1.037956\n",
      ">> Epoch 67 finished \tANN training loss 1.036712\n",
      ">> Epoch 68 finished \tANN training loss 1.014811\n",
      ">> Epoch 69 finished \tANN training loss 0.630239\n",
      ">> Epoch 70 finished \tANN training loss 0.530508\n",
      ">> Epoch 71 finished \tANN training loss 1.113836\n",
      ">> Epoch 72 finished \tANN training loss 1.067217\n",
      ">> Epoch 73 finished \tANN training loss 1.048198\n",
      ">> Epoch 74 finished \tANN training loss 1.040236\n",
      ">> Epoch 75 finished \tANN training loss 1.037027\n",
      ">> Epoch 76 finished \tANN training loss 1.037768\n",
      ">> Epoch 77 finished \tANN training loss 1.036752\n",
      ">> Epoch 78 finished \tANN training loss 1.036868\n",
      ">> Epoch 79 finished \tANN training loss 1.036710\n",
      ">> Epoch 80 finished \tANN training loss 1.037007\n",
      ">> Epoch 81 finished \tANN training loss 1.036733\n",
      ">> Epoch 82 finished \tANN training loss 1.036692\n",
      ">> Epoch 83 finished \tANN training loss 1.037177\n",
      ">> Epoch 84 finished \tANN training loss 1.037308\n",
      ">> Epoch 85 finished \tANN training loss 1.040790\n",
      ">> Epoch 86 finished \tANN training loss 1.037795\n",
      ">> Epoch 87 finished \tANN training loss 1.038396\n",
      ">> Epoch 88 finished \tANN training loss 1.038789\n",
      ">> Epoch 89 finished \tANN training loss 1.038774\n",
      ">> Epoch 90 finished \tANN training loss 1.037624\n",
      ">> Epoch 91 finished \tANN training loss 1.036933\n",
      ">> Epoch 92 finished \tANN training loss 1.036750\n",
      ">> Epoch 93 finished \tANN training loss 1.036869\n",
      ">> Epoch 94 finished \tANN training loss 1.036875\n",
      ">> Epoch 95 finished \tANN training loss 1.036698\n",
      ">> Epoch 96 finished \tANN training loss 1.036909\n",
      ">> Epoch 97 finished \tANN training loss 1.036757\n",
      ">> Epoch 98 finished \tANN training loss 1.036948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 99 finished \tANN training loss 1.037094\n",
      ">> Epoch 100 finished \tANN training loss 1.036674\n",
      ">> Epoch 101 finished \tANN training loss 1.036702\n",
      ">> Epoch 102 finished \tANN training loss 1.036673\n",
      ">> Epoch 103 finished \tANN training loss 1.036727\n",
      ">> Epoch 104 finished \tANN training loss 1.036948\n",
      ">> Epoch 105 finished \tANN training loss 1.036734\n",
      ">> Epoch 106 finished \tANN training loss 1.036670\n",
      ">> Epoch 107 finished \tANN training loss 1.036703\n",
      ">> Epoch 108 finished \tANN training loss 1.036895\n",
      ">> Epoch 109 finished \tANN training loss 1.036804\n",
      ">> Epoch 110 finished \tANN training loss 1.037143\n",
      ">> Epoch 111 finished \tANN training loss 1.036668\n",
      ">> Epoch 112 finished \tANN training loss 1.036987\n",
      ">> Epoch 113 finished \tANN training loss 1.037499\n",
      ">> Epoch 114 finished \tANN training loss 1.036805\n",
      ">> Epoch 115 finished \tANN training loss 1.037089\n",
      ">> Epoch 116 finished \tANN training loss 1.036911\n",
      ">> Epoch 117 finished \tANN training loss 1.037773\n",
      ">> Epoch 118 finished \tANN training loss 1.036663\n",
      ">> Epoch 119 finished \tANN training loss 1.036852\n",
      ">> Epoch 120 finished \tANN training loss 1.037116\n",
      ">> Epoch 121 finished \tANN training loss 1.036752\n",
      ">> Epoch 122 finished \tANN training loss 1.036865\n",
      ">> Epoch 123 finished \tANN training loss 1.036664\n",
      ">> Epoch 124 finished \tANN training loss 1.036708\n",
      ">> Epoch 125 finished \tANN training loss 1.037086\n",
      ">> Epoch 126 finished \tANN training loss 1.038121\n",
      ">> Epoch 127 finished \tANN training loss 1.037546\n",
      ">> Epoch 128 finished \tANN training loss 1.036744\n",
      ">> Epoch 129 finished \tANN training loss 1.037763\n",
      ">> Epoch 130 finished \tANN training loss 1.036659\n",
      ">> Epoch 131 finished \tANN training loss 1.036816\n",
      ">> Epoch 132 finished \tANN training loss 1.036824\n",
      ">> Epoch 133 finished \tANN training loss 1.036739\n",
      ">> Epoch 134 finished \tANN training loss 1.036728\n",
      ">> Epoch 135 finished \tANN training loss 1.036661\n",
      ">> Epoch 136 finished \tANN training loss 1.037050\n",
      ">> Epoch 137 finished \tANN training loss 1.036659\n",
      ">> Epoch 138 finished \tANN training loss 1.036855\n",
      ">> Epoch 139 finished \tANN training loss 1.036653\n",
      ">> Epoch 140 finished \tANN training loss 1.036651\n",
      ">> Epoch 141 finished \tANN training loss 1.036692\n",
      ">> Epoch 142 finished \tANN training loss 1.037051\n",
      ">> Epoch 143 finished \tANN training loss 1.037048\n",
      ">> Epoch 144 finished \tANN training loss 1.036663\n",
      ">> Epoch 145 finished \tANN training loss 1.036657\n",
      ">> Epoch 146 finished \tANN training loss 1.038127\n",
      ">> Epoch 147 finished \tANN training loss 1.036933\n",
      ">> Epoch 148 finished \tANN training loss 1.036660\n",
      ">> Epoch 149 finished \tANN training loss 1.036734\n",
      ">> Epoch 150 finished \tANN training loss 1.036656\n",
      ">> Epoch 151 finished \tANN training loss 1.036671\n",
      ">> Epoch 152 finished \tANN training loss 1.037205\n",
      ">> Epoch 153 finished \tANN training loss 1.037180\n",
      ">> Epoch 154 finished \tANN training loss 1.036650\n",
      ">> Epoch 155 finished \tANN training loss 1.036818\n",
      ">> Epoch 156 finished \tANN training loss 1.039538\n",
      ">> Epoch 157 finished \tANN training loss 1.038417\n",
      ">> Epoch 158 finished \tANN training loss 1.036672\n",
      ">> Epoch 159 finished \tANN training loss 1.038262\n",
      ">> Epoch 160 finished \tANN training loss 1.036677\n",
      ">> Epoch 161 finished \tANN training loss 1.037778\n",
      ">> Epoch 162 finished \tANN training loss 1.037065\n",
      ">> Epoch 163 finished \tANN training loss 1.036752\n",
      ">> Epoch 164 finished \tANN training loss 1.036643\n",
      ">> Epoch 165 finished \tANN training loss 1.036815\n",
      ">> Epoch 166 finished \tANN training loss 1.037123\n",
      ">> Epoch 167 finished \tANN training loss 1.037885\n",
      ">> Epoch 168 finished \tANN training loss 1.039736\n",
      ">> Epoch 169 finished \tANN training loss 1.036633\n",
      ">> Epoch 170 finished \tANN training loss 1.036814\n",
      ">> Epoch 171 finished \tANN training loss 1.036634\n",
      ">> Epoch 172 finished \tANN training loss 1.036632\n",
      ">> Epoch 173 finished \tANN training loss 1.036679\n",
      ">> Epoch 174 finished \tANN training loss 1.036950\n",
      ">> Epoch 175 finished \tANN training loss 1.037031\n",
      ">> Epoch 176 finished \tANN training loss 1.036685\n",
      ">> Epoch 177 finished \tANN training loss 1.037152\n",
      ">> Epoch 178 finished \tANN training loss 1.036730\n",
      ">> Epoch 179 finished \tANN training loss 1.036974\n",
      ">> Epoch 180 finished \tANN training loss 1.036829\n",
      ">> Epoch 181 finished \tANN training loss 1.037219\n",
      ">> Epoch 182 finished \tANN training loss 1.036638\n",
      ">> Epoch 183 finished \tANN training loss 1.036624\n",
      ">> Epoch 184 finished \tANN training loss 1.036635\n",
      ">> Epoch 185 finished \tANN training loss 1.037274\n",
      ">> Epoch 186 finished \tANN training loss 1.036797\n",
      ">> Epoch 187 finished \tANN training loss 1.036631\n",
      ">> Epoch 188 finished \tANN training loss 1.036697\n",
      ">> Epoch 189 finished \tANN training loss 1.036791\n",
      ">> Epoch 190 finished \tANN training loss 1.036619\n",
      ">> Epoch 191 finished \tANN training loss 1.037074\n",
      ">> Epoch 192 finished \tANN training loss 1.036622\n",
      ">> Epoch 193 finished \tANN training loss 1.036951\n",
      ">> Epoch 194 finished \tANN training loss 1.037119\n",
      ">> Epoch 195 finished \tANN training loss 1.036812\n",
      ">> Epoch 196 finished \tANN training loss 1.036809\n",
      ">> Epoch 197 finished \tANN training loss 1.036643\n",
      ">> Epoch 198 finished \tANN training loss 1.036619\n",
      ">> Epoch 199 finished \tANN training loss 1.036978\n",
      ">> Epoch 200 finished \tANN training loss 1.036836\n",
      ">> Epoch 201 finished \tANN training loss 1.036641\n",
      ">> Epoch 202 finished \tANN training loss 1.036795\n",
      ">> Epoch 203 finished \tANN training loss 1.038334\n",
      ">> Epoch 204 finished \tANN training loss 1.036720\n",
      ">> Epoch 205 finished \tANN training loss 1.036751\n",
      ">> Epoch 206 finished \tANN training loss 1.038250\n",
      ">> Epoch 207 finished \tANN training loss 1.037646\n",
      ">> Epoch 208 finished \tANN training loss 1.036628\n",
      ">> Epoch 209 finished \tANN training loss 1.037560\n",
      ">> Epoch 210 finished \tANN training loss 1.037113\n",
      ">> Epoch 211 finished \tANN training loss 1.037832\n",
      ">> Epoch 212 finished \tANN training loss 1.037295\n",
      ">> Epoch 213 finished \tANN training loss 1.037003\n",
      ">> Epoch 214 finished \tANN training loss 1.037146\n",
      ">> Epoch 215 finished \tANN training loss 1.036812\n",
      ">> Epoch 216 finished \tANN training loss 1.036580\n",
      ">> Epoch 217 finished \tANN training loss 1.036769\n",
      ">> Epoch 218 finished \tANN training loss 1.036616\n",
      ">> Epoch 219 finished \tANN training loss 1.036829\n",
      ">> Epoch 220 finished \tANN training loss 1.036600\n",
      ">> Epoch 221 finished \tANN training loss 1.036646\n",
      ">> Epoch 222 finished \tANN training loss 1.036621\n",
      ">> Epoch 223 finished \tANN training loss 1.036703\n",
      ">> Epoch 224 finished \tANN training loss 1.037314\n",
      ">> Epoch 225 finished \tANN training loss 1.040036\n",
      ">> Epoch 226 finished \tANN training loss 1.037783\n",
      ">> Epoch 227 finished \tANN training loss 1.036583\n",
      ">> Epoch 228 finished \tANN training loss 1.036612\n",
      ">> Epoch 229 finished \tANN training loss 1.037187\n",
      ">> Epoch 230 finished \tANN training loss 1.036680\n",
      ">> Epoch 231 finished \tANN training loss 1.036620\n",
      ">> Epoch 232 finished \tANN training loss 1.036751\n",
      ">> Epoch 233 finished \tANN training loss 1.036541\n",
      ">> Epoch 234 finished \tANN training loss 1.036591\n",
      ">> Epoch 235 finished \tANN training loss 1.036524\n",
      ">> Epoch 236 finished \tANN training loss 1.036698\n",
      ">> Epoch 237 finished \tANN training loss 1.037265\n",
      ">> Epoch 238 finished \tANN training loss 1.036613\n",
      ">> Epoch 239 finished \tANN training loss 1.036796\n",
      ">> Epoch 240 finished \tANN training loss 1.037168\n",
      ">> Epoch 241 finished \tANN training loss 1.037776\n",
      ">> Epoch 242 finished \tANN training loss 1.036448\n",
      ">> Epoch 243 finished \tANN training loss 1.036437\n",
      ">> Epoch 244 finished \tANN training loss 1.036562\n",
      ">> Epoch 245 finished \tANN training loss 1.036677\n",
      ">> Epoch 246 finished \tANN training loss 1.036447\n",
      ">> Epoch 247 finished \tANN training loss 1.036189\n",
      ">> Epoch 248 finished \tANN training loss 1.036076\n",
      ">> Epoch 249 finished \tANN training loss 1.036090\n",
      ">> Epoch 250 finished \tANN training loss 1.036941\n",
      ">> Epoch 251 finished \tANN training loss 1.035651\n",
      ">> Epoch 252 finished \tANN training loss 1.034730\n",
      ">> Epoch 253 finished \tANN training loss 0.413746\n",
      ">> Epoch 254 finished \tANN training loss 0.617365\n",
      ">> Epoch 255 finished \tANN training loss 0.388430\n",
      ">> Epoch 256 finished \tANN training loss 0.327178\n",
      ">> Epoch 257 finished \tANN training loss 0.312331\n",
      ">> Epoch 258 finished \tANN training loss 0.310757\n",
      ">> Epoch 259 finished \tANN training loss 0.302294\n",
      ">> Epoch 260 finished \tANN training loss 0.328023\n",
      ">> Epoch 261 finished \tANN training loss 1.109979\n",
      ">> Epoch 262 finished \tANN training loss 1.069330\n",
      ">> Epoch 263 finished \tANN training loss 0.501837\n",
      ">> Epoch 264 finished \tANN training loss 0.325792\n",
      ">> Epoch 265 finished \tANN training loss 0.684004\n",
      ">> Epoch 266 finished \tANN training loss 0.837191\n",
      ">> Epoch 267 finished \tANN training loss 4.394363\n",
      ">> Epoch 268 finished \tANN training loss 1.132326\n",
      ">> Epoch 269 finished \tANN training loss 1.059800\n",
      ">> Epoch 270 finished \tANN training loss 1.061042\n",
      ">> Epoch 271 finished \tANN training loss 1.041888\n",
      ">> Epoch 272 finished \tANN training loss 1.038573\n",
      ">> Epoch 273 finished \tANN training loss 1.038318\n",
      ">> Epoch 274 finished \tANN training loss 1.038555\n",
      ">> Epoch 275 finished \tANN training loss 1.037473\n",
      ">> Epoch 276 finished \tANN training loss 1.036971\n",
      ">> Epoch 277 finished \tANN training loss 1.036831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 278 finished \tANN training loss 1.036670\n",
      ">> Epoch 279 finished \tANN training loss 0.661339\n",
      ">> Epoch 280 finished \tANN training loss 0.527357\n",
      ">> Epoch 281 finished \tANN training loss 0.495864\n",
      ">> Epoch 282 finished \tANN training loss 0.424734\n",
      ">> Epoch 283 finished \tANN training loss 0.946143\n",
      ">> Epoch 284 finished \tANN training loss 1.211732\n",
      ">> Epoch 285 finished \tANN training loss 1.106225\n",
      ">> Epoch 286 finished \tANN training loss 1.074155\n",
      ">> Epoch 287 finished \tANN training loss 1.056711\n",
      ">> Epoch 288 finished \tANN training loss 1.047841\n",
      ">> Epoch 289 finished \tANN training loss 1.039222\n",
      ">> Epoch 290 finished \tANN training loss 1.036981\n",
      ">> Epoch 291 finished \tANN training loss 1.036988\n",
      ">> Epoch 292 finished \tANN training loss 1.037654\n",
      ">> Epoch 293 finished \tANN training loss 1.036631\n",
      ">> Epoch 294 finished \tANN training loss 1.036637\n",
      ">> Epoch 295 finished \tANN training loss 1.036775\n",
      ">> Epoch 296 finished \tANN training loss 1.036683\n",
      ">> Epoch 297 finished \tANN training loss 1.036793\n",
      ">> Epoch 298 finished \tANN training loss 1.036762\n",
      ">> Epoch 299 finished \tANN training loss 1.037335\n",
      ">> Epoch 300 finished \tANN training loss 1.036631\n",
      ">> Epoch 301 finished \tANN training loss 1.037522\n",
      ">> Epoch 302 finished \tANN training loss 1.037744\n",
      ">> Epoch 303 finished \tANN training loss 1.038758\n",
      ">> Epoch 304 finished \tANN training loss 1.037399\n",
      ">> Epoch 305 finished \tANN training loss 1.036807\n",
      ">> Epoch 306 finished \tANN training loss 1.036645\n",
      ">> Epoch 307 finished \tANN training loss 1.036606\n",
      ">> Epoch 308 finished \tANN training loss 1.036715\n",
      ">> Epoch 309 finished \tANN training loss 1.036608\n",
      ">> Epoch 310 finished \tANN training loss 1.036646\n",
      ">> Epoch 311 finished \tANN training loss 1.036632\n",
      ">> Epoch 312 finished \tANN training loss 1.036877\n",
      ">> Epoch 313 finished \tANN training loss 1.037231\n",
      ">> Epoch 314 finished \tANN training loss 1.036602\n",
      ">> Epoch 315 finished \tANN training loss 1.037201\n",
      ">> Epoch 316 finished \tANN training loss 1.037181\n",
      ">> Epoch 317 finished \tANN training loss 1.036685\n",
      ">> Epoch 318 finished \tANN training loss 1.036588\n",
      ">> Epoch 319 finished \tANN training loss 1.036684\n",
      ">> Epoch 320 finished \tANN training loss 1.036592\n",
      ">> Epoch 321 finished \tANN training loss 1.036602\n",
      ">> Epoch 322 finished \tANN training loss 1.036629\n",
      ">> Epoch 323 finished \tANN training loss 1.036576\n",
      ">> Epoch 324 finished \tANN training loss 1.036687\n",
      ">> Epoch 325 finished \tANN training loss 1.036583\n",
      ">> Epoch 326 finished \tANN training loss 1.036853\n",
      ">> Epoch 327 finished \tANN training loss 1.036932\n",
      ">> Epoch 328 finished \tANN training loss 1.036635\n",
      ">> Epoch 329 finished \tANN training loss 1.036719\n",
      ">> Epoch 330 finished \tANN training loss 1.036564\n",
      ">> Epoch 331 finished \tANN training loss 1.036649\n",
      ">> Epoch 332 finished \tANN training loss 1.037228\n",
      ">> Epoch 333 finished \tANN training loss 1.037277\n",
      ">> Epoch 334 finished \tANN training loss 1.038267\n",
      ">> Epoch 335 finished \tANN training loss 1.038281\n",
      ">> Epoch 336 finished \tANN training loss 1.036606\n",
      ">> Epoch 337 finished \tANN training loss 1.036530\n",
      ">> Epoch 338 finished \tANN training loss 1.036730\n",
      ">> Epoch 339 finished \tANN training loss 1.036971\n",
      ">> Epoch 340 finished \tANN training loss 1.037178\n",
      ">> Epoch 341 finished \tANN training loss 1.036615\n",
      ">> Epoch 342 finished \tANN training loss 1.036522\n",
      ">> Epoch 343 finished \tANN training loss 1.036535\n",
      ">> Epoch 344 finished \tANN training loss 1.036519\n",
      ">> Epoch 345 finished \tANN training loss 1.036702\n",
      ">> Epoch 346 finished \tANN training loss 1.036500\n",
      ">> Epoch 347 finished \tANN training loss 1.036538\n",
      ">> Epoch 348 finished \tANN training loss 1.036518\n",
      ">> Epoch 349 finished \tANN training loss 1.036497\n",
      ">> Epoch 350 finished \tANN training loss 1.037022\n",
      ">> Epoch 351 finished \tANN training loss 1.036752\n",
      ">> Epoch 352 finished \tANN training loss 1.036570\n",
      ">> Epoch 353 finished \tANN training loss 1.037261\n",
      ">> Epoch 354 finished \tANN training loss 1.038318\n",
      ">> Epoch 355 finished \tANN training loss 1.037832\n",
      ">> Epoch 356 finished \tANN training loss 1.036683\n",
      ">> Epoch 357 finished \tANN training loss 1.037238\n",
      ">> Epoch 358 finished \tANN training loss 1.036472\n",
      ">> Epoch 359 finished \tANN training loss 1.036471\n",
      ">> Epoch 360 finished \tANN training loss 1.036487\n",
      ">> Epoch 361 finished \tANN training loss 1.036483\n",
      ">> Epoch 362 finished \tANN training loss 1.036849\n",
      ">> Epoch 363 finished \tANN training loss 1.036623\n",
      ">> Epoch 364 finished \tANN training loss 1.036497\n",
      ">> Epoch 365 finished \tANN training loss 1.036482\n",
      ">> Epoch 366 finished \tANN training loss 1.037602\n",
      ">> Epoch 367 finished \tANN training loss 1.038031\n",
      ">> Epoch 368 finished \tANN training loss 1.036635\n",
      ">> Epoch 369 finished \tANN training loss 1.037112\n",
      ">> Epoch 370 finished \tANN training loss 1.037925\n",
      ">> Epoch 371 finished \tANN training loss 1.037288\n",
      ">> Epoch 372 finished \tANN training loss 1.036409\n",
      ">> Epoch 373 finished \tANN training loss 1.036414\n",
      ">> Epoch 374 finished \tANN training loss 1.036594\n",
      ">> Epoch 375 finished \tANN training loss 1.036460\n",
      ">> Epoch 376 finished \tANN training loss 1.037279\n",
      ">> Epoch 377 finished \tANN training loss 1.036423\n",
      ">> Epoch 378 finished \tANN training loss 1.036852\n",
      ">> Epoch 379 finished \tANN training loss 1.036381\n",
      ">> Epoch 380 finished \tANN training loss 1.036553\n",
      ">> Epoch 381 finished \tANN training loss 1.037035\n",
      ">> Epoch 382 finished \tANN training loss 1.036398\n",
      ">> Epoch 383 finished \tANN training loss 1.036970\n",
      ">> Epoch 384 finished \tANN training loss 1.036692\n",
      ">> Epoch 385 finished \tANN training loss 1.036552\n",
      ">> Epoch 386 finished \tANN training loss 1.036802\n",
      ">> Epoch 387 finished \tANN training loss 1.036619\n",
      ">> Epoch 388 finished \tANN training loss 1.036665\n",
      ">> Epoch 389 finished \tANN training loss 1.036295\n",
      ">> Epoch 390 finished \tANN training loss 1.036279\n",
      ">> Epoch 391 finished \tANN training loss 1.036263\n",
      ">> Epoch 392 finished \tANN training loss 1.036249\n",
      ">> Epoch 393 finished \tANN training loss 1.037055\n",
      ">> Epoch 394 finished \tANN training loss 1.036480\n",
      ">> Epoch 395 finished \tANN training loss 1.036279\n",
      ">> Epoch 396 finished \tANN training loss 1.038140\n",
      ">> Epoch 397 finished \tANN training loss 1.036215\n",
      ">> Epoch 398 finished \tANN training loss 1.036243\n",
      ">> Epoch 399 finished \tANN training loss 1.036160\n",
      ">> Epoch 400 finished \tANN training loss 1.036604\n",
      ">> Epoch 401 finished \tANN training loss 1.036256\n",
      ">> Epoch 402 finished \tANN training loss 1.036874\n",
      ">> Epoch 403 finished \tANN training loss 1.036936\n",
      ">> Epoch 404 finished \tANN training loss 1.036833\n",
      ">> Epoch 405 finished \tANN training loss 1.037061\n",
      ">> Epoch 406 finished \tANN training loss 1.036622\n",
      ">> Epoch 407 finished \tANN training loss 1.037072\n",
      ">> Epoch 408 finished \tANN training loss 1.036830\n",
      ">> Epoch 409 finished \tANN training loss 1.036205\n",
      ">> Epoch 410 finished \tANN training loss 1.036054\n",
      ">> Epoch 411 finished \tANN training loss 1.036062\n",
      ">> Epoch 412 finished \tANN training loss 1.036547\n",
      ">> Epoch 413 finished \tANN training loss 1.037046\n",
      ">> Epoch 414 finished \tANN training loss 1.036278\n",
      ">> Epoch 415 finished \tANN training loss 1.036092\n",
      ">> Epoch 416 finished \tANN training loss 1.036003\n",
      ">> Epoch 417 finished \tANN training loss 1.035954\n",
      ">> Epoch 418 finished \tANN training loss 1.036879\n",
      ">> Epoch 419 finished \tANN training loss 1.037545\n",
      ">> Epoch 420 finished \tANN training loss 1.036235\n",
      ">> Epoch 421 finished \tANN training loss 1.036617\n",
      ">> Epoch 422 finished \tANN training loss 1.036170\n",
      ">> Epoch 423 finished \tANN training loss 1.035819\n",
      ">> Epoch 424 finished \tANN training loss 1.036295\n",
      ">> Epoch 425 finished \tANN training loss 1.036126\n",
      ">> Epoch 426 finished \tANN training loss 1.036149\n",
      ">> Epoch 427 finished \tANN training loss 1.036657\n",
      ">> Epoch 428 finished \tANN training loss 1.035667\n",
      ">> Epoch 429 finished \tANN training loss 1.035648\n",
      ">> Epoch 430 finished \tANN training loss 1.035628\n",
      ">> Epoch 431 finished \tANN training loss 1.036100\n",
      ">> Epoch 432 finished \tANN training loss 1.035513\n",
      ">> Epoch 433 finished \tANN training loss 1.035461\n",
      ">> Epoch 434 finished \tANN training loss 1.035436\n",
      ">> Epoch 435 finished \tANN training loss 1.035326\n",
      ">> Epoch 436 finished \tANN training loss 1.035343\n",
      ">> Epoch 437 finished \tANN training loss 1.035148\n",
      ">> Epoch 438 finished \tANN training loss 1.035119\n",
      ">> Epoch 439 finished \tANN training loss 1.034908\n",
      ">> Epoch 440 finished \tANN training loss 1.034817\n",
      ">> Epoch 441 finished \tANN training loss 1.034397\n",
      ">> Epoch 442 finished \tANN training loss 1.034408\n",
      ">> Epoch 443 finished \tANN training loss 1.033100\n",
      ">> Epoch 444 finished \tANN training loss 1.031806\n",
      ">> Epoch 445 finished \tANN training loss 0.394219\n",
      ">> Epoch 446 finished \tANN training loss 0.758734\n",
      ">> Epoch 447 finished \tANN training loss 0.332496\n",
      ">> Epoch 448 finished \tANN training loss 0.467448\n",
      ">> Epoch 449 finished \tANN training loss 1.090989\n",
      ">> Epoch 450 finished \tANN training loss 1.059199\n",
      ">> Epoch 451 finished \tANN training loss 1.044321\n",
      ">> Epoch 452 finished \tANN training loss 1.043203\n",
      ">> Epoch 453 finished \tANN training loss 1.041195\n",
      ">> Epoch 454 finished \tANN training loss 1.041063\n",
      ">> Epoch 455 finished \tANN training loss 1.039469\n",
      ">> Epoch 456 finished \tANN training loss 1.040228\n",
      ">> Epoch 457 finished \tANN training loss 1.039319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 458 finished \tANN training loss 1.038892\n",
      ">> Epoch 459 finished \tANN training loss 1.037348\n",
      ">> Epoch 460 finished \tANN training loss 1.036949\n",
      ">> Epoch 461 finished \tANN training loss 1.037379\n",
      ">> Epoch 462 finished \tANN training loss 1.038850\n",
      ">> Epoch 463 finished \tANN training loss 1.036445\n",
      ">> Epoch 464 finished \tANN training loss 1.035787\n",
      ">> Epoch 465 finished \tANN training loss 1.035238\n",
      ">> Epoch 466 finished \tANN training loss 1.035250\n",
      ">> Epoch 467 finished \tANN training loss 1.035223\n",
      ">> Epoch 468 finished \tANN training loss 1.035429\n",
      ">> Epoch 469 finished \tANN training loss 1.035412\n",
      ">> Epoch 470 finished \tANN training loss 1.035295\n",
      ">> Epoch 471 finished \tANN training loss 1.037073\n",
      ">> Epoch 472 finished \tANN training loss 1.036619\n",
      ">> Epoch 473 finished \tANN training loss 1.036099\n",
      ">> Epoch 474 finished \tANN training loss 1.035406\n",
      ">> Epoch 475 finished \tANN training loss 1.036465\n",
      ">> Epoch 476 finished \tANN training loss 1.035245\n",
      ">> Epoch 477 finished \tANN training loss 1.035376\n",
      ">> Epoch 478 finished \tANN training loss 1.034606\n",
      ">> Epoch 479 finished \tANN training loss 1.034588\n",
      ">> Epoch 480 finished \tANN training loss 1.034611\n",
      ">> Epoch 481 finished \tANN training loss 1.035392\n",
      ">> Epoch 482 finished \tANN training loss 1.035111\n",
      ">> Epoch 483 finished \tANN training loss 1.034413\n",
      ">> Epoch 484 finished \tANN training loss 1.035716\n",
      ">> Epoch 485 finished \tANN training loss 1.034794\n",
      ">> Epoch 486 finished \tANN training loss 1.036174\n",
      ">> Epoch 487 finished \tANN training loss 1.034067\n",
      ">> Epoch 488 finished \tANN training loss 1.033844\n",
      ">> Epoch 489 finished \tANN training loss 1.034180\n",
      ">> Epoch 490 finished \tANN training loss 1.034911\n",
      ">> Epoch 491 finished \tANN training loss 1.033738\n",
      ">> Epoch 492 finished \tANN training loss 1.035472\n",
      ">> Epoch 493 finished \tANN training loss 1.033636\n",
      ">> Epoch 494 finished \tANN training loss 1.032462\n",
      ">> Epoch 495 finished \tANN training loss 1.031748\n",
      ">> Epoch 496 finished \tANN training loss 1.031413\n",
      ">> Epoch 497 finished \tANN training loss 1.030687\n",
      ">> Epoch 498 finished \tANN training loss 1.029704\n",
      ">> Epoch 499 finished \tANN training loss 1.027793\n",
      ">> Epoch 500 finished \tANN training loss 1.023892\n",
      ">> Epoch 501 finished \tANN training loss 1.009010\n",
      ">> Epoch 502 finished \tANN training loss 1.042529\n",
      ">> Epoch 503 finished \tANN training loss 1.040257\n",
      ">> Epoch 504 finished \tANN training loss 1.040367\n",
      ">> Epoch 505 finished \tANN training loss 1.039994\n",
      ">> Epoch 506 finished \tANN training loss 1.040769\n",
      ">> Epoch 507 finished \tANN training loss 1.039604\n",
      ">> Epoch 508 finished \tANN training loss 1.039263\n",
      ">> Epoch 509 finished \tANN training loss 1.039577\n",
      ">> Epoch 510 finished \tANN training loss 1.039223\n",
      ">> Epoch 511 finished \tANN training loss 1.039018\n",
      ">> Epoch 512 finished \tANN training loss 1.038962\n",
      ">> Epoch 513 finished \tANN training loss 1.038970\n",
      ">> Epoch 514 finished \tANN training loss 1.038827\n",
      ">> Epoch 515 finished \tANN training loss 1.039719\n",
      ">> Epoch 516 finished \tANN training loss 1.041366\n",
      ">> Epoch 517 finished \tANN training loss 1.040416\n",
      ">> Epoch 518 finished \tANN training loss 1.038756\n",
      ">> Epoch 519 finished \tANN training loss 1.038711\n",
      ">> Epoch 520 finished \tANN training loss 1.038520\n",
      ">> Epoch 521 finished \tANN training loss 1.038940\n",
      ">> Epoch 522 finished \tANN training loss 1.040137\n",
      ">> Epoch 523 finished \tANN training loss 1.038465\n",
      ">> Epoch 524 finished \tANN training loss 1.038907\n",
      ">> Epoch 525 finished \tANN training loss 1.038285\n",
      ">> Epoch 526 finished \tANN training loss 1.038293\n",
      ">> Epoch 527 finished \tANN training loss 1.038341\n",
      ">> Epoch 528 finished \tANN training loss 1.038888\n",
      ">> Epoch 529 finished \tANN training loss 1.039344\n",
      ">> Epoch 530 finished \tANN training loss 1.038565\n",
      ">> Epoch 531 finished \tANN training loss 1.038348\n",
      ">> Epoch 532 finished \tANN training loss 1.038010\n",
      ">> Epoch 533 finished \tANN training loss 1.038326\n",
      ">> Epoch 534 finished \tANN training loss 1.039077\n",
      ">> Epoch 535 finished \tANN training loss 1.038001\n",
      ">> Epoch 536 finished \tANN training loss 1.037977\n",
      ">> Epoch 537 finished \tANN training loss 1.038148\n",
      ">> Epoch 538 finished \tANN training loss 1.038253\n",
      ">> Epoch 539 finished \tANN training loss 1.037848\n",
      ">> Epoch 540 finished \tANN training loss 1.038217\n",
      ">> Epoch 541 finished \tANN training loss 1.037838\n",
      ">> Epoch 542 finished \tANN training loss 1.037824\n",
      ">> Epoch 543 finished \tANN training loss 1.038152\n",
      ">> Epoch 544 finished \tANN training loss 1.039177\n",
      ">> Epoch 545 finished \tANN training loss 1.038121\n",
      ">> Epoch 546 finished \tANN training loss 1.039256\n",
      ">> Epoch 547 finished \tANN training loss 1.038992\n",
      ">> Epoch 548 finished \tANN training loss 1.037643\n",
      ">> Epoch 549 finished \tANN training loss 1.038040\n",
      ">> Epoch 550 finished \tANN training loss 1.037582\n",
      ">> Epoch 551 finished \tANN training loss 1.037650\n",
      ">> Epoch 552 finished \tANN training loss 1.037565\n",
      ">> Epoch 553 finished \tANN training loss 1.037581\n",
      ">> Epoch 554 finished \tANN training loss 1.037630\n",
      ">> Epoch 555 finished \tANN training loss 1.037502\n",
      ">> Epoch 556 finished \tANN training loss 1.037472\n",
      ">> Epoch 557 finished \tANN training loss 1.037559\n",
      ">> Epoch 558 finished \tANN training loss 1.037452\n",
      ">> Epoch 559 finished \tANN training loss 1.037411\n",
      ">> Epoch 560 finished \tANN training loss 1.037619\n",
      ">> Epoch 561 finished \tANN training loss 1.038032\n",
      ">> Epoch 562 finished \tANN training loss 1.038446\n",
      ">> Epoch 563 finished \tANN training loss 1.038016\n",
      ">> Epoch 564 finished \tANN training loss 1.037648\n",
      ">> Epoch 565 finished \tANN training loss 1.037440\n",
      ">> Epoch 566 finished \tANN training loss 1.037650\n",
      ">> Epoch 567 finished \tANN training loss 1.037565\n",
      ">> Epoch 568 finished \tANN training loss 1.038801\n",
      ">> Epoch 569 finished \tANN training loss 1.037597\n",
      ">> Epoch 570 finished \tANN training loss 1.037763\n",
      ">> Epoch 571 finished \tANN training loss 1.037483\n",
      ">> Epoch 572 finished \tANN training loss 1.037294\n",
      ">> Epoch 573 finished \tANN training loss 1.037402\n",
      ">> Epoch 574 finished \tANN training loss 1.037828\n",
      ">> Epoch 575 finished \tANN training loss 1.037577\n",
      ">> Epoch 576 finished \tANN training loss 1.037188\n",
      ">> Epoch 577 finished \tANN training loss 1.037176\n",
      ">> Epoch 578 finished \tANN training loss 1.038176\n",
      ">> Epoch 579 finished \tANN training loss 1.037152\n",
      ">> Epoch 580 finished \tANN training loss 1.037119\n",
      ">> Epoch 581 finished \tANN training loss 1.037104\n",
      ">> Epoch 582 finished \tANN training loss 1.037102\n",
      ">> Epoch 583 finished \tANN training loss 1.038508\n",
      ">> Epoch 584 finished \tANN training loss 1.037976\n",
      ">> Epoch 585 finished \tANN training loss 1.037114\n",
      ">> Epoch 586 finished \tANN training loss 1.037082\n",
      ">> Epoch 587 finished \tANN training loss 1.037218\n",
      ">> Epoch 588 finished \tANN training loss 1.037638\n",
      ">> Epoch 589 finished \tANN training loss 1.037126\n",
      ">> Epoch 590 finished \tANN training loss 1.037202\n",
      ">> Epoch 591 finished \tANN training loss 1.037055\n",
      ">> Epoch 592 finished \tANN training loss 1.037024\n",
      ">> Epoch 593 finished \tANN training loss 1.037011\n",
      ">> Epoch 594 finished \tANN training loss 1.036997\n",
      ">> Epoch 595 finished \tANN training loss 1.037173\n",
      ">> Epoch 596 finished \tANN training loss 1.037419\n",
      ">> Epoch 597 finished \tANN training loss 1.037001\n",
      ">> Epoch 598 finished \tANN training loss 1.037507\n",
      ">> Epoch 599 finished \tANN training loss 1.037270\n",
      ">> Epoch 600 finished \tANN training loss 1.037380\n",
      ">> Epoch 601 finished \tANN training loss 1.037974\n",
      ">> Epoch 602 finished \tANN training loss 1.037157\n",
      ">> Epoch 603 finished \tANN training loss 1.037089\n",
      ">> Epoch 604 finished \tANN training loss 1.036963\n",
      ">> Epoch 605 finished \tANN training loss 1.037243\n",
      ">> Epoch 606 finished \tANN training loss 1.037065\n",
      ">> Epoch 607 finished \tANN training loss 1.036971\n",
      ">> Epoch 608 finished \tANN training loss 1.036927\n",
      ">> Epoch 609 finished \tANN training loss 1.036935\n",
      ">> Epoch 610 finished \tANN training loss 1.036990\n",
      ">> Epoch 611 finished \tANN training loss 1.037044\n",
      ">> Epoch 612 finished \tANN training loss 1.036939\n",
      ">> Epoch 613 finished \tANN training loss 1.036970\n",
      ">> Epoch 614 finished \tANN training loss 1.036986\n",
      ">> Epoch 615 finished \tANN training loss 1.036926\n",
      ">> Epoch 616 finished \tANN training loss 1.037121\n",
      ">> Epoch 617 finished \tANN training loss 1.037219\n",
      ">> Epoch 618 finished \tANN training loss 1.037040\n",
      ">> Epoch 619 finished \tANN training loss 1.037012\n",
      ">> Epoch 620 finished \tANN training loss 1.037953\n",
      ">> Epoch 621 finished \tANN training loss 1.037085\n",
      ">> Epoch 622 finished \tANN training loss 1.037491\n",
      ">> Epoch 623 finished \tANN training loss 1.038081\n",
      ">> Epoch 624 finished \tANN training loss 1.037966\n",
      ">> Epoch 625 finished \tANN training loss 1.036893\n",
      ">> Epoch 626 finished \tANN training loss 1.037474\n",
      ">> Epoch 627 finished \tANN training loss 1.036913\n",
      ">> Epoch 628 finished \tANN training loss 1.036885\n",
      ">> Epoch 629 finished \tANN training loss 1.037227\n",
      ">> Epoch 630 finished \tANN training loss 1.036890\n",
      ">> Epoch 631 finished \tANN training loss 1.037476\n",
      ">> Epoch 632 finished \tANN training loss 1.037137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 633 finished \tANN training loss 1.037193\n",
      ">> Epoch 634 finished \tANN training loss 1.038018\n",
      ">> Epoch 635 finished \tANN training loss 1.036842\n",
      ">> Epoch 636 finished \tANN training loss 1.036953\n",
      ">> Epoch 637 finished \tANN training loss 1.037379\n",
      ">> Epoch 638 finished \tANN training loss 1.037011\n",
      ">> Epoch 639 finished \tANN training loss 1.037181\n",
      ">> Epoch 640 finished \tANN training loss 1.037787\n",
      ">> Epoch 641 finished \tANN training loss 1.037319\n",
      ">> Epoch 642 finished \tANN training loss 1.036917\n",
      ">> Epoch 643 finished \tANN training loss 1.036964\n",
      ">> Epoch 644 finished \tANN training loss 1.037422\n",
      ">> Epoch 645 finished \tANN training loss 1.037270\n",
      ">> Epoch 646 finished \tANN training loss 1.037311\n",
      ">> Epoch 647 finished \tANN training loss 1.036852\n",
      ">> Epoch 648 finished \tANN training loss 1.037892\n",
      ">> Epoch 649 finished \tANN training loss 1.037465\n",
      ">> Epoch 650 finished \tANN training loss 1.036942\n",
      ">> Epoch 651 finished \tANN training loss 1.036824\n",
      ">> Epoch 652 finished \tANN training loss 1.037136\n",
      ">> Epoch 653 finished \tANN training loss 1.036862\n",
      ">> Epoch 654 finished \tANN training loss 1.038243\n",
      ">> Epoch 655 finished \tANN training loss 1.037177\n",
      ">> Epoch 656 finished \tANN training loss 1.036793\n",
      ">> Epoch 657 finished \tANN training loss 1.036793\n",
      ">> Epoch 658 finished \tANN training loss 1.036838\n",
      ">> Epoch 659 finished \tANN training loss 1.036986\n",
      ">> Epoch 660 finished \tANN training loss 1.036820\n",
      ">> Epoch 661 finished \tANN training loss 1.036918\n",
      ">> Epoch 662 finished \tANN training loss 1.036889\n",
      ">> Epoch 663 finished \tANN training loss 1.036810\n",
      ">> Epoch 664 finished \tANN training loss 1.036798\n",
      ">> Epoch 665 finished \tANN training loss 1.036881\n",
      ">> Epoch 666 finished \tANN training loss 1.037294\n",
      ">> Epoch 667 finished \tANN training loss 1.037846\n",
      ">> Epoch 668 finished \tANN training loss 1.036784\n",
      ">> Epoch 669 finished \tANN training loss 1.036993\n",
      ">> Epoch 670 finished \tANN training loss 1.036841\n",
      ">> Epoch 671 finished \tANN training loss 1.037197\n",
      ">> Epoch 672 finished \tANN training loss 1.036905\n",
      ">> Epoch 673 finished \tANN training loss 1.036907\n",
      ">> Epoch 674 finished \tANN training loss 1.036977\n",
      ">> Epoch 675 finished \tANN training loss 1.037179\n",
      ">> Epoch 676 finished \tANN training loss 1.036858\n",
      ">> Epoch 677 finished \tANN training loss 1.036879\n",
      ">> Epoch 678 finished \tANN training loss 1.036970\n",
      ">> Epoch 679 finished \tANN training loss 1.037058\n",
      ">> Epoch 680 finished \tANN training loss 1.036764\n",
      ">> Epoch 681 finished \tANN training loss 1.036775\n",
      ">> Epoch 682 finished \tANN training loss 1.036758\n",
      ">> Epoch 683 finished \tANN training loss 1.036968\n",
      ">> Epoch 684 finished \tANN training loss 1.037256\n",
      ">> Epoch 685 finished \tANN training loss 1.036756\n",
      ">> Epoch 686 finished \tANN training loss 1.037199\n",
      ">> Epoch 687 finished \tANN training loss 1.036842\n",
      ">> Epoch 688 finished \tANN training loss 1.036829\n",
      ">> Epoch 689 finished \tANN training loss 1.036771\n",
      ">> Epoch 690 finished \tANN training loss 1.036852\n",
      ">> Epoch 691 finished \tANN training loss 1.036858\n",
      ">> Epoch 692 finished \tANN training loss 1.036747\n",
      ">> Epoch 693 finished \tANN training loss 1.036850\n",
      ">> Epoch 694 finished \tANN training loss 1.036748\n",
      ">> Epoch 695 finished \tANN training loss 1.037330\n",
      ">> Epoch 696 finished \tANN training loss 1.037872\n",
      ">> Epoch 697 finished \tANN training loss 1.036752\n",
      ">> Epoch 698 finished \tANN training loss 1.037133\n",
      ">> Epoch 699 finished \tANN training loss 1.037270\n",
      ">> Epoch 700 finished \tANN training loss 1.036745\n",
      ">> Epoch 701 finished \tANN training loss 1.036826\n",
      ">> Epoch 702 finished \tANN training loss 1.036867\n",
      ">> Epoch 703 finished \tANN training loss 1.037419\n",
      ">> Epoch 704 finished \tANN training loss 1.037032\n",
      ">> Epoch 705 finished \tANN training loss 1.038452\n",
      ">> Epoch 706 finished \tANN training loss 1.037600\n",
      ">> Epoch 707 finished \tANN training loss 1.036786\n",
      ">> Epoch 708 finished \tANN training loss 1.036738\n",
      ">> Epoch 709 finished \tANN training loss 1.036817\n",
      ">> Epoch 710 finished \tANN training loss 1.037062\n",
      ">> Epoch 711 finished \tANN training loss 1.036814\n",
      ">> Epoch 712 finished \tANN training loss 1.037012\n",
      ">> Epoch 713 finished \tANN training loss 1.036914\n",
      ">> Epoch 714 finished \tANN training loss 1.037975\n",
      ">> Epoch 715 finished \tANN training loss 1.037891\n",
      ">> Epoch 716 finished \tANN training loss 1.037723\n",
      ">> Epoch 717 finished \tANN training loss 1.038111\n",
      ">> Epoch 718 finished \tANN training loss 1.036722\n",
      ">> Epoch 719 finished \tANN training loss 1.036842\n",
      ">> Epoch 720 finished \tANN training loss 1.037568\n",
      ">> Epoch 721 finished \tANN training loss 1.036720\n",
      ">> Epoch 722 finished \tANN training loss 1.036726\n",
      ">> Epoch 723 finished \tANN training loss 1.036760\n",
      ">> Epoch 724 finished \tANN training loss 1.036838\n",
      ">> Epoch 725 finished \tANN training loss 1.036932\n",
      ">> Epoch 726 finished \tANN training loss 1.036741\n",
      ">> Epoch 727 finished \tANN training loss 1.036757\n",
      ">> Epoch 728 finished \tANN training loss 1.036862\n",
      ">> Epoch 729 finished \tANN training loss 1.037539\n",
      ">> Epoch 730 finished \tANN training loss 1.037103\n",
      ">> Epoch 731 finished \tANN training loss 1.036733\n",
      ">> Epoch 732 finished \tANN training loss 1.037585\n",
      ">> Epoch 733 finished \tANN training loss 1.036713\n",
      ">> Epoch 734 finished \tANN training loss 1.036732\n",
      ">> Epoch 735 finished \tANN training loss 1.036970\n",
      ">> Epoch 736 finished \tANN training loss 1.036734\n",
      ">> Epoch 737 finished \tANN training loss 1.036906\n",
      ">> Epoch 738 finished \tANN training loss 1.036707\n",
      ">> Epoch 739 finished \tANN training loss 1.037030\n",
      ">> Epoch 740 finished \tANN training loss 1.036703\n",
      ">> Epoch 741 finished \tANN training loss 1.037210\n",
      ">> Epoch 742 finished \tANN training loss 1.036722\n",
      ">> Epoch 743 finished \tANN training loss 1.036715\n",
      ">> Epoch 744 finished \tANN training loss 1.036951\n",
      ">> Epoch 745 finished \tANN training loss 1.039960\n",
      ">> Epoch 746 finished \tANN training loss 1.038268\n",
      ">> Epoch 747 finished \tANN training loss 1.038622\n",
      ">> Epoch 748 finished \tANN training loss 1.038254\n",
      ">> Epoch 749 finished \tANN training loss 1.037953\n",
      ">> Epoch 750 finished \tANN training loss 1.037159\n",
      ">> Epoch 751 finished \tANN training loss 1.036700\n",
      ">> Epoch 752 finished \tANN training loss 1.036818\n",
      ">> Epoch 753 finished \tANN training loss 1.037133\n",
      ">> Epoch 754 finished \tANN training loss 1.036824\n",
      ">> Epoch 755 finished \tANN training loss 1.037204\n",
      ">> Epoch 756 finished \tANN training loss 1.036749\n",
      ">> Epoch 757 finished \tANN training loss 1.036695\n",
      ">> Epoch 758 finished \tANN training loss 1.038983\n",
      ">> Epoch 759 finished \tANN training loss 1.037258\n",
      ">> Epoch 760 finished \tANN training loss 1.037085\n",
      ">> Epoch 761 finished \tANN training loss 1.036852\n",
      ">> Epoch 762 finished \tANN training loss 1.036755\n",
      ">> Epoch 763 finished \tANN training loss 1.036715\n",
      ">> Epoch 764 finished \tANN training loss 1.036820\n",
      ">> Epoch 765 finished \tANN training loss 1.036717\n",
      ">> Epoch 766 finished \tANN training loss 1.036898\n",
      ">> Epoch 767 finished \tANN training loss 1.036814\n",
      ">> Epoch 768 finished \tANN training loss 1.036759\n",
      ">> Epoch 769 finished \tANN training loss 1.036889\n",
      ">> Epoch 770 finished \tANN training loss 1.036853\n",
      ">> Epoch 771 finished \tANN training loss 1.036711\n",
      ">> Epoch 772 finished \tANN training loss 1.037757\n",
      ">> Epoch 773 finished \tANN training loss 1.037430\n",
      ">> Epoch 774 finished \tANN training loss 1.036923\n",
      ">> Epoch 775 finished \tANN training loss 1.036730\n",
      ">> Epoch 776 finished \tANN training loss 1.036990\n",
      ">> Epoch 777 finished \tANN training loss 1.036704\n",
      ">> Epoch 778 finished \tANN training loss 1.036963\n",
      ">> Epoch 779 finished \tANN training loss 1.036739\n",
      ">> Epoch 780 finished \tANN training loss 1.037896\n",
      ">> Epoch 781 finished \tANN training loss 1.036686\n",
      ">> Epoch 782 finished \tANN training loss 1.037201\n",
      ">> Epoch 783 finished \tANN training loss 1.036715\n",
      ">> Epoch 784 finished \tANN training loss 1.036948\n",
      ">> Epoch 785 finished \tANN training loss 1.037367\n",
      ">> Epoch 786 finished \tANN training loss 1.037293\n",
      ">> Epoch 787 finished \tANN training loss 1.037374\n",
      ">> Epoch 788 finished \tANN training loss 1.037857\n",
      ">> Epoch 789 finished \tANN training loss 1.037173\n",
      ">> Epoch 790 finished \tANN training loss 1.037000\n",
      ">> Epoch 791 finished \tANN training loss 1.036739\n",
      ">> Epoch 792 finished \tANN training loss 1.037440\n",
      ">> Epoch 793 finished \tANN training loss 1.036809\n",
      ">> Epoch 794 finished \tANN training loss 1.036726\n",
      ">> Epoch 795 finished \tANN training loss 1.037172\n",
      ">> Epoch 796 finished \tANN training loss 1.038280\n",
      ">> Epoch 797 finished \tANN training loss 1.036679\n",
      ">> Epoch 798 finished \tANN training loss 1.036678\n",
      ">> Epoch 799 finished \tANN training loss 1.037391\n",
      ">> Epoch 800 finished \tANN training loss 1.037339\n",
      ">> Epoch 801 finished \tANN training loss 1.037080\n",
      ">> Epoch 802 finished \tANN training loss 1.037175\n",
      ">> Epoch 803 finished \tANN training loss 1.036741\n",
      ">> Epoch 804 finished \tANN training loss 1.037056\n",
      ">> Epoch 805 finished \tANN training loss 1.038541\n",
      ">> Epoch 806 finished \tANN training loss 1.038865\n",
      ">> Epoch 807 finished \tANN training loss 1.037794\n",
      ">> Epoch 808 finished \tANN training loss 1.037156\n",
      ">> Epoch 809 finished \tANN training loss 1.036687\n",
      ">> Epoch 810 finished \tANN training loss 1.036817\n",
      ">> Epoch 811 finished \tANN training loss 1.036681\n",
      ">> Epoch 812 finished \tANN training loss 1.036700\n",
      ">> Epoch 813 finished \tANN training loss 1.036711\n",
      ">> Epoch 814 finished \tANN training loss 1.037197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 815 finished \tANN training loss 1.036703\n",
      ">> Epoch 816 finished \tANN training loss 1.036674\n",
      ">> Epoch 817 finished \tANN training loss 1.036888\n",
      ">> Epoch 818 finished \tANN training loss 1.036677\n",
      ">> Epoch 819 finished \tANN training loss 1.036708\n",
      ">> Epoch 820 finished \tANN training loss 1.036762\n",
      ">> Epoch 821 finished \tANN training loss 1.036673\n",
      ">> Epoch 822 finished \tANN training loss 1.036735\n",
      ">> Epoch 823 finished \tANN training loss 1.036944\n",
      ">> Epoch 824 finished \tANN training loss 1.036819\n",
      ">> Epoch 825 finished \tANN training loss 1.037384\n",
      ">> Epoch 826 finished \tANN training loss 1.036918\n",
      ">> Epoch 827 finished \tANN training loss 1.036685\n",
      ">> Epoch 828 finished \tANN training loss 1.036699\n",
      ">> Epoch 829 finished \tANN training loss 1.036674\n",
      ">> Epoch 830 finished \tANN training loss 1.036696\n",
      ">> Epoch 831 finished \tANN training loss 1.036679\n",
      ">> Epoch 832 finished \tANN training loss 1.036679\n",
      ">> Epoch 833 finished \tANN training loss 1.037490\n",
      ">> Epoch 834 finished \tANN training loss 1.036751\n",
      ">> Epoch 835 finished \tANN training loss 1.036914\n",
      ">> Epoch 836 finished \tANN training loss 1.037012\n",
      ">> Epoch 837 finished \tANN training loss 1.036824\n",
      ">> Epoch 838 finished \tANN training loss 1.036675\n",
      ">> Epoch 839 finished \tANN training loss 1.036669\n",
      ">> Epoch 840 finished \tANN training loss 1.036872\n",
      ">> Epoch 841 finished \tANN training loss 1.037204\n",
      ">> Epoch 842 finished \tANN training loss 1.036685\n",
      ">> Epoch 843 finished \tANN training loss 1.036668\n",
      ">> Epoch 844 finished \tANN training loss 1.036700\n",
      ">> Epoch 845 finished \tANN training loss 1.036705\n",
      ">> Epoch 846 finished \tANN training loss 1.036675\n",
      ">> Epoch 847 finished \tANN training loss 1.037104\n",
      ">> Epoch 848 finished \tANN training loss 1.037444\n",
      ">> Epoch 849 finished \tANN training loss 1.036733\n",
      ">> Epoch 850 finished \tANN training loss 1.036877\n",
      ">> Epoch 851 finished \tANN training loss 1.036794\n",
      ">> Epoch 852 finished \tANN training loss 1.037888\n",
      ">> Epoch 853 finished \tANN training loss 1.037137\n",
      ">> Epoch 854 finished \tANN training loss 1.036769\n",
      ">> Epoch 855 finished \tANN training loss 1.036701\n",
      ">> Epoch 856 finished \tANN training loss 1.037058\n",
      ">> Epoch 857 finished \tANN training loss 1.037192\n",
      ">> Epoch 858 finished \tANN training loss 1.037207\n",
      ">> Epoch 859 finished \tANN training loss 1.036669\n",
      ">> Epoch 860 finished \tANN training loss 1.036665\n",
      ">> Epoch 861 finished \tANN training loss 1.036944\n",
      ">> Epoch 862 finished \tANN training loss 1.037594\n",
      ">> Epoch 863 finished \tANN training loss 1.038269\n",
      ">> Epoch 864 finished \tANN training loss 1.037548\n",
      ">> Epoch 865 finished \tANN training loss 1.036769\n",
      ">> Epoch 866 finished \tANN training loss 1.037530\n",
      ">> Epoch 867 finished \tANN training loss 1.036940\n",
      ">> Epoch 868 finished \tANN training loss 1.036819\n",
      ">> Epoch 869 finished \tANN training loss 1.036778\n",
      ">> Epoch 870 finished \tANN training loss 1.036675\n",
      ">> Epoch 871 finished \tANN training loss 1.036759\n",
      ">> Epoch 872 finished \tANN training loss 1.037221\n",
      ">> Epoch 873 finished \tANN training loss 1.036993\n",
      ">> Epoch 874 finished \tANN training loss 1.036837\n",
      ">> Epoch 875 finished \tANN training loss 1.037450\n",
      ">> Epoch 876 finished \tANN training loss 1.037575\n",
      ">> Epoch 877 finished \tANN training loss 1.038346\n",
      ">> Epoch 878 finished \tANN training loss 1.036814\n",
      ">> Epoch 879 finished \tANN training loss 1.037031\n",
      ">> Epoch 880 finished \tANN training loss 1.037439\n",
      ">> Epoch 881 finished \tANN training loss 1.037183\n",
      ">> Epoch 882 finished \tANN training loss 1.036743\n",
      ">> Epoch 883 finished \tANN training loss 1.036688\n",
      ">> Epoch 884 finished \tANN training loss 1.036669\n",
      ">> Epoch 885 finished \tANN training loss 1.036662\n",
      ">> Epoch 886 finished \tANN training loss 1.036661\n",
      ">> Epoch 887 finished \tANN training loss 1.037798\n",
      ">> Epoch 888 finished \tANN training loss 1.036682\n",
      ">> Epoch 889 finished \tANN training loss 1.036669\n",
      ">> Epoch 890 finished \tANN training loss 1.036669\n",
      ">> Epoch 891 finished \tANN training loss 1.036711\n",
      ">> Epoch 892 finished \tANN training loss 1.036716\n",
      ">> Epoch 893 finished \tANN training loss 1.036850\n",
      ">> Epoch 894 finished \tANN training loss 1.036813\n",
      ">> Epoch 895 finished \tANN training loss 1.036665\n",
      ">> Epoch 896 finished \tANN training loss 1.036864\n",
      ">> Epoch 897 finished \tANN training loss 1.036829\n",
      ">> Epoch 898 finished \tANN training loss 1.036668\n",
      ">> Epoch 899 finished \tANN training loss 1.036773\n",
      ">> Epoch 900 finished \tANN training loss 1.037763\n",
      ">> Epoch 901 finished \tANN training loss 1.038939\n",
      ">> Epoch 902 finished \tANN training loss 1.039520\n",
      ">> Epoch 903 finished \tANN training loss 1.038522\n",
      ">> Epoch 904 finished \tANN training loss 1.037145\n",
      ">> Epoch 905 finished \tANN training loss 1.037094\n",
      ">> Epoch 906 finished \tANN training loss 1.037298\n",
      ">> Epoch 907 finished \tANN training loss 1.037877\n",
      ">> Epoch 908 finished \tANN training loss 1.037126\n",
      ">> Epoch 909 finished \tANN training loss 1.036750\n",
      ">> Epoch 910 finished \tANN training loss 1.036815\n",
      ">> Epoch 911 finished \tANN training loss 1.037459\n",
      ">> Epoch 912 finished \tANN training loss 1.037756\n",
      ">> Epoch 913 finished \tANN training loss 1.037664\n",
      ">> Epoch 914 finished \tANN training loss 1.036867\n",
      ">> Epoch 915 finished \tANN training loss 1.036841\n",
      ">> Epoch 916 finished \tANN training loss 1.036742\n",
      ">> Epoch 917 finished \tANN training loss 1.036703\n",
      ">> Epoch 918 finished \tANN training loss 1.037725\n",
      ">> Epoch 919 finished \tANN training loss 1.036778\n",
      ">> Epoch 920 finished \tANN training loss 1.039755\n",
      ">> Epoch 921 finished \tANN training loss 1.037689\n",
      ">> Epoch 922 finished \tANN training loss 1.036689\n",
      ">> Epoch 923 finished \tANN training loss 1.036957\n",
      ">> Epoch 924 finished \tANN training loss 1.037140\n",
      ">> Epoch 925 finished \tANN training loss 1.036699\n",
      ">> Epoch 926 finished \tANN training loss 1.036663\n",
      ">> Epoch 927 finished \tANN training loss 1.036682\n",
      ">> Epoch 928 finished \tANN training loss 1.038097\n",
      ">> Epoch 929 finished \tANN training loss 1.036704\n",
      ">> Epoch 930 finished \tANN training loss 1.036666\n",
      ">> Epoch 931 finished \tANN training loss 1.038372\n",
      ">> Epoch 932 finished \tANN training loss 1.037311\n",
      ">> Epoch 933 finished \tANN training loss 1.037785\n",
      ">> Epoch 934 finished \tANN training loss 1.037661\n",
      ">> Epoch 935 finished \tANN training loss 1.036778\n",
      ">> Epoch 936 finished \tANN training loss 1.036963\n",
      ">> Epoch 937 finished \tANN training loss 1.037277\n",
      ">> Epoch 938 finished \tANN training loss 1.039469\n",
      ">> Epoch 939 finished \tANN training loss 1.040370\n",
      ">> Epoch 940 finished \tANN training loss 1.037280\n",
      ">> Epoch 941 finished \tANN training loss 1.038955\n",
      ">> Epoch 942 finished \tANN training loss 1.038202\n",
      ">> Epoch 943 finished \tANN training loss 1.036725\n",
      ">> Epoch 944 finished \tANN training loss 1.037294\n",
      ">> Epoch 945 finished \tANN training loss 1.036665\n",
      ">> Epoch 946 finished \tANN training loss 1.036674\n",
      ">> Epoch 947 finished \tANN training loss 1.037303\n",
      ">> Epoch 948 finished \tANN training loss 1.036790\n",
      ">> Epoch 949 finished \tANN training loss 1.036703\n",
      ">> Epoch 950 finished \tANN training loss 1.036769\n",
      ">> Epoch 951 finished \tANN training loss 1.036673\n",
      ">> Epoch 952 finished \tANN training loss 1.037196\n",
      ">> Epoch 953 finished \tANN training loss 1.036670\n",
      ">> Epoch 954 finished \tANN training loss 1.036657\n",
      ">> Epoch 955 finished \tANN training loss 1.036972\n",
      ">> Epoch 956 finished \tANN training loss 1.036655\n",
      ">> Epoch 957 finished \tANN training loss 1.036657\n",
      ">> Epoch 958 finished \tANN training loss 1.037396\n",
      ">> Epoch 959 finished \tANN training loss 1.036813\n",
      ">> Epoch 960 finished \tANN training loss 1.037101\n",
      ">> Epoch 961 finished \tANN training loss 1.036683\n",
      ">> Epoch 962 finished \tANN training loss 1.036669\n",
      ">> Epoch 963 finished \tANN training loss 1.036733\n",
      ">> Epoch 964 finished \tANN training loss 1.036656\n",
      ">> Epoch 965 finished \tANN training loss 1.037253\n",
      ">> Epoch 966 finished \tANN training loss 1.037027\n",
      ">> Epoch 967 finished \tANN training loss 1.038065\n",
      ">> Epoch 968 finished \tANN training loss 1.037670\n",
      ">> Epoch 969 finished \tANN training loss 1.037587\n",
      ">> Epoch 970 finished \tANN training loss 1.037547\n",
      ">> Epoch 971 finished \tANN training loss 1.037005\n",
      ">> Epoch 972 finished \tANN training loss 1.036654\n",
      ">> Epoch 973 finished \tANN training loss 1.036853\n",
      ">> Epoch 974 finished \tANN training loss 1.036664\n",
      ">> Epoch 975 finished \tANN training loss 1.036833\n",
      ">> Epoch 976 finished \tANN training loss 1.037287\n",
      ">> Epoch 977 finished \tANN training loss 1.037899\n",
      ">> Epoch 978 finished \tANN training loss 1.037115\n",
      ">> Epoch 979 finished \tANN training loss 1.036882\n",
      ">> Epoch 980 finished \tANN training loss 1.037128\n",
      ">> Epoch 981 finished \tANN training loss 1.037091\n",
      ">> Epoch 982 finished \tANN training loss 1.036842\n",
      ">> Epoch 983 finished \tANN training loss 1.036688\n",
      ">> Epoch 984 finished \tANN training loss 1.036888\n",
      ">> Epoch 985 finished \tANN training loss 1.038530\n",
      ">> Epoch 986 finished \tANN training loss 1.037369\n",
      ">> Epoch 987 finished \tANN training loss 1.036731\n",
      ">> Epoch 988 finished \tANN training loss 1.036662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 989 finished \tANN training loss 1.036653\n",
      ">> Epoch 990 finished \tANN training loss 1.036682\n",
      ">> Epoch 991 finished \tANN training loss 1.036982\n",
      ">> Epoch 992 finished \tANN training loss 1.038402\n",
      ">> Epoch 993 finished \tANN training loss 1.037063\n",
      ">> Epoch 994 finished \tANN training loss 1.036838\n",
      ">> Epoch 995 finished \tANN training loss 1.036886\n",
      ">> Epoch 996 finished \tANN training loss 1.038012\n",
      ">> Epoch 997 finished \tANN training loss 1.039173\n",
      ">> Epoch 998 finished \tANN training loss 1.040326\n",
      ">> Epoch 999 finished \tANN training loss 1.039360\n",
      "[END] Fine tuning step\n",
      "\n",
      "\n",
      "TRIAL:  5\n",
      "\n",
      "\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 5.623683\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 4.564537\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 4.437744\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 4.964074\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 5.948040\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 7.290006\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 9.381418\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 12.125279\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 19.064467\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 24.154513\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 288.820679\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 1751.164307\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 2811.362793\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 3394.583008\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 3730.412842\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 3880.657227\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 3863.493652\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 3843.302734\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 3827.601562\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 4006.683838\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.296848\n",
      ">> Epoch 1 finished \tANN training loss 1.129181\n",
      ">> Epoch 2 finished \tANN training loss 1.076922\n",
      ">> Epoch 3 finished \tANN training loss 1.042706\n",
      ">> Epoch 4 finished \tANN training loss 1.018564\n",
      ">> Epoch 5 finished \tANN training loss 0.996412\n",
      ">> Epoch 6 finished \tANN training loss 0.915022\n",
      ">> Epoch 7 finished \tANN training loss 0.623171\n",
      ">> Epoch 8 finished \tANN training loss 0.358822\n",
      ">> Epoch 9 finished \tANN training loss 0.301162\n",
      ">> Epoch 10 finished \tANN training loss 0.269584\n",
      ">> Epoch 11 finished \tANN training loss 0.232431\n",
      ">> Epoch 12 finished \tANN training loss 0.205452\n",
      ">> Epoch 13 finished \tANN training loss 0.191597\n",
      ">> Epoch 14 finished \tANN training loss 0.175858\n",
      ">> Epoch 15 finished \tANN training loss 0.156352\n",
      ">> Epoch 16 finished \tANN training loss 0.160553\n",
      ">> Epoch 17 finished \tANN training loss 0.156747\n",
      ">> Epoch 18 finished \tANN training loss 0.145146\n",
      ">> Epoch 19 finished \tANN training loss 0.152515\n",
      ">> Epoch 20 finished \tANN training loss 0.140465\n",
      ">> Epoch 21 finished \tANN training loss 0.137518\n",
      ">> Epoch 22 finished \tANN training loss 0.128328\n",
      ">> Epoch 23 finished \tANN training loss 0.136375\n",
      ">> Epoch 24 finished \tANN training loss 0.115045\n",
      ">> Epoch 25 finished \tANN training loss 0.114923\n",
      ">> Epoch 26 finished \tANN training loss 0.113886\n",
      ">> Epoch 27 finished \tANN training loss 0.122018\n",
      ">> Epoch 28 finished \tANN training loss 0.110400\n",
      ">> Epoch 29 finished \tANN training loss 0.105373\n",
      ">> Epoch 30 finished \tANN training loss 0.105472\n",
      ">> Epoch 31 finished \tANN training loss 0.110487\n",
      ">> Epoch 32 finished \tANN training loss 0.096099\n",
      ">> Epoch 33 finished \tANN training loss 0.104902\n",
      ">> Epoch 34 finished \tANN training loss 0.105823\n",
      ">> Epoch 35 finished \tANN training loss 0.100684\n",
      ">> Epoch 36 finished \tANN training loss 0.130206\n",
      ">> Epoch 37 finished \tANN training loss 0.105502\n",
      ">> Epoch 38 finished \tANN training loss 0.089751\n",
      ">> Epoch 39 finished \tANN training loss 0.098151\n",
      ">> Epoch 40 finished \tANN training loss 0.091680\n",
      ">> Epoch 41 finished \tANN training loss 0.093436\n",
      ">> Epoch 42 finished \tANN training loss 0.090159\n",
      ">> Epoch 43 finished \tANN training loss 0.097017\n",
      ">> Epoch 44 finished \tANN training loss 0.103870\n",
      ">> Epoch 45 finished \tANN training loss 0.086013\n",
      ">> Epoch 46 finished \tANN training loss 0.081165\n",
      ">> Epoch 47 finished \tANN training loss 0.087839\n",
      ">> Epoch 48 finished \tANN training loss 0.080509\n",
      ">> Epoch 49 finished \tANN training loss 0.096003\n",
      ">> Epoch 50 finished \tANN training loss 0.082137\n",
      ">> Epoch 51 finished \tANN training loss 0.101915\n",
      ">> Epoch 52 finished \tANN training loss 0.075080\n",
      ">> Epoch 53 finished \tANN training loss 0.084168\n",
      ">> Epoch 54 finished \tANN training loss 0.075662\n",
      ">> Epoch 55 finished \tANN training loss 0.095626\n",
      ">> Epoch 56 finished \tANN training loss 0.072639\n",
      ">> Epoch 57 finished \tANN training loss 0.071696\n",
      ">> Epoch 58 finished \tANN training loss 0.078778\n",
      ">> Epoch 59 finished \tANN training loss 0.085069\n",
      ">> Epoch 60 finished \tANN training loss 0.068407\n",
      ">> Epoch 61 finished \tANN training loss 0.072067\n",
      ">> Epoch 62 finished \tANN training loss 0.073116\n",
      ">> Epoch 63 finished \tANN training loss 0.096644\n",
      ">> Epoch 64 finished \tANN training loss 0.066045\n",
      ">> Epoch 65 finished \tANN training loss 0.067273\n",
      ">> Epoch 66 finished \tANN training loss 0.068891\n",
      ">> Epoch 67 finished \tANN training loss 0.076202\n",
      ">> Epoch 68 finished \tANN training loss 0.076504\n",
      ">> Epoch 69 finished \tANN training loss 0.072888\n",
      ">> Epoch 70 finished \tANN training loss 0.064012\n",
      ">> Epoch 71 finished \tANN training loss 0.073641\n",
      ">> Epoch 72 finished \tANN training loss 0.066889\n",
      ">> Epoch 73 finished \tANN training loss 0.069185\n",
      ">> Epoch 74 finished \tANN training loss 0.063400\n",
      ">> Epoch 75 finished \tANN training loss 0.062188\n",
      ">> Epoch 76 finished \tANN training loss 0.065352\n",
      ">> Epoch 77 finished \tANN training loss 0.065261\n",
      ">> Epoch 78 finished \tANN training loss 0.061146\n",
      ">> Epoch 79 finished \tANN training loss 0.065333\n",
      ">> Epoch 80 finished \tANN training loss 0.079069\n",
      ">> Epoch 81 finished \tANN training loss 0.061146\n",
      ">> Epoch 82 finished \tANN training loss 0.100154\n",
      ">> Epoch 83 finished \tANN training loss 0.061564\n",
      ">> Epoch 84 finished \tANN training loss 0.061832\n",
      ">> Epoch 85 finished \tANN training loss 0.067085\n",
      ">> Epoch 86 finished \tANN training loss 0.068471\n",
      ">> Epoch 87 finished \tANN training loss 0.059180\n",
      ">> Epoch 88 finished \tANN training loss 0.085819\n",
      ">> Epoch 89 finished \tANN training loss 0.057780\n",
      ">> Epoch 90 finished \tANN training loss 0.064808\n",
      ">> Epoch 91 finished \tANN training loss 0.061822\n",
      ">> Epoch 92 finished \tANN training loss 0.069773\n",
      ">> Epoch 93 finished \tANN training loss 0.071176\n",
      ">> Epoch 94 finished \tANN training loss 0.076260\n",
      ">> Epoch 95 finished \tANN training loss 0.059135\n",
      ">> Epoch 96 finished \tANN training loss 0.061040\n",
      ">> Epoch 97 finished \tANN training loss 0.060125\n",
      ">> Epoch 98 finished \tANN training loss 0.062115\n",
      ">> Epoch 99 finished \tANN training loss 0.063979\n",
      ">> Epoch 100 finished \tANN training loss 0.070666\n",
      ">> Epoch 101 finished \tANN training loss 0.067575\n",
      ">> Epoch 102 finished \tANN training loss 0.058457\n",
      ">> Epoch 103 finished \tANN training loss 0.061130\n",
      ">> Epoch 104 finished \tANN training loss 0.064203\n",
      ">> Epoch 105 finished \tANN training loss 0.058876\n",
      ">> Epoch 106 finished \tANN training loss 0.057847\n",
      ">> Epoch 107 finished \tANN training loss 0.056881\n",
      ">> Epoch 108 finished \tANN training loss 0.056572\n",
      ">> Epoch 109 finished \tANN training loss 0.058924\n",
      ">> Epoch 110 finished \tANN training loss 0.055484\n",
      ">> Epoch 111 finished \tANN training loss 0.069514\n",
      ">> Epoch 112 finished \tANN training loss 0.057376\n",
      ">> Epoch 113 finished \tANN training loss 0.081762\n",
      ">> Epoch 114 finished \tANN training loss 0.066482\n",
      ">> Epoch 115 finished \tANN training loss 0.072481\n",
      ">> Epoch 116 finished \tANN training loss 0.055509\n",
      ">> Epoch 117 finished \tANN training loss 0.061033\n",
      ">> Epoch 118 finished \tANN training loss 0.081600\n",
      ">> Epoch 119 finished \tANN training loss 0.054475\n",
      ">> Epoch 120 finished \tANN training loss 0.060399\n",
      ">> Epoch 121 finished \tANN training loss 0.057366\n",
      ">> Epoch 122 finished \tANN training loss 0.054080\n",
      ">> Epoch 123 finished \tANN training loss 0.068334\n",
      ">> Epoch 124 finished \tANN training loss 0.058661\n",
      ">> Epoch 125 finished \tANN training loss 0.059545\n",
      ">> Epoch 126 finished \tANN training loss 0.071685\n",
      ">> Epoch 127 finished \tANN training loss 0.062970\n",
      ">> Epoch 128 finished \tANN training loss 0.057111\n",
      ">> Epoch 129 finished \tANN training loss 0.055789\n",
      ">> Epoch 130 finished \tANN training loss 0.056981\n",
      ">> Epoch 131 finished \tANN training loss 0.060322\n",
      ">> Epoch 132 finished \tANN training loss 0.061552\n",
      ">> Epoch 133 finished \tANN training loss 0.052313\n",
      ">> Epoch 134 finished \tANN training loss 0.052858\n",
      ">> Epoch 135 finished \tANN training loss 0.051855\n",
      ">> Epoch 136 finished \tANN training loss 0.055882\n",
      ">> Epoch 137 finished \tANN training loss 0.051442\n",
      ">> Epoch 138 finished \tANN training loss 0.050242\n",
      ">> Epoch 139 finished \tANN training loss 0.056504\n",
      ">> Epoch 140 finished \tANN training loss 0.058818\n",
      ">> Epoch 141 finished \tANN training loss 0.054656\n",
      ">> Epoch 142 finished \tANN training loss 0.097584\n",
      ">> Epoch 143 finished \tANN training loss 0.055360\n",
      ">> Epoch 144 finished \tANN training loss 0.089198\n",
      ">> Epoch 145 finished \tANN training loss 0.053713\n",
      ">> Epoch 146 finished \tANN training loss 0.050596\n",
      ">> Epoch 147 finished \tANN training loss 0.053763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 148 finished \tANN training loss 0.056106\n",
      ">> Epoch 149 finished \tANN training loss 0.049556\n",
      ">> Epoch 150 finished \tANN training loss 0.054563\n",
      ">> Epoch 151 finished \tANN training loss 0.067119\n",
      ">> Epoch 152 finished \tANN training loss 0.055102\n",
      ">> Epoch 153 finished \tANN training loss 0.062698\n",
      ">> Epoch 154 finished \tANN training loss 0.059525\n",
      ">> Epoch 155 finished \tANN training loss 0.072309\n",
      ">> Epoch 156 finished \tANN training loss 0.055912\n",
      ">> Epoch 157 finished \tANN training loss 0.056825\n",
      ">> Epoch 158 finished \tANN training loss 0.048930\n",
      ">> Epoch 159 finished \tANN training loss 0.077534\n",
      ">> Epoch 160 finished \tANN training loss 0.051203\n",
      ">> Epoch 161 finished \tANN training loss 0.053609\n",
      ">> Epoch 162 finished \tANN training loss 0.050122\n",
      ">> Epoch 163 finished \tANN training loss 0.048930\n",
      ">> Epoch 164 finished \tANN training loss 0.047971\n",
      ">> Epoch 165 finished \tANN training loss 0.048796\n",
      ">> Epoch 166 finished \tANN training loss 0.048276\n",
      ">> Epoch 167 finished \tANN training loss 0.049129\n",
      ">> Epoch 168 finished \tANN training loss 0.085767\n",
      ">> Epoch 169 finished \tANN training loss 0.075712\n",
      ">> Epoch 170 finished \tANN training loss 0.051465\n",
      ">> Epoch 171 finished \tANN training loss 0.054832\n",
      ">> Epoch 172 finished \tANN training loss 0.068718\n",
      ">> Epoch 173 finished \tANN training loss 0.049205\n",
      ">> Epoch 174 finished \tANN training loss 0.052173\n",
      ">> Epoch 175 finished \tANN training loss 0.080859\n",
      ">> Epoch 176 finished \tANN training loss 0.055140\n",
      ">> Epoch 177 finished \tANN training loss 0.052191\n",
      ">> Epoch 178 finished \tANN training loss 0.046826\n",
      ">> Epoch 179 finished \tANN training loss 0.049033\n",
      ">> Epoch 180 finished \tANN training loss 0.054433\n",
      ">> Epoch 181 finished \tANN training loss 0.057471\n",
      ">> Epoch 182 finished \tANN training loss 0.049329\n",
      ">> Epoch 183 finished \tANN training loss 0.071922\n",
      ">> Epoch 184 finished \tANN training loss 0.051982\n",
      ">> Epoch 185 finished \tANN training loss 0.051507\n",
      ">> Epoch 186 finished \tANN training loss 0.063084\n",
      ">> Epoch 187 finished \tANN training loss 0.049871\n",
      ">> Epoch 188 finished \tANN training loss 0.049046\n",
      ">> Epoch 189 finished \tANN training loss 0.063094\n",
      ">> Epoch 190 finished \tANN training loss 0.062775\n",
      ">> Epoch 191 finished \tANN training loss 0.064591\n",
      ">> Epoch 192 finished \tANN training loss 0.051928\n",
      ">> Epoch 193 finished \tANN training loss 0.046758\n",
      ">> Epoch 194 finished \tANN training loss 0.047629\n",
      ">> Epoch 195 finished \tANN training loss 0.054064\n",
      ">> Epoch 196 finished \tANN training loss 0.054780\n",
      ">> Epoch 197 finished \tANN training loss 0.047324\n",
      ">> Epoch 198 finished \tANN training loss 0.054391\n",
      ">> Epoch 199 finished \tANN training loss 0.118309\n",
      ">> Epoch 200 finished \tANN training loss 0.059065\n",
      ">> Epoch 201 finished \tANN training loss 0.053831\n",
      ">> Epoch 202 finished \tANN training loss 0.097391\n",
      ">> Epoch 203 finished \tANN training loss 0.093258\n",
      ">> Epoch 204 finished \tANN training loss 0.052383\n",
      ">> Epoch 205 finished \tANN training loss 0.045284\n",
      ">> Epoch 206 finished \tANN training loss 0.057917\n",
      ">> Epoch 207 finished \tANN training loss 0.056450\n",
      ">> Epoch 208 finished \tANN training loss 0.057194\n",
      ">> Epoch 209 finished \tANN training loss 0.046022\n",
      ">> Epoch 210 finished \tANN training loss 0.060076\n",
      ">> Epoch 211 finished \tANN training loss 0.072815\n",
      ">> Epoch 212 finished \tANN training loss 0.060437\n",
      ">> Epoch 213 finished \tANN training loss 0.059207\n",
      ">> Epoch 214 finished \tANN training loss 0.058757\n",
      ">> Epoch 215 finished \tANN training loss 0.061580\n",
      ">> Epoch 216 finished \tANN training loss 0.049432\n",
      ">> Epoch 217 finished \tANN training loss 0.050986\n",
      ">> Epoch 218 finished \tANN training loss 0.071204\n",
      ">> Epoch 219 finished \tANN training loss 0.049352\n",
      ">> Epoch 220 finished \tANN training loss 0.044992\n",
      ">> Epoch 221 finished \tANN training loss 0.070480\n",
      ">> Epoch 222 finished \tANN training loss 0.050189\n",
      ">> Epoch 223 finished \tANN training loss 0.055971\n",
      ">> Epoch 224 finished \tANN training loss 0.053670\n",
      ">> Epoch 225 finished \tANN training loss 0.043646\n",
      ">> Epoch 226 finished \tANN training loss 0.055471\n",
      ">> Epoch 227 finished \tANN training loss 0.099275\n",
      ">> Epoch 228 finished \tANN training loss 0.048600\n",
      ">> Epoch 229 finished \tANN training loss 0.049425\n",
      ">> Epoch 230 finished \tANN training loss 0.044941\n",
      ">> Epoch 231 finished \tANN training loss 0.045866\n",
      ">> Epoch 232 finished \tANN training loss 0.049420\n",
      ">> Epoch 233 finished \tANN training loss 0.051032\n",
      ">> Epoch 234 finished \tANN training loss 0.046374\n",
      ">> Epoch 235 finished \tANN training loss 0.059194\n",
      ">> Epoch 236 finished \tANN training loss 0.053859\n",
      ">> Epoch 237 finished \tANN training loss 0.073634\n",
      ">> Epoch 238 finished \tANN training loss 0.067020\n",
      ">> Epoch 239 finished \tANN training loss 0.055992\n",
      ">> Epoch 240 finished \tANN training loss 0.045850\n",
      ">> Epoch 241 finished \tANN training loss 0.056498\n",
      ">> Epoch 242 finished \tANN training loss 0.060215\n",
      ">> Epoch 243 finished \tANN training loss 0.044280\n",
      ">> Epoch 244 finished \tANN training loss 0.044637\n",
      ">> Epoch 245 finished \tANN training loss 0.044104\n",
      ">> Epoch 246 finished \tANN training loss 0.060737\n",
      ">> Epoch 247 finished \tANN training loss 0.048690\n",
      ">> Epoch 248 finished \tANN training loss 0.099136\n",
      ">> Epoch 249 finished \tANN training loss 0.062680\n",
      ">> Epoch 250 finished \tANN training loss 0.042238\n",
      ">> Epoch 251 finished \tANN training loss 0.057807\n",
      ">> Epoch 252 finished \tANN training loss 0.044980\n",
      ">> Epoch 253 finished \tANN training loss 0.086686\n",
      ">> Epoch 254 finished \tANN training loss 0.072916\n",
      ">> Epoch 255 finished \tANN training loss 0.077980\n",
      ">> Epoch 256 finished \tANN training loss 0.041320\n",
      ">> Epoch 257 finished \tANN training loss 0.059468\n",
      ">> Epoch 258 finished \tANN training loss 0.050058\n",
      ">> Epoch 259 finished \tANN training loss 0.069266\n",
      ">> Epoch 260 finished \tANN training loss 0.047445\n",
      ">> Epoch 261 finished \tANN training loss 0.066614\n",
      ">> Epoch 262 finished \tANN training loss 0.054575\n",
      ">> Epoch 263 finished \tANN training loss 0.061035\n",
      ">> Epoch 264 finished \tANN training loss 0.041189\n",
      ">> Epoch 265 finished \tANN training loss 0.055768\n",
      ">> Epoch 266 finished \tANN training loss 0.069028\n",
      ">> Epoch 267 finished \tANN training loss 0.043457\n",
      ">> Epoch 268 finished \tANN training loss 0.053468\n",
      ">> Epoch 269 finished \tANN training loss 0.083593\n",
      ">> Epoch 270 finished \tANN training loss 0.049679\n",
      ">> Epoch 271 finished \tANN training loss 0.047650\n",
      ">> Epoch 272 finished \tANN training loss 0.069505\n",
      ">> Epoch 273 finished \tANN training loss 0.041710\n",
      ">> Epoch 274 finished \tANN training loss 0.059635\n",
      ">> Epoch 275 finished \tANN training loss 0.040369\n",
      ">> Epoch 276 finished \tANN training loss 0.040364\n",
      ">> Epoch 277 finished \tANN training loss 0.043285\n",
      ">> Epoch 278 finished \tANN training loss 0.049441\n",
      ">> Epoch 279 finished \tANN training loss 0.051392\n",
      ">> Epoch 280 finished \tANN training loss 0.070681\n",
      ">> Epoch 281 finished \tANN training loss 0.045545\n",
      ">> Epoch 282 finished \tANN training loss 0.046840\n",
      ">> Epoch 283 finished \tANN training loss 0.073475\n",
      ">> Epoch 284 finished \tANN training loss 0.041280\n",
      ">> Epoch 285 finished \tANN training loss 0.074676\n",
      ">> Epoch 286 finished \tANN training loss 0.077319\n",
      ">> Epoch 287 finished \tANN training loss 0.047385\n",
      ">> Epoch 288 finished \tANN training loss 0.053373\n",
      ">> Epoch 289 finished \tANN training loss 0.044472\n",
      ">> Epoch 290 finished \tANN training loss 0.067867\n",
      ">> Epoch 291 finished \tANN training loss 0.043102\n",
      ">> Epoch 292 finished \tANN training loss 0.039732\n",
      ">> Epoch 293 finished \tANN training loss 0.042109\n",
      ">> Epoch 294 finished \tANN training loss 0.042188\n",
      ">> Epoch 295 finished \tANN training loss 0.071534\n",
      ">> Epoch 296 finished \tANN training loss 0.069707\n",
      ">> Epoch 297 finished \tANN training loss 0.047684\n",
      ">> Epoch 298 finished \tANN training loss 0.042289\n",
      ">> Epoch 299 finished \tANN training loss 0.063511\n",
      ">> Epoch 300 finished \tANN training loss 0.047229\n",
      ">> Epoch 301 finished \tANN training loss 0.045510\n",
      ">> Epoch 302 finished \tANN training loss 0.052838\n",
      ">> Epoch 303 finished \tANN training loss 0.054133\n",
      ">> Epoch 304 finished \tANN training loss 0.050987\n",
      ">> Epoch 305 finished \tANN training loss 0.052790\n",
      ">> Epoch 306 finished \tANN training loss 0.042203\n",
      ">> Epoch 307 finished \tANN training loss 0.042444\n",
      ">> Epoch 308 finished \tANN training loss 0.080631\n",
      ">> Epoch 309 finished \tANN training loss 0.061134\n",
      ">> Epoch 310 finished \tANN training loss 0.048775\n",
      ">> Epoch 311 finished \tANN training loss 0.054733\n",
      ">> Epoch 312 finished \tANN training loss 0.044516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 313 finished \tANN training loss 0.043695\n",
      ">> Epoch 314 finished \tANN training loss 0.103693\n",
      ">> Epoch 315 finished \tANN training loss 0.042466\n",
      ">> Epoch 316 finished \tANN training loss 0.057936\n",
      ">> Epoch 317 finished \tANN training loss 0.042968\n",
      ">> Epoch 318 finished \tANN training loss 0.053963\n",
      ">> Epoch 319 finished \tANN training loss 0.043873\n",
      ">> Epoch 320 finished \tANN training loss 0.060631\n",
      ">> Epoch 321 finished \tANN training loss 0.044680\n",
      ">> Epoch 322 finished \tANN training loss 0.040939\n",
      ">> Epoch 323 finished \tANN training loss 0.044940\n",
      ">> Epoch 324 finished \tANN training loss 0.051821\n",
      ">> Epoch 325 finished \tANN training loss 0.060934\n",
      ">> Epoch 326 finished \tANN training loss 0.065815\n",
      ">> Epoch 327 finished \tANN training loss 0.038462\n",
      ">> Epoch 328 finished \tANN training loss 0.054767\n",
      ">> Epoch 329 finished \tANN training loss 0.073524\n",
      ">> Epoch 330 finished \tANN training loss 0.050706\n",
      ">> Epoch 331 finished \tANN training loss 0.048268\n",
      ">> Epoch 332 finished \tANN training loss 0.054423\n",
      ">> Epoch 333 finished \tANN training loss 0.045348\n",
      ">> Epoch 334 finished \tANN training loss 0.060424\n",
      ">> Epoch 335 finished \tANN training loss 0.038871\n",
      ">> Epoch 336 finished \tANN training loss 0.059322\n",
      ">> Epoch 337 finished \tANN training loss 0.059102\n",
      ">> Epoch 338 finished \tANN training loss 0.040897\n",
      ">> Epoch 339 finished \tANN training loss 0.047092\n",
      ">> Epoch 340 finished \tANN training loss 0.057743\n",
      ">> Epoch 341 finished \tANN training loss 0.057699\n",
      ">> Epoch 342 finished \tANN training loss 0.050076\n",
      ">> Epoch 343 finished \tANN training loss 0.045620\n",
      ">> Epoch 344 finished \tANN training loss 0.046339\n",
      ">> Epoch 345 finished \tANN training loss 0.046224\n",
      ">> Epoch 346 finished \tANN training loss 0.045173\n",
      ">> Epoch 347 finished \tANN training loss 0.053275\n",
      ">> Epoch 348 finished \tANN training loss 0.043910\n",
      ">> Epoch 349 finished \tANN training loss 0.073969\n",
      ">> Epoch 350 finished \tANN training loss 0.043478\n",
      ">> Epoch 351 finished \tANN training loss 0.066058\n",
      ">> Epoch 352 finished \tANN training loss 0.051904\n",
      ">> Epoch 353 finished \tANN training loss 0.055989\n",
      ">> Epoch 354 finished \tANN training loss 0.052499\n",
      ">> Epoch 355 finished \tANN training loss 0.037546\n",
      ">> Epoch 356 finished \tANN training loss 0.043585\n",
      ">> Epoch 357 finished \tANN training loss 0.066570\n",
      ">> Epoch 358 finished \tANN training loss 0.045937\n",
      ">> Epoch 359 finished \tANN training loss 0.056152\n",
      ">> Epoch 360 finished \tANN training loss 0.041617\n",
      ">> Epoch 361 finished \tANN training loss 0.043776\n",
      ">> Epoch 362 finished \tANN training loss 0.081274\n",
      ">> Epoch 363 finished \tANN training loss 0.053557\n",
      ">> Epoch 364 finished \tANN training loss 0.059184\n",
      ">> Epoch 365 finished \tANN training loss 0.064594\n",
      ">> Epoch 366 finished \tANN training loss 0.051017\n",
      ">> Epoch 367 finished \tANN training loss 0.045628\n",
      ">> Epoch 368 finished \tANN training loss 0.039243\n",
      ">> Epoch 369 finished \tANN training loss 0.055035\n",
      ">> Epoch 370 finished \tANN training loss 0.045169\n",
      ">> Epoch 371 finished \tANN training loss 0.043597\n",
      ">> Epoch 372 finished \tANN training loss 0.042836\n",
      ">> Epoch 373 finished \tANN training loss 0.040062\n",
      ">> Epoch 374 finished \tANN training loss 0.038363\n",
      ">> Epoch 375 finished \tANN training loss 0.040907\n",
      ">> Epoch 376 finished \tANN training loss 0.044551\n",
      ">> Epoch 377 finished \tANN training loss 0.044809\n",
      ">> Epoch 378 finished \tANN training loss 0.037785\n",
      ">> Epoch 379 finished \tANN training loss 0.093527\n",
      ">> Epoch 380 finished \tANN training loss 0.090734\n",
      ">> Epoch 381 finished \tANN training loss 0.064957\n",
      ">> Epoch 382 finished \tANN training loss 0.049043\n",
      ">> Epoch 383 finished \tANN training loss 0.045725\n",
      ">> Epoch 384 finished \tANN training loss 0.055613\n",
      ">> Epoch 385 finished \tANN training loss 0.080877\n",
      ">> Epoch 386 finished \tANN training loss 0.059739\n",
      ">> Epoch 387 finished \tANN training loss 0.047136\n",
      ">> Epoch 388 finished \tANN training loss 0.043725\n",
      ">> Epoch 389 finished \tANN training loss 0.052322\n",
      ">> Epoch 390 finished \tANN training loss 0.038410\n",
      ">> Epoch 391 finished \tANN training loss 0.062222\n",
      ">> Epoch 392 finished \tANN training loss 0.053900\n",
      ">> Epoch 393 finished \tANN training loss 0.046474\n",
      ">> Epoch 394 finished \tANN training loss 0.038471\n",
      ">> Epoch 395 finished \tANN training loss 0.050889\n",
      ">> Epoch 396 finished \tANN training loss 0.039872\n",
      ">> Epoch 397 finished \tANN training loss 0.041975\n",
      ">> Epoch 398 finished \tANN training loss 0.049431\n",
      ">> Epoch 399 finished \tANN training loss 0.045633\n",
      ">> Epoch 400 finished \tANN training loss 0.047448\n",
      ">> Epoch 401 finished \tANN training loss 0.038553\n",
      ">> Epoch 402 finished \tANN training loss 0.043021\n",
      ">> Epoch 403 finished \tANN training loss 0.047487\n",
      ">> Epoch 404 finished \tANN training loss 0.042496\n",
      ">> Epoch 405 finished \tANN training loss 0.040851\n",
      ">> Epoch 406 finished \tANN training loss 0.050614\n",
      ">> Epoch 407 finished \tANN training loss 0.071592\n",
      ">> Epoch 408 finished \tANN training loss 0.050234\n",
      ">> Epoch 409 finished \tANN training loss 0.047570\n",
      ">> Epoch 410 finished \tANN training loss 0.068899\n",
      ">> Epoch 411 finished \tANN training loss 0.039013\n",
      ">> Epoch 412 finished \tANN training loss 0.050750\n",
      ">> Epoch 413 finished \tANN training loss 0.051853\n",
      ">> Epoch 414 finished \tANN training loss 0.054776\n",
      ">> Epoch 415 finished \tANN training loss 0.037400\n",
      ">> Epoch 416 finished \tANN training loss 0.037123\n",
      ">> Epoch 417 finished \tANN training loss 0.062265\n",
      ">> Epoch 418 finished \tANN training loss 0.040604\n",
      ">> Epoch 419 finished \tANN training loss 0.047761\n",
      ">> Epoch 420 finished \tANN training loss 0.045090\n",
      ">> Epoch 421 finished \tANN training loss 0.060529\n",
      ">> Epoch 422 finished \tANN training loss 0.038598\n",
      ">> Epoch 423 finished \tANN training loss 0.048600\n",
      ">> Epoch 424 finished \tANN training loss 0.038636\n",
      ">> Epoch 425 finished \tANN training loss 0.040312\n",
      ">> Epoch 426 finished \tANN training loss 0.040886\n",
      ">> Epoch 427 finished \tANN training loss 0.039840\n",
      ">> Epoch 428 finished \tANN training loss 0.038594\n",
      ">> Epoch 429 finished \tANN training loss 0.044878\n",
      ">> Epoch 430 finished \tANN training loss 0.048164\n",
      ">> Epoch 431 finished \tANN training loss 0.051032\n",
      ">> Epoch 432 finished \tANN training loss 0.048348\n",
      ">> Epoch 433 finished \tANN training loss 0.073948\n",
      ">> Epoch 434 finished \tANN training loss 0.059660\n",
      ">> Epoch 435 finished \tANN training loss 0.048386\n",
      ">> Epoch 436 finished \tANN training loss 0.075956\n",
      ">> Epoch 437 finished \tANN training loss 0.039350\n",
      ">> Epoch 438 finished \tANN training loss 0.056831\n",
      ">> Epoch 439 finished \tANN training loss 0.079900\n",
      ">> Epoch 440 finished \tANN training loss 0.060248\n",
      ">> Epoch 441 finished \tANN training loss 0.048993\n",
      ">> Epoch 442 finished \tANN training loss 0.045761\n",
      ">> Epoch 443 finished \tANN training loss 0.077701\n",
      ">> Epoch 444 finished \tANN training loss 0.056817\n",
      ">> Epoch 445 finished \tANN training loss 0.080702\n",
      ">> Epoch 446 finished \tANN training loss 0.037993\n",
      ">> Epoch 447 finished \tANN training loss 0.058853\n",
      ">> Epoch 448 finished \tANN training loss 0.048533\n",
      ">> Epoch 449 finished \tANN training loss 0.053256\n",
      ">> Epoch 450 finished \tANN training loss 0.048638\n",
      ">> Epoch 451 finished \tANN training loss 0.071745\n",
      ">> Epoch 452 finished \tANN training loss 0.042171\n",
      ">> Epoch 453 finished \tANN training loss 0.044398\n",
      ">> Epoch 454 finished \tANN training loss 0.051686\n",
      ">> Epoch 455 finished \tANN training loss 0.063493\n",
      ">> Epoch 456 finished \tANN training loss 0.042269\n",
      ">> Epoch 457 finished \tANN training loss 0.056141\n",
      ">> Epoch 458 finished \tANN training loss 0.054618\n",
      ">> Epoch 459 finished \tANN training loss 0.038358\n",
      ">> Epoch 460 finished \tANN training loss 0.038529\n",
      ">> Epoch 461 finished \tANN training loss 0.133695\n",
      ">> Epoch 462 finished \tANN training loss 0.062974\n",
      ">> Epoch 463 finished \tANN training loss 0.039305\n",
      ">> Epoch 464 finished \tANN training loss 0.051320\n",
      ">> Epoch 465 finished \tANN training loss 0.045161\n",
      ">> Epoch 466 finished \tANN training loss 0.065706\n",
      ">> Epoch 467 finished \tANN training loss 0.051997\n",
      ">> Epoch 468 finished \tANN training loss 0.037285\n",
      ">> Epoch 469 finished \tANN training loss 0.045120\n",
      ">> Epoch 470 finished \tANN training loss 0.041363\n",
      ">> Epoch 471 finished \tANN training loss 0.048335\n",
      ">> Epoch 472 finished \tANN training loss 0.037248\n",
      ">> Epoch 473 finished \tANN training loss 0.064709\n",
      ">> Epoch 474 finished \tANN training loss 0.060469\n",
      ">> Epoch 475 finished \tANN training loss 0.037736\n",
      ">> Epoch 476 finished \tANN training loss 0.052341\n",
      ">> Epoch 477 finished \tANN training loss 0.040937\n",
      ">> Epoch 478 finished \tANN training loss 0.053033\n",
      ">> Epoch 479 finished \tANN training loss 0.039379\n",
      ">> Epoch 480 finished \tANN training loss 0.053435\n",
      ">> Epoch 481 finished \tANN training loss 0.051819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 482 finished \tANN training loss 0.053708\n",
      ">> Epoch 483 finished \tANN training loss 0.087163\n",
      ">> Epoch 484 finished \tANN training loss 0.045110\n",
      ">> Epoch 485 finished \tANN training loss 0.036864\n",
      ">> Epoch 486 finished \tANN training loss 0.044214\n",
      ">> Epoch 487 finished \tANN training loss 0.046381\n",
      ">> Epoch 488 finished \tANN training loss 0.049013\n",
      ">> Epoch 489 finished \tANN training loss 0.037629\n",
      ">> Epoch 490 finished \tANN training loss 0.051023\n",
      ">> Epoch 491 finished \tANN training loss 0.060212\n",
      ">> Epoch 492 finished \tANN training loss 0.054183\n",
      ">> Epoch 493 finished \tANN training loss 0.037744\n",
      ">> Epoch 494 finished \tANN training loss 0.036470\n",
      ">> Epoch 495 finished \tANN training loss 0.044721\n",
      ">> Epoch 496 finished \tANN training loss 0.080103\n",
      ">> Epoch 497 finished \tANN training loss 0.042300\n",
      ">> Epoch 498 finished \tANN training loss 0.047875\n",
      ">> Epoch 499 finished \tANN training loss 0.081429\n",
      ">> Epoch 500 finished \tANN training loss 0.045803\n",
      ">> Epoch 501 finished \tANN training loss 0.087674\n",
      ">> Epoch 502 finished \tANN training loss 0.046645\n",
      ">> Epoch 503 finished \tANN training loss 0.054697\n",
      ">> Epoch 504 finished \tANN training loss 0.057235\n",
      ">> Epoch 505 finished \tANN training loss 0.063960\n",
      ">> Epoch 506 finished \tANN training loss 0.046649\n",
      ">> Epoch 507 finished \tANN training loss 0.054692\n",
      ">> Epoch 508 finished \tANN training loss 0.041704\n",
      ">> Epoch 509 finished \tANN training loss 0.078199\n",
      ">> Epoch 510 finished \tANN training loss 0.043266\n",
      ">> Epoch 511 finished \tANN training loss 0.041944\n",
      ">> Epoch 512 finished \tANN training loss 0.060285\n",
      ">> Epoch 513 finished \tANN training loss 0.055011\n",
      ">> Epoch 514 finished \tANN training loss 0.053355\n",
      ">> Epoch 515 finished \tANN training loss 0.040275\n",
      ">> Epoch 516 finished \tANN training loss 0.067998\n",
      ">> Epoch 517 finished \tANN training loss 0.044475\n",
      ">> Epoch 518 finished \tANN training loss 0.043623\n",
      ">> Epoch 519 finished \tANN training loss 0.038160\n",
      ">> Epoch 520 finished \tANN training loss 0.039055\n",
      ">> Epoch 521 finished \tANN training loss 0.048877\n",
      ">> Epoch 522 finished \tANN training loss 0.046366\n",
      ">> Epoch 523 finished \tANN training loss 0.038015\n",
      ">> Epoch 524 finished \tANN training loss 0.044931\n",
      ">> Epoch 525 finished \tANN training loss 0.073667\n",
      ">> Epoch 526 finished \tANN training loss 0.043313\n",
      ">> Epoch 527 finished \tANN training loss 0.048320\n",
      ">> Epoch 528 finished \tANN training loss 0.056366\n",
      ">> Epoch 529 finished \tANN training loss 0.044702\n",
      ">> Epoch 530 finished \tANN training loss 0.036431\n",
      ">> Epoch 531 finished \tANN training loss 0.047775\n",
      ">> Epoch 532 finished \tANN training loss 0.042379\n",
      ">> Epoch 533 finished \tANN training loss 0.038823\n",
      ">> Epoch 534 finished \tANN training loss 0.047358\n",
      ">> Epoch 535 finished \tANN training loss 0.046446\n",
      ">> Epoch 536 finished \tANN training loss 0.050799\n",
      ">> Epoch 537 finished \tANN training loss 0.048588\n",
      ">> Epoch 538 finished \tANN training loss 0.073534\n",
      ">> Epoch 539 finished \tANN training loss 0.045305\n",
      ">> Epoch 540 finished \tANN training loss 0.049878\n",
      ">> Epoch 541 finished \tANN training loss 0.037534\n",
      ">> Epoch 542 finished \tANN training loss 0.043700\n",
      ">> Epoch 543 finished \tANN training loss 0.053100\n",
      ">> Epoch 544 finished \tANN training loss 0.050419\n",
      ">> Epoch 545 finished \tANN training loss 0.035982\n",
      ">> Epoch 546 finished \tANN training loss 0.036283\n",
      ">> Epoch 547 finished \tANN training loss 0.037802\n",
      ">> Epoch 548 finished \tANN training loss 0.036035\n",
      ">> Epoch 549 finished \tANN training loss 0.039094\n",
      ">> Epoch 550 finished \tANN training loss 0.048596\n",
      ">> Epoch 551 finished \tANN training loss 0.050572\n",
      ">> Epoch 552 finished \tANN training loss 0.041619\n",
      ">> Epoch 553 finished \tANN training loss 0.040983\n",
      ">> Epoch 554 finished \tANN training loss 0.069637\n",
      ">> Epoch 555 finished \tANN training loss 0.061272\n",
      ">> Epoch 556 finished \tANN training loss 0.054578\n",
      ">> Epoch 557 finished \tANN training loss 0.038756\n",
      ">> Epoch 558 finished \tANN training loss 0.047253\n",
      ">> Epoch 559 finished \tANN training loss 0.041520\n",
      ">> Epoch 560 finished \tANN training loss 0.055452\n",
      ">> Epoch 561 finished \tANN training loss 0.038313\n",
      ">> Epoch 562 finished \tANN training loss 0.040763\n",
      ">> Epoch 563 finished \tANN training loss 0.042057\n",
      ">> Epoch 564 finished \tANN training loss 0.041451\n",
      ">> Epoch 565 finished \tANN training loss 0.060733\n",
      ">> Epoch 566 finished \tANN training loss 0.038419\n",
      ">> Epoch 567 finished \tANN training loss 0.042709\n",
      ">> Epoch 568 finished \tANN training loss 0.062650\n",
      ">> Epoch 569 finished \tANN training loss 0.048819\n",
      ">> Epoch 570 finished \tANN training loss 0.051595\n",
      ">> Epoch 571 finished \tANN training loss 0.037337\n",
      ">> Epoch 572 finished \tANN training loss 0.049130\n",
      ">> Epoch 573 finished \tANN training loss 0.041847\n",
      ">> Epoch 574 finished \tANN training loss 0.035337\n",
      ">> Epoch 575 finished \tANN training loss 0.051220\n",
      ">> Epoch 576 finished \tANN training loss 0.043260\n",
      ">> Epoch 577 finished \tANN training loss 0.054820\n",
      ">> Epoch 578 finished \tANN training loss 0.038645\n",
      ">> Epoch 579 finished \tANN training loss 0.038869\n",
      ">> Epoch 580 finished \tANN training loss 0.037365\n",
      ">> Epoch 581 finished \tANN training loss 0.046665\n",
      ">> Epoch 582 finished \tANN training loss 0.039768\n",
      ">> Epoch 583 finished \tANN training loss 0.036050\n",
      ">> Epoch 584 finished \tANN training loss 0.037373\n",
      ">> Epoch 585 finished \tANN training loss 0.047744\n",
      ">> Epoch 586 finished \tANN training loss 0.038472\n",
      ">> Epoch 587 finished \tANN training loss 0.043458\n",
      ">> Epoch 588 finished \tANN training loss 0.037494\n",
      ">> Epoch 589 finished \tANN training loss 0.039205\n",
      ">> Epoch 590 finished \tANN training loss 0.036321\n",
      ">> Epoch 591 finished \tANN training loss 0.039897\n",
      ">> Epoch 592 finished \tANN training loss 0.061330\n",
      ">> Epoch 593 finished \tANN training loss 0.052717\n",
      ">> Epoch 594 finished \tANN training loss 0.054731\n",
      ">> Epoch 595 finished \tANN training loss 0.041122\n",
      ">> Epoch 596 finished \tANN training loss 0.036991\n",
      ">> Epoch 597 finished \tANN training loss 0.063588\n",
      ">> Epoch 598 finished \tANN training loss 0.042311\n",
      ">> Epoch 599 finished \tANN training loss 0.045409\n",
      ">> Epoch 600 finished \tANN training loss 0.044115\n",
      ">> Epoch 601 finished \tANN training loss 0.049168\n",
      ">> Epoch 602 finished \tANN training loss 0.048262\n",
      ">> Epoch 603 finished \tANN training loss 0.050254\n",
      ">> Epoch 604 finished \tANN training loss 0.039457\n",
      ">> Epoch 605 finished \tANN training loss 0.034979\n",
      ">> Epoch 606 finished \tANN training loss 0.041730\n",
      ">> Epoch 607 finished \tANN training loss 0.036964\n",
      ">> Epoch 608 finished \tANN training loss 0.041255\n",
      ">> Epoch 609 finished \tANN training loss 0.037494\n",
      ">> Epoch 610 finished \tANN training loss 0.055837\n",
      ">> Epoch 611 finished \tANN training loss 0.034940\n",
      ">> Epoch 612 finished \tANN training loss 0.039508\n",
      ">> Epoch 613 finished \tANN training loss 0.039838\n",
      ">> Epoch 614 finished \tANN training loss 0.035418\n",
      ">> Epoch 615 finished \tANN training loss 0.047192\n",
      ">> Epoch 616 finished \tANN training loss 0.042338\n",
      ">> Epoch 617 finished \tANN training loss 0.035769\n",
      ">> Epoch 618 finished \tANN training loss 0.084964\n",
      ">> Epoch 619 finished \tANN training loss 0.041807\n",
      ">> Epoch 620 finished \tANN training loss 0.055375\n",
      ">> Epoch 621 finished \tANN training loss 0.047163\n",
      ">> Epoch 622 finished \tANN training loss 0.045338\n",
      ">> Epoch 623 finished \tANN training loss 0.049264\n",
      ">> Epoch 624 finished \tANN training loss 0.127695\n",
      ">> Epoch 625 finished \tANN training loss 0.038743\n",
      ">> Epoch 626 finished \tANN training loss 0.036352\n",
      ">> Epoch 627 finished \tANN training loss 0.045916\n",
      ">> Epoch 628 finished \tANN training loss 0.047243\n",
      ">> Epoch 629 finished \tANN training loss 0.037048\n",
      ">> Epoch 630 finished \tANN training loss 0.041137\n",
      ">> Epoch 631 finished \tANN training loss 0.036781\n",
      ">> Epoch 632 finished \tANN training loss 0.039740\n",
      ">> Epoch 633 finished \tANN training loss 0.035214\n",
      ">> Epoch 634 finished \tANN training loss 0.045612\n",
      ">> Epoch 635 finished \tANN training loss 0.051642\n",
      ">> Epoch 636 finished \tANN training loss 0.044367\n",
      ">> Epoch 637 finished \tANN training loss 0.038004\n",
      ">> Epoch 638 finished \tANN training loss 0.040929\n",
      ">> Epoch 639 finished \tANN training loss 0.047815\n",
      ">> Epoch 640 finished \tANN training loss 0.040216\n",
      ">> Epoch 641 finished \tANN training loss 0.033695\n",
      ">> Epoch 642 finished \tANN training loss 0.036730\n",
      ">> Epoch 643 finished \tANN training loss 0.035433\n",
      ">> Epoch 644 finished \tANN training loss 0.039215\n",
      ">> Epoch 645 finished \tANN training loss 0.035608\n",
      ">> Epoch 646 finished \tANN training loss 0.035947\n",
      ">> Epoch 647 finished \tANN training loss 0.046582\n",
      ">> Epoch 648 finished \tANN training loss 0.038934\n",
      ">> Epoch 649 finished \tANN training loss 0.040029\n",
      ">> Epoch 650 finished \tANN training loss 0.034005\n",
      ">> Epoch 651 finished \tANN training loss 0.036499\n",
      ">> Epoch 652 finished \tANN training loss 0.057588\n",
      ">> Epoch 653 finished \tANN training loss 0.054380\n",
      ">> Epoch 654 finished \tANN training loss 0.050266\n",
      ">> Epoch 655 finished \tANN training loss 0.032498\n",
      ">> Epoch 656 finished \tANN training loss 0.032901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 657 finished \tANN training loss 0.044350\n",
      ">> Epoch 658 finished \tANN training loss 0.046806\n",
      ">> Epoch 659 finished \tANN training loss 0.044626\n",
      ">> Epoch 660 finished \tANN training loss 0.053649\n",
      ">> Epoch 661 finished \tANN training loss 0.047305\n",
      ">> Epoch 662 finished \tANN training loss 0.060845\n",
      ">> Epoch 663 finished \tANN training loss 0.049622\n",
      ">> Epoch 664 finished \tANN training loss 0.034255\n",
      ">> Epoch 665 finished \tANN training loss 0.037533\n",
      ">> Epoch 666 finished \tANN training loss 0.034528\n",
      ">> Epoch 667 finished \tANN training loss 0.045782\n",
      ">> Epoch 668 finished \tANN training loss 0.055713\n",
      ">> Epoch 669 finished \tANN training loss 0.041755\n",
      ">> Epoch 670 finished \tANN training loss 0.039584\n",
      ">> Epoch 671 finished \tANN training loss 0.045444\n",
      ">> Epoch 672 finished \tANN training loss 0.039738\n",
      ">> Epoch 673 finished \tANN training loss 0.033406\n",
      ">> Epoch 674 finished \tANN training loss 0.040484\n",
      ">> Epoch 675 finished \tANN training loss 0.081363\n",
      ">> Epoch 676 finished \tANN training loss 0.035843\n",
      ">> Epoch 677 finished \tANN training loss 0.043704\n",
      ">> Epoch 678 finished \tANN training loss 0.040432\n",
      ">> Epoch 679 finished \tANN training loss 0.039513\n",
      ">> Epoch 680 finished \tANN training loss 0.036686\n",
      ">> Epoch 681 finished \tANN training loss 0.056197\n",
      ">> Epoch 682 finished \tANN training loss 0.053993\n",
      ">> Epoch 683 finished \tANN training loss 0.059203\n",
      ">> Epoch 684 finished \tANN training loss 0.040186\n",
      ">> Epoch 685 finished \tANN training loss 0.043815\n",
      ">> Epoch 686 finished \tANN training loss 0.034915\n",
      ">> Epoch 687 finished \tANN training loss 0.043446\n",
      ">> Epoch 688 finished \tANN training loss 0.037193\n",
      ">> Epoch 689 finished \tANN training loss 0.058459\n",
      ">> Epoch 690 finished \tANN training loss 0.077433\n",
      ">> Epoch 691 finished \tANN training loss 0.037066\n",
      ">> Epoch 692 finished \tANN training loss 0.051229\n",
      ">> Epoch 693 finished \tANN training loss 0.040072\n",
      ">> Epoch 694 finished \tANN training loss 0.052237\n",
      ">> Epoch 695 finished \tANN training loss 0.046411\n",
      ">> Epoch 696 finished \tANN training loss 0.062097\n",
      ">> Epoch 697 finished \tANN training loss 0.045420\n",
      ">> Epoch 698 finished \tANN training loss 0.042360\n",
      ">> Epoch 699 finished \tANN training loss 0.040233\n",
      ">> Epoch 700 finished \tANN training loss 0.046991\n",
      ">> Epoch 701 finished \tANN training loss 0.045183\n",
      ">> Epoch 702 finished \tANN training loss 0.049375\n",
      ">> Epoch 703 finished \tANN training loss 0.039180\n",
      ">> Epoch 704 finished \tANN training loss 0.038300\n",
      ">> Epoch 705 finished \tANN training loss 0.040721\n",
      ">> Epoch 706 finished \tANN training loss 0.045991\n",
      ">> Epoch 707 finished \tANN training loss 0.056674\n",
      ">> Epoch 708 finished \tANN training loss 0.038390\n",
      ">> Epoch 709 finished \tANN training loss 0.044880\n",
      ">> Epoch 710 finished \tANN training loss 0.062688\n",
      ">> Epoch 711 finished \tANN training loss 0.043734\n",
      ">> Epoch 712 finished \tANN training loss 0.046984\n",
      ">> Epoch 713 finished \tANN training loss 0.036843\n",
      ">> Epoch 714 finished \tANN training loss 0.037118\n",
      ">> Epoch 715 finished \tANN training loss 0.043604\n",
      ">> Epoch 716 finished \tANN training loss 0.054908\n",
      ">> Epoch 717 finished \tANN training loss 0.032942\n",
      ">> Epoch 718 finished \tANN training loss 0.033096\n",
      ">> Epoch 719 finished \tANN training loss 0.047080\n",
      ">> Epoch 720 finished \tANN training loss 0.045237\n",
      ">> Epoch 721 finished \tANN training loss 0.054094\n",
      ">> Epoch 722 finished \tANN training loss 0.036993\n",
      ">> Epoch 723 finished \tANN training loss 0.032862\n",
      ">> Epoch 724 finished \tANN training loss 0.037473\n",
      ">> Epoch 725 finished \tANN training loss 0.036375\n",
      ">> Epoch 726 finished \tANN training loss 0.070645\n",
      ">> Epoch 727 finished \tANN training loss 0.035480\n",
      ">> Epoch 728 finished \tANN training loss 0.034967\n",
      ">> Epoch 729 finished \tANN training loss 0.044740\n",
      ">> Epoch 730 finished \tANN training loss 0.046969\n",
      ">> Epoch 731 finished \tANN training loss 0.059573\n",
      ">> Epoch 732 finished \tANN training loss 0.034540\n",
      ">> Epoch 733 finished \tANN training loss 0.053357\n",
      ">> Epoch 734 finished \tANN training loss 0.052654\n",
      ">> Epoch 735 finished \tANN training loss 0.039267\n",
      ">> Epoch 736 finished \tANN training loss 0.048480\n",
      ">> Epoch 737 finished \tANN training loss 0.061031\n",
      ">> Epoch 738 finished \tANN training loss 0.036454\n",
      ">> Epoch 739 finished \tANN training loss 0.060213\n",
      ">> Epoch 740 finished \tANN training loss 0.035963\n",
      ">> Epoch 741 finished \tANN training loss 0.039130\n",
      ">> Epoch 742 finished \tANN training loss 0.040094\n",
      ">> Epoch 743 finished \tANN training loss 0.041024\n",
      ">> Epoch 744 finished \tANN training loss 0.042116\n",
      ">> Epoch 745 finished \tANN training loss 0.052302\n",
      ">> Epoch 746 finished \tANN training loss 0.033817\n",
      ">> Epoch 747 finished \tANN training loss 0.041587\n",
      ">> Epoch 748 finished \tANN training loss 0.039245\n",
      ">> Epoch 749 finished \tANN training loss 0.042145\n",
      ">> Epoch 750 finished \tANN training loss 0.049024\n",
      ">> Epoch 751 finished \tANN training loss 0.043690\n",
      ">> Epoch 752 finished \tANN training loss 0.052371\n",
      ">> Epoch 753 finished \tANN training loss 0.038419\n",
      ">> Epoch 754 finished \tANN training loss 0.057359\n",
      ">> Epoch 755 finished \tANN training loss 0.047707\n",
      ">> Epoch 756 finished \tANN training loss 0.035666\n",
      ">> Epoch 757 finished \tANN training loss 0.044943\n",
      ">> Epoch 758 finished \tANN training loss 0.040686\n",
      ">> Epoch 759 finished \tANN training loss 0.044673\n",
      ">> Epoch 760 finished \tANN training loss 0.051486\n",
      ">> Epoch 761 finished \tANN training loss 0.037858\n",
      ">> Epoch 762 finished \tANN training loss 0.046371\n",
      ">> Epoch 763 finished \tANN training loss 0.037433\n",
      ">> Epoch 764 finished \tANN training loss 0.040040\n",
      ">> Epoch 765 finished \tANN training loss 0.044369\n",
      ">> Epoch 766 finished \tANN training loss 0.034063\n",
      ">> Epoch 767 finished \tANN training loss 0.046493\n",
      ">> Epoch 768 finished \tANN training loss 0.046706\n",
      ">> Epoch 769 finished \tANN training loss 0.044017\n",
      ">> Epoch 770 finished \tANN training loss 0.038912\n",
      ">> Epoch 771 finished \tANN training loss 0.042028\n",
      ">> Epoch 772 finished \tANN training loss 0.043330\n",
      ">> Epoch 773 finished \tANN training loss 0.054549\n",
      ">> Epoch 774 finished \tANN training loss 0.054988\n",
      ">> Epoch 775 finished \tANN training loss 0.037856\n",
      ">> Epoch 776 finished \tANN training loss 0.057755\n",
      ">> Epoch 777 finished \tANN training loss 0.050814\n",
      ">> Epoch 778 finished \tANN training loss 0.048651\n",
      ">> Epoch 779 finished \tANN training loss 0.036675\n",
      ">> Epoch 780 finished \tANN training loss 0.041610\n",
      ">> Epoch 781 finished \tANN training loss 0.045254\n",
      ">> Epoch 782 finished \tANN training loss 0.055830\n",
      ">> Epoch 783 finished \tANN training loss 0.032535\n",
      ">> Epoch 784 finished \tANN training loss 0.061886\n",
      ">> Epoch 785 finished \tANN training loss 0.035836\n",
      ">> Epoch 786 finished \tANN training loss 0.033285\n",
      ">> Epoch 787 finished \tANN training loss 0.037115\n",
      ">> Epoch 788 finished \tANN training loss 0.048918\n",
      ">> Epoch 789 finished \tANN training loss 0.036132\n",
      ">> Epoch 790 finished \tANN training loss 0.033261\n",
      ">> Epoch 791 finished \tANN training loss 0.053988\n",
      ">> Epoch 792 finished \tANN training loss 0.044537\n",
      ">> Epoch 793 finished \tANN training loss 0.050569\n",
      ">> Epoch 794 finished \tANN training loss 0.046907\n",
      ">> Epoch 795 finished \tANN training loss 0.048367\n",
      ">> Epoch 796 finished \tANN training loss 0.033591\n",
      ">> Epoch 797 finished \tANN training loss 0.045995\n",
      ">> Epoch 798 finished \tANN training loss 0.048706\n",
      ">> Epoch 799 finished \tANN training loss 0.038319\n",
      ">> Epoch 800 finished \tANN training loss 0.034988\n",
      ">> Epoch 801 finished \tANN training loss 0.055580\n",
      ">> Epoch 802 finished \tANN training loss 0.049539\n",
      ">> Epoch 803 finished \tANN training loss 0.038749\n",
      ">> Epoch 804 finished \tANN training loss 0.051290\n",
      ">> Epoch 805 finished \tANN training loss 0.044112\n",
      ">> Epoch 806 finished \tANN training loss 0.039755\n",
      ">> Epoch 807 finished \tANN training loss 0.050586\n",
      ">> Epoch 808 finished \tANN training loss 0.053220\n",
      ">> Epoch 809 finished \tANN training loss 0.051652\n",
      ">> Epoch 810 finished \tANN training loss 0.051108\n",
      ">> Epoch 811 finished \tANN training loss 0.035007\n",
      ">> Epoch 812 finished \tANN training loss 0.038296\n",
      ">> Epoch 813 finished \tANN training loss 0.054188\n",
      ">> Epoch 814 finished \tANN training loss 0.052643\n",
      ">> Epoch 815 finished \tANN training loss 0.035002\n",
      ">> Epoch 816 finished \tANN training loss 0.058945\n",
      ">> Epoch 817 finished \tANN training loss 0.038444\n",
      ">> Epoch 818 finished \tANN training loss 0.039249\n",
      ">> Epoch 819 finished \tANN training loss 0.044339\n",
      ">> Epoch 820 finished \tANN training loss 0.044342\n",
      ">> Epoch 821 finished \tANN training loss 0.038370\n",
      ">> Epoch 822 finished \tANN training loss 0.037113\n",
      ">> Epoch 823 finished \tANN training loss 0.048446\n",
      ">> Epoch 824 finished \tANN training loss 0.040073\n",
      ">> Epoch 825 finished \tANN training loss 0.055499\n",
      ">> Epoch 826 finished \tANN training loss 0.070699\n",
      ">> Epoch 827 finished \tANN training loss 0.080048\n",
      ">> Epoch 828 finished \tANN training loss 0.033955\n",
      ">> Epoch 829 finished \tANN training loss 0.051570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 830 finished \tANN training loss 0.056735\n",
      ">> Epoch 831 finished \tANN training loss 0.043671\n",
      ">> Epoch 832 finished \tANN training loss 0.035836\n",
      ">> Epoch 833 finished \tANN training loss 0.038044\n",
      ">> Epoch 834 finished \tANN training loss 0.040328\n",
      ">> Epoch 835 finished \tANN training loss 0.034832\n",
      ">> Epoch 836 finished \tANN training loss 0.041914\n",
      ">> Epoch 837 finished \tANN training loss 0.038792\n",
      ">> Epoch 838 finished \tANN training loss 0.048086\n",
      ">> Epoch 839 finished \tANN training loss 0.033694\n",
      ">> Epoch 840 finished \tANN training loss 0.036161\n",
      ">> Epoch 841 finished \tANN training loss 0.046109\n",
      ">> Epoch 842 finished \tANN training loss 0.059975\n",
      ">> Epoch 843 finished \tANN training loss 0.043237\n",
      ">> Epoch 844 finished \tANN training loss 0.063030\n",
      ">> Epoch 845 finished \tANN training loss 0.049700\n",
      ">> Epoch 846 finished \tANN training loss 0.034409\n",
      ">> Epoch 847 finished \tANN training loss 0.043128\n",
      ">> Epoch 848 finished \tANN training loss 0.047619\n",
      ">> Epoch 849 finished \tANN training loss 0.041068\n",
      ">> Epoch 850 finished \tANN training loss 0.061747\n",
      ">> Epoch 851 finished \tANN training loss 0.035202\n",
      ">> Epoch 852 finished \tANN training loss 0.042437\n",
      ">> Epoch 853 finished \tANN training loss 0.042655\n",
      ">> Epoch 854 finished \tANN training loss 0.036570\n",
      ">> Epoch 855 finished \tANN training loss 0.039704\n",
      ">> Epoch 856 finished \tANN training loss 0.034384\n",
      ">> Epoch 857 finished \tANN training loss 0.056782\n",
      ">> Epoch 858 finished \tANN training loss 0.047050\n",
      ">> Epoch 859 finished \tANN training loss 0.041223\n",
      ">> Epoch 860 finished \tANN training loss 0.048356\n",
      ">> Epoch 861 finished \tANN training loss 0.039284\n",
      ">> Epoch 862 finished \tANN training loss 0.081730\n",
      ">> Epoch 863 finished \tANN training loss 0.061791\n",
      ">> Epoch 864 finished \tANN training loss 0.052660\n",
      ">> Epoch 865 finished \tANN training loss 0.034148\n",
      ">> Epoch 866 finished \tANN training loss 0.057039\n",
      ">> Epoch 867 finished \tANN training loss 0.037035\n",
      ">> Epoch 868 finished \tANN training loss 0.102819\n",
      ">> Epoch 869 finished \tANN training loss 0.045700\n",
      ">> Epoch 870 finished \tANN training loss 0.055932\n",
      ">> Epoch 871 finished \tANN training loss 0.033183\n",
      ">> Epoch 872 finished \tANN training loss 0.047987\n",
      ">> Epoch 873 finished \tANN training loss 0.039864\n",
      ">> Epoch 874 finished \tANN training loss 0.038020\n",
      ">> Epoch 875 finished \tANN training loss 0.061166\n",
      ">> Epoch 876 finished \tANN training loss 0.047068\n",
      ">> Epoch 877 finished \tANN training loss 0.058000\n",
      ">> Epoch 878 finished \tANN training loss 0.038368\n",
      ">> Epoch 879 finished \tANN training loss 0.047602\n",
      ">> Epoch 880 finished \tANN training loss 0.044173\n",
      ">> Epoch 881 finished \tANN training loss 0.050021\n",
      ">> Epoch 882 finished \tANN training loss 0.060233\n",
      ">> Epoch 883 finished \tANN training loss 0.046403\n",
      ">> Epoch 884 finished \tANN training loss 0.039709\n",
      ">> Epoch 885 finished \tANN training loss 0.035056\n",
      ">> Epoch 886 finished \tANN training loss 0.064492\n",
      ">> Epoch 887 finished \tANN training loss 0.037749\n",
      ">> Epoch 888 finished \tANN training loss 0.041722\n",
      ">> Epoch 889 finished \tANN training loss 0.049427\n",
      ">> Epoch 890 finished \tANN training loss 0.050791\n",
      ">> Epoch 891 finished \tANN training loss 0.060889\n",
      ">> Epoch 892 finished \tANN training loss 0.044987\n",
      ">> Epoch 893 finished \tANN training loss 0.043645\n",
      ">> Epoch 894 finished \tANN training loss 0.059749\n",
      ">> Epoch 895 finished \tANN training loss 0.053136\n",
      ">> Epoch 896 finished \tANN training loss 0.039378\n",
      ">> Epoch 897 finished \tANN training loss 0.042382\n",
      ">> Epoch 898 finished \tANN training loss 0.048471\n",
      ">> Epoch 899 finished \tANN training loss 0.059248\n",
      ">> Epoch 900 finished \tANN training loss 0.032607\n",
      ">> Epoch 901 finished \tANN training loss 0.097058\n",
      ">> Epoch 902 finished \tANN training loss 0.035165\n",
      ">> Epoch 903 finished \tANN training loss 0.032251\n",
      ">> Epoch 904 finished \tANN training loss 0.053720\n",
      ">> Epoch 905 finished \tANN training loss 0.059130\n",
      ">> Epoch 906 finished \tANN training loss 0.044538\n",
      ">> Epoch 907 finished \tANN training loss 0.046396\n",
      ">> Epoch 908 finished \tANN training loss 0.047094\n",
      ">> Epoch 909 finished \tANN training loss 0.041379\n",
      ">> Epoch 910 finished \tANN training loss 0.057138\n",
      ">> Epoch 911 finished \tANN training loss 0.042591\n",
      ">> Epoch 912 finished \tANN training loss 0.043484\n",
      ">> Epoch 913 finished \tANN training loss 0.035770\n",
      ">> Epoch 914 finished \tANN training loss 0.042539\n",
      ">> Epoch 915 finished \tANN training loss 0.039304\n",
      ">> Epoch 916 finished \tANN training loss 0.034690\n",
      ">> Epoch 917 finished \tANN training loss 0.057845\n",
      ">> Epoch 918 finished \tANN training loss 0.040445\n",
      ">> Epoch 919 finished \tANN training loss 0.054044\n",
      ">> Epoch 920 finished \tANN training loss 0.039982\n",
      ">> Epoch 921 finished \tANN training loss 0.057484\n",
      ">> Epoch 922 finished \tANN training loss 0.032469\n",
      ">> Epoch 923 finished \tANN training loss 0.046986\n",
      ">> Epoch 924 finished \tANN training loss 0.033500\n",
      ">> Epoch 925 finished \tANN training loss 0.032472\n",
      ">> Epoch 926 finished \tANN training loss 0.044388\n",
      ">> Epoch 927 finished \tANN training loss 0.046094\n",
      ">> Epoch 928 finished \tANN training loss 0.042941\n",
      ">> Epoch 929 finished \tANN training loss 0.037929\n",
      ">> Epoch 930 finished \tANN training loss 0.039976\n",
      ">> Epoch 931 finished \tANN training loss 0.032914\n",
      ">> Epoch 932 finished \tANN training loss 0.055453\n",
      ">> Epoch 933 finished \tANN training loss 0.032015\n",
      ">> Epoch 934 finished \tANN training loss 0.041839\n",
      ">> Epoch 935 finished \tANN training loss 0.035871\n",
      ">> Epoch 936 finished \tANN training loss 0.044240\n",
      ">> Epoch 937 finished \tANN training loss 0.038210\n",
      ">> Epoch 938 finished \tANN training loss 0.049606\n",
      ">> Epoch 939 finished \tANN training loss 0.048165\n",
      ">> Epoch 940 finished \tANN training loss 0.039207\n",
      ">> Epoch 941 finished \tANN training loss 0.037596\n",
      ">> Epoch 942 finished \tANN training loss 0.034154\n",
      ">> Epoch 943 finished \tANN training loss 0.044021\n",
      ">> Epoch 944 finished \tANN training loss 0.033013\n",
      ">> Epoch 945 finished \tANN training loss 0.036119\n",
      ">> Epoch 946 finished \tANN training loss 0.047808\n",
      ">> Epoch 947 finished \tANN training loss 0.036058\n",
      ">> Epoch 948 finished \tANN training loss 0.033912\n",
      ">> Epoch 949 finished \tANN training loss 0.039679\n",
      ">> Epoch 950 finished \tANN training loss 0.049157\n",
      ">> Epoch 951 finished \tANN training loss 0.036443\n",
      ">> Epoch 952 finished \tANN training loss 0.047316\n",
      ">> Epoch 953 finished \tANN training loss 0.039312\n",
      ">> Epoch 954 finished \tANN training loss 0.112396\n",
      ">> Epoch 955 finished \tANN training loss 0.055102\n",
      ">> Epoch 956 finished \tANN training loss 0.041149\n",
      ">> Epoch 957 finished \tANN training loss 0.039640\n",
      ">> Epoch 958 finished \tANN training loss 0.032967\n",
      ">> Epoch 959 finished \tANN training loss 0.032103\n",
      ">> Epoch 960 finished \tANN training loss 0.035658\n",
      ">> Epoch 961 finished \tANN training loss 0.050534\n",
      ">> Epoch 962 finished \tANN training loss 0.046051\n",
      ">> Epoch 963 finished \tANN training loss 0.035233\n",
      ">> Epoch 964 finished \tANN training loss 0.058575\n",
      ">> Epoch 965 finished \tANN training loss 0.038193\n",
      ">> Epoch 966 finished \tANN training loss 0.033319\n",
      ">> Epoch 967 finished \tANN training loss 0.033239\n",
      ">> Epoch 968 finished \tANN training loss 0.036452\n",
      ">> Epoch 969 finished \tANN training loss 0.036986\n",
      ">> Epoch 970 finished \tANN training loss 0.037031\n",
      ">> Epoch 971 finished \tANN training loss 0.047473\n",
      ">> Epoch 972 finished \tANN training loss 0.048541\n",
      ">> Epoch 973 finished \tANN training loss 0.038302\n",
      ">> Epoch 974 finished \tANN training loss 0.045257\n",
      ">> Epoch 975 finished \tANN training loss 0.070612\n",
      ">> Epoch 976 finished \tANN training loss 0.044270\n",
      ">> Epoch 977 finished \tANN training loss 0.033157\n",
      ">> Epoch 978 finished \tANN training loss 0.046623\n",
      ">> Epoch 979 finished \tANN training loss 0.043704\n",
      ">> Epoch 980 finished \tANN training loss 0.042947\n",
      ">> Epoch 981 finished \tANN training loss 0.032187\n",
      ">> Epoch 982 finished \tANN training loss 0.037716\n",
      ">> Epoch 983 finished \tANN training loss 0.041573\n",
      ">> Epoch 984 finished \tANN training loss 0.049389\n",
      ">> Epoch 985 finished \tANN training loss 0.045455\n",
      ">> Epoch 986 finished \tANN training loss 0.064865\n",
      ">> Epoch 987 finished \tANN training loss 0.037821\n",
      ">> Epoch 988 finished \tANN training loss 0.055375\n",
      ">> Epoch 989 finished \tANN training loss 0.051392\n",
      ">> Epoch 990 finished \tANN training loss 0.040976\n",
      ">> Epoch 991 finished \tANN training loss 0.039694\n",
      ">> Epoch 992 finished \tANN training loss 0.036078\n",
      ">> Epoch 993 finished \tANN training loss 0.035919\n",
      ">> Epoch 994 finished \tANN training loss 0.047138\n",
      ">> Epoch 995 finished \tANN training loss 0.037119\n",
      ">> Epoch 996 finished \tANN training loss 0.039612\n",
      ">> Epoch 997 finished \tANN training loss 0.054371\n",
      ">> Epoch 998 finished \tANN training loss 0.072267\n",
      ">> Epoch 999 finished \tANN training loss 0.039882\n",
      "[END] Fine tuning step\n",
      "\n",
      "\n",
      "TRIAL:  6\n",
      "\n",
      "\n",
      "[START] Pre-training step:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 1 finished \tRBM Reconstruction error 5.589903\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 4.519089\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 4.737075\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 5.334951\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 6.454946\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 7.964031\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 10.087322\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 14.883307\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 19.912150\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 26.137814\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 87.355003\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 285.291870\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 708.968323\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 1256.623413\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 2045.286743\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 2684.414551\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 3278.830322\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 3372.296387\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 3475.604004\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 3662.457275\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 5631271.000000\n",
      ">> Epoch 1 finished \tANN training loss 2034456.125000\n",
      ">> Epoch 2 finished \tANN training loss 8382920191989776384.000000\n",
      ">> Epoch 3 finished \tANN training loss 3525211574899834880.000000\n",
      ">> Epoch 4 finished \tANN training loss 1482434144335036416.000000\n",
      ">> Epoch 5 finished \tANN training loss 623398010339983360.000000\n",
      ">> Epoch 6 finished \tANN training loss 262153361954963456.000000\n",
      ">> Epoch 7 finished \tANN training loss 110241468207071232.000000\n",
      ">> Epoch 8 finished \tANN training loss 46359172618387456.000000\n",
      ">> Epoch 9 finished \tANN training loss 19495079894843392.000000\n",
      ">> Epoch 10 finished \tANN training loss 8198139622195200.000000\n",
      ">> Epoch 11 finished \tANN training loss 3447506092425216.000000\n",
      ">> Epoch 12 finished \tANN training loss 1449757620830208.000000\n",
      ">> Epoch 13 finished \tANN training loss 609657320112128.000000\n",
      ">> Epoch 14 finished \tANN training loss 257228963577856.000000\n",
      ">> Epoch 15 finished \tANN training loss 108170856890368.000000\n",
      ">> Epoch 16 finished \tANN training loss 45488313204736.000000\n",
      ">> Epoch 17 finished \tANN training loss 19128888852480.000000\n",
      ">> Epoch 18 finished \tANN training loss 8044141346816.000000\n",
      ">> Epoch 19 finished \tANN training loss 3382752575488.000000\n",
      ">> Epoch 20 finished \tANN training loss 1422524416000.000000\n",
      ">> Epoch 21 finished \tANN training loss 598204809216.000000\n",
      ">> Epoch 22 finished \tANN training loss 251559018496.000000\n",
      ">> Epoch 23 finished \tANN training loss 158327409475584.000000\n",
      ">> Epoch 24 finished \tANN training loss 66580272644096.000000\n",
      ">> Epoch 25 finished \tANN training loss 27998579326976.000000\n",
      ">> Epoch 26 finished \tANN training loss 11774043619328.000000\n",
      ">> Epoch 27 finished \tANN training loss 4951256858624.000000\n",
      ">> Epoch 28 finished \tANN training loss 2082118303744.000000\n",
      ">> Epoch 29 finished \tANN training loss 875580030976.000000\n",
      ">> Epoch 30 finished \tANN training loss 368201728000.000000\n",
      ">> Epoch 31 finished \tANN training loss 154837352448.000000\n",
      ">> Epoch 32 finished \tANN training loss 65112666112.000000\n",
      ">> Epoch 33 finished \tANN training loss 46365241507840.000000\n",
      ">> Epoch 34 finished \tANN training loss 19497668837376.000000\n",
      ">> Epoch 35 finished \tANN training loss 8199216300032.000000\n",
      ">> Epoch 36 finished \tANN training loss 3447958274048.000000\n",
      ">> Epoch 37 finished \tANN training loss 1449947299840.000000\n",
      ">> Epoch 38 finished \tANN training loss 609736196096.000000\n",
      ">> Epoch 39 finished \tANN training loss 256408305664.000000\n",
      ">> Epoch 40 finished \tANN training loss 107825610752.000000\n",
      ">> Epoch 41 finished \tANN training loss 45343178752.000000\n",
      ">> Epoch 42 finished \tANN training loss 19067865088.000000\n",
      ">> Epoch 43 finished \tANN training loss 8018486272.000000\n",
      ">> Epoch 44 finished \tANN training loss 3371960832.000000\n",
      ">> Epoch 45 finished \tANN training loss 1417988992.000000\n",
      ">> Epoch 46 finished \tANN training loss 596297472.000000\n",
      ">> Epoch 47 finished \tANN training loss 250757312.000000\n",
      ">> Epoch 48 finished \tANN training loss 105449448.000000\n",
      ">> Epoch 49 finished \tANN training loss 1453893563399103971168616448.000000\n",
      ">> Epoch 50 finished \tANN training loss 611395417591143521365000192.000000\n",
      ">> Epoch 51 finished \tANN training loss 257105922745904565940387840.000000\n",
      ">> Epoch 52 finished \tANN training loss 108119124804250701599342592.000000\n",
      ">> Epoch 53 finished \tANN training loss 45466566338815433071656960.000000\n",
      ">> Epoch 54 finished \tANN training loss 19119740096515211008147456.000000\n",
      ">> Epoch 55 finished \tANN training loss 8040299329059450569883648.000000\n",
      ">> Epoch 56 finished \tANN training loss 3381127644391444307771392.000000\n",
      ">> Epoch 57 finished \tANN training loss 1421844624896848232972288.000000\n",
      ">> Epoch 58 finished \tANN training loss 597918783231978048061440.000000\n",
      ">> Epoch 59 finished \tANN training loss 251438435168690590711808.000000\n",
      ">> Epoch 60 finished \tANN training loss 105735710209788109389824.000000\n",
      ">> Epoch 61 finished \tANN training loss 44464349869403195572224.000000\n",
      ">> Epoch 62 finished \tANN training loss 18698283623697075929088.000000\n",
      ">> Epoch 63 finished \tANN training loss 7863067087756911968256.000000\n",
      ">> Epoch 64 finished \tANN training loss 3306598296985830162432.000000\n",
      ">> Epoch 65 finished \tANN training loss 1390503554924219990016.000000\n",
      ">> Epoch 66 finished \tANN training loss 584738438787274637312.000000\n",
      ">> Epoch 67 finished \tANN training loss 245896275771638415360.000000\n",
      ">> Epoch 68 finished \tANN training loss 103404943224705384448.000000\n",
      ">> Epoch 69 finished \tANN training loss 43484215931006091264.000000\n",
      ">> Epoch 70 finished \tANN training loss 18286114220984500224.000000\n",
      ">> Epoch 71 finished \tANN training loss 7689729237967699968.000000\n",
      ">> Epoch 72 finished \tANN training loss 3233709052143861760.000000\n",
      ">> Epoch 73 finished \tANN training loss 1359849867832197120.000000\n",
      ">> Epoch 74 finished \tANN training loss 571848713341763584.000000\n",
      ">> Epoch 75 finished \tANN training loss 240475510960816128.000000\n",
      ">> Epoch 76 finished \tANN training loss 101125528970330112.000000\n",
      ">> Epoch 77 finished \tANN training loss 42525635133833216.000000\n",
      ">> Epoch 78 finished \tANN training loss 17883017607380992.000000\n",
      ">> Epoch 79 finished \tANN training loss 7520221447323648.000000\n",
      ">> Epoch 80 finished \tANN training loss 3162432201555968.000000\n",
      ">> Epoch 81 finished \tANN training loss 1329876225228800.000000\n",
      ">> Epoch 82 finished \tANN training loss 559243530862592.000000\n",
      ">> Epoch 83 finished \tANN training loss 235175061487616.000000\n",
      ">> Epoch 84 finished \tANN training loss 98896495771648.000000\n",
      ">> Epoch 85 finished \tANN training loss 41588260601856.000000\n",
      ">> Epoch 86 finished \tANN training loss 17488840491008.000000\n",
      ">> Epoch 87 finished \tANN training loss 7354461978624.000000\n",
      ">> Epoch 88 finished \tANN training loss 3092718551040.000000\n",
      ">> Epoch 89 finished \tANN training loss 1300561657856.000000\n",
      ">> Epoch 90 finished \tANN training loss 546916302848.000000\n",
      ">> Epoch 91 finished \tANN training loss 229991006208.000000\n",
      ">> Epoch 92 finished \tANN training loss 96716546048.000000\n",
      ">> Epoch 93 finished \tANN training loss 40671567872.000000\n",
      ">> Epoch 94 finished \tANN training loss 17103336448.000000\n",
      ">> Epoch 95 finished \tANN training loss 7192353792.000000\n",
      ">> Epoch 96 finished \tANN training loss 3024550144.000000\n",
      ">> Epoch 97 finished \tANN training loss 1271893632.000000\n",
      ">> Epoch 98 finished \tANN training loss 534859264.000000\n",
      ">> Epoch 99 finished \tANN training loss 224920480.000000\n",
      ">> Epoch 100 finished \tANN training loss 94584320.000000\n",
      ">> Epoch 101 finished \tANN training loss 39775008.000000\n",
      ">> Epoch 102 finished \tANN training loss 16726219.000000\n",
      ">> Epoch 103 finished \tANN training loss 7033867.500000\n",
      ">> Epoch 104 finished \tANN training loss 2957856.500000\n",
      ">> Epoch 105 finished \tANN training loss 1243864.375000\n",
      ">> Epoch 106 finished \tANN training loss 523089.187500\n",
      ">> Epoch 107 finished \tANN training loss 219965.906250\n",
      ">> Epoch 108 finished \tANN training loss 92502.500000\n",
      ">> Epoch 109 finished \tANN training loss 38896.695312\n",
      ">> Epoch 110 finished \tANN training loss 16358.588867\n",
      ">> Epoch 111 finished \tANN training loss 6878.788574\n",
      ">> Epoch 112 finished \tANN training loss 2895.133789\n",
      ">> Epoch 113 finished \tANN training loss 1218.037598\n",
      ">> Epoch 114 finished \tANN training loss 511.985168\n",
      ">> Epoch 115 finished \tANN training loss 216.506332\n",
      ">> Epoch 116 finished \tANN training loss 91.694107\n",
      ">> Epoch 117 finished \tANN training loss 39.069546\n",
      ">> Epoch 118 finished \tANN training loss 17.025537\n",
      ">> Epoch 119 finished \tANN training loss 7.866897\n",
      ">> Epoch 120 finished \tANN training loss 3.864299\n",
      ">> Epoch 121 finished \tANN training loss 2.237110\n",
      ">> Epoch 122 finished \tANN training loss 1.537411\n",
      ">> Epoch 123 finished \tANN training loss 1.259577\n",
      ">> Epoch 124 finished \tANN training loss 1.131230\n",
      ">> Epoch 125 finished \tANN training loss 1.065353\n",
      ">> Epoch 126 finished \tANN training loss 1.049257\n",
      ">> Epoch 127 finished \tANN training loss 1.041355\n",
      ">> Epoch 128 finished \tANN training loss 1.041757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 129 finished \tANN training loss 1.037081\n",
      ">> Epoch 130 finished \tANN training loss 1.037461\n",
      ">> Epoch 131 finished \tANN training loss 1.037789\n",
      ">> Epoch 132 finished \tANN training loss 1.036736\n",
      ">> Epoch 133 finished \tANN training loss 1.036745\n",
      ">> Epoch 134 finished \tANN training loss 1.037619\n",
      ">> Epoch 135 finished \tANN training loss 1.036960\n",
      ">> Epoch 136 finished \tANN training loss 1.037328\n",
      ">> Epoch 137 finished \tANN training loss 1.036884\n",
      ">> Epoch 138 finished \tANN training loss 1.036929\n",
      ">> Epoch 139 finished \tANN training loss 1.036748\n",
      ">> Epoch 140 finished \tANN training loss 1.036693\n",
      ">> Epoch 141 finished \tANN training loss 1.036695\n",
      ">> Epoch 142 finished \tANN training loss 1.036664\n",
      ">> Epoch 143 finished \tANN training loss 1.037357\n",
      ">> Epoch 144 finished \tANN training loss 1.036664\n",
      ">> Epoch 145 finished \tANN training loss 1.036695\n",
      ">> Epoch 146 finished \tANN training loss 1.036792\n",
      ">> Epoch 147 finished \tANN training loss 1.036661\n",
      ">> Epoch 148 finished \tANN training loss 1.037012\n",
      ">> Epoch 149 finished \tANN training loss 1.037357\n",
      ">> Epoch 150 finished \tANN training loss 1.036960\n",
      ">> Epoch 151 finished \tANN training loss 1.037995\n",
      ">> Epoch 152 finished \tANN training loss 1.037407\n",
      ">> Epoch 153 finished \tANN training loss 1.036660\n",
      ">> Epoch 154 finished \tANN training loss 1.037026\n",
      ">> Epoch 155 finished \tANN training loss 1.037318\n",
      ">> Epoch 156 finished \tANN training loss 1.036942\n",
      ">> Epoch 157 finished \tANN training loss 1.036672\n",
      ">> Epoch 158 finished \tANN training loss 1.036951\n",
      ">> Epoch 159 finished \tANN training loss 1.036696\n",
      ">> Epoch 160 finished \tANN training loss 1.037293\n",
      ">> Epoch 161 finished \tANN training loss 1.036865\n",
      ">> Epoch 162 finished \tANN training loss 1.036722\n",
      ">> Epoch 163 finished \tANN training loss 1.036663\n",
      ">> Epoch 164 finished \tANN training loss 1.036963\n",
      ">> Epoch 165 finished \tANN training loss 1.036888\n",
      ">> Epoch 166 finished \tANN training loss 1.037651\n",
      ">> Epoch 167 finished \tANN training loss 1.037993\n",
      ">> Epoch 168 finished \tANN training loss 1.037262\n",
      ">> Epoch 169 finished \tANN training loss 1.036781\n",
      ">> Epoch 170 finished \tANN training loss 1.037141\n",
      ">> Epoch 171 finished \tANN training loss 1.037105\n",
      ">> Epoch 172 finished \tANN training loss 1.036698\n",
      ">> Epoch 173 finished \tANN training loss 1.036660\n",
      ">> Epoch 174 finished \tANN training loss 1.036925\n",
      ">> Epoch 175 finished \tANN training loss 1.036799\n",
      ">> Epoch 176 finished \tANN training loss 1.036858\n",
      ">> Epoch 177 finished \tANN training loss 1.036789\n",
      ">> Epoch 178 finished \tANN training loss 1.036658\n",
      ">> Epoch 179 finished \tANN training loss 1.036655\n",
      ">> Epoch 180 finished \tANN training loss 1.037462\n",
      ">> Epoch 181 finished \tANN training loss 1.036661\n",
      ">> Epoch 182 finished \tANN training loss 1.036665\n",
      ">> Epoch 183 finished \tANN training loss 1.036705\n",
      ">> Epoch 184 finished \tANN training loss 1.037466\n",
      ">> Epoch 185 finished \tANN training loss 1.036856\n",
      ">> Epoch 186 finished \tANN training loss 1.036721\n",
      ">> Epoch 187 finished \tANN training loss 1.036669\n",
      ">> Epoch 188 finished \tANN training loss 1.036886\n",
      ">> Epoch 189 finished \tANN training loss 1.036883\n",
      ">> Epoch 190 finished \tANN training loss 1.037067\n",
      ">> Epoch 191 finished \tANN training loss 1.037836\n",
      ">> Epoch 192 finished \tANN training loss 1.036853\n",
      ">> Epoch 193 finished \tANN training loss 1.037050\n",
      ">> Epoch 194 finished \tANN training loss 1.037657\n",
      ">> Epoch 195 finished \tANN training loss 1.036814\n",
      ">> Epoch 196 finished \tANN training loss 1.036998\n",
      ">> Epoch 197 finished \tANN training loss 1.037337\n",
      ">> Epoch 198 finished \tANN training loss 1.036714\n",
      ">> Epoch 199 finished \tANN training loss 1.036956\n",
      ">> Epoch 200 finished \tANN training loss 1.036714\n",
      ">> Epoch 201 finished \tANN training loss 1.037103\n",
      ">> Epoch 202 finished \tANN training loss 1.037782\n",
      ">> Epoch 203 finished \tANN training loss 1.037043\n",
      ">> Epoch 204 finished \tANN training loss 1.036692\n",
      ">> Epoch 205 finished \tANN training loss 1.036719\n",
      ">> Epoch 206 finished \tANN training loss 1.037220\n",
      ">> Epoch 207 finished \tANN training loss 1.036955\n",
      ">> Epoch 208 finished \tANN training loss 1.037159\n",
      ">> Epoch 209 finished \tANN training loss 1.037525\n",
      ">> Epoch 210 finished \tANN training loss 1.036757\n",
      ">> Epoch 211 finished \tANN training loss 1.036697\n",
      ">> Epoch 212 finished \tANN training loss 1.036885\n",
      ">> Epoch 213 finished \tANN training loss 1.036961\n",
      ">> Epoch 214 finished \tANN training loss 1.036769\n",
      ">> Epoch 215 finished \tANN training loss 1.038107\n",
      ">> Epoch 216 finished \tANN training loss 1.037107\n",
      ">> Epoch 217 finished \tANN training loss 1.036880\n",
      ">> Epoch 218 finished \tANN training loss 1.036689\n",
      ">> Epoch 219 finished \tANN training loss 1.036779\n",
      ">> Epoch 220 finished \tANN training loss 1.036655\n",
      ">> Epoch 221 finished \tANN training loss 1.036803\n",
      ">> Epoch 222 finished \tANN training loss 1.037254\n",
      ">> Epoch 223 finished \tANN training loss 1.036893\n",
      ">> Epoch 224 finished \tANN training loss 1.036893\n",
      ">> Epoch 225 finished \tANN training loss 1.036681\n",
      ">> Epoch 226 finished \tANN training loss 1.036770\n",
      ">> Epoch 227 finished \tANN training loss 1.036705\n",
      ">> Epoch 228 finished \tANN training loss 1.037063\n",
      ">> Epoch 229 finished \tANN training loss 1.037008\n",
      ">> Epoch 230 finished \tANN training loss 1.036856\n",
      ">> Epoch 231 finished \tANN training loss 1.037069\n",
      ">> Epoch 232 finished \tANN training loss 1.037469\n",
      ">> Epoch 233 finished \tANN training loss 1.036891\n",
      ">> Epoch 234 finished \tANN training loss 1.037024\n",
      ">> Epoch 235 finished \tANN training loss 1.036656\n",
      ">> Epoch 236 finished \tANN training loss 1.036777\n",
      ">> Epoch 237 finished \tANN training loss 1.036660\n",
      ">> Epoch 238 finished \tANN training loss 1.037061\n",
      ">> Epoch 239 finished \tANN training loss 1.036658\n",
      ">> Epoch 240 finished \tANN training loss 1.037570\n",
      ">> Epoch 241 finished \tANN training loss 1.037130\n",
      ">> Epoch 242 finished \tANN training loss 1.036872\n",
      ">> Epoch 243 finished \tANN training loss 1.036715\n",
      ">> Epoch 244 finished \tANN training loss 1.036663\n",
      ">> Epoch 245 finished \tANN training loss 1.037084\n",
      ">> Epoch 246 finished \tANN training loss 1.037293\n",
      ">> Epoch 247 finished \tANN training loss 1.036676\n",
      ">> Epoch 248 finished \tANN training loss 1.036673\n",
      ">> Epoch 249 finished \tANN training loss 1.036679\n",
      ">> Epoch 250 finished \tANN training loss 1.036850\n",
      ">> Epoch 251 finished \tANN training loss 1.036718\n",
      ">> Epoch 252 finished \tANN training loss 1.036737\n",
      ">> Epoch 253 finished \tANN training loss 1.036661\n",
      ">> Epoch 254 finished \tANN training loss 1.036673\n",
      ">> Epoch 255 finished \tANN training loss 1.036671\n",
      ">> Epoch 256 finished \tANN training loss 1.036656\n",
      ">> Epoch 257 finished \tANN training loss 1.036737\n",
      ">> Epoch 258 finished \tANN training loss 1.036768\n",
      ">> Epoch 259 finished \tANN training loss 1.036901\n",
      ">> Epoch 260 finished \tANN training loss 1.037927\n",
      ">> Epoch 261 finished \tANN training loss 1.037626\n",
      ">> Epoch 262 finished \tANN training loss 1.038769\n",
      ">> Epoch 263 finished \tANN training loss 1.037561\n",
      ">> Epoch 264 finished \tANN training loss 1.036709\n",
      ">> Epoch 265 finished \tANN training loss 1.036711\n",
      ">> Epoch 266 finished \tANN training loss 1.036723\n",
      ">> Epoch 267 finished \tANN training loss 1.036689\n",
      ">> Epoch 268 finished \tANN training loss 1.036656\n",
      ">> Epoch 269 finished \tANN training loss 1.036675\n",
      ">> Epoch 270 finished \tANN training loss 1.036736\n",
      ">> Epoch 271 finished \tANN training loss 1.036674\n",
      ">> Epoch 272 finished \tANN training loss 1.036963\n",
      ">> Epoch 273 finished \tANN training loss 1.036674\n",
      ">> Epoch 274 finished \tANN training loss 1.036705\n",
      ">> Epoch 275 finished \tANN training loss 1.036682\n",
      ">> Epoch 276 finished \tANN training loss 1.036808\n",
      ">> Epoch 277 finished \tANN training loss 1.036991\n",
      ">> Epoch 278 finished \tANN training loss 1.036722\n",
      ">> Epoch 279 finished \tANN training loss 1.036663\n",
      ">> Epoch 280 finished \tANN training loss 1.037032\n",
      ">> Epoch 281 finished \tANN training loss 1.036813\n",
      ">> Epoch 282 finished \tANN training loss 1.037468\n",
      ">> Epoch 283 finished \tANN training loss 1.038434\n",
      ">> Epoch 284 finished \tANN training loss 1.036726\n",
      ">> Epoch 285 finished \tANN training loss 1.036671\n",
      ">> Epoch 286 finished \tANN training loss 1.036678\n",
      ">> Epoch 287 finished \tANN training loss 1.036696\n",
      ">> Epoch 288 finished \tANN training loss 1.036996\n",
      ">> Epoch 289 finished \tANN training loss 1.036655\n",
      ">> Epoch 290 finished \tANN training loss 1.037396\n",
      ">> Epoch 291 finished \tANN training loss 1.036950\n",
      ">> Epoch 292 finished \tANN training loss 1.036755\n",
      ">> Epoch 293 finished \tANN training loss 1.036754\n",
      ">> Epoch 294 finished \tANN training loss 1.037745\n",
      ">> Epoch 295 finished \tANN training loss 1.037023\n",
      ">> Epoch 296 finished \tANN training loss 1.037254\n",
      ">> Epoch 297 finished \tANN training loss 1.036983\n",
      ">> Epoch 298 finished \tANN training loss 1.037162\n",
      ">> Epoch 299 finished \tANN training loss 1.037291\n",
      ">> Epoch 300 finished \tANN training loss 1.036924\n",
      ">> Epoch 301 finished \tANN training loss 1.037806\n",
      ">> Epoch 302 finished \tANN training loss 1.036685\n",
      ">> Epoch 303 finished \tANN training loss 1.036929\n",
      ">> Epoch 304 finished \tANN training loss 1.036673\n",
      ">> Epoch 305 finished \tANN training loss 1.036879\n",
      ">> Epoch 306 finished \tANN training loss 1.037020\n",
      ">> Epoch 307 finished \tANN training loss 1.036833\n",
      ">> Epoch 308 finished \tANN training loss 1.037015\n",
      ">> Epoch 309 finished \tANN training loss 1.037571\n",
      ">> Epoch 310 finished \tANN training loss 1.037522\n",
      ">> Epoch 311 finished \tANN training loss 1.036877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 312 finished \tANN training loss 1.036895\n",
      ">> Epoch 313 finished \tANN training loss 1.036873\n",
      ">> Epoch 314 finished \tANN training loss 1.037673\n",
      ">> Epoch 315 finished \tANN training loss 1.037681\n",
      ">> Epoch 316 finished \tANN training loss 1.039533\n",
      ">> Epoch 317 finished \tANN training loss 1.038050\n",
      ">> Epoch 318 finished \tANN training loss 1.036735\n",
      ">> Epoch 319 finished \tANN training loss 1.036823\n",
      ">> Epoch 320 finished \tANN training loss 1.036711\n",
      ">> Epoch 321 finished \tANN training loss 1.036658\n",
      ">> Epoch 322 finished \tANN training loss 1.036918\n",
      ">> Epoch 323 finished \tANN training loss 1.036907\n",
      ">> Epoch 324 finished \tANN training loss 1.037730\n",
      ">> Epoch 325 finished \tANN training loss 1.036944\n",
      ">> Epoch 326 finished \tANN training loss 1.037942\n",
      ">> Epoch 327 finished \tANN training loss 1.037403\n",
      ">> Epoch 328 finished \tANN training loss 1.036655\n",
      ">> Epoch 329 finished \tANN training loss 1.036931\n",
      ">> Epoch 330 finished \tANN training loss 1.036659\n",
      ">> Epoch 331 finished \tANN training loss 1.036655\n",
      ">> Epoch 332 finished \tANN training loss 1.036805\n",
      ">> Epoch 333 finished \tANN training loss 1.036828\n",
      ">> Epoch 334 finished \tANN training loss 1.036862\n",
      ">> Epoch 335 finished \tANN training loss 1.036903\n",
      ">> Epoch 336 finished \tANN training loss 1.036722\n",
      ">> Epoch 337 finished \tANN training loss 1.037850\n",
      ">> Epoch 338 finished \tANN training loss 1.037300\n",
      ">> Epoch 339 finished \tANN training loss 1.038586\n",
      ">> Epoch 340 finished \tANN training loss 1.037014\n",
      ">> Epoch 341 finished \tANN training loss 1.037812\n",
      ">> Epoch 342 finished \tANN training loss 1.036810\n",
      ">> Epoch 343 finished \tANN training loss 1.036656\n",
      ">> Epoch 344 finished \tANN training loss 1.036717\n",
      ">> Epoch 345 finished \tANN training loss 1.037128\n",
      ">> Epoch 346 finished \tANN training loss 1.037934\n",
      ">> Epoch 347 finished \tANN training loss 1.037380\n",
      ">> Epoch 348 finished \tANN training loss 1.038002\n",
      ">> Epoch 349 finished \tANN training loss 1.039376\n",
      ">> Epoch 350 finished \tANN training loss 1.037656\n",
      ">> Epoch 351 finished \tANN training loss 1.036930\n",
      ">> Epoch 352 finished \tANN training loss 1.036658\n",
      ">> Epoch 353 finished \tANN training loss 1.036681\n",
      ">> Epoch 354 finished \tANN training loss 1.036658\n",
      ">> Epoch 355 finished \tANN training loss 1.036675\n",
      ">> Epoch 356 finished \tANN training loss 1.036728\n",
      ">> Epoch 357 finished \tANN training loss 1.037131\n",
      ">> Epoch 358 finished \tANN training loss 1.036852\n",
      ">> Epoch 359 finished \tANN training loss 1.037654\n",
      ">> Epoch 360 finished \tANN training loss 1.036950\n",
      ">> Epoch 361 finished \tANN training loss 1.036738\n",
      ">> Epoch 362 finished \tANN training loss 1.036781\n",
      ">> Epoch 363 finished \tANN training loss 1.036801\n",
      ">> Epoch 364 finished \tANN training loss 1.036945\n",
      ">> Epoch 365 finished \tANN training loss 1.036657\n",
      ">> Epoch 366 finished \tANN training loss 1.037675\n",
      ">> Epoch 367 finished \tANN training loss 1.036714\n",
      ">> Epoch 368 finished \tANN training loss 1.036900\n",
      ">> Epoch 369 finished \tANN training loss 1.037179\n",
      ">> Epoch 370 finished \tANN training loss 1.036865\n",
      ">> Epoch 371 finished \tANN training loss 1.036656\n",
      ">> Epoch 372 finished \tANN training loss 1.037045\n",
      ">> Epoch 373 finished \tANN training loss 1.036917\n",
      ">> Epoch 374 finished \tANN training loss 1.036785\n",
      ">> Epoch 375 finished \tANN training loss 1.036755\n",
      ">> Epoch 376 finished \tANN training loss 1.036968\n",
      ">> Epoch 377 finished \tANN training loss 1.036735\n",
      ">> Epoch 378 finished \tANN training loss 1.036656\n",
      ">> Epoch 379 finished \tANN training loss 1.036909\n",
      ">> Epoch 380 finished \tANN training loss 1.036675\n",
      ">> Epoch 381 finished \tANN training loss 1.038024\n",
      ">> Epoch 382 finished \tANN training loss 1.036708\n",
      ">> Epoch 383 finished \tANN training loss 1.036806\n",
      ">> Epoch 384 finished \tANN training loss 1.037291\n",
      ">> Epoch 385 finished \tANN training loss 1.036655\n",
      ">> Epoch 386 finished \tANN training loss 1.036829\n",
      ">> Epoch 387 finished \tANN training loss 1.036904\n",
      ">> Epoch 388 finished \tANN training loss 1.036987\n",
      ">> Epoch 389 finished \tANN training loss 1.036758\n",
      ">> Epoch 390 finished \tANN training loss 1.036657\n",
      ">> Epoch 391 finished \tANN training loss 1.036927\n",
      ">> Epoch 392 finished \tANN training loss 1.037369\n",
      ">> Epoch 393 finished \tANN training loss 1.037053\n",
      ">> Epoch 394 finished \tANN training loss 1.036800\n",
      ">> Epoch 395 finished \tANN training loss 1.037691\n",
      ">> Epoch 396 finished \tANN training loss 1.036673\n",
      ">> Epoch 397 finished \tANN training loss 1.036739\n",
      ">> Epoch 398 finished \tANN training loss 1.037268\n",
      ">> Epoch 399 finished \tANN training loss 1.037857\n",
      ">> Epoch 400 finished \tANN training loss 1.037434\n",
      ">> Epoch 401 finished \tANN training loss 1.037675\n",
      ">> Epoch 402 finished \tANN training loss 1.038308\n",
      ">> Epoch 403 finished \tANN training loss 1.037243\n",
      ">> Epoch 404 finished \tANN training loss 1.036824\n",
      ">> Epoch 405 finished \tANN training loss 1.036762\n",
      ">> Epoch 406 finished \tANN training loss 1.036674\n",
      ">> Epoch 407 finished \tANN training loss 1.036981\n",
      ">> Epoch 408 finished \tANN training loss 1.036658\n",
      ">> Epoch 409 finished \tANN training loss 1.036656\n",
      ">> Epoch 410 finished \tANN training loss 1.036925\n",
      ">> Epoch 411 finished \tANN training loss 1.037976\n",
      ">> Epoch 412 finished \tANN training loss 1.037123\n",
      ">> Epoch 413 finished \tANN training loss 1.036891\n",
      ">> Epoch 414 finished \tANN training loss 1.036655\n",
      ">> Epoch 415 finished \tANN training loss 1.036696\n",
      ">> Epoch 416 finished \tANN training loss 1.036937\n",
      ">> Epoch 417 finished \tANN training loss 1.036729\n",
      ">> Epoch 418 finished \tANN training loss 1.037025\n",
      ">> Epoch 419 finished \tANN training loss 1.036883\n",
      ">> Epoch 420 finished \tANN training loss 1.037227\n",
      ">> Epoch 421 finished \tANN training loss 1.037003\n",
      ">> Epoch 422 finished \tANN training loss 1.037091\n",
      ">> Epoch 423 finished \tANN training loss 1.038074\n",
      ">> Epoch 424 finished \tANN training loss 1.038710\n",
      ">> Epoch 425 finished \tANN training loss 1.038430\n",
      ">> Epoch 426 finished \tANN training loss 1.037273\n",
      ">> Epoch 427 finished \tANN training loss 1.038225\n",
      ">> Epoch 428 finished \tANN training loss 1.037149\n",
      ">> Epoch 429 finished \tANN training loss 1.036719\n",
      ">> Epoch 430 finished \tANN training loss 1.036662\n",
      ">> Epoch 431 finished \tANN training loss 1.036847\n",
      ">> Epoch 432 finished \tANN training loss 1.036841\n",
      ">> Epoch 433 finished \tANN training loss 1.036677\n",
      ">> Epoch 434 finished \tANN training loss 1.036896\n",
      ">> Epoch 435 finished \tANN training loss 1.037668\n",
      ">> Epoch 436 finished \tANN training loss 1.036658\n",
      ">> Epoch 437 finished \tANN training loss 1.036765\n",
      ">> Epoch 438 finished \tANN training loss 1.036665\n",
      ">> Epoch 439 finished \tANN training loss 1.037183\n",
      ">> Epoch 440 finished \tANN training loss 1.036706\n",
      ">> Epoch 441 finished \tANN training loss 1.036688\n",
      ">> Epoch 442 finished \tANN training loss 1.036750\n",
      ">> Epoch 443 finished \tANN training loss 1.036988\n",
      ">> Epoch 444 finished \tANN training loss 1.036817\n",
      ">> Epoch 445 finished \tANN training loss 1.039596\n",
      ">> Epoch 446 finished \tANN training loss 1.036927\n",
      ">> Epoch 447 finished \tANN training loss 1.036725\n",
      ">> Epoch 448 finished \tANN training loss 1.036717\n",
      ">> Epoch 449 finished \tANN training loss 1.037075\n",
      ">> Epoch 450 finished \tANN training loss 1.036673\n",
      ">> Epoch 451 finished \tANN training loss 1.036659\n",
      ">> Epoch 452 finished \tANN training loss 1.037341\n",
      ">> Epoch 453 finished \tANN training loss 1.037371\n",
      ">> Epoch 454 finished \tANN training loss 1.037070\n",
      ">> Epoch 455 finished \tANN training loss 1.036746\n",
      ">> Epoch 456 finished \tANN training loss 1.037020\n",
      ">> Epoch 457 finished \tANN training loss 1.036962\n",
      ">> Epoch 458 finished \tANN training loss 1.037114\n",
      ">> Epoch 459 finished \tANN training loss 1.037994\n",
      ">> Epoch 460 finished \tANN training loss 1.036738\n",
      ">> Epoch 461 finished \tANN training loss 1.036667\n",
      ">> Epoch 462 finished \tANN training loss 1.037291\n",
      ">> Epoch 463 finished \tANN training loss 1.036679\n",
      ">> Epoch 464 finished \tANN training loss 1.036808\n",
      ">> Epoch 465 finished \tANN training loss 1.036695\n",
      ">> Epoch 466 finished \tANN training loss 1.036933\n",
      ">> Epoch 467 finished \tANN training loss 1.037026\n",
      ">> Epoch 468 finished \tANN training loss 1.037191\n",
      ">> Epoch 469 finished \tANN training loss 1.036834\n",
      ">> Epoch 470 finished \tANN training loss 1.036713\n",
      ">> Epoch 471 finished \tANN training loss 1.036656\n",
      ">> Epoch 472 finished \tANN training loss 1.036820\n",
      ">> Epoch 473 finished \tANN training loss 1.036655\n",
      ">> Epoch 474 finished \tANN training loss 1.036798\n",
      ">> Epoch 475 finished \tANN training loss 1.036943\n",
      ">> Epoch 476 finished \tANN training loss 1.037284\n",
      ">> Epoch 477 finished \tANN training loss 1.036658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 478 finished \tANN training loss 1.036668\n",
      ">> Epoch 479 finished \tANN training loss 1.036731\n",
      ">> Epoch 480 finished \tANN training loss 1.036663\n",
      ">> Epoch 481 finished \tANN training loss 1.036807\n",
      ">> Epoch 482 finished \tANN training loss 1.036683\n",
      ">> Epoch 483 finished \tANN training loss 1.036792\n",
      ">> Epoch 484 finished \tANN training loss 1.036885\n",
      ">> Epoch 485 finished \tANN training loss 1.037090\n",
      ">> Epoch 486 finished \tANN training loss 1.036828\n",
      ">> Epoch 487 finished \tANN training loss 1.037366\n",
      ">> Epoch 488 finished \tANN training loss 1.037001\n",
      ">> Epoch 489 finished \tANN training loss 1.036689\n",
      ">> Epoch 490 finished \tANN training loss 1.037569\n",
      ">> Epoch 491 finished \tANN training loss 1.037200\n",
      ">> Epoch 492 finished \tANN training loss 1.036658\n",
      ">> Epoch 493 finished \tANN training loss 1.036936\n",
      ">> Epoch 494 finished \tANN training loss 1.037642\n",
      ">> Epoch 495 finished \tANN training loss 1.038003\n",
      ">> Epoch 496 finished \tANN training loss 1.038402\n",
      ">> Epoch 497 finished \tANN training loss 1.036713\n",
      ">> Epoch 498 finished \tANN training loss 1.036692\n",
      ">> Epoch 499 finished \tANN training loss 1.036664\n",
      ">> Epoch 500 finished \tANN training loss 1.037380\n",
      ">> Epoch 501 finished \tANN training loss 1.036664\n",
      ">> Epoch 502 finished \tANN training loss 1.036909\n",
      ">> Epoch 503 finished \tANN training loss 1.036686\n",
      ">> Epoch 504 finished \tANN training loss 1.037265\n",
      ">> Epoch 505 finished \tANN training loss 1.036671\n",
      ">> Epoch 506 finished \tANN training loss 1.036655\n",
      ">> Epoch 507 finished \tANN training loss 1.036824\n",
      ">> Epoch 508 finished \tANN training loss 1.036657\n",
      ">> Epoch 509 finished \tANN training loss 1.036966\n",
      ">> Epoch 510 finished \tANN training loss 1.036668\n",
      ">> Epoch 511 finished \tANN training loss 1.036663\n",
      ">> Epoch 512 finished \tANN training loss 1.036911\n",
      ">> Epoch 513 finished \tANN training loss 1.036661\n",
      ">> Epoch 514 finished \tANN training loss 1.036732\n",
      ">> Epoch 515 finished \tANN training loss 1.037258\n",
      ">> Epoch 516 finished \tANN training loss 1.036662\n",
      ">> Epoch 517 finished \tANN training loss 1.036688\n",
      ">> Epoch 518 finished \tANN training loss 1.036659\n",
      ">> Epoch 519 finished \tANN training loss 1.036682\n",
      ">> Epoch 520 finished \tANN training loss 1.036956\n",
      ">> Epoch 521 finished \tANN training loss 1.038250\n",
      ">> Epoch 522 finished \tANN training loss 1.036685\n",
      ">> Epoch 523 finished \tANN training loss 1.037387\n",
      ">> Epoch 524 finished \tANN training loss 1.036667\n",
      ">> Epoch 525 finished \tANN training loss 1.038149\n",
      ">> Epoch 526 finished \tANN training loss 1.038699\n",
      ">> Epoch 527 finished \tANN training loss 1.036975\n",
      ">> Epoch 528 finished \tANN training loss 1.038419\n",
      ">> Epoch 529 finished \tANN training loss 1.037269\n",
      ">> Epoch 530 finished \tANN training loss 1.036794\n",
      ">> Epoch 531 finished \tANN training loss 1.037244\n",
      ">> Epoch 532 finished \tANN training loss 1.036658\n",
      ">> Epoch 533 finished \tANN training loss 1.036660\n",
      ">> Epoch 534 finished \tANN training loss 1.036664\n",
      ">> Epoch 535 finished \tANN training loss 1.037372\n",
      ">> Epoch 536 finished \tANN training loss 1.037189\n",
      ">> Epoch 537 finished \tANN training loss 1.036958\n",
      ">> Epoch 538 finished \tANN training loss 1.037181\n",
      ">> Epoch 539 finished \tANN training loss 1.036957\n",
      ">> Epoch 540 finished \tANN training loss 1.036907\n",
      ">> Epoch 541 finished \tANN training loss 1.037274\n",
      ">> Epoch 542 finished \tANN training loss 1.036976\n",
      ">> Epoch 543 finished \tANN training loss 1.036872\n",
      ">> Epoch 544 finished \tANN training loss 1.036791\n",
      ">> Epoch 545 finished \tANN training loss 1.036678\n",
      ">> Epoch 546 finished \tANN training loss 1.036822\n",
      ">> Epoch 547 finished \tANN training loss 1.036960\n",
      ">> Epoch 548 finished \tANN training loss 1.038572\n",
      ">> Epoch 549 finished \tANN training loss 1.037045\n",
      ">> Epoch 550 finished \tANN training loss 1.037101\n",
      ">> Epoch 551 finished \tANN training loss 1.036706\n",
      ">> Epoch 552 finished \tANN training loss 1.036891\n",
      ">> Epoch 553 finished \tANN training loss 1.037285\n",
      ">> Epoch 554 finished \tANN training loss 1.036864\n",
      ">> Epoch 555 finished \tANN training loss 1.037031\n",
      ">> Epoch 556 finished \tANN training loss 1.036694\n",
      ">> Epoch 557 finished \tANN training loss 1.036942\n",
      ">> Epoch 558 finished \tANN training loss 1.036655\n",
      ">> Epoch 559 finished \tANN training loss 1.036663\n",
      ">> Epoch 560 finished \tANN training loss 1.036743\n",
      ">> Epoch 561 finished \tANN training loss 1.036692\n",
      ">> Epoch 562 finished \tANN training loss 1.037097\n",
      ">> Epoch 563 finished \tANN training loss 1.037549\n",
      ">> Epoch 564 finished \tANN training loss 1.036656\n",
      ">> Epoch 565 finished \tANN training loss 1.036661\n",
      ">> Epoch 566 finished \tANN training loss 1.036890\n",
      ">> Epoch 567 finished \tANN training loss 1.036674\n",
      ">> Epoch 568 finished \tANN training loss 1.036780\n",
      ">> Epoch 569 finished \tANN training loss 1.037696\n",
      ">> Epoch 570 finished \tANN training loss 1.037882\n",
      ">> Epoch 571 finished \tANN training loss 1.037661\n",
      ">> Epoch 572 finished \tANN training loss 1.037539\n",
      ">> Epoch 573 finished \tANN training loss 1.038636\n",
      ">> Epoch 574 finished \tANN training loss 1.037858\n",
      ">> Epoch 575 finished \tANN training loss 1.036899\n",
      ">> Epoch 576 finished \tANN training loss 1.036777\n",
      ">> Epoch 577 finished \tANN training loss 1.037073\n",
      ">> Epoch 578 finished \tANN training loss 1.037532\n",
      ">> Epoch 579 finished \tANN training loss 1.036923\n",
      ">> Epoch 580 finished \tANN training loss 1.036672\n",
      ">> Epoch 581 finished \tANN training loss 1.036743\n",
      ">> Epoch 582 finished \tANN training loss 1.038802\n",
      ">> Epoch 583 finished \tANN training loss 1.037081\n",
      ">> Epoch 584 finished \tANN training loss 1.036683\n",
      ">> Epoch 585 finished \tANN training loss 1.036780\n",
      ">> Epoch 586 finished \tANN training loss 1.036660\n",
      ">> Epoch 587 finished \tANN training loss 1.036826\n",
      ">> Epoch 588 finished \tANN training loss 1.036784\n",
      ">> Epoch 589 finished \tANN training loss 1.036744\n",
      ">> Epoch 590 finished \tANN training loss 1.036790\n",
      ">> Epoch 591 finished \tANN training loss 1.037355\n",
      ">> Epoch 592 finished \tANN training loss 1.036655\n",
      ">> Epoch 593 finished \tANN training loss 1.037323\n",
      ">> Epoch 594 finished \tANN training loss 1.037005\n",
      ">> Epoch 595 finished \tANN training loss 1.036759\n",
      ">> Epoch 596 finished \tANN training loss 1.036656\n",
      ">> Epoch 597 finished \tANN training loss 1.036858\n",
      ">> Epoch 598 finished \tANN training loss 1.036657\n",
      ">> Epoch 599 finished \tANN training loss 1.036714\n",
      ">> Epoch 600 finished \tANN training loss 1.037476\n",
      ">> Epoch 601 finished \tANN training loss 1.037435\n",
      ">> Epoch 602 finished \tANN training loss 1.036681\n",
      ">> Epoch 603 finished \tANN training loss 1.036719\n",
      ">> Epoch 604 finished \tANN training loss 1.037112\n",
      ">> Epoch 605 finished \tANN training loss 1.036953\n",
      ">> Epoch 606 finished \tANN training loss 1.037060\n",
      ">> Epoch 607 finished \tANN training loss 1.036656\n",
      ">> Epoch 608 finished \tANN training loss 1.038454\n",
      ">> Epoch 609 finished \tANN training loss 1.038297\n",
      ">> Epoch 610 finished \tANN training loss 1.037215\n",
      ">> Epoch 611 finished \tANN training loss 1.036700\n",
      ">> Epoch 612 finished \tANN training loss 1.036884\n",
      ">> Epoch 613 finished \tANN training loss 1.036690\n",
      ">> Epoch 614 finished \tANN training loss 1.036999\n",
      ">> Epoch 615 finished \tANN training loss 1.036714\n",
      ">> Epoch 616 finished \tANN training loss 1.036658\n",
      ">> Epoch 617 finished \tANN training loss 1.036805\n",
      ">> Epoch 618 finished \tANN training loss 1.036710\n",
      ">> Epoch 619 finished \tANN training loss 1.036693\n",
      ">> Epoch 620 finished \tANN training loss 1.036700\n",
      ">> Epoch 621 finished \tANN training loss 1.037161\n",
      ">> Epoch 622 finished \tANN training loss 1.037670\n",
      ">> Epoch 623 finished \tANN training loss 1.036674\n",
      ">> Epoch 624 finished \tANN training loss 1.036993\n",
      ">> Epoch 625 finished \tANN training loss 1.036917\n",
      ">> Epoch 626 finished \tANN training loss 1.036883\n",
      ">> Epoch 627 finished \tANN training loss 1.036663\n",
      ">> Epoch 628 finished \tANN training loss 1.036765\n",
      ">> Epoch 629 finished \tANN training loss 1.036933\n",
      ">> Epoch 630 finished \tANN training loss 1.036666\n",
      ">> Epoch 631 finished \tANN training loss 1.036709\n",
      ">> Epoch 632 finished \tANN training loss 1.036853\n",
      ">> Epoch 633 finished \tANN training loss 1.037777\n",
      ">> Epoch 634 finished \tANN training loss 1.036655\n",
      ">> Epoch 635 finished \tANN training loss 1.036740\n",
      ">> Epoch 636 finished \tANN training loss 1.038170\n",
      ">> Epoch 637 finished \tANN training loss 1.037578\n",
      ">> Epoch 638 finished \tANN training loss 1.037746\n",
      ">> Epoch 639 finished \tANN training loss 1.038368\n",
      ">> Epoch 640 finished \tANN training loss 1.037210\n",
      ">> Epoch 641 finished \tANN training loss 1.036669\n",
      ">> Epoch 642 finished \tANN training loss 1.036657\n",
      ">> Epoch 643 finished \tANN training loss 1.037032\n",
      ">> Epoch 644 finished \tANN training loss 1.036674\n",
      ">> Epoch 645 finished \tANN training loss 1.036795\n",
      ">> Epoch 646 finished \tANN training loss 1.036880\n",
      ">> Epoch 647 finished \tANN training loss 1.037130\n",
      ">> Epoch 648 finished \tANN training loss 1.036655\n",
      ">> Epoch 649 finished \tANN training loss 1.037430\n",
      ">> Epoch 650 finished \tANN training loss 1.037042\n",
      ">> Epoch 651 finished \tANN training loss 1.037004\n",
      ">> Epoch 652 finished \tANN training loss 1.037741\n",
      ">> Epoch 653 finished \tANN training loss 1.037792\n",
      ">> Epoch 654 finished \tANN training loss 1.036787\n",
      ">> Epoch 655 finished \tANN training loss 1.036662\n",
      ">> Epoch 656 finished \tANN training loss 1.036722\n",
      ">> Epoch 657 finished \tANN training loss 1.036704\n",
      ">> Epoch 658 finished \tANN training loss 1.036665\n",
      ">> Epoch 659 finished \tANN training loss 1.036830\n",
      ">> Epoch 660 finished \tANN training loss 1.037022\n",
      ">> Epoch 661 finished \tANN training loss 1.037616\n",
      ">> Epoch 662 finished \tANN training loss 1.036674\n",
      ">> Epoch 663 finished \tANN training loss 1.036910\n",
      ">> Epoch 664 finished \tANN training loss 1.037987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 665 finished \tANN training loss 1.036759\n",
      ">> Epoch 666 finished \tANN training loss 1.037264\n",
      ">> Epoch 667 finished \tANN training loss 1.039240\n",
      ">> Epoch 668 finished \tANN training loss 1.037908\n",
      ">> Epoch 669 finished \tANN training loss 1.037141\n",
      ">> Epoch 670 finished \tANN training loss 1.037042\n",
      ">> Epoch 671 finished \tANN training loss 1.036762\n",
      ">> Epoch 672 finished \tANN training loss 1.036763\n",
      ">> Epoch 673 finished \tANN training loss 1.037289\n",
      ">> Epoch 674 finished \tANN training loss 1.037027\n",
      ">> Epoch 675 finished \tANN training loss 1.037379\n",
      ">> Epoch 676 finished \tANN training loss 1.036926\n",
      ">> Epoch 677 finished \tANN training loss 1.037409\n",
      ">> Epoch 678 finished \tANN training loss 1.036715\n",
      ">> Epoch 679 finished \tANN training loss 1.037424\n",
      ">> Epoch 680 finished \tANN training loss 1.037949\n",
      ">> Epoch 681 finished \tANN training loss 1.036703\n",
      ">> Epoch 682 finished \tANN training loss 1.036806\n",
      ">> Epoch 683 finished \tANN training loss 1.037487\n",
      ">> Epoch 684 finished \tANN training loss 1.038477\n",
      ">> Epoch 685 finished \tANN training loss 1.037333\n",
      ">> Epoch 686 finished \tANN training loss 1.037105\n",
      ">> Epoch 687 finished \tANN training loss 1.036893\n",
      ">> Epoch 688 finished \tANN training loss 1.036672\n",
      ">> Epoch 689 finished \tANN training loss 1.037049\n",
      ">> Epoch 690 finished \tANN training loss 1.036931\n",
      ">> Epoch 691 finished \tANN training loss 1.037050\n",
      ">> Epoch 692 finished \tANN training loss 1.037641\n",
      ">> Epoch 693 finished \tANN training loss 1.037685\n",
      ">> Epoch 694 finished \tANN training loss 1.036707\n",
      ">> Epoch 695 finished \tANN training loss 1.036816\n",
      ">> Epoch 696 finished \tANN training loss 1.036888\n",
      ">> Epoch 697 finished \tANN training loss 1.037223\n",
      ">> Epoch 698 finished \tANN training loss 1.036734\n",
      ">> Epoch 699 finished \tANN training loss 1.036790\n",
      ">> Epoch 700 finished \tANN training loss 1.036802\n",
      ">> Epoch 701 finished \tANN training loss 1.036731\n",
      ">> Epoch 702 finished \tANN training loss 1.036658\n",
      ">> Epoch 703 finished \tANN training loss 1.036668\n",
      ">> Epoch 704 finished \tANN training loss 1.036667\n",
      ">> Epoch 705 finished \tANN training loss 1.036675\n",
      ">> Epoch 706 finished \tANN training loss 1.036864\n",
      ">> Epoch 707 finished \tANN training loss 1.037149\n",
      ">> Epoch 708 finished \tANN training loss 1.036818\n",
      ">> Epoch 709 finished \tANN training loss 1.038444\n",
      ">> Epoch 710 finished \tANN training loss 1.036978\n",
      ">> Epoch 711 finished \tANN training loss 1.037796\n",
      ">> Epoch 712 finished \tANN training loss 1.036887\n",
      ">> Epoch 713 finished \tANN training loss 1.038684\n",
      ">> Epoch 714 finished \tANN training loss 1.037287\n",
      ">> Epoch 715 finished \tANN training loss 1.036681\n",
      ">> Epoch 716 finished \tANN training loss 1.036848\n",
      ">> Epoch 717 finished \tANN training loss 1.036750\n",
      ">> Epoch 718 finished \tANN training loss 1.036987\n",
      ">> Epoch 719 finished \tANN training loss 1.037338\n",
      ">> Epoch 720 finished \tANN training loss 1.037002\n",
      ">> Epoch 721 finished \tANN training loss 1.036915\n",
      ">> Epoch 722 finished \tANN training loss 1.037750\n",
      ">> Epoch 723 finished \tANN training loss 1.036988\n",
      ">> Epoch 724 finished \tANN training loss 1.038112\n",
      ">> Epoch 725 finished \tANN training loss 1.037768\n",
      ">> Epoch 726 finished \tANN training loss 1.037852\n",
      ">> Epoch 727 finished \tANN training loss 1.036672\n",
      ">> Epoch 728 finished \tANN training loss 1.036819\n",
      ">> Epoch 729 finished \tANN training loss 1.036819\n",
      ">> Epoch 730 finished \tANN training loss 1.036659\n",
      ">> Epoch 731 finished \tANN training loss 1.037655\n",
      ">> Epoch 732 finished \tANN training loss 1.037594\n",
      ">> Epoch 733 finished \tANN training loss 1.038526\n",
      ">> Epoch 734 finished \tANN training loss 1.038458\n",
      ">> Epoch 735 finished \tANN training loss 1.037734\n",
      ">> Epoch 736 finished \tANN training loss 1.037652\n",
      ">> Epoch 737 finished \tANN training loss 1.036784\n",
      ">> Epoch 738 finished \tANN training loss 1.036759\n",
      ">> Epoch 739 finished \tANN training loss 1.036701\n",
      ">> Epoch 740 finished \tANN training loss 1.036662\n",
      ">> Epoch 741 finished \tANN training loss 1.036660\n",
      ">> Epoch 742 finished \tANN training loss 1.037175\n",
      ">> Epoch 743 finished \tANN training loss 1.036708\n",
      ">> Epoch 744 finished \tANN training loss 1.037192\n",
      ">> Epoch 745 finished \tANN training loss 1.038544\n",
      ">> Epoch 746 finished \tANN training loss 1.036772\n",
      ">> Epoch 747 finished \tANN training loss 1.036963\n",
      ">> Epoch 748 finished \tANN training loss 1.036656\n",
      ">> Epoch 749 finished \tANN training loss 1.036786\n",
      ">> Epoch 750 finished \tANN training loss 1.036682\n",
      ">> Epoch 751 finished \tANN training loss 1.036853\n",
      ">> Epoch 752 finished \tANN training loss 1.036670\n",
      ">> Epoch 753 finished \tANN training loss 1.036847\n",
      ">> Epoch 754 finished \tANN training loss 1.036837\n",
      ">> Epoch 755 finished \tANN training loss 1.036743\n",
      ">> Epoch 756 finished \tANN training loss 1.036755\n",
      ">> Epoch 757 finished \tANN training loss 1.036901\n",
      ">> Epoch 758 finished \tANN training loss 1.036713\n",
      ">> Epoch 759 finished \tANN training loss 1.036964\n",
      ">> Epoch 760 finished \tANN training loss 1.036896\n",
      ">> Epoch 761 finished \tANN training loss 1.036657\n",
      ">> Epoch 762 finished \tANN training loss 1.036829\n",
      ">> Epoch 763 finished \tANN training loss 1.036845\n",
      ">> Epoch 764 finished \tANN training loss 1.036685\n",
      ">> Epoch 765 finished \tANN training loss 1.037812\n",
      ">> Epoch 766 finished \tANN training loss 1.038322\n",
      ">> Epoch 767 finished \tANN training loss 1.038391\n",
      ">> Epoch 768 finished \tANN training loss 1.037319\n",
      ">> Epoch 769 finished \tANN training loss 1.037272\n",
      ">> Epoch 770 finished \tANN training loss 1.037722\n",
      ">> Epoch 771 finished \tANN training loss 1.038305\n",
      ">> Epoch 772 finished \tANN training loss 1.039768\n",
      ">> Epoch 773 finished \tANN training loss 1.039993\n",
      ">> Epoch 774 finished \tANN training loss 1.036815\n",
      ">> Epoch 775 finished \tANN training loss 1.037146\n",
      ">> Epoch 776 finished \tANN training loss 1.036707\n",
      ">> Epoch 777 finished \tANN training loss 1.039191\n",
      ">> Epoch 778 finished \tANN training loss 1.037505\n",
      ">> Epoch 779 finished \tANN training loss 1.036851\n",
      ">> Epoch 780 finished \tANN training loss 1.037757\n",
      ">> Epoch 781 finished \tANN training loss 1.036890\n",
      ">> Epoch 782 finished \tANN training loss 1.036673\n",
      ">> Epoch 783 finished \tANN training loss 1.036684\n",
      ">> Epoch 784 finished \tANN training loss 1.036660\n",
      ">> Epoch 785 finished \tANN training loss 1.037060\n",
      ">> Epoch 786 finished \tANN training loss 1.036666\n",
      ">> Epoch 787 finished \tANN training loss 1.037591\n",
      ">> Epoch 788 finished \tANN training loss 1.040018\n",
      ">> Epoch 789 finished \tANN training loss 1.038398\n",
      ">> Epoch 790 finished \tANN training loss 1.038309\n",
      ">> Epoch 791 finished \tANN training loss 1.037417\n",
      ">> Epoch 792 finished \tANN training loss 1.037056\n",
      ">> Epoch 793 finished \tANN training loss 1.036693\n",
      ">> Epoch 794 finished \tANN training loss 1.036839\n",
      ">> Epoch 795 finished \tANN training loss 1.037227\n",
      ">> Epoch 796 finished \tANN training loss 1.037050\n",
      ">> Epoch 797 finished \tANN training loss 1.036693\n",
      ">> Epoch 798 finished \tANN training loss 1.037165\n",
      ">> Epoch 799 finished \tANN training loss 1.036822\n",
      ">> Epoch 800 finished \tANN training loss 1.036765\n",
      ">> Epoch 801 finished \tANN training loss 1.036913\n",
      ">> Epoch 802 finished \tANN training loss 1.036730\n",
      ">> Epoch 803 finished \tANN training loss 1.037603\n",
      ">> Epoch 804 finished \tANN training loss 1.036808\n",
      ">> Epoch 805 finished \tANN training loss 1.036967\n",
      ">> Epoch 806 finished \tANN training loss 1.037035\n",
      ">> Epoch 807 finished \tANN training loss 1.036764\n",
      ">> Epoch 808 finished \tANN training loss 1.037051\n",
      ">> Epoch 809 finished \tANN training loss 1.037095\n",
      ">> Epoch 810 finished \tANN training loss 1.039206\n",
      ">> Epoch 811 finished \tANN training loss 1.038290\n",
      ">> Epoch 812 finished \tANN training loss 1.037017\n",
      ">> Epoch 813 finished \tANN training loss 1.036998\n",
      ">> Epoch 814 finished \tANN training loss 1.037224\n",
      ">> Epoch 815 finished \tANN training loss 1.038294\n",
      ">> Epoch 816 finished \tANN training loss 1.036819\n",
      ">> Epoch 817 finished \tANN training loss 1.038445\n",
      ">> Epoch 818 finished \tANN training loss 1.040948\n",
      ">> Epoch 819 finished \tANN training loss 1.037130\n",
      ">> Epoch 820 finished \tANN training loss 1.038056\n",
      ">> Epoch 821 finished \tANN training loss 1.036742\n",
      ">> Epoch 822 finished \tANN training loss 1.036656\n",
      ">> Epoch 823 finished \tANN training loss 1.036718\n",
      ">> Epoch 824 finished \tANN training loss 1.037322\n",
      ">> Epoch 825 finished \tANN training loss 1.036860\n",
      ">> Epoch 826 finished \tANN training loss 1.036907\n",
      ">> Epoch 827 finished \tANN training loss 1.036707\n",
      ">> Epoch 828 finished \tANN training loss 1.036842\n",
      ">> Epoch 829 finished \tANN training loss 1.036965\n",
      ">> Epoch 830 finished \tANN training loss 1.036689\n",
      ">> Epoch 831 finished \tANN training loss 1.036670\n",
      ">> Epoch 832 finished \tANN training loss 1.036659\n",
      ">> Epoch 833 finished \tANN training loss 1.037359\n",
      ">> Epoch 834 finished \tANN training loss 1.036658\n",
      ">> Epoch 835 finished \tANN training loss 1.037168\n",
      ">> Epoch 836 finished \tANN training loss 1.037819\n",
      ">> Epoch 837 finished \tANN training loss 1.037999\n",
      ">> Epoch 838 finished \tANN training loss 1.036757\n",
      ">> Epoch 839 finished \tANN training loss 1.037861\n",
      ">> Epoch 840 finished \tANN training loss 1.036766\n",
      ">> Epoch 841 finished \tANN training loss 1.036665\n",
      ">> Epoch 842 finished \tANN training loss 1.038260\n",
      ">> Epoch 843 finished \tANN training loss 1.036694\n",
      ">> Epoch 844 finished \tANN training loss 1.036941\n",
      ">> Epoch 845 finished \tANN training loss 1.036681\n",
      ">> Epoch 846 finished \tANN training loss 1.037188\n",
      ">> Epoch 847 finished \tANN training loss 1.037145\n",
      ">> Epoch 848 finished \tANN training loss 1.037789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 849 finished \tANN training loss 1.036727\n",
      ">> Epoch 850 finished \tANN training loss 1.036800\n",
      ">> Epoch 851 finished \tANN training loss 1.036825\n",
      ">> Epoch 852 finished \tANN training loss 1.037405\n",
      ">> Epoch 853 finished \tANN training loss 1.036746\n",
      ">> Epoch 854 finished \tANN training loss 1.036671\n",
      ">> Epoch 855 finished \tANN training loss 1.037142\n",
      ">> Epoch 856 finished \tANN training loss 1.036854\n",
      ">> Epoch 857 finished \tANN training loss 1.037572\n",
      ">> Epoch 858 finished \tANN training loss 1.037104\n",
      ">> Epoch 859 finished \tANN training loss 1.036826\n",
      ">> Epoch 860 finished \tANN training loss 1.036666\n",
      ">> Epoch 861 finished \tANN training loss 1.036942\n",
      ">> Epoch 862 finished \tANN training loss 1.037149\n",
      ">> Epoch 863 finished \tANN training loss 1.037450\n",
      ">> Epoch 864 finished \tANN training loss 1.038333\n",
      ">> Epoch 865 finished \tANN training loss 1.037028\n",
      ">> Epoch 866 finished \tANN training loss 1.036896\n",
      ">> Epoch 867 finished \tANN training loss 1.036937\n",
      ">> Epoch 868 finished \tANN training loss 1.037388\n",
      ">> Epoch 869 finished \tANN training loss 1.038038\n",
      ">> Epoch 870 finished \tANN training loss 1.036973\n",
      ">> Epoch 871 finished \tANN training loss 1.036726\n",
      ">> Epoch 872 finished \tANN training loss 1.036806\n",
      ">> Epoch 873 finished \tANN training loss 1.036695\n",
      ">> Epoch 874 finished \tANN training loss 1.036661\n",
      ">> Epoch 875 finished \tANN training loss 1.036663\n",
      ">> Epoch 876 finished \tANN training loss 1.036826\n",
      ">> Epoch 877 finished \tANN training loss 1.036743\n",
      ">> Epoch 878 finished \tANN training loss 1.037415\n",
      ">> Epoch 879 finished \tANN training loss 1.037658\n",
      ">> Epoch 880 finished \tANN training loss 1.036671\n",
      ">> Epoch 881 finished \tANN training loss 1.036655\n",
      ">> Epoch 882 finished \tANN training loss 1.036753\n",
      ">> Epoch 883 finished \tANN training loss 1.036657\n",
      ">> Epoch 884 finished \tANN training loss 1.036821\n",
      ">> Epoch 885 finished \tANN training loss 1.037035\n",
      ">> Epoch 886 finished \tANN training loss 1.036771\n",
      ">> Epoch 887 finished \tANN training loss 1.036858\n",
      ">> Epoch 888 finished \tANN training loss 1.036829\n",
      ">> Epoch 889 finished \tANN training loss 1.037569\n",
      ">> Epoch 890 finished \tANN training loss 1.039209\n",
      ">> Epoch 891 finished \tANN training loss 1.037257\n",
      ">> Epoch 892 finished \tANN training loss 1.036805\n",
      ">> Epoch 893 finished \tANN training loss 1.037230\n",
      ">> Epoch 894 finished \tANN training loss 1.036664\n",
      ">> Epoch 895 finished \tANN training loss 1.036694\n",
      ">> Epoch 896 finished \tANN training loss 1.036694\n",
      ">> Epoch 897 finished \tANN training loss 1.036744\n",
      ">> Epoch 898 finished \tANN training loss 1.037076\n",
      ">> Epoch 899 finished \tANN training loss 1.036754\n",
      ">> Epoch 900 finished \tANN training loss 1.036927\n",
      ">> Epoch 901 finished \tANN training loss 1.036677\n",
      ">> Epoch 902 finished \tANN training loss 1.036739\n",
      ">> Epoch 903 finished \tANN training loss 1.037096\n",
      ">> Epoch 904 finished \tANN training loss 1.036993\n",
      ">> Epoch 905 finished \tANN training loss 1.036675\n",
      ">> Epoch 906 finished \tANN training loss 1.036655\n",
      ">> Epoch 907 finished \tANN training loss 1.037455\n",
      ">> Epoch 908 finished \tANN training loss 1.037119\n",
      ">> Epoch 909 finished \tANN training loss 1.036664\n",
      ">> Epoch 910 finished \tANN training loss 1.036658\n",
      ">> Epoch 911 finished \tANN training loss 1.036964\n",
      ">> Epoch 912 finished \tANN training loss 1.038279\n",
      ">> Epoch 913 finished \tANN training loss 1.037086\n",
      ">> Epoch 914 finished \tANN training loss 1.036656\n",
      ">> Epoch 915 finished \tANN training loss 1.037133\n",
      ">> Epoch 916 finished \tANN training loss 1.037303\n",
      ">> Epoch 917 finished \tANN training loss 1.039424\n",
      ">> Epoch 918 finished \tANN training loss 1.039151\n",
      ">> Epoch 919 finished \tANN training loss 1.037153\n",
      ">> Epoch 920 finished \tANN training loss 1.036669\n",
      ">> Epoch 921 finished \tANN training loss 1.036659\n",
      ">> Epoch 922 finished \tANN training loss 1.036661\n",
      ">> Epoch 923 finished \tANN training loss 1.036658\n",
      ">> Epoch 924 finished \tANN training loss 1.037513\n",
      ">> Epoch 925 finished \tANN training loss 1.036831\n",
      ">> Epoch 926 finished \tANN training loss 1.036673\n",
      ">> Epoch 927 finished \tANN training loss 1.036670\n",
      ">> Epoch 928 finished \tANN training loss 1.036747\n",
      ">> Epoch 929 finished \tANN training loss 1.036797\n",
      ">> Epoch 930 finished \tANN training loss 1.036861\n",
      ">> Epoch 931 finished \tANN training loss 1.037099\n",
      ">> Epoch 932 finished \tANN training loss 1.037053\n",
      ">> Epoch 933 finished \tANN training loss 1.036656\n",
      ">> Epoch 934 finished \tANN training loss 1.036783\n",
      ">> Epoch 935 finished \tANN training loss 1.036991\n",
      ">> Epoch 936 finished \tANN training loss 1.037694\n",
      ">> Epoch 937 finished \tANN training loss 1.036655\n",
      ">> Epoch 938 finished \tANN training loss 1.036684\n",
      ">> Epoch 939 finished \tANN training loss 1.036670\n",
      ">> Epoch 940 finished \tANN training loss 1.036748\n",
      ">> Epoch 941 finished \tANN training loss 1.036721\n",
      ">> Epoch 942 finished \tANN training loss 1.036733\n",
      ">> Epoch 943 finished \tANN training loss 1.036718\n",
      ">> Epoch 944 finished \tANN training loss 1.036928\n",
      ">> Epoch 945 finished \tANN training loss 1.036655\n",
      ">> Epoch 946 finished \tANN training loss 1.036856\n",
      ">> Epoch 947 finished \tANN training loss 1.037813\n",
      ">> Epoch 948 finished \tANN training loss 1.039356\n",
      ">> Epoch 949 finished \tANN training loss 1.036661\n",
      ">> Epoch 950 finished \tANN training loss 1.037232\n",
      ">> Epoch 951 finished \tANN training loss 1.036671\n",
      ">> Epoch 952 finished \tANN training loss 1.036665\n",
      ">> Epoch 953 finished \tANN training loss 1.036724\n",
      ">> Epoch 954 finished \tANN training loss 1.036822\n",
      ">> Epoch 955 finished \tANN training loss 1.037386\n",
      ">> Epoch 956 finished \tANN training loss 1.037162\n",
      ">> Epoch 957 finished \tANN training loss 1.036828\n",
      ">> Epoch 958 finished \tANN training loss 1.036848\n",
      ">> Epoch 959 finished \tANN training loss 1.036983\n",
      ">> Epoch 960 finished \tANN training loss 1.036677\n",
      ">> Epoch 961 finished \tANN training loss 1.037196\n",
      ">> Epoch 962 finished \tANN training loss 1.037152\n",
      ">> Epoch 963 finished \tANN training loss 1.036658\n",
      ">> Epoch 964 finished \tANN training loss 1.036918\n",
      ">> Epoch 965 finished \tANN training loss 1.036736\n",
      ">> Epoch 966 finished \tANN training loss 1.036663\n",
      ">> Epoch 967 finished \tANN training loss 1.036655\n",
      ">> Epoch 968 finished \tANN training loss 1.036706\n",
      ">> Epoch 969 finished \tANN training loss 1.038236\n",
      ">> Epoch 970 finished \tANN training loss 1.037472\n",
      ">> Epoch 971 finished \tANN training loss 1.036724\n",
      ">> Epoch 972 finished \tANN training loss 1.036806\n",
      ">> Epoch 973 finished \tANN training loss 1.036824\n",
      ">> Epoch 974 finished \tANN training loss 1.037653\n",
      ">> Epoch 975 finished \tANN training loss 1.037145\n",
      ">> Epoch 976 finished \tANN training loss 1.037465\n",
      ">> Epoch 977 finished \tANN training loss 1.036676\n",
      ">> Epoch 978 finished \tANN training loss 1.036669\n",
      ">> Epoch 979 finished \tANN training loss 1.036665\n",
      ">> Epoch 980 finished \tANN training loss 1.036856\n",
      ">> Epoch 981 finished \tANN training loss 1.036739\n",
      ">> Epoch 982 finished \tANN training loss 1.036658\n",
      ">> Epoch 983 finished \tANN training loss 1.036656\n",
      ">> Epoch 984 finished \tANN training loss 1.036712\n",
      ">> Epoch 985 finished \tANN training loss 1.036667\n",
      ">> Epoch 986 finished \tANN training loss 1.037176\n",
      ">> Epoch 987 finished \tANN training loss 1.036657\n",
      ">> Epoch 988 finished \tANN training loss 1.036680\n",
      ">> Epoch 989 finished \tANN training loss 1.036684\n",
      ">> Epoch 990 finished \tANN training loss 1.036787\n",
      ">> Epoch 991 finished \tANN training loss 1.037026\n",
      ">> Epoch 992 finished \tANN training loss 1.037517\n",
      ">> Epoch 993 finished \tANN training loss 1.038512\n",
      ">> Epoch 994 finished \tANN training loss 1.037795\n",
      ">> Epoch 995 finished \tANN training loss 1.036658\n",
      ">> Epoch 996 finished \tANN training loss 1.036747\n",
      ">> Epoch 997 finished \tANN training loss 1.036760\n",
      ">> Epoch 998 finished \tANN training loss 1.036729\n",
      ">> Epoch 999 finished \tANN training loss 1.036733\n",
      "[END] Fine tuning step\n",
      "\n",
      "\n",
      "TRIAL:  7\n",
      "\n",
      "\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 5.798211\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 4.771605\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 4.330514\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 4.703944\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 5.419348\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 6.597163\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 8.489375\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 11.888603\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 17.222807\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 23.164849\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 61.398518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 2 finished \tRBM Reconstruction error 662.502869\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 1310.465698\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 1937.673950\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 2547.290283\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 2815.357910\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 3164.204102\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 3264.854492\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 3346.274658\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 3533.567627\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 614567744.000000\n",
      ">> Epoch 1 finished \tANN training loss 673769.500000\n",
      ">> Epoch 2 finished \tANN training loss 283308.562500\n",
      ">> Epoch 3 finished \tANN training loss 119152.617188\n",
      ">> Epoch 4 finished \tANN training loss 50099.914062\n",
      ">> Epoch 5 finished \tANN training loss 21068.574219\n",
      ">> Epoch 6 finished \tANN training loss 8855.821289\n",
      ">> Epoch 7 finished \tANN training loss 3726.868652\n",
      ">> Epoch 8 finished \tANN training loss 1566.757812\n",
      ">> Epoch 9 finished \tANN training loss 660.020203\n",
      ">> Epoch 10 finished \tANN training loss 278.876129\n",
      ">> Epoch 11 finished \tANN training loss 117.557358\n",
      ">> Epoch 12 finished \tANN training loss 49.890388\n",
      ">> Epoch 13 finished \tANN training loss 21.358387\n",
      ">> Epoch 14 finished \tANN training loss 9.583838\n",
      ">> Epoch 15 finished \tANN training loss 4.594770\n",
      ">> Epoch 16 finished \tANN training loss 2.532008\n",
      ">> Epoch 17 finished \tANN training loss 1.665489\n",
      ">> Epoch 18 finished \tANN training loss 1.299626\n",
      ">> Epoch 19 finished \tANN training loss 1.159672\n",
      ">> Epoch 20 finished \tANN training loss 1.087226\n",
      ">> Epoch 21 finished \tANN training loss 1.059545\n",
      ">> Epoch 22 finished \tANN training loss 1.051737\n",
      ">> Epoch 23 finished \tANN training loss 1.043288\n",
      ">> Epoch 24 finished \tANN training loss 1.037345\n",
      ">> Epoch 25 finished \tANN training loss 1.036705\n",
      ">> Epoch 26 finished \tANN training loss 1.037691\n",
      ">> Epoch 27 finished \tANN training loss 1.036715\n",
      ">> Epoch 28 finished \tANN training loss 1.036713\n",
      ">> Epoch 29 finished \tANN training loss 1.036884\n",
      ">> Epoch 30 finished \tANN training loss 1.036720\n",
      ">> Epoch 31 finished \tANN training loss 1.036919\n",
      ">> Epoch 32 finished \tANN training loss 1.036733\n",
      ">> Epoch 33 finished \tANN training loss 1.036828\n",
      ">> Epoch 34 finished \tANN training loss 1.036717\n",
      ">> Epoch 35 finished \tANN training loss 1.036656\n",
      ">> Epoch 36 finished \tANN training loss 1.037099\n",
      ">> Epoch 37 finished \tANN training loss 1.038049\n",
      ">> Epoch 38 finished \tANN training loss 1.037597\n",
      ">> Epoch 39 finished \tANN training loss 1.037454\n",
      ">> Epoch 40 finished \tANN training loss 1.038001\n",
      ">> Epoch 41 finished \tANN training loss 1.039609\n",
      ">> Epoch 42 finished \tANN training loss 1.037336\n",
      ">> Epoch 43 finished \tANN training loss 1.037751\n",
      ">> Epoch 44 finished \tANN training loss 1.037741\n",
      ">> Epoch 45 finished \tANN training loss 1.038111\n",
      ">> Epoch 46 finished \tANN training loss 1.036915\n",
      ">> Epoch 47 finished \tANN training loss 1.037045\n",
      ">> Epoch 48 finished \tANN training loss 1.036668\n",
      ">> Epoch 49 finished \tANN training loss 1.036656\n",
      ">> Epoch 50 finished \tANN training loss 1.037489\n",
      ">> Epoch 51 finished \tANN training loss 1.037025\n",
      ">> Epoch 52 finished \tANN training loss 1.036655\n",
      ">> Epoch 53 finished \tANN training loss 1.036895\n",
      ">> Epoch 54 finished \tANN training loss 1.039279\n",
      ">> Epoch 55 finished \tANN training loss 1.037927\n",
      ">> Epoch 56 finished \tANN training loss 1.036839\n",
      ">> Epoch 57 finished \tANN training loss 1.037518\n",
      ">> Epoch 58 finished \tANN training loss 1.037210\n",
      ">> Epoch 59 finished \tANN training loss 1.037330\n",
      ">> Epoch 60 finished \tANN training loss 1.036911\n",
      ">> Epoch 61 finished \tANN training loss 1.036658\n",
      ">> Epoch 62 finished \tANN training loss 1.036952\n",
      ">> Epoch 63 finished \tANN training loss 1.039771\n",
      ">> Epoch 64 finished \tANN training loss 1.036826\n",
      ">> Epoch 65 finished \tANN training loss 1.036973\n",
      ">> Epoch 66 finished \tANN training loss 1.036777\n",
      ">> Epoch 67 finished \tANN training loss 1.036668\n",
      ">> Epoch 68 finished \tANN training loss 1.036788\n",
      ">> Epoch 69 finished \tANN training loss 1.036801\n",
      ">> Epoch 70 finished \tANN training loss 1.037956\n",
      ">> Epoch 71 finished \tANN training loss 1.038758\n",
      ">> Epoch 72 finished \tANN training loss 1.036957\n",
      ">> Epoch 73 finished \tANN training loss 1.036703\n",
      ">> Epoch 74 finished \tANN training loss 1.036655\n",
      ">> Epoch 75 finished \tANN training loss 1.036762\n",
      ">> Epoch 76 finished \tANN training loss 1.036683\n",
      ">> Epoch 77 finished \tANN training loss 1.036812\n",
      ">> Epoch 78 finished \tANN training loss 1.036986\n",
      ">> Epoch 79 finished \tANN training loss 1.037897\n",
      ">> Epoch 80 finished \tANN training loss 1.036872\n",
      ">> Epoch 81 finished \tANN training loss 1.037459\n",
      ">> Epoch 82 finished \tANN training loss 1.037131\n",
      ">> Epoch 83 finished \tANN training loss 1.037077\n",
      ">> Epoch 84 finished \tANN training loss 1.037307\n",
      ">> Epoch 85 finished \tANN training loss 1.037321\n",
      ">> Epoch 86 finished \tANN training loss 1.037805\n",
      ">> Epoch 87 finished \tANN training loss 1.036698\n",
      ">> Epoch 88 finished \tANN training loss 1.037210\n",
      ">> Epoch 89 finished \tANN training loss 1.036679\n",
      ">> Epoch 90 finished \tANN training loss 1.037085\n",
      ">> Epoch 91 finished \tANN training loss 1.037932\n",
      ">> Epoch 92 finished \tANN training loss 1.036871\n",
      ">> Epoch 93 finished \tANN training loss 1.036755\n",
      ">> Epoch 94 finished \tANN training loss 1.036673\n",
      ">> Epoch 95 finished \tANN training loss 1.036742\n",
      ">> Epoch 96 finished \tANN training loss 1.036795\n",
      ">> Epoch 97 finished \tANN training loss 1.036889\n",
      ">> Epoch 98 finished \tANN training loss 1.036713\n",
      ">> Epoch 99 finished \tANN training loss 1.036664\n",
      ">> Epoch 100 finished \tANN training loss 1.036678\n",
      ">> Epoch 101 finished \tANN training loss 1.036679\n",
      ">> Epoch 102 finished \tANN training loss 1.037474\n",
      ">> Epoch 103 finished \tANN training loss 1.036994\n",
      ">> Epoch 104 finished \tANN training loss 1.036693\n",
      ">> Epoch 105 finished \tANN training loss 1.036657\n",
      ">> Epoch 106 finished \tANN training loss 1.037133\n",
      ">> Epoch 107 finished \tANN training loss 1.036759\n",
      ">> Epoch 108 finished \tANN training loss 1.036658\n",
      ">> Epoch 109 finished \tANN training loss 1.036701\n",
      ">> Epoch 110 finished \tANN training loss 1.036725\n",
      ">> Epoch 111 finished \tANN training loss 1.036687\n",
      ">> Epoch 112 finished \tANN training loss 1.037147\n",
      ">> Epoch 113 finished \tANN training loss 1.036814\n",
      ">> Epoch 114 finished \tANN training loss 1.036662\n",
      ">> Epoch 115 finished \tANN training loss 1.036787\n",
      ">> Epoch 116 finished \tANN training loss 1.036684\n",
      ">> Epoch 117 finished \tANN training loss 1.036700\n",
      ">> Epoch 118 finished \tANN training loss 1.037269\n",
      ">> Epoch 119 finished \tANN training loss 1.036704\n",
      ">> Epoch 120 finished \tANN training loss 1.037956\n",
      ">> Epoch 121 finished \tANN training loss 1.037763\n",
      ">> Epoch 122 finished \tANN training loss 1.037553\n",
      ">> Epoch 123 finished \tANN training loss 1.036722\n",
      ">> Epoch 124 finished \tANN training loss 1.036856\n",
      ">> Epoch 125 finished \tANN training loss 1.036949\n",
      ">> Epoch 126 finished \tANN training loss 1.036754\n",
      ">> Epoch 127 finished \tANN training loss 1.036663\n",
      ">> Epoch 128 finished \tANN training loss 1.037638\n",
      ">> Epoch 129 finished \tANN training loss 1.037764\n",
      ">> Epoch 130 finished \tANN training loss 1.038246\n",
      ">> Epoch 131 finished \tANN training loss 1.036906\n",
      ">> Epoch 132 finished \tANN training loss 1.036663\n",
      ">> Epoch 133 finished \tANN training loss 1.037914\n",
      ">> Epoch 134 finished \tANN training loss 1.040329\n",
      ">> Epoch 135 finished \tANN training loss 1.037649\n",
      ">> Epoch 136 finished \tANN training loss 1.039483\n",
      ">> Epoch 137 finished \tANN training loss 1.037558\n",
      ">> Epoch 138 finished \tANN training loss 1.036720\n",
      ">> Epoch 139 finished \tANN training loss 1.037688\n",
      ">> Epoch 140 finished \tANN training loss 1.036933\n",
      ">> Epoch 141 finished \tANN training loss 1.036655\n",
      ">> Epoch 142 finished \tANN training loss 1.036726\n",
      ">> Epoch 143 finished \tANN training loss 1.037364\n",
      ">> Epoch 144 finished \tANN training loss 1.036668\n",
      ">> Epoch 145 finished \tANN training loss 1.037033\n",
      ">> Epoch 146 finished \tANN training loss 1.038160\n",
      ">> Epoch 147 finished \tANN training loss 1.037015\n",
      ">> Epoch 148 finished \tANN training loss 1.037120\n",
      ">> Epoch 149 finished \tANN training loss 1.036916\n",
      ">> Epoch 150 finished \tANN training loss 1.036674\n",
      ">> Epoch 151 finished \tANN training loss 1.036858\n",
      ">> Epoch 152 finished \tANN training loss 1.036903\n",
      ">> Epoch 153 finished \tANN training loss 1.037818\n",
      ">> Epoch 154 finished \tANN training loss 1.037360\n",
      ">> Epoch 155 finished \tANN training loss 1.038686\n",
      ">> Epoch 156 finished \tANN training loss 1.037281\n",
      ">> Epoch 157 finished \tANN training loss 1.037174\n",
      ">> Epoch 158 finished \tANN training loss 1.037520\n",
      ">> Epoch 159 finished \tANN training loss 1.036655\n",
      ">> Epoch 160 finished \tANN training loss 1.036665\n",
      ">> Epoch 161 finished \tANN training loss 1.036702\n",
      ">> Epoch 162 finished \tANN training loss 1.036806\n",
      ">> Epoch 163 finished \tANN training loss 1.036775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 164 finished \tANN training loss 1.036683\n",
      ">> Epoch 165 finished \tANN training loss 1.037315\n",
      ">> Epoch 166 finished \tANN training loss 1.037050\n",
      ">> Epoch 167 finished \tANN training loss 1.036659\n",
      ">> Epoch 168 finished \tANN training loss 1.036987\n",
      ">> Epoch 169 finished \tANN training loss 1.036934\n",
      ">> Epoch 170 finished \tANN training loss 1.037145\n",
      ">> Epoch 171 finished \tANN training loss 1.036773\n",
      ">> Epoch 172 finished \tANN training loss 1.037102\n",
      ">> Epoch 173 finished \tANN training loss 1.036657\n",
      ">> Epoch 174 finished \tANN training loss 1.037083\n",
      ">> Epoch 175 finished \tANN training loss 1.036777\n",
      ">> Epoch 176 finished \tANN training loss 1.036850\n",
      ">> Epoch 177 finished \tANN training loss 1.036870\n",
      ">> Epoch 178 finished \tANN training loss 1.037728\n",
      ">> Epoch 179 finished \tANN training loss 1.036877\n",
      ">> Epoch 180 finished \tANN training loss 1.036954\n",
      ">> Epoch 181 finished \tANN training loss 1.036819\n",
      ">> Epoch 182 finished \tANN training loss 1.036957\n",
      ">> Epoch 183 finished \tANN training loss 1.036834\n",
      ">> Epoch 184 finished \tANN training loss 1.036911\n",
      ">> Epoch 185 finished \tANN training loss 1.036779\n",
      ">> Epoch 186 finished \tANN training loss 1.038159\n",
      ">> Epoch 187 finished \tANN training loss 1.038358\n",
      ">> Epoch 188 finished \tANN training loss 1.037089\n",
      ">> Epoch 189 finished \tANN training loss 1.036827\n",
      ">> Epoch 190 finished \tANN training loss 1.037430\n",
      ">> Epoch 191 finished \tANN training loss 1.036729\n",
      ">> Epoch 192 finished \tANN training loss 1.036661\n",
      ">> Epoch 193 finished \tANN training loss 1.036655\n",
      ">> Epoch 194 finished \tANN training loss 1.038001\n",
      ">> Epoch 195 finished \tANN training loss 1.038494\n",
      ">> Epoch 196 finished \tANN training loss 1.036657\n",
      ">> Epoch 197 finished \tANN training loss 1.036790\n",
      ">> Epoch 198 finished \tANN training loss 1.037849\n",
      ">> Epoch 199 finished \tANN training loss 1.036660\n",
      ">> Epoch 200 finished \tANN training loss 1.036734\n",
      ">> Epoch 201 finished \tANN training loss 1.036659\n",
      ">> Epoch 202 finished \tANN training loss 1.036854\n",
      ">> Epoch 203 finished \tANN training loss 1.036752\n",
      ">> Epoch 204 finished \tANN training loss 1.036902\n",
      ">> Epoch 205 finished \tANN training loss 1.036655\n",
      ">> Epoch 206 finished \tANN training loss 1.036924\n",
      ">> Epoch 207 finished \tANN training loss 1.036689\n",
      ">> Epoch 208 finished \tANN training loss 1.036703\n",
      ">> Epoch 209 finished \tANN training loss 1.036668\n",
      ">> Epoch 210 finished \tANN training loss 1.036659\n",
      ">> Epoch 211 finished \tANN training loss 1.036656\n",
      ">> Epoch 212 finished \tANN training loss 1.036662\n",
      ">> Epoch 213 finished \tANN training loss 1.036845\n",
      ">> Epoch 214 finished \tANN training loss 1.037331\n",
      ">> Epoch 215 finished \tANN training loss 1.037227\n",
      ">> Epoch 216 finished \tANN training loss 1.037129\n",
      ">> Epoch 217 finished \tANN training loss 1.036775\n",
      ">> Epoch 218 finished \tANN training loss 1.036946\n",
      ">> Epoch 219 finished \tANN training loss 1.036655\n",
      ">> Epoch 220 finished \tANN training loss 1.037462\n",
      ">> Epoch 221 finished \tANN training loss 1.037355\n",
      ">> Epoch 222 finished \tANN training loss 1.036661\n",
      ">> Epoch 223 finished \tANN training loss 1.036697\n",
      ">> Epoch 224 finished \tANN training loss 1.036657\n",
      ">> Epoch 225 finished \tANN training loss 1.036719\n",
      ">> Epoch 226 finished \tANN training loss 1.037107\n",
      ">> Epoch 227 finished \tANN training loss 1.036670\n",
      ">> Epoch 228 finished \tANN training loss 1.036746\n",
      ">> Epoch 229 finished \tANN training loss 1.036920\n",
      ">> Epoch 230 finished \tANN training loss 1.036810\n",
      ">> Epoch 231 finished \tANN training loss 1.036655\n",
      ">> Epoch 232 finished \tANN training loss 1.036689\n",
      ">> Epoch 233 finished \tANN training loss 1.036655\n",
      ">> Epoch 234 finished \tANN training loss 1.036659\n",
      ">> Epoch 235 finished \tANN training loss 1.037065\n",
      ">> Epoch 236 finished \tANN training loss 1.037758\n",
      ">> Epoch 237 finished \tANN training loss 1.038360\n",
      ">> Epoch 238 finished \tANN training loss 1.036847\n",
      ">> Epoch 239 finished \tANN training loss 1.036706\n",
      ">> Epoch 240 finished \tANN training loss 1.036660\n",
      ">> Epoch 241 finished \tANN training loss 1.036655\n",
      ">> Epoch 242 finished \tANN training loss 1.036722\n",
      ">> Epoch 243 finished \tANN training loss 1.036722\n",
      ">> Epoch 244 finished \tANN training loss 1.036659\n",
      ">> Epoch 245 finished \tANN training loss 1.037035\n",
      ">> Epoch 246 finished \tANN training loss 1.036962\n",
      ">> Epoch 247 finished \tANN training loss 1.037325\n",
      ">> Epoch 248 finished \tANN training loss 1.037104\n",
      ">> Epoch 249 finished \tANN training loss 1.037029\n",
      ">> Epoch 250 finished \tANN training loss 1.036704\n",
      ">> Epoch 251 finished \tANN training loss 1.036664\n",
      ">> Epoch 252 finished \tANN training loss 1.036667\n",
      ">> Epoch 253 finished \tANN training loss 1.036674\n",
      ">> Epoch 254 finished \tANN training loss 1.036678\n",
      ">> Epoch 255 finished \tANN training loss 1.037486\n",
      ">> Epoch 256 finished \tANN training loss 1.040837\n",
      ">> Epoch 257 finished \tANN training loss 1.037858\n",
      ">> Epoch 258 finished \tANN training loss 1.036672\n",
      ">> Epoch 259 finished \tANN training loss 1.036705\n",
      ">> Epoch 260 finished \tANN training loss 1.036679\n",
      ">> Epoch 261 finished \tANN training loss 1.036851\n",
      ">> Epoch 262 finished \tANN training loss 1.037946\n",
      ">> Epoch 263 finished \tANN training loss 1.037142\n",
      ">> Epoch 264 finished \tANN training loss 1.037161\n",
      ">> Epoch 265 finished \tANN training loss 1.037044\n",
      ">> Epoch 266 finished \tANN training loss 1.038457\n",
      ">> Epoch 267 finished \tANN training loss 1.037764\n",
      ">> Epoch 268 finished \tANN training loss 1.036773\n",
      ">> Epoch 269 finished \tANN training loss 1.037035\n",
      ">> Epoch 270 finished \tANN training loss 1.036940\n",
      ">> Epoch 271 finished \tANN training loss 1.036726\n",
      ">> Epoch 272 finished \tANN training loss 1.036663\n",
      ">> Epoch 273 finished \tANN training loss 1.037693\n",
      ">> Epoch 274 finished \tANN training loss 1.037177\n",
      ">> Epoch 275 finished \tANN training loss 1.036655\n",
      ">> Epoch 276 finished \tANN training loss 1.036843\n",
      ">> Epoch 277 finished \tANN training loss 1.036707\n",
      ">> Epoch 278 finished \tANN training loss 1.036667\n",
      ">> Epoch 279 finished \tANN training loss 1.036692\n",
      ">> Epoch 280 finished \tANN training loss 1.036669\n",
      ">> Epoch 281 finished \tANN training loss 1.036937\n",
      ">> Epoch 282 finished \tANN training loss 1.037156\n",
      ">> Epoch 283 finished \tANN training loss 1.036663\n",
      ">> Epoch 284 finished \tANN training loss 1.036702\n",
      ">> Epoch 285 finished \tANN training loss 1.036779\n",
      ">> Epoch 286 finished \tANN training loss 1.036698\n",
      ">> Epoch 287 finished \tANN training loss 1.037423\n",
      ">> Epoch 288 finished \tANN training loss 1.037116\n",
      ">> Epoch 289 finished \tANN training loss 1.037867\n",
      ">> Epoch 290 finished \tANN training loss 1.037356\n",
      ">> Epoch 291 finished \tANN training loss 1.036860\n",
      ">> Epoch 292 finished \tANN training loss 1.037598\n",
      ">> Epoch 293 finished \tANN training loss 1.036665\n",
      ">> Epoch 294 finished \tANN training loss 1.037374\n",
      ">> Epoch 295 finished \tANN training loss 1.036656\n",
      ">> Epoch 296 finished \tANN training loss 1.036663\n",
      ">> Epoch 297 finished \tANN training loss 1.036869\n",
      ">> Epoch 298 finished \tANN training loss 1.037504\n",
      ">> Epoch 299 finished \tANN training loss 1.037202\n",
      ">> Epoch 300 finished \tANN training loss 1.038378\n",
      ">> Epoch 301 finished \tANN training loss 1.037720\n",
      ">> Epoch 302 finished \tANN training loss 1.037604\n",
      ">> Epoch 303 finished \tANN training loss 1.037212\n",
      ">> Epoch 304 finished \tANN training loss 1.037685\n",
      ">> Epoch 305 finished \tANN training loss 1.037098\n",
      ">> Epoch 306 finished \tANN training loss 1.036772\n",
      ">> Epoch 307 finished \tANN training loss 1.037131\n",
      ">> Epoch 308 finished \tANN training loss 1.037915\n",
      ">> Epoch 309 finished \tANN training loss 1.036865\n",
      ">> Epoch 310 finished \tANN training loss 1.037169\n",
      ">> Epoch 311 finished \tANN training loss 1.036978\n",
      ">> Epoch 312 finished \tANN training loss 1.037821\n",
      ">> Epoch 313 finished \tANN training loss 1.037686\n",
      ">> Epoch 314 finished \tANN training loss 1.037425\n",
      ">> Epoch 315 finished \tANN training loss 1.037155\n",
      ">> Epoch 316 finished \tANN training loss 1.037056\n",
      ">> Epoch 317 finished \tANN training loss 1.037435\n",
      ">> Epoch 318 finished \tANN training loss 1.036918\n",
      ">> Epoch 319 finished \tANN training loss 1.036929\n",
      ">> Epoch 320 finished \tANN training loss 1.036813\n",
      ">> Epoch 321 finished \tANN training loss 1.036792\n",
      ">> Epoch 322 finished \tANN training loss 1.037735\n",
      ">> Epoch 323 finished \tANN training loss 1.037550\n",
      ">> Epoch 324 finished \tANN training loss 1.037518\n",
      ">> Epoch 325 finished \tANN training loss 1.036667\n",
      ">> Epoch 326 finished \tANN training loss 1.036745\n",
      ">> Epoch 327 finished \tANN training loss 1.036717\n",
      ">> Epoch 328 finished \tANN training loss 1.036806\n",
      ">> Epoch 329 finished \tANN training loss 1.036803\n",
      ">> Epoch 330 finished \tANN training loss 1.036971\n",
      ">> Epoch 331 finished \tANN training loss 1.036666\n",
      ">> Epoch 332 finished \tANN training loss 1.036783\n",
      ">> Epoch 333 finished \tANN training loss 1.036757\n",
      ">> Epoch 334 finished \tANN training loss 1.036830\n",
      ">> Epoch 335 finished \tANN training loss 1.037793\n",
      ">> Epoch 336 finished \tANN training loss 1.037248\n",
      ">> Epoch 337 finished \tANN training loss 1.036767\n",
      ">> Epoch 338 finished \tANN training loss 1.036826\n",
      ">> Epoch 339 finished \tANN training loss 1.036702\n",
      ">> Epoch 340 finished \tANN training loss 1.036673\n",
      ">> Epoch 341 finished \tANN training loss 1.036680\n",
      ">> Epoch 342 finished \tANN training loss 1.036678\n",
      ">> Epoch 343 finished \tANN training loss 1.037027\n",
      ">> Epoch 344 finished \tANN training loss 1.036656\n",
      ">> Epoch 345 finished \tANN training loss 1.036662\n",
      ">> Epoch 346 finished \tANN training loss 1.037009\n",
      ">> Epoch 347 finished \tANN training loss 1.037122\n",
      ">> Epoch 348 finished \tANN training loss 1.036735\n",
      ">> Epoch 349 finished \tANN training loss 1.037025\n",
      ">> Epoch 350 finished \tANN training loss 1.037165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 351 finished \tANN training loss 1.036737\n",
      ">> Epoch 352 finished \tANN training loss 1.036658\n",
      ">> Epoch 353 finished \tANN training loss 1.036700\n",
      ">> Epoch 354 finished \tANN training loss 1.037124\n",
      ">> Epoch 355 finished \tANN training loss 1.036664\n",
      ">> Epoch 356 finished \tANN training loss 1.036700\n",
      ">> Epoch 357 finished \tANN training loss 1.036790\n",
      ">> Epoch 358 finished \tANN training loss 1.036655\n",
      ">> Epoch 359 finished \tANN training loss 1.036765\n",
      ">> Epoch 360 finished \tANN training loss 1.036681\n",
      ">> Epoch 361 finished \tANN training loss 1.036662\n",
      ">> Epoch 362 finished \tANN training loss 1.036727\n",
      ">> Epoch 363 finished \tANN training loss 1.036700\n",
      ">> Epoch 364 finished \tANN training loss 1.036813\n",
      ">> Epoch 365 finished \tANN training loss 1.036694\n",
      ">> Epoch 366 finished \tANN training loss 1.036835\n",
      ">> Epoch 367 finished \tANN training loss 1.037187\n",
      ">> Epoch 368 finished \tANN training loss 1.036660\n",
      ">> Epoch 369 finished \tANN training loss 1.036671\n",
      ">> Epoch 370 finished \tANN training loss 1.036948\n",
      ">> Epoch 371 finished \tANN training loss 1.036694\n",
      ">> Epoch 372 finished \tANN training loss 1.036656\n",
      ">> Epoch 373 finished \tANN training loss 1.036672\n",
      ">> Epoch 374 finished \tANN training loss 1.036656\n",
      ">> Epoch 375 finished \tANN training loss 1.038312\n",
      ">> Epoch 376 finished \tANN training loss 1.037604\n",
      ">> Epoch 377 finished \tANN training loss 1.038066\n",
      ">> Epoch 378 finished \tANN training loss 1.037603\n",
      ">> Epoch 379 finished \tANN training loss 1.037400\n",
      ">> Epoch 380 finished \tANN training loss 1.036894\n",
      ">> Epoch 381 finished \tANN training loss 1.036668\n",
      ">> Epoch 382 finished \tANN training loss 1.036804\n",
      ">> Epoch 383 finished \tANN training loss 1.036704\n",
      ">> Epoch 384 finished \tANN training loss 1.036857\n",
      ">> Epoch 385 finished \tANN training loss 1.036798\n",
      ">> Epoch 386 finished \tANN training loss 1.036724\n",
      ">> Epoch 387 finished \tANN training loss 1.037066\n",
      ">> Epoch 388 finished \tANN training loss 1.038834\n",
      ">> Epoch 389 finished \tANN training loss 1.037782\n",
      ">> Epoch 390 finished \tANN training loss 1.038692\n",
      ">> Epoch 391 finished \tANN training loss 1.038151\n",
      ">> Epoch 392 finished \tANN training loss 1.038995\n",
      ">> Epoch 393 finished \tANN training loss 1.036899\n",
      ">> Epoch 394 finished \tANN training loss 1.036682\n",
      ">> Epoch 395 finished \tANN training loss 1.036749\n",
      ">> Epoch 396 finished \tANN training loss 1.037336\n",
      ">> Epoch 397 finished \tANN training loss 1.036721\n",
      ">> Epoch 398 finished \tANN training loss 1.036952\n",
      ">> Epoch 399 finished \tANN training loss 1.038812\n",
      ">> Epoch 400 finished \tANN training loss 1.037316\n",
      ">> Epoch 401 finished \tANN training loss 1.036937\n",
      ">> Epoch 402 finished \tANN training loss 1.037305\n",
      ">> Epoch 403 finished \tANN training loss 1.037257\n",
      ">> Epoch 404 finished \tANN training loss 1.036770\n",
      ">> Epoch 405 finished \tANN training loss 1.036655\n",
      ">> Epoch 406 finished \tANN training loss 1.037756\n",
      ">> Epoch 407 finished \tANN training loss 1.036929\n",
      ">> Epoch 408 finished \tANN training loss 1.036656\n",
      ">> Epoch 409 finished \tANN training loss 1.036938\n",
      ">> Epoch 410 finished \tANN training loss 1.036794\n",
      ">> Epoch 411 finished \tANN training loss 1.037086\n",
      ">> Epoch 412 finished \tANN training loss 1.037885\n",
      ">> Epoch 413 finished \tANN training loss 1.037773\n",
      ">> Epoch 414 finished \tANN training loss 1.036781\n",
      ">> Epoch 415 finished \tANN training loss 1.036658\n",
      ">> Epoch 416 finished \tANN training loss 1.036674\n",
      ">> Epoch 417 finished \tANN training loss 1.037949\n",
      ">> Epoch 418 finished \tANN training loss 1.040734\n",
      ">> Epoch 419 finished \tANN training loss 1.037155\n",
      ">> Epoch 420 finished \tANN training loss 1.036655\n",
      ">> Epoch 421 finished \tANN training loss 1.036663\n",
      ">> Epoch 422 finished \tANN training loss 1.036769\n",
      ">> Epoch 423 finished \tANN training loss 1.036700\n",
      ">> Epoch 424 finished \tANN training loss 1.036770\n",
      ">> Epoch 425 finished \tANN training loss 1.036715\n",
      ">> Epoch 426 finished \tANN training loss 1.036659\n",
      ">> Epoch 427 finished \tANN training loss 1.036734\n",
      ">> Epoch 428 finished \tANN training loss 1.037579\n",
      ">> Epoch 429 finished \tANN training loss 1.037061\n",
      ">> Epoch 430 finished \tANN training loss 1.036880\n",
      ">> Epoch 431 finished \tANN training loss 1.037093\n",
      ">> Epoch 432 finished \tANN training loss 1.036701\n",
      ">> Epoch 433 finished \tANN training loss 1.037572\n",
      ">> Epoch 434 finished \tANN training loss 1.036884\n",
      ">> Epoch 435 finished \tANN training loss 1.036780\n",
      ">> Epoch 436 finished \tANN training loss 1.036681\n",
      ">> Epoch 437 finished \tANN training loss 1.036813\n",
      ">> Epoch 438 finished \tANN training loss 1.036831\n",
      ">> Epoch 439 finished \tANN training loss 1.037298\n",
      ">> Epoch 440 finished \tANN training loss 1.037589\n",
      ">> Epoch 441 finished \tANN training loss 1.038187\n",
      ">> Epoch 442 finished \tANN training loss 1.037362\n",
      ">> Epoch 443 finished \tANN training loss 1.037455\n",
      ">> Epoch 444 finished \tANN training loss 1.038236\n",
      ">> Epoch 445 finished \tANN training loss 1.037498\n",
      ">> Epoch 446 finished \tANN training loss 12157.454102\n",
      ">> Epoch 447 finished \tANN training loss 5110.505859\n",
      ">> Epoch 448 finished \tANN training loss 2149.821045\n",
      ">> Epoch 449 finished \tANN training loss 905.887512\n",
      ">> Epoch 450 finished \tANN training loss 381.977081\n",
      ">> Epoch 451 finished \tANN training loss 160.719635\n",
      ">> Epoch 452 finished \tANN training loss 68.329308\n",
      ">> Epoch 453 finished \tANN training loss 29.475107\n",
      ">> Epoch 454 finished \tANN training loss 13.073334\n",
      ">> Epoch 455 finished \tANN training loss 6.207751\n",
      ">> Epoch 456 finished \tANN training loss 3.198475\n",
      ">> Epoch 457 finished \tANN training loss 1.916502\n",
      ">> Epoch 458 finished \tANN training loss 1.398000\n",
      ">> Epoch 459 finished \tANN training loss 1.167961\n",
      ">> Epoch 460 finished \tANN training loss 1.082660\n",
      ">> Epoch 461 finished \tANN training loss 1.068387\n",
      ">> Epoch 462 finished \tANN training loss 1.053212\n",
      ">> Epoch 463 finished \tANN training loss 1.043015\n",
      ">> Epoch 464 finished \tANN training loss 1.037523\n",
      ">> Epoch 465 finished \tANN training loss 1.036695\n",
      ">> Epoch 466 finished \tANN training loss 1.036766\n",
      ">> Epoch 467 finished \tANN training loss 1.037005\n",
      ">> Epoch 468 finished \tANN training loss 1.036672\n",
      ">> Epoch 469 finished \tANN training loss 1.036727\n",
      ">> Epoch 470 finished \tANN training loss 1.036660\n",
      ">> Epoch 471 finished \tANN training loss 1.036669\n",
      ">> Epoch 472 finished \tANN training loss 1.036658\n",
      ">> Epoch 473 finished \tANN training loss 1.036706\n",
      ">> Epoch 474 finished \tANN training loss 1.036739\n",
      ">> Epoch 475 finished \tANN training loss 1.037378\n",
      ">> Epoch 476 finished \tANN training loss 1.037030\n",
      ">> Epoch 477 finished \tANN training loss 1.036676\n",
      ">> Epoch 478 finished \tANN training loss 1.037050\n",
      ">> Epoch 479 finished \tANN training loss 1.036809\n",
      ">> Epoch 480 finished \tANN training loss 1.037888\n",
      ">> Epoch 481 finished \tANN training loss 1.036907\n",
      ">> Epoch 482 finished \tANN training loss 1.037214\n",
      ">> Epoch 483 finished \tANN training loss 1.036766\n",
      ">> Epoch 484 finished \tANN training loss 1.036868\n",
      ">> Epoch 485 finished \tANN training loss 1.036672\n",
      ">> Epoch 486 finished \tANN training loss 1.036738\n",
      ">> Epoch 487 finished \tANN training loss 1.036657\n",
      ">> Epoch 488 finished \tANN training loss 1.037861\n",
      ">> Epoch 489 finished \tANN training loss 1.036771\n",
      ">> Epoch 490 finished \tANN training loss 1.037092\n",
      ">> Epoch 491 finished \tANN training loss 1.036659\n",
      ">> Epoch 492 finished \tANN training loss 1.036868\n",
      ">> Epoch 493 finished \tANN training loss 1.036655\n",
      ">> Epoch 494 finished \tANN training loss 1.037452\n",
      ">> Epoch 495 finished \tANN training loss 1.037205\n",
      ">> Epoch 496 finished \tANN training loss 1.036951\n",
      ">> Epoch 497 finished \tANN training loss 1.036719\n",
      ">> Epoch 498 finished \tANN training loss 1.036984\n",
      ">> Epoch 499 finished \tANN training loss 1.036932\n",
      ">> Epoch 500 finished \tANN training loss 1.036705\n",
      ">> Epoch 501 finished \tANN training loss 1.036823\n",
      ">> Epoch 502 finished \tANN training loss 1.037250\n",
      ">> Epoch 503 finished \tANN training loss 1.036705\n",
      ">> Epoch 504 finished \tANN training loss 1.036664\n",
      ">> Epoch 505 finished \tANN training loss 1.036803\n",
      ">> Epoch 506 finished \tANN training loss 1.036662\n",
      ">> Epoch 507 finished \tANN training loss 1.036723\n",
      ">> Epoch 508 finished \tANN training loss 1.036998\n",
      ">> Epoch 509 finished \tANN training loss 1.036721\n",
      ">> Epoch 510 finished \tANN training loss 1.036826\n",
      ">> Epoch 511 finished \tANN training loss 1.036787\n",
      ">> Epoch 512 finished \tANN training loss 1.036715\n",
      ">> Epoch 513 finished \tANN training loss 1.036675\n",
      ">> Epoch 514 finished \tANN training loss 1.037296\n",
      ">> Epoch 515 finished \tANN training loss 1.036780\n",
      ">> Epoch 516 finished \tANN training loss 1.036847\n",
      ">> Epoch 517 finished \tANN training loss 1.037186\n",
      ">> Epoch 518 finished \tANN training loss 1.037574\n",
      ">> Epoch 519 finished \tANN training loss 1.037139\n",
      ">> Epoch 520 finished \tANN training loss 1.037434\n",
      ">> Epoch 521 finished \tANN training loss 1.036709\n",
      ">> Epoch 522 finished \tANN training loss 1.036820\n",
      ">> Epoch 523 finished \tANN training loss 1.036941\n",
      ">> Epoch 524 finished \tANN training loss 1.036695\n",
      ">> Epoch 525 finished \tANN training loss 1.036977\n",
      ">> Epoch 526 finished \tANN training loss 1.036685\n",
      ">> Epoch 527 finished \tANN training loss 1.036795\n",
      ">> Epoch 528 finished \tANN training loss 1.036845\n",
      ">> Epoch 529 finished \tANN training loss 1.036863\n",
      ">> Epoch 530 finished \tANN training loss 1.036896\n",
      ">> Epoch 531 finished \tANN training loss 1.036697\n",
      ">> Epoch 532 finished \tANN training loss 1.036920\n",
      ">> Epoch 533 finished \tANN training loss 1.036655\n",
      ">> Epoch 534 finished \tANN training loss 1.036708\n",
      ">> Epoch 535 finished \tANN training loss 1.036669\n",
      ">> Epoch 536 finished \tANN training loss 1.036698\n",
      ">> Epoch 537 finished \tANN training loss 1.037041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 538 finished \tANN training loss 1.036703\n",
      ">> Epoch 539 finished \tANN training loss 1.036655\n",
      ">> Epoch 540 finished \tANN training loss 1.036717\n",
      ">> Epoch 541 finished \tANN training loss 1.037295\n",
      ">> Epoch 542 finished \tANN training loss 1.036933\n",
      ">> Epoch 543 finished \tANN training loss 1.036659\n",
      ">> Epoch 544 finished \tANN training loss 1.036724\n",
      ">> Epoch 545 finished \tANN training loss 1.036668\n",
      ">> Epoch 546 finished \tANN training loss 1.036660\n",
      ">> Epoch 547 finished \tANN training loss 1.036907\n",
      ">> Epoch 548 finished \tANN training loss 1.038412\n",
      ">> Epoch 549 finished \tANN training loss 1.036668\n",
      ">> Epoch 550 finished \tANN training loss 1.037147\n",
      ">> Epoch 551 finished \tANN training loss 1.036902\n",
      ">> Epoch 552 finished \tANN training loss 1.036824\n",
      ">> Epoch 553 finished \tANN training loss 1.036685\n",
      ">> Epoch 554 finished \tANN training loss 1.036664\n",
      ">> Epoch 555 finished \tANN training loss 1.036682\n",
      ">> Epoch 556 finished \tANN training loss 1.037633\n",
      ">> Epoch 557 finished \tANN training loss 1.038292\n",
      ">> Epoch 558 finished \tANN training loss 1.039414\n",
      ">> Epoch 559 finished \tANN training loss 1.038506\n",
      ">> Epoch 560 finished \tANN training loss 1.036692\n",
      ">> Epoch 561 finished \tANN training loss 1.036668\n",
      ">> Epoch 562 finished \tANN training loss 1.036802\n",
      ">> Epoch 563 finished \tANN training loss 1.038340\n",
      ">> Epoch 564 finished \tANN training loss 1.037250\n",
      ">> Epoch 565 finished \tANN training loss 1.037032\n",
      ">> Epoch 566 finished \tANN training loss 1.036683\n",
      ">> Epoch 567 finished \tANN training loss 1.037520\n",
      ">> Epoch 568 finished \tANN training loss 1.038900\n",
      ">> Epoch 569 finished \tANN training loss 1.037420\n",
      ">> Epoch 570 finished \tANN training loss 1.037478\n",
      ">> Epoch 571 finished \tANN training loss 1.037861\n",
      ">> Epoch 572 finished \tANN training loss 1.036662\n",
      ">> Epoch 573 finished \tANN training loss 1.036655\n",
      ">> Epoch 574 finished \tANN training loss 1.036724\n",
      ">> Epoch 575 finished \tANN training loss 1.036668\n",
      ">> Epoch 576 finished \tANN training loss 1.036659\n",
      ">> Epoch 577 finished \tANN training loss 1.036699\n",
      ">> Epoch 578 finished \tANN training loss 1.036662\n",
      ">> Epoch 579 finished \tANN training loss 1.037335\n",
      ">> Epoch 580 finished \tANN training loss 1.036971\n",
      ">> Epoch 581 finished \tANN training loss 1.036688\n",
      ">> Epoch 582 finished \tANN training loss 1.036845\n",
      ">> Epoch 583 finished \tANN training loss 1.038297\n",
      ">> Epoch 584 finished \tANN training loss 1.037031\n",
      ">> Epoch 585 finished \tANN training loss 1.037063\n",
      ">> Epoch 586 finished \tANN training loss 1.037166\n",
      ">> Epoch 587 finished \tANN training loss 1.036839\n",
      ">> Epoch 588 finished \tANN training loss 1.039810\n",
      ">> Epoch 589 finished \tANN training loss 1.039528\n",
      ">> Epoch 590 finished \tANN training loss 1.037108\n",
      ">> Epoch 591 finished \tANN training loss 1.036866\n",
      ">> Epoch 592 finished \tANN training loss 1.036710\n",
      ">> Epoch 593 finished \tANN training loss 1.037326\n",
      ">> Epoch 594 finished \tANN training loss 1.036879\n",
      ">> Epoch 595 finished \tANN training loss 1.036861\n",
      ">> Epoch 596 finished \tANN training loss 1.036947\n",
      ">> Epoch 597 finished \tANN training loss 1.036741\n",
      ">> Epoch 598 finished \tANN training loss 1.036803\n",
      ">> Epoch 599 finished \tANN training loss 1.036833\n",
      ">> Epoch 600 finished \tANN training loss 1.037215\n",
      ">> Epoch 601 finished \tANN training loss 1.036656\n",
      ">> Epoch 602 finished \tANN training loss 1.037036\n",
      ">> Epoch 603 finished \tANN training loss 1.037111\n",
      ">> Epoch 604 finished \tANN training loss 1.036768\n",
      ">> Epoch 605 finished \tANN training loss 1.036940\n",
      ">> Epoch 606 finished \tANN training loss 1.038611\n",
      ">> Epoch 607 finished \tANN training loss 1.038616\n",
      ">> Epoch 608 finished \tANN training loss 1.040130\n",
      ">> Epoch 609 finished \tANN training loss 1.038013\n",
      ">> Epoch 610 finished \tANN training loss 1.037514\n",
      ">> Epoch 611 finished \tANN training loss 1.036659\n",
      ">> Epoch 612 finished \tANN training loss 1.036737\n",
      ">> Epoch 613 finished \tANN training loss 1.036659\n",
      ">> Epoch 614 finished \tANN training loss 1.036846\n",
      ">> Epoch 615 finished \tANN training loss 1.037174\n",
      ">> Epoch 616 finished \tANN training loss 1.037356\n",
      ">> Epoch 617 finished \tANN training loss 1.037219\n",
      ">> Epoch 618 finished \tANN training loss 1.036690\n",
      ">> Epoch 619 finished \tANN training loss 1.037081\n",
      ">> Epoch 620 finished \tANN training loss 1.036731\n",
      ">> Epoch 621 finished \tANN training loss 1.036757\n",
      ">> Epoch 622 finished \tANN training loss 1.037220\n",
      ">> Epoch 623 finished \tANN training loss 1.036699\n",
      ">> Epoch 624 finished \tANN training loss 1.036838\n",
      ">> Epoch 625 finished \tANN training loss 1.037296\n",
      ">> Epoch 626 finished \tANN training loss 1.036966\n",
      ">> Epoch 627 finished \tANN training loss 1.036900\n",
      ">> Epoch 628 finished \tANN training loss 1.036659\n",
      ">> Epoch 629 finished \tANN training loss 1.036655\n",
      ">> Epoch 630 finished \tANN training loss 1.036655\n",
      ">> Epoch 631 finished \tANN training loss 1.037199\n",
      ">> Epoch 632 finished \tANN training loss 1.041342\n",
      ">> Epoch 633 finished \tANN training loss 1.039096\n",
      ">> Epoch 634 finished \tANN training loss 1.039856\n",
      ">> Epoch 635 finished \tANN training loss 1.038108\n",
      ">> Epoch 636 finished \tANN training loss 1.037371\n",
      ">> Epoch 637 finished \tANN training loss 1.036661\n",
      ">> Epoch 638 finished \tANN training loss 1.036800\n",
      ">> Epoch 639 finished \tANN training loss 1.036696\n",
      ">> Epoch 640 finished \tANN training loss 1.037211\n",
      ">> Epoch 641 finished \tANN training loss 1.036947\n",
      ">> Epoch 642 finished \tANN training loss 1.036691\n",
      ">> Epoch 643 finished \tANN training loss 1.037008\n",
      ">> Epoch 644 finished \tANN training loss 1.036788\n",
      ">> Epoch 645 finished \tANN training loss 1.036663\n",
      ">> Epoch 646 finished \tANN training loss 1.037597\n",
      ">> Epoch 647 finished \tANN training loss 1.037311\n",
      ">> Epoch 648 finished \tANN training loss 1.036655\n",
      ">> Epoch 649 finished \tANN training loss 1.036696\n",
      ">> Epoch 650 finished \tANN training loss 1.036655\n",
      ">> Epoch 651 finished \tANN training loss 1.037010\n",
      ">> Epoch 652 finished \tANN training loss 1.037390\n",
      ">> Epoch 653 finished \tANN training loss 1.036694\n",
      ">> Epoch 654 finished \tANN training loss 1.036682\n",
      ">> Epoch 655 finished \tANN training loss 1.037751\n",
      ">> Epoch 656 finished \tANN training loss 1.036656\n",
      ">> Epoch 657 finished \tANN training loss 1.036844\n",
      ">> Epoch 658 finished \tANN training loss 1.037933\n",
      ">> Epoch 659 finished \tANN training loss 1.036938\n",
      ">> Epoch 660 finished \tANN training loss 1.036681\n",
      ">> Epoch 661 finished \tANN training loss 1.036675\n",
      ">> Epoch 662 finished \tANN training loss 1.036674\n",
      ">> Epoch 663 finished \tANN training loss 1.037687\n",
      ">> Epoch 664 finished \tANN training loss 1.036675\n",
      ">> Epoch 665 finished \tANN training loss 1.037111\n",
      ">> Epoch 666 finished \tANN training loss 1.036752\n",
      ">> Epoch 667 finished \tANN training loss 1.036765\n",
      ">> Epoch 668 finished \tANN training loss 1.036794\n",
      ">> Epoch 669 finished \tANN training loss 1.036758\n",
      ">> Epoch 670 finished \tANN training loss 1.036757\n",
      ">> Epoch 671 finished \tANN training loss 1.036844\n",
      ">> Epoch 672 finished \tANN training loss 1.038106\n",
      ">> Epoch 673 finished \tANN training loss 1.036676\n",
      ">> Epoch 674 finished \tANN training loss 1.037119\n",
      ">> Epoch 675 finished \tANN training loss 1.036659\n",
      ">> Epoch 676 finished \tANN training loss 1.036737\n",
      ">> Epoch 677 finished \tANN training loss 1.036998\n",
      ">> Epoch 678 finished \tANN training loss 1.036742\n",
      ">> Epoch 679 finished \tANN training loss 1.036922\n",
      ">> Epoch 680 finished \tANN training loss 1.037283\n",
      ">> Epoch 681 finished \tANN training loss 1.037030\n",
      ">> Epoch 682 finished \tANN training loss 1.036802\n",
      ">> Epoch 683 finished \tANN training loss 1.036953\n",
      ">> Epoch 684 finished \tANN training loss 1.037009\n",
      ">> Epoch 685 finished \tANN training loss 1.036924\n",
      ">> Epoch 686 finished \tANN training loss 1.036656\n",
      ">> Epoch 687 finished \tANN training loss 1.036887\n",
      ">> Epoch 688 finished \tANN training loss 1.036920\n",
      ">> Epoch 689 finished \tANN training loss 1.037558\n",
      ">> Epoch 690 finished \tANN training loss 1.036659\n",
      ">> Epoch 691 finished \tANN training loss 1.036702\n",
      ">> Epoch 692 finished \tANN training loss 1.036873\n",
      ">> Epoch 693 finished \tANN training loss 1.038376\n",
      ">> Epoch 694 finished \tANN training loss 1.038459\n",
      ">> Epoch 695 finished \tANN training loss 1.038155\n",
      ">> Epoch 696 finished \tANN training loss 1.036862\n",
      ">> Epoch 697 finished \tANN training loss 1.037044\n",
      ">> Epoch 698 finished \tANN training loss 1.036687\n",
      ">> Epoch 699 finished \tANN training loss 1.037103\n",
      ">> Epoch 700 finished \tANN training loss 1.038291\n",
      ">> Epoch 701 finished \tANN training loss 1.036707\n",
      ">> Epoch 702 finished \tANN training loss 1.036835\n",
      ">> Epoch 703 finished \tANN training loss 1.038028\n",
      ">> Epoch 704 finished \tANN training loss 1.037048\n",
      ">> Epoch 705 finished \tANN training loss 1.038501\n",
      ">> Epoch 706 finished \tANN training loss 1.037470\n",
      ">> Epoch 707 finished \tANN training loss 1.038281\n",
      ">> Epoch 708 finished \tANN training loss 1.037832\n",
      ">> Epoch 709 finished \tANN training loss 1.037012\n",
      ">> Epoch 710 finished \tANN training loss 1.038517\n",
      ">> Epoch 711 finished \tANN training loss 1.037312\n",
      ">> Epoch 712 finished \tANN training loss 1.036861\n",
      ">> Epoch 713 finished \tANN training loss 1.036822\n",
      ">> Epoch 714 finished \tANN training loss 1.037092\n",
      ">> Epoch 715 finished \tANN training loss 1.038342\n",
      ">> Epoch 716 finished \tANN training loss 1.036693\n",
      ">> Epoch 717 finished \tANN training loss 1.036661\n",
      ">> Epoch 718 finished \tANN training loss 1.036760\n",
      ">> Epoch 719 finished \tANN training loss 1.037281\n",
      ">> Epoch 720 finished \tANN training loss 1.038045\n",
      ">> Epoch 721 finished \tANN training loss 1.037070\n",
      ">> Epoch 722 finished \tANN training loss 1.036792\n",
      ">> Epoch 723 finished \tANN training loss 1.036666\n",
      ">> Epoch 724 finished \tANN training loss 1.036924\n",
      ">> Epoch 725 finished \tANN training loss 1.037691\n",
      ">> Epoch 726 finished \tANN training loss 1.037260\n",
      ">> Epoch 727 finished \tANN training loss 1.038293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 728 finished \tANN training loss 1.036938\n",
      ">> Epoch 729 finished \tANN training loss 1.036655\n",
      ">> Epoch 730 finished \tANN training loss 1.037241\n",
      ">> Epoch 731 finished \tANN training loss 1.036719\n",
      ">> Epoch 732 finished \tANN training loss 1.036764\n",
      ">> Epoch 733 finished \tANN training loss 1.036683\n",
      ">> Epoch 734 finished \tANN training loss 1.037195\n",
      ">> Epoch 735 finished \tANN training loss 1.037121\n",
      ">> Epoch 736 finished \tANN training loss 1.036748\n",
      ">> Epoch 737 finished \tANN training loss 1.036716\n",
      ">> Epoch 738 finished \tANN training loss 1.036913\n",
      ">> Epoch 739 finished \tANN training loss 1.037395\n",
      ">> Epoch 740 finished \tANN training loss 1.036676\n",
      ">> Epoch 741 finished \tANN training loss 1.037467\n",
      ">> Epoch 742 finished \tANN training loss 1.036671\n",
      ">> Epoch 743 finished \tANN training loss 1.036759\n",
      ">> Epoch 744 finished \tANN training loss 1.036962\n",
      ">> Epoch 745 finished \tANN training loss 1.036681\n",
      ">> Epoch 746 finished \tANN training loss 1.037004\n",
      ">> Epoch 747 finished \tANN training loss 1.036945\n",
      ">> Epoch 748 finished \tANN training loss 1.037005\n",
      ">> Epoch 749 finished \tANN training loss 1.036681\n",
      ">> Epoch 750 finished \tANN training loss 1.036734\n",
      ">> Epoch 751 finished \tANN training loss 1.036720\n",
      ">> Epoch 752 finished \tANN training loss 1.036898\n",
      ">> Epoch 753 finished \tANN training loss 1.036840\n",
      ">> Epoch 754 finished \tANN training loss 1.037230\n",
      ">> Epoch 755 finished \tANN training loss 1.036692\n",
      ">> Epoch 756 finished \tANN training loss 1.036658\n",
      ">> Epoch 757 finished \tANN training loss 1.037208\n",
      ">> Epoch 758 finished \tANN training loss 1.036655\n",
      ">> Epoch 759 finished \tANN training loss 1.036655\n",
      ">> Epoch 760 finished \tANN training loss 1.036682\n",
      ">> Epoch 761 finished \tANN training loss 1.036781\n",
      ">> Epoch 762 finished \tANN training loss 1.036815\n",
      ">> Epoch 763 finished \tANN training loss 1.037454\n",
      ">> Epoch 764 finished \tANN training loss 1.036656\n",
      ">> Epoch 765 finished \tANN training loss 1.036687\n",
      ">> Epoch 766 finished \tANN training loss 1.036742\n",
      ">> Epoch 767 finished \tANN training loss 1.036668\n",
      ">> Epoch 768 finished \tANN training loss 1.036740\n",
      ">> Epoch 769 finished \tANN training loss 1.036787\n",
      ">> Epoch 770 finished \tANN training loss 1.036791\n",
      ">> Epoch 771 finished \tANN training loss 1.036776\n",
      ">> Epoch 772 finished \tANN training loss 1.036675\n",
      ">> Epoch 773 finished \tANN training loss 1.036684\n",
      ">> Epoch 774 finished \tANN training loss 1.037730\n",
      ">> Epoch 775 finished \tANN training loss 1.036854\n",
      ">> Epoch 776 finished \tANN training loss 1.037090\n",
      ">> Epoch 777 finished \tANN training loss 1.036819\n",
      ">> Epoch 778 finished \tANN training loss 1.037500\n",
      ">> Epoch 779 finished \tANN training loss 1.036865\n",
      ">> Epoch 780 finished \tANN training loss 1.036883\n",
      ">> Epoch 781 finished \tANN training loss 1.036780\n",
      ">> Epoch 782 finished \tANN training loss 1.036656\n",
      ">> Epoch 783 finished \tANN training loss 1.036655\n",
      ">> Epoch 784 finished \tANN training loss 1.037116\n",
      ">> Epoch 785 finished \tANN training loss 1.037932\n",
      ">> Epoch 786 finished \tANN training loss 1.038310\n",
      ">> Epoch 787 finished \tANN training loss 1.037825\n",
      ">> Epoch 788 finished \tANN training loss 1.037284\n",
      ">> Epoch 789 finished \tANN training loss 1.036684\n",
      ">> Epoch 790 finished \tANN training loss 1.036997\n",
      ">> Epoch 791 finished \tANN training loss 1.036659\n",
      ">> Epoch 792 finished \tANN training loss 1.036661\n",
      ">> Epoch 793 finished \tANN training loss 1.036723\n",
      ">> Epoch 794 finished \tANN training loss 1.036704\n",
      ">> Epoch 795 finished \tANN training loss 1.036979\n",
      ">> Epoch 796 finished \tANN training loss 1.036665\n",
      ">> Epoch 797 finished \tANN training loss 1.036957\n",
      ">> Epoch 798 finished \tANN training loss 1.037245\n",
      ">> Epoch 799 finished \tANN training loss 1.036772\n",
      ">> Epoch 800 finished \tANN training loss 1.036672\n",
      ">> Epoch 801 finished \tANN training loss 1.036673\n",
      ">> Epoch 802 finished \tANN training loss 1.036656\n",
      ">> Epoch 803 finished \tANN training loss 1.037082\n",
      ">> Epoch 804 finished \tANN training loss 1.036699\n",
      ">> Epoch 805 finished \tANN training loss 1.036782\n",
      ">> Epoch 806 finished \tANN training loss 1.036718\n",
      ">> Epoch 807 finished \tANN training loss 1.036792\n",
      ">> Epoch 808 finished \tANN training loss 1.036802\n",
      ">> Epoch 809 finished \tANN training loss 1.037363\n",
      ">> Epoch 810 finished \tANN training loss 1.036665\n",
      ">> Epoch 811 finished \tANN training loss 1.036686\n",
      ">> Epoch 812 finished \tANN training loss 1.036663\n",
      ">> Epoch 813 finished \tANN training loss 1.037238\n",
      ">> Epoch 814 finished \tANN training loss 1.036829\n",
      ">> Epoch 815 finished \tANN training loss 1.038105\n",
      ">> Epoch 816 finished \tANN training loss 1.037052\n",
      ">> Epoch 817 finished \tANN training loss 1.036810\n",
      ">> Epoch 818 finished \tANN training loss 1.037937\n",
      ">> Epoch 819 finished \tANN training loss 1.037081\n",
      ">> Epoch 820 finished \tANN training loss 1.036655\n",
      ">> Epoch 821 finished \tANN training loss 1.036977\n",
      ">> Epoch 822 finished \tANN training loss 1.036896\n",
      ">> Epoch 823 finished \tANN training loss 1.036945\n",
      ">> Epoch 824 finished \tANN training loss 1.036674\n",
      ">> Epoch 825 finished \tANN training loss 1.036655\n",
      ">> Epoch 826 finished \tANN training loss 1.036801\n",
      ">> Epoch 827 finished \tANN training loss 1.037132\n",
      ">> Epoch 828 finished \tANN training loss 1.038180\n",
      ">> Epoch 829 finished \tANN training loss 1.037284\n",
      ">> Epoch 830 finished \tANN training loss 1.036974\n",
      ">> Epoch 831 finished \tANN training loss 1.036658\n",
      ">> Epoch 832 finished \tANN training loss 1.036899\n",
      ">> Epoch 833 finished \tANN training loss 1.036947\n",
      ">> Epoch 834 finished \tANN training loss 1.037046\n",
      ">> Epoch 835 finished \tANN training loss 1.038790\n",
      ">> Epoch 836 finished \tANN training loss 1.037282\n",
      ">> Epoch 837 finished \tANN training loss 1.038686\n",
      ">> Epoch 838 finished \tANN training loss 1.038436\n",
      ">> Epoch 839 finished \tANN training loss 1.038684\n",
      ">> Epoch 840 finished \tANN training loss 1.037050\n",
      ">> Epoch 841 finished \tANN training loss 1.036759\n",
      ">> Epoch 842 finished \tANN training loss 1.036656\n",
      ">> Epoch 843 finished \tANN training loss 1.036689\n",
      ">> Epoch 844 finished \tANN training loss 1.036655\n",
      ">> Epoch 845 finished \tANN training loss 1.036902\n",
      ">> Epoch 846 finished \tANN training loss 1.037276\n",
      ">> Epoch 847 finished \tANN training loss 1.037490\n",
      ">> Epoch 848 finished \tANN training loss 1.037148\n",
      ">> Epoch 849 finished \tANN training loss 1.037263\n",
      ">> Epoch 850 finished \tANN training loss 1.036797\n",
      ">> Epoch 851 finished \tANN training loss 1.038181\n",
      ">> Epoch 852 finished \tANN training loss 1.036845\n",
      ">> Epoch 853 finished \tANN training loss 1.036670\n",
      ">> Epoch 854 finished \tANN training loss 1.036699\n",
      ">> Epoch 855 finished \tANN training loss 1.036940\n",
      ">> Epoch 856 finished \tANN training loss 1.036716\n",
      ">> Epoch 857 finished \tANN training loss 1.036655\n",
      ">> Epoch 858 finished \tANN training loss 1.036724\n",
      ">> Epoch 859 finished \tANN training loss 1.036659\n",
      ">> Epoch 860 finished \tANN training loss 1.036764\n",
      ">> Epoch 861 finished \tANN training loss 1.037030\n",
      ">> Epoch 862 finished \tANN training loss 1.036945\n",
      ">> Epoch 863 finished \tANN training loss 1.038368\n",
      ">> Epoch 864 finished \tANN training loss 1.037251\n",
      ">> Epoch 865 finished \tANN training loss 1.036922\n",
      ">> Epoch 866 finished \tANN training loss 1.036667\n",
      ">> Epoch 867 finished \tANN training loss 1.037531\n",
      ">> Epoch 868 finished \tANN training loss 1.039240\n",
      ">> Epoch 869 finished \tANN training loss 1.038249\n",
      ">> Epoch 870 finished \tANN training loss 1.037120\n",
      ">> Epoch 871 finished \tANN training loss 1.037021\n",
      ">> Epoch 872 finished \tANN training loss 1.037256\n",
      ">> Epoch 873 finished \tANN training loss 1.036666\n",
      ">> Epoch 874 finished \tANN training loss 1.037251\n",
      ">> Epoch 875 finished \tANN training loss 1.036858\n",
      ">> Epoch 876 finished \tANN training loss 1.036776\n",
      ">> Epoch 877 finished \tANN training loss 1.037096\n",
      ">> Epoch 878 finished \tANN training loss 1.038118\n",
      ">> Epoch 879 finished \tANN training loss 1.036794\n",
      ">> Epoch 880 finished \tANN training loss 1.036796\n",
      ">> Epoch 881 finished \tANN training loss 1.037332\n",
      ">> Epoch 882 finished \tANN training loss 1.037549\n",
      ">> Epoch 883 finished \tANN training loss 1.037163\n",
      ">> Epoch 884 finished \tANN training loss 1.037678\n",
      ">> Epoch 885 finished \tANN training loss 1.037334\n",
      ">> Epoch 886 finished \tANN training loss 1.036751\n",
      ">> Epoch 887 finished \tANN training loss 1.036958\n",
      ">> Epoch 888 finished \tANN training loss 1.037336\n",
      ">> Epoch 889 finished \tANN training loss 1.036985\n",
      ">> Epoch 890 finished \tANN training loss 1.037875\n",
      ">> Epoch 891 finished \tANN training loss 1.036907\n",
      ">> Epoch 892 finished \tANN training loss 1.036876\n",
      ">> Epoch 893 finished \tANN training loss 1.036949\n",
      ">> Epoch 894 finished \tANN training loss 1.036757\n",
      ">> Epoch 895 finished \tANN training loss 1.036669\n",
      ">> Epoch 896 finished \tANN training loss 1.036815\n",
      ">> Epoch 897 finished \tANN training loss 1.037556\n",
      ">> Epoch 898 finished \tANN training loss 1.037148\n",
      ">> Epoch 899 finished \tANN training loss 1.036817\n",
      ">> Epoch 900 finished \tANN training loss 1.036763\n",
      ">> Epoch 901 finished \tANN training loss 1.036746\n",
      ">> Epoch 902 finished \tANN training loss 1.036893\n",
      ">> Epoch 903 finished \tANN training loss 1.036731\n",
      ">> Epoch 904 finished \tANN training loss 1.036723\n",
      ">> Epoch 905 finished \tANN training loss 1.036655\n",
      ">> Epoch 906 finished \tANN training loss 1.037042\n",
      ">> Epoch 907 finished \tANN training loss 1.037425\n",
      ">> Epoch 908 finished \tANN training loss 1.036800\n",
      ">> Epoch 909 finished \tANN training loss 1.036938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 910 finished \tANN training loss 1.038260\n",
      ">> Epoch 911 finished \tANN training loss 1.037258\n",
      ">> Epoch 912 finished \tANN training loss 1.037738\n",
      ">> Epoch 913 finished \tANN training loss 1.036763\n",
      ">> Epoch 914 finished \tANN training loss 1.036655\n",
      ">> Epoch 915 finished \tANN training loss 1.036655\n",
      ">> Epoch 916 finished \tANN training loss 1.036951\n",
      ">> Epoch 917 finished \tANN training loss 1.036715\n",
      ">> Epoch 918 finished \tANN training loss 1.036667\n",
      ">> Epoch 919 finished \tANN training loss 1.037114\n",
      ">> Epoch 920 finished \tANN training loss 1.036776\n",
      ">> Epoch 921 finished \tANN training loss 1.036689\n",
      ">> Epoch 922 finished \tANN training loss 1.037020\n",
      ">> Epoch 923 finished \tANN training loss 1.037035\n",
      ">> Epoch 924 finished \tANN training loss 1.036858\n",
      ">> Epoch 925 finished \tANN training loss 1.036973\n",
      ">> Epoch 926 finished \tANN training loss 1.037269\n",
      ">> Epoch 927 finished \tANN training loss 1.036657\n",
      ">> Epoch 928 finished \tANN training loss 1.036795\n",
      ">> Epoch 929 finished \tANN training loss 1.037842\n",
      ">> Epoch 930 finished \tANN training loss 1.037144\n",
      ">> Epoch 931 finished \tANN training loss 1.036993\n",
      ">> Epoch 932 finished \tANN training loss 1.037282\n",
      ">> Epoch 933 finished \tANN training loss 1.036658\n",
      ">> Epoch 934 finished \tANN training loss 1.036662\n",
      ">> Epoch 935 finished \tANN training loss 1.036834\n",
      ">> Epoch 936 finished \tANN training loss 1.037078\n",
      ">> Epoch 937 finished \tANN training loss 1.036804\n",
      ">> Epoch 938 finished \tANN training loss 1.037404\n",
      ">> Epoch 939 finished \tANN training loss 1.036789\n",
      ">> Epoch 940 finished \tANN training loss 1.036808\n",
      ">> Epoch 941 finished \tANN training loss 1.036788\n",
      ">> Epoch 942 finished \tANN training loss 1.037152\n",
      ">> Epoch 943 finished \tANN training loss 1.037529\n",
      ">> Epoch 944 finished \tANN training loss 1.036674\n",
      ">> Epoch 945 finished \tANN training loss 1.036656\n",
      ">> Epoch 946 finished \tANN training loss 1.037346\n",
      ">> Epoch 947 finished \tANN training loss 1.038048\n",
      ">> Epoch 948 finished \tANN training loss 1.037461\n",
      ">> Epoch 949 finished \tANN training loss 1.037209\n",
      ">> Epoch 950 finished \tANN training loss 1.036682\n",
      ">> Epoch 951 finished \tANN training loss 1.037190\n",
      ">> Epoch 952 finished \tANN training loss 1.036796\n",
      ">> Epoch 953 finished \tANN training loss 1.036731\n",
      ">> Epoch 954 finished \tANN training loss 1.036660\n",
      ">> Epoch 955 finished \tANN training loss 1.036688\n",
      ">> Epoch 956 finished \tANN training loss 1.036780\n",
      ">> Epoch 957 finished \tANN training loss 1.036673\n",
      ">> Epoch 958 finished \tANN training loss 1.036661\n",
      ">> Epoch 959 finished \tANN training loss 1.036883\n",
      ">> Epoch 960 finished \tANN training loss 1.036758\n",
      ">> Epoch 961 finished \tANN training loss 1.037037\n",
      ">> Epoch 962 finished \tANN training loss 1.036655\n",
      ">> Epoch 963 finished \tANN training loss 1.036884\n",
      ">> Epoch 964 finished \tANN training loss 1.038005\n",
      ">> Epoch 965 finished \tANN training loss 1.036866\n",
      ">> Epoch 966 finished \tANN training loss 1.037960\n",
      ">> Epoch 967 finished \tANN training loss 1.038013\n",
      ">> Epoch 968 finished \tANN training loss 1.036997\n",
      ">> Epoch 969 finished \tANN training loss 1.036660\n",
      ">> Epoch 970 finished \tANN training loss 1.036739\n",
      ">> Epoch 971 finished \tANN training loss 1.037474\n",
      ">> Epoch 972 finished \tANN training loss 1.036674\n",
      ">> Epoch 973 finished \tANN training loss 1.037059\n",
      ">> Epoch 974 finished \tANN training loss 1.036686\n",
      ">> Epoch 975 finished \tANN training loss 1.036655\n",
      ">> Epoch 976 finished \tANN training loss 1.037118\n",
      ">> Epoch 977 finished \tANN training loss 1.036804\n",
      ">> Epoch 978 finished \tANN training loss 1.037134\n",
      ">> Epoch 979 finished \tANN training loss 1.036655\n",
      ">> Epoch 980 finished \tANN training loss 1.036682\n",
      ">> Epoch 981 finished \tANN training loss 1.036716\n",
      ">> Epoch 982 finished \tANN training loss 1.036743\n",
      ">> Epoch 983 finished \tANN training loss 1.036665\n",
      ">> Epoch 984 finished \tANN training loss 1.037332\n",
      ">> Epoch 985 finished \tANN training loss 1.036736\n",
      ">> Epoch 986 finished \tANN training loss 1.036666\n",
      ">> Epoch 987 finished \tANN training loss 1.036676\n",
      ">> Epoch 988 finished \tANN training loss 1.036699\n",
      ">> Epoch 989 finished \tANN training loss 1.037341\n",
      ">> Epoch 990 finished \tANN training loss 1.037157\n",
      ">> Epoch 991 finished \tANN training loss 1.037235\n",
      ">> Epoch 992 finished \tANN training loss 1.036773\n",
      ">> Epoch 993 finished \tANN training loss 1.038293\n",
      ">> Epoch 994 finished \tANN training loss 1.037398\n",
      ">> Epoch 995 finished \tANN training loss 1.036956\n",
      ">> Epoch 996 finished \tANN training loss 1.036684\n",
      ">> Epoch 997 finished \tANN training loss 1.036704\n",
      ">> Epoch 998 finished \tANN training loss 1.036664\n",
      ">> Epoch 999 finished \tANN training loss 1.037016\n",
      "[END] Fine tuning step\n",
      "\n",
      "\n",
      "TRIAL:  8\n",
      "\n",
      "\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 5.738114\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 4.849667\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 4.299739\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 4.742702\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 5.598973\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 6.885141\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 9.040172\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 12.134219\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 14.839504\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 22.521604\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 265.914856\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 972.583496\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 1806.092896\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 2310.465820\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 2572.000000\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 2655.656982\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 2732.633545\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 2825.347656\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 2899.024902\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 3076.005615\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 5.130596\n",
      ">> Epoch 1 finished \tANN training loss 2.742185\n",
      ">> Epoch 2 finished \tANN training loss 1.752989\n",
      ">> Epoch 3 finished \tANN training loss 1.352548\n",
      ">> Epoch 4 finished \tANN training loss 1.174138\n",
      ">> Epoch 5 finished \tANN training loss 1.083895\n",
      ">> Epoch 6 finished \tANN training loss 1.022026\n",
      ">> Epoch 7 finished \tANN training loss 0.986358\n",
      ">> Epoch 8 finished \tANN training loss 0.958147\n",
      ">> Epoch 9 finished \tANN training loss 0.924793\n",
      ">> Epoch 10 finished \tANN training loss 0.879486\n",
      ">> Epoch 11 finished \tANN training loss 0.823741\n",
      ">> Epoch 12 finished \tANN training loss 0.760638\n",
      ">> Epoch 13 finished \tANN training loss 0.691972\n",
      ">> Epoch 14 finished \tANN training loss 0.621980\n",
      ">> Epoch 15 finished \tANN training loss 0.554728\n",
      ">> Epoch 16 finished \tANN training loss 0.497622\n",
      ">> Epoch 17 finished \tANN training loss 0.449573\n",
      ">> Epoch 18 finished \tANN training loss 0.412056\n",
      ">> Epoch 19 finished \tANN training loss 0.377957\n",
      ">> Epoch 20 finished \tANN training loss 0.351616\n",
      ">> Epoch 21 finished \tANN training loss 0.326750\n",
      ">> Epoch 22 finished \tANN training loss 0.308012\n",
      ">> Epoch 23 finished \tANN training loss 0.290812\n",
      ">> Epoch 24 finished \tANN training loss 0.288721\n",
      ">> Epoch 25 finished \tANN training loss 0.275574\n",
      ">> Epoch 26 finished \tANN training loss 0.262870\n",
      ">> Epoch 27 finished \tANN training loss 0.248584\n",
      ">> Epoch 28 finished \tANN training loss 0.265238\n",
      ">> Epoch 29 finished \tANN training loss 0.239940\n",
      ">> Epoch 30 finished \tANN training loss 0.236361\n",
      ">> Epoch 31 finished \tANN training loss 0.233921\n",
      ">> Epoch 32 finished \tANN training loss 0.233381\n",
      ">> Epoch 33 finished \tANN training loss 0.238872\n",
      ">> Epoch 34 finished \tANN training loss 0.227546\n",
      ">> Epoch 35 finished \tANN training loss 0.227244\n",
      ">> Epoch 36 finished \tANN training loss 0.218043\n",
      ">> Epoch 37 finished \tANN training loss 0.221941\n",
      ">> Epoch 38 finished \tANN training loss 0.219512\n",
      ">> Epoch 39 finished \tANN training loss 0.365527\n",
      ">> Epoch 40 finished \tANN training loss 0.247013\n",
      ">> Epoch 41 finished \tANN training loss 0.228422\n",
      ">> Epoch 42 finished \tANN training loss 0.233448\n",
      ">> Epoch 43 finished \tANN training loss 0.238747\n",
      ">> Epoch 44 finished \tANN training loss 0.225917\n",
      ">> Epoch 45 finished \tANN training loss 0.228704\n",
      ">> Epoch 46 finished \tANN training loss 0.224664\n",
      ">> Epoch 47 finished \tANN training loss 0.224886\n",
      ">> Epoch 48 finished \tANN training loss 0.229076\n",
      ">> Epoch 49 finished \tANN training loss 0.225462\n",
      ">> Epoch 50 finished \tANN training loss 0.224424\n",
      ">> Epoch 51 finished \tANN training loss 0.234579\n",
      ">> Epoch 52 finished \tANN training loss 0.220194\n",
      ">> Epoch 53 finished \tANN training loss 0.216648\n",
      ">> Epoch 54 finished \tANN training loss 0.229853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 55 finished \tANN training loss 0.214635\n",
      ">> Epoch 56 finished \tANN training loss 0.220210\n",
      ">> Epoch 57 finished \tANN training loss 0.216523\n",
      ">> Epoch 58 finished \tANN training loss 0.217248\n",
      ">> Epoch 59 finished \tANN training loss 0.217539\n",
      ">> Epoch 60 finished \tANN training loss 0.215087\n",
      ">> Epoch 61 finished \tANN training loss 0.212002\n",
      ">> Epoch 62 finished \tANN training loss 0.217575\n",
      ">> Epoch 63 finished \tANN training loss 0.214123\n",
      ">> Epoch 64 finished \tANN training loss 0.220896\n",
      ">> Epoch 65 finished \tANN training loss 0.212535\n",
      ">> Epoch 66 finished \tANN training loss 0.224014\n",
      ">> Epoch 67 finished \tANN training loss 0.210153\n",
      ">> Epoch 68 finished \tANN training loss 0.207304\n",
      ">> Epoch 69 finished \tANN training loss 0.214948\n",
      ">> Epoch 70 finished \tANN training loss 0.204124\n",
      ">> Epoch 71 finished \tANN training loss 0.203309\n",
      ">> Epoch 72 finished \tANN training loss 0.208225\n",
      ">> Epoch 73 finished \tANN training loss 0.209303\n",
      ">> Epoch 74 finished \tANN training loss 0.215485\n",
      ">> Epoch 75 finished \tANN training loss 0.210506\n",
      ">> Epoch 76 finished \tANN training loss 0.198700\n",
      ">> Epoch 77 finished \tANN training loss 0.204588\n",
      ">> Epoch 78 finished \tANN training loss 0.210986\n",
      ">> Epoch 79 finished \tANN training loss 0.210381\n",
      ">> Epoch 80 finished \tANN training loss 0.202673\n",
      ">> Epoch 81 finished \tANN training loss 0.204721\n",
      ">> Epoch 82 finished \tANN training loss 0.209593\n",
      ">> Epoch 83 finished \tANN training loss 0.212104\n",
      ">> Epoch 84 finished \tANN training loss 0.212425\n",
      ">> Epoch 85 finished \tANN training loss 0.204619\n",
      ">> Epoch 86 finished \tANN training loss 0.196695\n",
      ">> Epoch 87 finished \tANN training loss 0.204093\n",
      ">> Epoch 88 finished \tANN training loss 0.202331\n",
      ">> Epoch 89 finished \tANN training loss 0.198504\n",
      ">> Epoch 90 finished \tANN training loss 0.202204\n",
      ">> Epoch 91 finished \tANN training loss 0.203476\n",
      ">> Epoch 92 finished \tANN training loss 0.204198\n",
      ">> Epoch 93 finished \tANN training loss 0.202714\n",
      ">> Epoch 94 finished \tANN training loss 0.199143\n",
      ">> Epoch 95 finished \tANN training loss 0.200534\n",
      ">> Epoch 96 finished \tANN training loss 0.199469\n",
      ">> Epoch 97 finished \tANN training loss 0.197683\n",
      ">> Epoch 98 finished \tANN training loss 0.195810\n",
      ">> Epoch 99 finished \tANN training loss 0.212162\n",
      ">> Epoch 100 finished \tANN training loss 0.207606\n",
      ">> Epoch 101 finished \tANN training loss 0.197351\n",
      ">> Epoch 102 finished \tANN training loss 0.206048\n",
      ">> Epoch 103 finished \tANN training loss 0.197508\n",
      ">> Epoch 104 finished \tANN training loss 0.197633\n",
      ">> Epoch 105 finished \tANN training loss 0.203988\n",
      ">> Epoch 106 finished \tANN training loss 0.203923\n",
      ">> Epoch 107 finished \tANN training loss 0.201280\n",
      ">> Epoch 108 finished \tANN training loss 0.195464\n",
      ">> Epoch 109 finished \tANN training loss 0.198214\n",
      ">> Epoch 110 finished \tANN training loss 0.193465\n",
      ">> Epoch 111 finished \tANN training loss 0.192323\n",
      ">> Epoch 112 finished \tANN training loss 0.194708\n",
      ">> Epoch 113 finished \tANN training loss 0.193045\n",
      ">> Epoch 114 finished \tANN training loss 0.200158\n",
      ">> Epoch 115 finished \tANN training loss 0.194776\n",
      ">> Epoch 116 finished \tANN training loss 0.199177\n",
      ">> Epoch 117 finished \tANN training loss 0.191553\n",
      ">> Epoch 118 finished \tANN training loss 0.197427\n",
      ">> Epoch 119 finished \tANN training loss 0.207108\n",
      ">> Epoch 120 finished \tANN training loss 0.201597\n",
      ">> Epoch 121 finished \tANN training loss 0.191343\n",
      ">> Epoch 122 finished \tANN training loss 0.191637\n",
      ">> Epoch 123 finished \tANN training loss 0.191575\n",
      ">> Epoch 124 finished \tANN training loss 0.199985\n",
      ">> Epoch 125 finished \tANN training loss 0.195338\n",
      ">> Epoch 126 finished \tANN training loss 0.198661\n",
      ">> Epoch 127 finished \tANN training loss 0.196424\n",
      ">> Epoch 128 finished \tANN training loss 0.195096\n",
      ">> Epoch 129 finished \tANN training loss 0.193622\n",
      ">> Epoch 130 finished \tANN training loss 0.200043\n",
      ">> Epoch 131 finished \tANN training loss 0.189320\n",
      ">> Epoch 132 finished \tANN training loss 0.190827\n",
      ">> Epoch 133 finished \tANN training loss 0.201261\n",
      ">> Epoch 134 finished \tANN training loss 0.192296\n",
      ">> Epoch 135 finished \tANN training loss 0.192690\n",
      ">> Epoch 136 finished \tANN training loss 0.191566\n",
      ">> Epoch 137 finished \tANN training loss 0.196726\n",
      ">> Epoch 138 finished \tANN training loss 0.190068\n",
      ">> Epoch 139 finished \tANN training loss 0.196036\n",
      ">> Epoch 140 finished \tANN training loss 0.189814\n",
      ">> Epoch 141 finished \tANN training loss 0.189963\n",
      ">> Epoch 142 finished \tANN training loss 0.198357\n",
      ">> Epoch 143 finished \tANN training loss 0.196201\n",
      ">> Epoch 144 finished \tANN training loss 0.198935\n",
      ">> Epoch 145 finished \tANN training loss 0.206235\n",
      ">> Epoch 146 finished \tANN training loss 0.199633\n",
      ">> Epoch 147 finished \tANN training loss 0.190458\n",
      ">> Epoch 148 finished \tANN training loss 0.196089\n",
      ">> Epoch 149 finished \tANN training loss 0.193427\n",
      ">> Epoch 150 finished \tANN training loss 0.192563\n",
      ">> Epoch 151 finished \tANN training loss 0.196111\n",
      ">> Epoch 152 finished \tANN training loss 0.193196\n",
      ">> Epoch 153 finished \tANN training loss 0.193525\n",
      ">> Epoch 154 finished \tANN training loss 0.200845\n",
      ">> Epoch 155 finished \tANN training loss 0.200317\n",
      ">> Epoch 156 finished \tANN training loss 0.199954\n",
      ">> Epoch 157 finished \tANN training loss 0.196415\n",
      ">> Epoch 158 finished \tANN training loss 0.192839\n",
      ">> Epoch 159 finished \tANN training loss 0.193148\n",
      ">> Epoch 160 finished \tANN training loss 0.201327\n",
      ">> Epoch 161 finished \tANN training loss 0.203277\n",
      ">> Epoch 162 finished \tANN training loss 0.192127\n",
      ">> Epoch 163 finished \tANN training loss 0.190824\n",
      ">> Epoch 164 finished \tANN training loss 0.189085\n",
      ">> Epoch 165 finished \tANN training loss 0.193738\n",
      ">> Epoch 166 finished \tANN training loss 0.188886\n",
      ">> Epoch 167 finished \tANN training loss 0.191144\n",
      ">> Epoch 168 finished \tANN training loss 0.193719\n",
      ">> Epoch 169 finished \tANN training loss 0.202892\n",
      ">> Epoch 170 finished \tANN training loss 0.193604\n",
      ">> Epoch 171 finished \tANN training loss 0.190693\n",
      ">> Epoch 172 finished \tANN training loss 0.194607\n",
      ">> Epoch 173 finished \tANN training loss 0.188475\n",
      ">> Epoch 174 finished \tANN training loss 0.189958\n",
      ">> Epoch 175 finished \tANN training loss 0.199325\n",
      ">> Epoch 176 finished \tANN training loss 0.195008\n",
      ">> Epoch 177 finished \tANN training loss 0.193645\n",
      ">> Epoch 178 finished \tANN training loss 0.190584\n",
      ">> Epoch 179 finished \tANN training loss 0.197307\n",
      ">> Epoch 180 finished \tANN training loss 0.190441\n",
      ">> Epoch 181 finished \tANN training loss 0.187327\n",
      ">> Epoch 182 finished \tANN training loss 0.188825\n",
      ">> Epoch 183 finished \tANN training loss 0.189596\n",
      ">> Epoch 184 finished \tANN training loss 0.188079\n",
      ">> Epoch 185 finished \tANN training loss 0.197572\n",
      ">> Epoch 186 finished \tANN training loss 0.194336\n",
      ">> Epoch 187 finished \tANN training loss 0.193407\n",
      ">> Epoch 188 finished \tANN training loss 0.199802\n",
      ">> Epoch 189 finished \tANN training loss 0.192408\n",
      ">> Epoch 190 finished \tANN training loss 0.187935\n",
      ">> Epoch 191 finished \tANN training loss 0.191261\n",
      ">> Epoch 192 finished \tANN training loss 0.195602\n",
      ">> Epoch 193 finished \tANN training loss 0.193285\n",
      ">> Epoch 194 finished \tANN training loss 0.189662\n",
      ">> Epoch 195 finished \tANN training loss 0.196717\n",
      ">> Epoch 196 finished \tANN training loss 0.192646\n",
      ">> Epoch 197 finished \tANN training loss 0.199482\n",
      ">> Epoch 198 finished \tANN training loss 0.196352\n",
      ">> Epoch 199 finished \tANN training loss 0.186469\n",
      ">> Epoch 200 finished \tANN training loss 0.204455\n",
      ">> Epoch 201 finished \tANN training loss 0.190575\n",
      ">> Epoch 202 finished \tANN training loss 0.194539\n",
      ">> Epoch 203 finished \tANN training loss 0.197164\n",
      ">> Epoch 204 finished \tANN training loss 0.202208\n",
      ">> Epoch 205 finished \tANN training loss 0.192553\n",
      ">> Epoch 206 finished \tANN training loss 0.198776\n",
      ">> Epoch 207 finished \tANN training loss 0.202176\n",
      ">> Epoch 208 finished \tANN training loss 0.196851\n",
      ">> Epoch 209 finished \tANN training loss 0.194298\n",
      ">> Epoch 210 finished \tANN training loss 0.198201\n",
      ">> Epoch 211 finished \tANN training loss 0.193932\n",
      ">> Epoch 212 finished \tANN training loss 0.187010\n",
      ">> Epoch 213 finished \tANN training loss 0.189166\n",
      ">> Epoch 214 finished \tANN training loss 0.190429\n",
      ">> Epoch 215 finished \tANN training loss 0.190448\n",
      ">> Epoch 216 finished \tANN training loss 0.187757\n",
      ">> Epoch 217 finished \tANN training loss 0.187923\n",
      ">> Epoch 218 finished \tANN training loss 0.192697\n",
      ">> Epoch 219 finished \tANN training loss 0.189765\n",
      ">> Epoch 220 finished \tANN training loss 0.199358\n",
      ">> Epoch 221 finished \tANN training loss 0.195060\n",
      ">> Epoch 222 finished \tANN training loss 0.189120\n",
      ">> Epoch 223 finished \tANN training loss 0.189040\n",
      ">> Epoch 224 finished \tANN training loss 0.191088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 225 finished \tANN training loss 0.188015\n",
      ">> Epoch 226 finished \tANN training loss 0.194464\n",
      ">> Epoch 227 finished \tANN training loss 0.188403\n",
      ">> Epoch 228 finished \tANN training loss 0.196035\n",
      ">> Epoch 229 finished \tANN training loss 0.190473\n",
      ">> Epoch 230 finished \tANN training loss 0.198135\n",
      ">> Epoch 231 finished \tANN training loss 0.189337\n",
      ">> Epoch 232 finished \tANN training loss 0.193815\n",
      ">> Epoch 233 finished \tANN training loss 0.195049\n",
      ">> Epoch 234 finished \tANN training loss 0.186725\n",
      ">> Epoch 235 finished \tANN training loss 0.199389\n",
      ">> Epoch 236 finished \tANN training loss 0.183881\n",
      ">> Epoch 237 finished \tANN training loss 0.191346\n",
      ">> Epoch 238 finished \tANN training loss 0.194981\n",
      ">> Epoch 239 finished \tANN training loss 0.192935\n",
      ">> Epoch 240 finished \tANN training loss 0.191705\n",
      ">> Epoch 241 finished \tANN training loss 0.183807\n",
      ">> Epoch 242 finished \tANN training loss 0.190557\n",
      ">> Epoch 243 finished \tANN training loss 0.197431\n",
      ">> Epoch 244 finished \tANN training loss 0.198733\n",
      ">> Epoch 245 finished \tANN training loss 0.190808\n",
      ">> Epoch 246 finished \tANN training loss 0.189815\n",
      ">> Epoch 247 finished \tANN training loss 0.193905\n",
      ">> Epoch 248 finished \tANN training loss 0.195960\n",
      ">> Epoch 249 finished \tANN training loss 0.190690\n",
      ">> Epoch 250 finished \tANN training loss 0.195106\n",
      ">> Epoch 251 finished \tANN training loss 0.197850\n",
      ">> Epoch 252 finished \tANN training loss 0.193737\n",
      ">> Epoch 253 finished \tANN training loss 0.193941\n",
      ">> Epoch 254 finished \tANN training loss 0.194316\n",
      ">> Epoch 255 finished \tANN training loss 0.187806\n",
      ">> Epoch 256 finished \tANN training loss 0.193507\n",
      ">> Epoch 257 finished \tANN training loss 0.191114\n",
      ">> Epoch 258 finished \tANN training loss 0.196189\n",
      ">> Epoch 259 finished \tANN training loss 0.188494\n",
      ">> Epoch 260 finished \tANN training loss 0.197440\n",
      ">> Epoch 261 finished \tANN training loss 0.198140\n",
      ">> Epoch 262 finished \tANN training loss 0.193891\n",
      ">> Epoch 263 finished \tANN training loss 0.196161\n",
      ">> Epoch 264 finished \tANN training loss 0.191668\n",
      ">> Epoch 265 finished \tANN training loss 0.195073\n",
      ">> Epoch 266 finished \tANN training loss 0.187206\n",
      ">> Epoch 267 finished \tANN training loss 0.189770\n",
      ">> Epoch 268 finished \tANN training loss 0.184964\n",
      ">> Epoch 269 finished \tANN training loss 0.191586\n",
      ">> Epoch 270 finished \tANN training loss 0.189746\n",
      ">> Epoch 271 finished \tANN training loss 0.186979\n",
      ">> Epoch 272 finished \tANN training loss 0.184026\n",
      ">> Epoch 273 finished \tANN training loss 0.200088\n",
      ">> Epoch 274 finished \tANN training loss 0.206616\n",
      ">> Epoch 275 finished \tANN training loss 0.191292\n",
      ">> Epoch 276 finished \tANN training loss 0.187054\n",
      ">> Epoch 277 finished \tANN training loss 0.188919\n",
      ">> Epoch 278 finished \tANN training loss 0.186535\n",
      ">> Epoch 279 finished \tANN training loss 0.187217\n",
      ">> Epoch 280 finished \tANN training loss 0.196376\n",
      ">> Epoch 281 finished \tANN training loss 0.188022\n",
      ">> Epoch 282 finished \tANN training loss 0.194713\n",
      ">> Epoch 283 finished \tANN training loss 0.190372\n",
      ">> Epoch 284 finished \tANN training loss 0.188056\n",
      ">> Epoch 285 finished \tANN training loss 0.190511\n",
      ">> Epoch 286 finished \tANN training loss 0.196469\n",
      ">> Epoch 287 finished \tANN training loss 0.189203\n",
      ">> Epoch 288 finished \tANN training loss 0.200055\n",
      ">> Epoch 289 finished \tANN training loss 0.185932\n",
      ">> Epoch 290 finished \tANN training loss 0.188812\n",
      ">> Epoch 291 finished \tANN training loss 0.189599\n",
      ">> Epoch 292 finished \tANN training loss 0.198573\n",
      ">> Epoch 293 finished \tANN training loss 0.184862\n",
      ">> Epoch 294 finished \tANN training loss 0.188646\n",
      ">> Epoch 295 finished \tANN training loss 0.191609\n",
      ">> Epoch 296 finished \tANN training loss 0.188769\n",
      ">> Epoch 297 finished \tANN training loss 0.187212\n",
      ">> Epoch 298 finished \tANN training loss 0.198790\n",
      ">> Epoch 299 finished \tANN training loss 0.194997\n",
      ">> Epoch 300 finished \tANN training loss 0.189729\n",
      ">> Epoch 301 finished \tANN training loss 0.187750\n",
      ">> Epoch 302 finished \tANN training loss 0.187911\n",
      ">> Epoch 303 finished \tANN training loss 0.199722\n",
      ">> Epoch 304 finished \tANN training loss 0.188484\n",
      ">> Epoch 305 finished \tANN training loss 0.187303\n",
      ">> Epoch 306 finished \tANN training loss 0.201602\n",
      ">> Epoch 307 finished \tANN training loss 0.186434\n",
      ">> Epoch 308 finished \tANN training loss 0.188877\n",
      ">> Epoch 309 finished \tANN training loss 0.201509\n",
      ">> Epoch 310 finished \tANN training loss 0.187646\n",
      ">> Epoch 311 finished \tANN training loss 0.186737\n",
      ">> Epoch 312 finished \tANN training loss 0.206767\n",
      ">> Epoch 313 finished \tANN training loss 0.191748\n",
      ">> Epoch 314 finished \tANN training loss 0.200169\n",
      ">> Epoch 315 finished \tANN training loss 0.184516\n",
      ">> Epoch 316 finished \tANN training loss 0.192687\n",
      ">> Epoch 317 finished \tANN training loss 0.192558\n",
      ">> Epoch 318 finished \tANN training loss 0.186578\n",
      ">> Epoch 319 finished \tANN training loss 0.186694\n",
      ">> Epoch 320 finished \tANN training loss 0.186467\n",
      ">> Epoch 321 finished \tANN training loss 0.193118\n",
      ">> Epoch 322 finished \tANN training loss 0.187821\n",
      ">> Epoch 323 finished \tANN training loss 0.193506\n",
      ">> Epoch 324 finished \tANN training loss 0.189816\n",
      ">> Epoch 325 finished \tANN training loss 0.185209\n",
      ">> Epoch 326 finished \tANN training loss 0.193441\n",
      ">> Epoch 327 finished \tANN training loss 0.186569\n",
      ">> Epoch 328 finished \tANN training loss 0.190654\n",
      ">> Epoch 329 finished \tANN training loss 0.195106\n",
      ">> Epoch 330 finished \tANN training loss 0.189372\n",
      ">> Epoch 331 finished \tANN training loss 0.196609\n",
      ">> Epoch 332 finished \tANN training loss 0.198518\n",
      ">> Epoch 333 finished \tANN training loss 0.189190\n",
      ">> Epoch 334 finished \tANN training loss 0.189025\n",
      ">> Epoch 335 finished \tANN training loss 0.185710\n",
      ">> Epoch 336 finished \tANN training loss 0.186168\n",
      ">> Epoch 337 finished \tANN training loss 0.189701\n",
      ">> Epoch 338 finished \tANN training loss 0.191585\n",
      ">> Epoch 339 finished \tANN training loss 0.186271\n",
      ">> Epoch 340 finished \tANN training loss 0.192110\n",
      ">> Epoch 341 finished \tANN training loss 0.187231\n",
      ">> Epoch 342 finished \tANN training loss 0.183119\n",
      ">> Epoch 343 finished \tANN training loss 0.185084\n",
      ">> Epoch 344 finished \tANN training loss 0.191711\n",
      ">> Epoch 345 finished \tANN training loss 0.193307\n",
      ">> Epoch 346 finished \tANN training loss 0.190463\n",
      ">> Epoch 347 finished \tANN training loss 0.186816\n",
      ">> Epoch 348 finished \tANN training loss 0.195380\n",
      ">> Epoch 349 finished \tANN training loss 0.189525\n",
      ">> Epoch 350 finished \tANN training loss 0.201508\n",
      ">> Epoch 351 finished \tANN training loss 0.194988\n",
      ">> Epoch 352 finished \tANN training loss 0.188856\n",
      ">> Epoch 353 finished \tANN training loss 0.190257\n",
      ">> Epoch 354 finished \tANN training loss 0.199123\n",
      ">> Epoch 355 finished \tANN training loss 0.193651\n",
      ">> Epoch 356 finished \tANN training loss 0.193926\n",
      ">> Epoch 357 finished \tANN training loss 0.196511\n",
      ">> Epoch 358 finished \tANN training loss 0.193724\n",
      ">> Epoch 359 finished \tANN training loss 0.192308\n",
      ">> Epoch 360 finished \tANN training loss 0.190389\n",
      ">> Epoch 361 finished \tANN training loss 0.187783\n",
      ">> Epoch 362 finished \tANN training loss 0.194891\n",
      ">> Epoch 363 finished \tANN training loss 0.187112\n",
      ">> Epoch 364 finished \tANN training loss 0.185994\n",
      ">> Epoch 365 finished \tANN training loss 0.186775\n",
      ">> Epoch 366 finished \tANN training loss 0.187602\n",
      ">> Epoch 367 finished \tANN training loss 0.188326\n",
      ">> Epoch 368 finished \tANN training loss 0.191156\n",
      ">> Epoch 369 finished \tANN training loss 0.191617\n",
      ">> Epoch 370 finished \tANN training loss 0.194017\n",
      ">> Epoch 371 finished \tANN training loss 0.190180\n",
      ">> Epoch 372 finished \tANN training loss 0.188025\n",
      ">> Epoch 373 finished \tANN training loss 0.187167\n",
      ">> Epoch 374 finished \tANN training loss 0.187981\n",
      ">> Epoch 375 finished \tANN training loss 0.197792\n",
      ">> Epoch 376 finished \tANN training loss 0.191316\n",
      ">> Epoch 377 finished \tANN training loss 0.184455\n",
      ">> Epoch 378 finished \tANN training loss 0.186314\n",
      ">> Epoch 379 finished \tANN training loss 0.196520\n",
      ">> Epoch 380 finished \tANN training loss 0.188032\n",
      ">> Epoch 381 finished \tANN training loss 0.201651\n",
      ">> Epoch 382 finished \tANN training loss 0.189746\n",
      ">> Epoch 383 finished \tANN training loss 0.192408\n",
      ">> Epoch 384 finished \tANN training loss 0.193864\n",
      ">> Epoch 385 finished \tANN training loss 0.190373\n",
      ">> Epoch 386 finished \tANN training loss 0.186293\n",
      ">> Epoch 387 finished \tANN training loss 0.186410\n",
      ">> Epoch 388 finished \tANN training loss 0.188368\n",
      ">> Epoch 389 finished \tANN training loss 0.192755\n",
      ">> Epoch 390 finished \tANN training loss 0.186141\n",
      ">> Epoch 391 finished \tANN training loss 0.190824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 392 finished \tANN training loss 0.189940\n",
      ">> Epoch 393 finished \tANN training loss 0.192668\n",
      ">> Epoch 394 finished \tANN training loss 0.195201\n",
      ">> Epoch 395 finished \tANN training loss 0.187701\n",
      ">> Epoch 396 finished \tANN training loss 0.186664\n",
      ">> Epoch 397 finished \tANN training loss 0.192052\n",
      ">> Epoch 398 finished \tANN training loss 0.187811\n",
      ">> Epoch 399 finished \tANN training loss 0.186679\n",
      ">> Epoch 400 finished \tANN training loss 0.190932\n",
      ">> Epoch 401 finished \tANN training loss 0.186584\n",
      ">> Epoch 402 finished \tANN training loss 0.191706\n",
      ">> Epoch 403 finished \tANN training loss 0.189092\n",
      ">> Epoch 404 finished \tANN training loss 0.187629\n",
      ">> Epoch 405 finished \tANN training loss 0.188016\n",
      ">> Epoch 406 finished \tANN training loss 0.194124\n",
      ">> Epoch 407 finished \tANN training loss 0.200608\n",
      ">> Epoch 408 finished \tANN training loss 0.199624\n",
      ">> Epoch 409 finished \tANN training loss 0.188498\n",
      ">> Epoch 410 finished \tANN training loss 0.186176\n",
      ">> Epoch 411 finished \tANN training loss 0.194067\n",
      ">> Epoch 412 finished \tANN training loss 0.192747\n",
      ">> Epoch 413 finished \tANN training loss 0.188692\n",
      ">> Epoch 414 finished \tANN training loss 0.186909\n",
      ">> Epoch 415 finished \tANN training loss 0.191084\n",
      ">> Epoch 416 finished \tANN training loss 0.188154\n",
      ">> Epoch 417 finished \tANN training loss 0.188654\n",
      ">> Epoch 418 finished \tANN training loss 0.206231\n",
      ">> Epoch 419 finished \tANN training loss 0.189467\n",
      ">> Epoch 420 finished \tANN training loss 0.183460\n",
      ">> Epoch 421 finished \tANN training loss 0.193308\n",
      ">> Epoch 422 finished \tANN training loss 0.191223\n",
      ">> Epoch 423 finished \tANN training loss 0.195661\n",
      ">> Epoch 424 finished \tANN training loss 0.197022\n",
      ">> Epoch 425 finished \tANN training loss 0.185475\n",
      ">> Epoch 426 finished \tANN training loss 0.189542\n",
      ">> Epoch 427 finished \tANN training loss 0.197090\n",
      ">> Epoch 428 finished \tANN training loss 0.194793\n",
      ">> Epoch 429 finished \tANN training loss 0.189562\n",
      ">> Epoch 430 finished \tANN training loss 0.195693\n",
      ">> Epoch 431 finished \tANN training loss 0.190401\n",
      ">> Epoch 432 finished \tANN training loss 0.186636\n",
      ">> Epoch 433 finished \tANN training loss 0.184809\n",
      ">> Epoch 434 finished \tANN training loss 0.190947\n",
      ">> Epoch 435 finished \tANN training loss 0.186214\n",
      ">> Epoch 436 finished \tANN training loss 0.188817\n",
      ">> Epoch 437 finished \tANN training loss 0.189163\n",
      ">> Epoch 438 finished \tANN training loss 0.191740\n",
      ">> Epoch 439 finished \tANN training loss 0.182717\n",
      ">> Epoch 440 finished \tANN training loss 0.189885\n",
      ">> Epoch 441 finished \tANN training loss 0.199093\n",
      ">> Epoch 442 finished \tANN training loss 0.192351\n",
      ">> Epoch 443 finished \tANN training loss 0.192208\n",
      ">> Epoch 444 finished \tANN training loss 0.194071\n",
      ">> Epoch 445 finished \tANN training loss 0.182565\n",
      ">> Epoch 446 finished \tANN training loss 0.182491\n",
      ">> Epoch 447 finished \tANN training loss 0.192826\n",
      ">> Epoch 448 finished \tANN training loss 0.184255\n",
      ">> Epoch 449 finished \tANN training loss 0.190129\n",
      ">> Epoch 450 finished \tANN training loss 0.185865\n",
      ">> Epoch 451 finished \tANN training loss 0.187413\n",
      ">> Epoch 452 finished \tANN training loss 0.197520\n",
      ">> Epoch 453 finished \tANN training loss 0.187800\n",
      ">> Epoch 454 finished \tANN training loss 0.184679\n",
      ">> Epoch 455 finished \tANN training loss 0.188303\n",
      ">> Epoch 456 finished \tANN training loss 0.185251\n",
      ">> Epoch 457 finished \tANN training loss 0.193841\n",
      ">> Epoch 458 finished \tANN training loss 0.188165\n",
      ">> Epoch 459 finished \tANN training loss 0.188651\n",
      ">> Epoch 460 finished \tANN training loss 0.197735\n",
      ">> Epoch 461 finished \tANN training loss 0.193874\n",
      ">> Epoch 462 finished \tANN training loss 0.193098\n",
      ">> Epoch 463 finished \tANN training loss 0.187723\n",
      ">> Epoch 464 finished \tANN training loss 0.183847\n",
      ">> Epoch 465 finished \tANN training loss 0.187793\n",
      ">> Epoch 466 finished \tANN training loss 0.187663\n",
      ">> Epoch 467 finished \tANN training loss 0.191837\n",
      ">> Epoch 468 finished \tANN training loss 0.205518\n",
      ">> Epoch 469 finished \tANN training loss 0.197445\n",
      ">> Epoch 470 finished \tANN training loss 0.187360\n",
      ">> Epoch 471 finished \tANN training loss 0.185763\n",
      ">> Epoch 472 finished \tANN training loss 0.188935\n",
      ">> Epoch 473 finished \tANN training loss 0.186401\n",
      ">> Epoch 474 finished \tANN training loss 0.193142\n",
      ">> Epoch 475 finished \tANN training loss 0.192950\n",
      ">> Epoch 476 finished \tANN training loss 0.185231\n",
      ">> Epoch 477 finished \tANN training loss 0.184969\n",
      ">> Epoch 478 finished \tANN training loss 0.187051\n",
      ">> Epoch 479 finished \tANN training loss 0.185143\n",
      ">> Epoch 480 finished \tANN training loss 0.190389\n",
      ">> Epoch 481 finished \tANN training loss 0.184324\n",
      ">> Epoch 482 finished \tANN training loss 0.186203\n",
      ">> Epoch 483 finished \tANN training loss 0.189032\n",
      ">> Epoch 484 finished \tANN training loss 0.186003\n",
      ">> Epoch 485 finished \tANN training loss 0.184757\n",
      ">> Epoch 486 finished \tANN training loss 0.188503\n",
      ">> Epoch 487 finished \tANN training loss 0.192630\n",
      ">> Epoch 488 finished \tANN training loss 0.185660\n",
      ">> Epoch 489 finished \tANN training loss 0.187595\n",
      ">> Epoch 490 finished \tANN training loss 0.189336\n",
      ">> Epoch 491 finished \tANN training loss 0.185474\n",
      ">> Epoch 492 finished \tANN training loss 0.183842\n",
      ">> Epoch 493 finished \tANN training loss 0.190533\n",
      ">> Epoch 494 finished \tANN training loss 0.189924\n",
      ">> Epoch 495 finished \tANN training loss 0.186020\n",
      ">> Epoch 496 finished \tANN training loss 0.195734\n",
      ">> Epoch 497 finished \tANN training loss 0.186589\n",
      ">> Epoch 498 finished \tANN training loss 0.185880\n",
      ">> Epoch 499 finished \tANN training loss 0.184043\n",
      ">> Epoch 500 finished \tANN training loss 0.183729\n",
      ">> Epoch 501 finished \tANN training loss 0.183254\n",
      ">> Epoch 502 finished \tANN training loss 0.183768\n",
      ">> Epoch 503 finished \tANN training loss 0.188423\n",
      ">> Epoch 504 finished \tANN training loss 0.185295\n",
      ">> Epoch 505 finished \tANN training loss 0.188240\n",
      ">> Epoch 506 finished \tANN training loss 0.188969\n",
      ">> Epoch 507 finished \tANN training loss 0.192403\n",
      ">> Epoch 508 finished \tANN training loss 0.186346\n",
      ">> Epoch 509 finished \tANN training loss 0.195786\n",
      ">> Epoch 510 finished \tANN training loss 0.189246\n",
      ">> Epoch 511 finished \tANN training loss 0.190003\n",
      ">> Epoch 512 finished \tANN training loss 0.188884\n",
      ">> Epoch 513 finished \tANN training loss 0.189324\n",
      ">> Epoch 514 finished \tANN training loss 0.189749\n",
      ">> Epoch 515 finished \tANN training loss 0.184413\n",
      ">> Epoch 516 finished \tANN training loss 0.187067\n",
      ">> Epoch 517 finished \tANN training loss 0.183705\n",
      ">> Epoch 518 finished \tANN training loss 0.189732\n",
      ">> Epoch 519 finished \tANN training loss 0.187267\n",
      ">> Epoch 520 finished \tANN training loss 0.191048\n",
      ">> Epoch 521 finished \tANN training loss 0.190355\n",
      ">> Epoch 522 finished \tANN training loss 0.189550\n",
      ">> Epoch 523 finished \tANN training loss 0.194458\n",
      ">> Epoch 524 finished \tANN training loss 0.192090\n",
      ">> Epoch 525 finished \tANN training loss 0.190145\n",
      ">> Epoch 526 finished \tANN training loss 0.191765\n",
      ">> Epoch 527 finished \tANN training loss 0.186521\n",
      ">> Epoch 528 finished \tANN training loss 0.187160\n",
      ">> Epoch 529 finished \tANN training loss 0.186691\n",
      ">> Epoch 530 finished \tANN training loss 0.186217\n",
      ">> Epoch 531 finished \tANN training loss 0.189203\n",
      ">> Epoch 532 finished \tANN training loss 0.192196\n",
      ">> Epoch 533 finished \tANN training loss 0.190151\n",
      ">> Epoch 534 finished \tANN training loss 0.190211\n",
      ">> Epoch 535 finished \tANN training loss 0.191491\n",
      ">> Epoch 536 finished \tANN training loss 0.193185\n",
      ">> Epoch 537 finished \tANN training loss 0.188976\n",
      ">> Epoch 538 finished \tANN training loss 0.197682\n",
      ">> Epoch 539 finished \tANN training loss 0.188725\n",
      ">> Epoch 540 finished \tANN training loss 0.187692\n",
      ">> Epoch 541 finished \tANN training loss 0.188637\n",
      ">> Epoch 542 finished \tANN training loss 0.188900\n",
      ">> Epoch 543 finished \tANN training loss 0.192138\n",
      ">> Epoch 544 finished \tANN training loss 0.183014\n",
      ">> Epoch 545 finished \tANN training loss 0.184967\n",
      ">> Epoch 546 finished \tANN training loss 0.187322\n",
      ">> Epoch 547 finished \tANN training loss 0.191442\n",
      ">> Epoch 548 finished \tANN training loss 0.200784\n",
      ">> Epoch 549 finished \tANN training loss 0.195006\n",
      ">> Epoch 550 finished \tANN training loss 0.190205\n",
      ">> Epoch 551 finished \tANN training loss 0.186545\n",
      ">> Epoch 552 finished \tANN training loss 0.185164\n",
      ">> Epoch 553 finished \tANN training loss 0.188921\n",
      ">> Epoch 554 finished \tANN training loss 0.197443\n",
      ">> Epoch 555 finished \tANN training loss 0.190649\n",
      ">> Epoch 556 finished \tANN training loss 0.196999\n",
      ">> Epoch 557 finished \tANN training loss 0.186381\n",
      ">> Epoch 558 finished \tANN training loss 0.186334\n",
      ">> Epoch 559 finished \tANN training loss 0.196589\n",
      ">> Epoch 560 finished \tANN training loss 0.187033\n",
      ">> Epoch 561 finished \tANN training loss 0.186493\n",
      ">> Epoch 562 finished \tANN training loss 0.187343\n",
      ">> Epoch 563 finished \tANN training loss 0.187733\n",
      ">> Epoch 564 finished \tANN training loss 0.193134\n",
      ">> Epoch 565 finished \tANN training loss 0.188974\n",
      ">> Epoch 566 finished \tANN training loss 0.189217\n",
      ">> Epoch 567 finished \tANN training loss 0.190200\n",
      ">> Epoch 568 finished \tANN training loss 0.188710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 569 finished \tANN training loss 0.193244\n",
      ">> Epoch 570 finished \tANN training loss 0.191545\n",
      ">> Epoch 571 finished \tANN training loss 0.190673\n",
      ">> Epoch 572 finished \tANN training loss 0.183004\n",
      ">> Epoch 573 finished \tANN training loss 0.183734\n",
      ">> Epoch 574 finished \tANN training loss 0.188979\n",
      ">> Epoch 575 finished \tANN training loss 0.188496\n",
      ">> Epoch 576 finished \tANN training loss 0.184511\n",
      ">> Epoch 577 finished \tANN training loss 0.196364\n",
      ">> Epoch 578 finished \tANN training loss 0.193084\n",
      ">> Epoch 579 finished \tANN training loss 0.196721\n",
      ">> Epoch 580 finished \tANN training loss 0.193201\n",
      ">> Epoch 581 finished \tANN training loss 0.209536\n",
      ">> Epoch 582 finished \tANN training loss 0.193551\n",
      ">> Epoch 583 finished \tANN training loss 0.199238\n",
      ">> Epoch 584 finished \tANN training loss 0.183159\n",
      ">> Epoch 585 finished \tANN training loss 0.183016\n",
      ">> Epoch 586 finished \tANN training loss 0.184812\n",
      ">> Epoch 587 finished \tANN training loss 0.183323\n",
      ">> Epoch 588 finished \tANN training loss 0.202160\n",
      ">> Epoch 589 finished \tANN training loss 0.187811\n",
      ">> Epoch 590 finished \tANN training loss 0.188382\n",
      ">> Epoch 591 finished \tANN training loss 0.191247\n",
      ">> Epoch 592 finished \tANN training loss 0.190611\n",
      ">> Epoch 593 finished \tANN training loss 0.184712\n",
      ">> Epoch 594 finished \tANN training loss 0.195022\n",
      ">> Epoch 595 finished \tANN training loss 0.185523\n",
      ">> Epoch 596 finished \tANN training loss 0.194651\n",
      ">> Epoch 597 finished \tANN training loss 0.186180\n",
      ">> Epoch 598 finished \tANN training loss 0.187259\n",
      ">> Epoch 599 finished \tANN training loss 0.188308\n",
      ">> Epoch 600 finished \tANN training loss 0.185960\n",
      ">> Epoch 601 finished \tANN training loss 0.191010\n",
      ">> Epoch 602 finished \tANN training loss 0.190489\n",
      ">> Epoch 603 finished \tANN training loss 0.187373\n",
      ">> Epoch 604 finished \tANN training loss 0.183546\n",
      ">> Epoch 605 finished \tANN training loss 0.185545\n",
      ">> Epoch 606 finished \tANN training loss 0.190429\n",
      ">> Epoch 607 finished \tANN training loss 0.190470\n",
      ">> Epoch 608 finished \tANN training loss 0.187261\n",
      ">> Epoch 609 finished \tANN training loss 0.182804\n",
      ">> Epoch 610 finished \tANN training loss 0.188322\n",
      ">> Epoch 611 finished \tANN training loss 0.181675\n",
      ">> Epoch 612 finished \tANN training loss 0.181379\n",
      ">> Epoch 613 finished \tANN training loss 0.191921\n",
      ">> Epoch 614 finished \tANN training loss 0.190267\n",
      ">> Epoch 615 finished \tANN training loss 0.180816\n",
      ">> Epoch 616 finished \tANN training loss 0.180620\n",
      ">> Epoch 617 finished \tANN training loss 0.179972\n",
      ">> Epoch 618 finished \tANN training loss 0.188454\n",
      ">> Epoch 619 finished \tANN training loss 0.189275\n",
      ">> Epoch 620 finished \tANN training loss 0.195562\n",
      ">> Epoch 621 finished \tANN training loss 0.185313\n",
      ">> Epoch 622 finished \tANN training loss 0.186192\n",
      ">> Epoch 623 finished \tANN training loss 0.187473\n",
      ">> Epoch 624 finished \tANN training loss 0.186808\n",
      ">> Epoch 625 finished \tANN training loss 0.188216\n",
      ">> Epoch 626 finished \tANN training loss 0.187188\n",
      ">> Epoch 627 finished \tANN training loss 0.182883\n",
      ">> Epoch 628 finished \tANN training loss 0.189199\n",
      ">> Epoch 629 finished \tANN training loss 0.181955\n",
      ">> Epoch 630 finished \tANN training loss 0.192983\n",
      ">> Epoch 631 finished \tANN training loss 0.184339\n",
      ">> Epoch 632 finished \tANN training loss 0.189925\n",
      ">> Epoch 633 finished \tANN training loss 0.188124\n",
      ">> Epoch 634 finished \tANN training loss 0.191508\n",
      ">> Epoch 635 finished \tANN training loss 0.189427\n",
      ">> Epoch 636 finished \tANN training loss 0.184741\n",
      ">> Epoch 637 finished \tANN training loss 0.187279\n",
      ">> Epoch 638 finished \tANN training loss 0.208535\n",
      ">> Epoch 639 finished \tANN training loss 0.188829\n",
      ">> Epoch 640 finished \tANN training loss 0.198952\n",
      ">> Epoch 641 finished \tANN training loss 0.184402\n",
      ">> Epoch 642 finished \tANN training loss 0.184253\n",
      ">> Epoch 643 finished \tANN training loss 0.197515\n",
      ">> Epoch 644 finished \tANN training loss 0.194320\n",
      ">> Epoch 645 finished \tANN training loss 0.192038\n",
      ">> Epoch 646 finished \tANN training loss 0.184053\n",
      ">> Epoch 647 finished \tANN training loss 0.187454\n",
      ">> Epoch 648 finished \tANN training loss 0.187056\n",
      ">> Epoch 649 finished \tANN training loss 0.186195\n",
      ">> Epoch 650 finished \tANN training loss 0.187673\n",
      ">> Epoch 651 finished \tANN training loss 0.199052\n",
      ">> Epoch 652 finished \tANN training loss 0.191250\n",
      ">> Epoch 653 finished \tANN training loss 0.187091\n",
      ">> Epoch 654 finished \tANN training loss 0.189403\n",
      ">> Epoch 655 finished \tANN training loss 0.204388\n",
      ">> Epoch 656 finished \tANN training loss 0.186396\n",
      ">> Epoch 657 finished \tANN training loss 0.195262\n",
      ">> Epoch 658 finished \tANN training loss 0.184382\n",
      ">> Epoch 659 finished \tANN training loss 0.188329\n",
      ">> Epoch 660 finished \tANN training loss 0.189604\n",
      ">> Epoch 661 finished \tANN training loss 0.190985\n",
      ">> Epoch 662 finished \tANN training loss 0.192226\n",
      ">> Epoch 663 finished \tANN training loss 0.189945\n",
      ">> Epoch 664 finished \tANN training loss 0.185755\n",
      ">> Epoch 665 finished \tANN training loss 0.184598\n",
      ">> Epoch 666 finished \tANN training loss 0.190994\n",
      ">> Epoch 667 finished \tANN training loss 0.186774\n",
      ">> Epoch 668 finished \tANN training loss 0.194713\n",
      ">> Epoch 669 finished \tANN training loss 0.187492\n",
      ">> Epoch 670 finished \tANN training loss 0.194010\n",
      ">> Epoch 671 finished \tANN training loss 0.190219\n",
      ">> Epoch 672 finished \tANN training loss 0.195148\n",
      ">> Epoch 673 finished \tANN training loss 0.189131\n",
      ">> Epoch 674 finished \tANN training loss 0.191339\n",
      ">> Epoch 675 finished \tANN training loss 0.198940\n",
      ">> Epoch 676 finished \tANN training loss 0.193454\n",
      ">> Epoch 677 finished \tANN training loss 0.198575\n",
      ">> Epoch 678 finished \tANN training loss 0.188506\n",
      ">> Epoch 679 finished \tANN training loss 0.195161\n",
      ">> Epoch 680 finished \tANN training loss 0.189368\n",
      ">> Epoch 681 finished \tANN training loss 0.191268\n",
      ">> Epoch 682 finished \tANN training loss 0.187051\n",
      ">> Epoch 683 finished \tANN training loss 0.184958\n",
      ">> Epoch 684 finished \tANN training loss 0.189242\n",
      ">> Epoch 685 finished \tANN training loss 0.186602\n",
      ">> Epoch 686 finished \tANN training loss 0.191268\n",
      ">> Epoch 687 finished \tANN training loss 0.190951\n",
      ">> Epoch 688 finished \tANN training loss 0.187826\n",
      ">> Epoch 689 finished \tANN training loss 0.192990\n",
      ">> Epoch 690 finished \tANN training loss 0.193327\n",
      ">> Epoch 691 finished \tANN training loss 0.188220\n",
      ">> Epoch 692 finished \tANN training loss 0.189977\n",
      ">> Epoch 693 finished \tANN training loss 0.183592\n",
      ">> Epoch 694 finished \tANN training loss 0.190375\n",
      ">> Epoch 695 finished \tANN training loss 0.192032\n",
      ">> Epoch 696 finished \tANN training loss 0.185989\n",
      ">> Epoch 697 finished \tANN training loss 0.184875\n",
      ">> Epoch 698 finished \tANN training loss 0.187879\n",
      ">> Epoch 699 finished \tANN training loss 0.189065\n",
      ">> Epoch 700 finished \tANN training loss 0.189723\n",
      ">> Epoch 701 finished \tANN training loss 0.189362\n",
      ">> Epoch 702 finished \tANN training loss 0.187086\n",
      ">> Epoch 703 finished \tANN training loss 0.187441\n",
      ">> Epoch 704 finished \tANN training loss 0.186678\n",
      ">> Epoch 705 finished \tANN training loss 0.183147\n",
      ">> Epoch 706 finished \tANN training loss 0.192349\n",
      ">> Epoch 707 finished \tANN training loss 0.184503\n",
      ">> Epoch 708 finished \tANN training loss 0.184556\n",
      ">> Epoch 709 finished \tANN training loss 0.190410\n",
      ">> Epoch 710 finished \tANN training loss 0.207642\n",
      ">> Epoch 711 finished \tANN training loss 0.189725\n",
      ">> Epoch 712 finished \tANN training loss 0.186620\n",
      ">> Epoch 713 finished \tANN training loss 0.185926\n",
      ">> Epoch 714 finished \tANN training loss 0.193998\n",
      ">> Epoch 715 finished \tANN training loss 0.194905\n",
      ">> Epoch 716 finished \tANN training loss 0.184902\n",
      ">> Epoch 717 finished \tANN training loss 0.186974\n",
      ">> Epoch 718 finished \tANN training loss 0.199238\n",
      ">> Epoch 719 finished \tANN training loss 0.187112\n",
      ">> Epoch 720 finished \tANN training loss 0.184865\n",
      ">> Epoch 721 finished \tANN training loss 0.190533\n",
      ">> Epoch 722 finished \tANN training loss 0.192781\n",
      ">> Epoch 723 finished \tANN training loss 0.188813\n",
      ">> Epoch 724 finished \tANN training loss 0.189566\n",
      ">> Epoch 725 finished \tANN training loss 0.193186\n",
      ">> Epoch 726 finished \tANN training loss 0.189743\n",
      ">> Epoch 727 finished \tANN training loss 0.185786\n",
      ">> Epoch 728 finished \tANN training loss 0.194994\n",
      ">> Epoch 729 finished \tANN training loss 0.184334\n",
      ">> Epoch 730 finished \tANN training loss 0.183547\n",
      ">> Epoch 731 finished \tANN training loss 0.183182\n",
      ">> Epoch 732 finished \tANN training loss 0.194723\n",
      ">> Epoch 733 finished \tANN training loss 0.190510\n",
      ">> Epoch 734 finished \tANN training loss 0.185190\n",
      ">> Epoch 735 finished \tANN training loss 0.187973\n",
      ">> Epoch 736 finished \tANN training loss 0.183814\n",
      ">> Epoch 737 finished \tANN training loss 0.186534\n",
      ">> Epoch 738 finished \tANN training loss 0.185440\n",
      ">> Epoch 739 finished \tANN training loss 0.196606\n",
      ">> Epoch 740 finished \tANN training loss 0.189766\n",
      ">> Epoch 741 finished \tANN training loss 0.186778\n",
      ">> Epoch 742 finished \tANN training loss 0.183880\n",
      ">> Epoch 743 finished \tANN training loss 0.184515\n",
      ">> Epoch 744 finished \tANN training loss 0.184877\n",
      ">> Epoch 745 finished \tANN training loss 0.185794\n",
      ">> Epoch 746 finished \tANN training loss 0.186208\n",
      ">> Epoch 747 finished \tANN training loss 0.184243\n",
      ">> Epoch 748 finished \tANN training loss 0.184699\n",
      ">> Epoch 749 finished \tANN training loss 0.187669\n",
      ">> Epoch 750 finished \tANN training loss 0.186860\n",
      ">> Epoch 751 finished \tANN training loss 0.201257\n",
      ">> Epoch 752 finished \tANN training loss 0.190296\n",
      ">> Epoch 753 finished \tANN training loss 0.193374\n",
      ">> Epoch 754 finished \tANN training loss 0.189185\n",
      ">> Epoch 755 finished \tANN training loss 0.190205\n",
      ">> Epoch 756 finished \tANN training loss 0.187097\n",
      ">> Epoch 757 finished \tANN training loss 0.194141\n",
      ">> Epoch 758 finished \tANN training loss 0.190761\n",
      ">> Epoch 759 finished \tANN training loss 0.186453\n",
      ">> Epoch 760 finished \tANN training loss 0.185653\n",
      ">> Epoch 761 finished \tANN training loss 0.187335\n",
      ">> Epoch 762 finished \tANN training loss 0.189825\n",
      ">> Epoch 763 finished \tANN training loss 0.189557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 764 finished \tANN training loss 0.194292\n",
      ">> Epoch 765 finished \tANN training loss 0.200719\n",
      ">> Epoch 766 finished \tANN training loss 0.185675\n",
      ">> Epoch 767 finished \tANN training loss 0.184867\n",
      ">> Epoch 768 finished \tANN training loss 0.185397\n",
      ">> Epoch 769 finished \tANN training loss 0.190811\n",
      ">> Epoch 770 finished \tANN training loss 0.191109\n",
      ">> Epoch 771 finished \tANN training loss 0.202726\n",
      ">> Epoch 772 finished \tANN training loss 0.203938\n",
      ">> Epoch 773 finished \tANN training loss 0.191173\n",
      ">> Epoch 774 finished \tANN training loss 0.194550\n",
      ">> Epoch 775 finished \tANN training loss 0.191749\n",
      ">> Epoch 776 finished \tANN training loss 0.187620\n",
      ">> Epoch 777 finished \tANN training loss 0.189254\n",
      ">> Epoch 778 finished \tANN training loss 0.188346\n",
      ">> Epoch 779 finished \tANN training loss 0.190850\n",
      ">> Epoch 780 finished \tANN training loss 0.190589\n",
      ">> Epoch 781 finished \tANN training loss 0.194453\n",
      ">> Epoch 782 finished \tANN training loss 0.199621\n",
      ">> Epoch 783 finished \tANN training loss 0.190021\n",
      ">> Epoch 784 finished \tANN training loss 0.185472\n",
      ">> Epoch 785 finished \tANN training loss 0.186234\n",
      ">> Epoch 786 finished \tANN training loss 0.185107\n",
      ">> Epoch 787 finished \tANN training loss 0.184219\n",
      ">> Epoch 788 finished \tANN training loss 0.196127\n",
      ">> Epoch 789 finished \tANN training loss 0.184529\n",
      ">> Epoch 790 finished \tANN training loss 0.195291\n",
      ">> Epoch 791 finished \tANN training loss 0.185722\n",
      ">> Epoch 792 finished \tANN training loss 0.201868\n",
      ">> Epoch 793 finished \tANN training loss 0.184597\n",
      ">> Epoch 794 finished \tANN training loss 0.190603\n",
      ">> Epoch 795 finished \tANN training loss 0.191993\n",
      ">> Epoch 796 finished \tANN training loss 0.183769\n",
      ">> Epoch 797 finished \tANN training loss 0.185221\n",
      ">> Epoch 798 finished \tANN training loss 0.185818\n",
      ">> Epoch 799 finished \tANN training loss 0.186445\n",
      ">> Epoch 800 finished \tANN training loss 0.189688\n",
      ">> Epoch 801 finished \tANN training loss 0.210386\n",
      ">> Epoch 802 finished \tANN training loss 0.187755\n",
      ">> Epoch 803 finished \tANN training loss 0.190449\n",
      ">> Epoch 804 finished \tANN training loss 0.195185\n",
      ">> Epoch 805 finished \tANN training loss 0.188417\n",
      ">> Epoch 806 finished \tANN training loss 0.185167\n",
      ">> Epoch 807 finished \tANN training loss 0.183203\n",
      ">> Epoch 808 finished \tANN training loss 0.185017\n",
      ">> Epoch 809 finished \tANN training loss 0.189796\n",
      ">> Epoch 810 finished \tANN training loss 0.191637\n",
      ">> Epoch 811 finished \tANN training loss 0.189954\n",
      ">> Epoch 812 finished \tANN training loss 0.189692\n",
      ">> Epoch 813 finished \tANN training loss 0.189316\n",
      ">> Epoch 814 finished \tANN training loss 0.190860\n",
      ">> Epoch 815 finished \tANN training loss 0.188110\n",
      ">> Epoch 816 finished \tANN training loss 0.185332\n",
      ">> Epoch 817 finished \tANN training loss 0.190244\n",
      ">> Epoch 818 finished \tANN training loss 0.191082\n",
      ">> Epoch 819 finished \tANN training loss 0.196442\n",
      ">> Epoch 820 finished \tANN training loss 0.188330\n",
      ">> Epoch 821 finished \tANN training loss 0.187184\n",
      ">> Epoch 822 finished \tANN training loss 0.189852\n",
      ">> Epoch 823 finished \tANN training loss 0.187095\n",
      ">> Epoch 824 finished \tANN training loss 0.186468\n",
      ">> Epoch 825 finished \tANN training loss 0.188451\n",
      ">> Epoch 826 finished \tANN training loss 0.183685\n",
      ">> Epoch 827 finished \tANN training loss 0.194085\n",
      ">> Epoch 828 finished \tANN training loss 0.194598\n",
      ">> Epoch 829 finished \tANN training loss 0.188004\n",
      ">> Epoch 830 finished \tANN training loss 0.195509\n",
      ">> Epoch 831 finished \tANN training loss 0.185874\n",
      ">> Epoch 832 finished \tANN training loss 0.195536\n",
      ">> Epoch 833 finished \tANN training loss 0.188876\n",
      ">> Epoch 834 finished \tANN training loss 0.194354\n",
      ">> Epoch 835 finished \tANN training loss 0.184239\n",
      ">> Epoch 836 finished \tANN training loss 0.183311\n",
      ">> Epoch 837 finished \tANN training loss 0.189264\n",
      ">> Epoch 838 finished \tANN training loss 0.190790\n",
      ">> Epoch 839 finished \tANN training loss 0.184443\n",
      ">> Epoch 840 finished \tANN training loss 0.185660\n",
      ">> Epoch 841 finished \tANN training loss 0.184050\n",
      ">> Epoch 842 finished \tANN training loss 0.190424\n",
      ">> Epoch 843 finished \tANN training loss 0.188835\n",
      ">> Epoch 844 finished \tANN training loss 0.187172\n",
      ">> Epoch 845 finished \tANN training loss 0.181843\n",
      ">> Epoch 846 finished \tANN training loss 0.188847\n",
      ">> Epoch 847 finished \tANN training loss 0.188500\n",
      ">> Epoch 848 finished \tANN training loss 0.191319\n",
      ">> Epoch 849 finished \tANN training loss 0.190529\n",
      ">> Epoch 850 finished \tANN training loss 0.192750\n",
      ">> Epoch 851 finished \tANN training loss 0.187495\n",
      ">> Epoch 852 finished \tANN training loss 0.184566\n",
      ">> Epoch 853 finished \tANN training loss 0.195137\n",
      ">> Epoch 854 finished \tANN training loss 0.186135\n",
      ">> Epoch 855 finished \tANN training loss 0.187455\n",
      ">> Epoch 856 finished \tANN training loss 0.184452\n",
      ">> Epoch 857 finished \tANN training loss 0.187472\n",
      ">> Epoch 858 finished \tANN training loss 0.183989\n",
      ">> Epoch 859 finished \tANN training loss 0.183541\n",
      ">> Epoch 860 finished \tANN training loss 0.185648\n",
      ">> Epoch 861 finished \tANN training loss 0.183807\n",
      ">> Epoch 862 finished \tANN training loss 0.188900\n",
      ">> Epoch 863 finished \tANN training loss 0.184347\n",
      ">> Epoch 864 finished \tANN training loss 0.185368\n",
      ">> Epoch 865 finished \tANN training loss 0.185610\n",
      ">> Epoch 866 finished \tANN training loss 0.186706\n",
      ">> Epoch 867 finished \tANN training loss 0.184346\n",
      ">> Epoch 868 finished \tANN training loss 0.184237\n",
      ">> Epoch 869 finished \tANN training loss 0.189278\n",
      ">> Epoch 870 finished \tANN training loss 0.187880\n",
      ">> Epoch 871 finished \tANN training loss 0.195008\n",
      ">> Epoch 872 finished \tANN training loss 0.197503\n",
      ">> Epoch 873 finished \tANN training loss 0.218868\n",
      ">> Epoch 874 finished \tANN training loss 0.189891\n",
      ">> Epoch 875 finished \tANN training loss 0.188867\n",
      ">> Epoch 876 finished \tANN training loss 0.190034\n",
      ">> Epoch 877 finished \tANN training loss 0.191169\n",
      ">> Epoch 878 finished \tANN training loss 0.188210\n",
      ">> Epoch 879 finished \tANN training loss 0.190718\n",
      ">> Epoch 880 finished \tANN training loss 0.191003\n",
      ">> Epoch 881 finished \tANN training loss 0.194109\n",
      ">> Epoch 882 finished \tANN training loss 0.193921\n",
      ">> Epoch 883 finished \tANN training loss 0.190266\n",
      ">> Epoch 884 finished \tANN training loss 0.185853\n",
      ">> Epoch 885 finished \tANN training loss 0.183950\n",
      ">> Epoch 886 finished \tANN training loss 0.184734\n",
      ">> Epoch 887 finished \tANN training loss 0.186632\n",
      ">> Epoch 888 finished \tANN training loss 0.182420\n",
      ">> Epoch 889 finished \tANN training loss 0.182447\n",
      ">> Epoch 890 finished \tANN training loss 0.188772\n",
      ">> Epoch 891 finished \tANN training loss 0.195034\n",
      ">> Epoch 892 finished \tANN training loss 0.184639\n",
      ">> Epoch 893 finished \tANN training loss 0.181903\n",
      ">> Epoch 894 finished \tANN training loss 0.190085\n",
      ">> Epoch 895 finished \tANN training loss 0.187308\n",
      ">> Epoch 896 finished \tANN training loss 0.186797\n",
      ">> Epoch 897 finished \tANN training loss 0.186070\n",
      ">> Epoch 898 finished \tANN training loss 0.186003\n",
      ">> Epoch 899 finished \tANN training loss 0.188864\n",
      ">> Epoch 900 finished \tANN training loss 0.183563\n",
      ">> Epoch 901 finished \tANN training loss 0.184613\n",
      ">> Epoch 902 finished \tANN training loss 0.183905\n",
      ">> Epoch 903 finished \tANN training loss 0.188699\n",
      ">> Epoch 904 finished \tANN training loss 0.190128\n",
      ">> Epoch 905 finished \tANN training loss 0.184271\n",
      ">> Epoch 906 finished \tANN training loss 0.194209\n",
      ">> Epoch 907 finished \tANN training loss 0.187810\n",
      ">> Epoch 908 finished \tANN training loss 0.187592\n",
      ">> Epoch 909 finished \tANN training loss 0.184978\n",
      ">> Epoch 910 finished \tANN training loss 0.189904\n",
      ">> Epoch 911 finished \tANN training loss 0.186908\n",
      ">> Epoch 912 finished \tANN training loss 0.189920\n",
      ">> Epoch 913 finished \tANN training loss 0.185403\n",
      ">> Epoch 914 finished \tANN training loss 0.189789\n",
      ">> Epoch 915 finished \tANN training loss 0.185614\n",
      ">> Epoch 916 finished \tANN training loss 0.184232\n",
      ">> Epoch 917 finished \tANN training loss 0.185403\n",
      ">> Epoch 918 finished \tANN training loss 0.184958\n",
      ">> Epoch 919 finished \tANN training loss 0.186443\n",
      ">> Epoch 920 finished \tANN training loss 0.187175\n",
      ">> Epoch 921 finished \tANN training loss 0.187405\n",
      ">> Epoch 922 finished \tANN training loss 0.188230\n",
      ">> Epoch 923 finished \tANN training loss 0.185908\n",
      ">> Epoch 924 finished \tANN training loss 0.185624\n",
      ">> Epoch 925 finished \tANN training loss 0.187941\n",
      ">> Epoch 926 finished \tANN training loss 0.185900\n",
      ">> Epoch 927 finished \tANN training loss 0.187926\n",
      ">> Epoch 928 finished \tANN training loss 0.187654\n",
      ">> Epoch 929 finished \tANN training loss 0.186805\n",
      ">> Epoch 930 finished \tANN training loss 0.189634\n",
      ">> Epoch 931 finished \tANN training loss 0.185339\n",
      ">> Epoch 932 finished \tANN training loss 0.188520\n",
      ">> Epoch 933 finished \tANN training loss 0.186178\n",
      ">> Epoch 934 finished \tANN training loss 0.187678\n",
      ">> Epoch 935 finished \tANN training loss 0.190618\n",
      ">> Epoch 936 finished \tANN training loss 0.187541\n",
      ">> Epoch 937 finished \tANN training loss 0.188329\n",
      ">> Epoch 938 finished \tANN training loss 0.190570\n",
      ">> Epoch 939 finished \tANN training loss 0.191727\n",
      ">> Epoch 940 finished \tANN training loss 0.196397\n",
      ">> Epoch 941 finished \tANN training loss 0.198548\n",
      ">> Epoch 942 finished \tANN training loss 0.185491\n",
      ">> Epoch 943 finished \tANN training loss 0.186717\n",
      ">> Epoch 944 finished \tANN training loss 0.196001\n",
      ">> Epoch 945 finished \tANN training loss 0.184373\n",
      ">> Epoch 946 finished \tANN training loss 0.191549\n",
      ">> Epoch 947 finished \tANN training loss 0.190478\n",
      ">> Epoch 948 finished \tANN training loss 0.185111\n",
      ">> Epoch 949 finished \tANN training loss 0.185598\n",
      ">> Epoch 950 finished \tANN training loss 0.185068\n",
      ">> Epoch 951 finished \tANN training loss 0.187854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 952 finished \tANN training loss 0.189037\n",
      ">> Epoch 953 finished \tANN training loss 0.191711\n",
      ">> Epoch 954 finished \tANN training loss 0.188498\n",
      ">> Epoch 955 finished \tANN training loss 0.190166\n",
      ">> Epoch 956 finished \tANN training loss 0.191505\n",
      ">> Epoch 957 finished \tANN training loss 0.196608\n",
      ">> Epoch 958 finished \tANN training loss 0.187584\n",
      ">> Epoch 959 finished \tANN training loss 0.187529\n",
      ">> Epoch 960 finished \tANN training loss 0.190554\n",
      ">> Epoch 961 finished \tANN training loss 0.193733\n",
      ">> Epoch 962 finished \tANN training loss 0.196348\n",
      ">> Epoch 963 finished \tANN training loss 0.190066\n",
      ">> Epoch 964 finished \tANN training loss 0.186971\n",
      ">> Epoch 965 finished \tANN training loss 0.187615\n",
      ">> Epoch 966 finished \tANN training loss 0.189354\n",
      ">> Epoch 967 finished \tANN training loss 0.185415\n",
      ">> Epoch 968 finished \tANN training loss 0.186569\n",
      ">> Epoch 969 finished \tANN training loss 0.191723\n",
      ">> Epoch 970 finished \tANN training loss 0.191521\n",
      ">> Epoch 971 finished \tANN training loss 0.186207\n",
      ">> Epoch 972 finished \tANN training loss 0.189915\n",
      ">> Epoch 973 finished \tANN training loss 0.188471\n",
      ">> Epoch 974 finished \tANN training loss 0.190719\n",
      ">> Epoch 975 finished \tANN training loss 0.186157\n",
      ">> Epoch 976 finished \tANN training loss 0.184020\n",
      ">> Epoch 977 finished \tANN training loss 0.186494\n",
      ">> Epoch 978 finished \tANN training loss 0.193756\n",
      ">> Epoch 979 finished \tANN training loss 0.184004\n",
      ">> Epoch 980 finished \tANN training loss 0.195552\n",
      ">> Epoch 981 finished \tANN training loss 0.181954\n",
      ">> Epoch 982 finished \tANN training loss 0.190711\n",
      ">> Epoch 983 finished \tANN training loss 0.184613\n",
      ">> Epoch 984 finished \tANN training loss 0.184879\n",
      ">> Epoch 985 finished \tANN training loss 0.187455\n",
      ">> Epoch 986 finished \tANN training loss 0.185000\n",
      ">> Epoch 987 finished \tANN training loss 0.182989\n",
      ">> Epoch 988 finished \tANN training loss 0.187059\n",
      ">> Epoch 989 finished \tANN training loss 0.185712\n",
      ">> Epoch 990 finished \tANN training loss 0.191348\n",
      ">> Epoch 991 finished \tANN training loss 0.187081\n",
      ">> Epoch 992 finished \tANN training loss 0.187911\n",
      ">> Epoch 993 finished \tANN training loss 0.187988\n",
      ">> Epoch 994 finished \tANN training loss 0.192155\n",
      ">> Epoch 995 finished \tANN training loss 0.188298\n",
      ">> Epoch 996 finished \tANN training loss 0.190795\n",
      ">> Epoch 997 finished \tANN training loss 0.186924\n",
      ">> Epoch 998 finished \tANN training loss 0.185699\n",
      ">> Epoch 999 finished \tANN training loss 0.192836\n",
      "[END] Fine tuning step\n",
      "\n",
      "\n",
      "TRIAL:  9\n",
      "\n",
      "\n",
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 5.709869\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 4.682958\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 4.448588\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 4.935475\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 5.938766\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 7.534440\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 9.879947\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 12.456274\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 17.960306\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 26.074075\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 262.101013\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 959.510986\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 1621.310425\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 2042.525024\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 2672.910889\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 2994.413330\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 3034.897949\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 3163.120117\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 3340.847900\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 3435.924072\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 50751930616000806912.000000\n",
      ">> Epoch 1 finished \tANN training loss 21342378314201825280.000000\n",
      ">> Epoch 2 finished \tANN training loss 6282026150193528832.000000\n",
      ">> Epoch 3 finished \tANN training loss 2789059403045666816.000000\n",
      ">> Epoch 4 finished \tANN training loss 1172862522366099456.000000\n",
      ">> Epoch 5 finished \tANN training loss nan\n",
      ">> Epoch 6 finished \tANN training loss nan\n",
      ">> Epoch 7 finished \tANN training loss nan\n",
      ">> Epoch 8 finished \tANN training loss nan\n",
      ">> Epoch 9 finished \tANN training loss nan\n",
      ">> Epoch 10 finished \tANN training loss nan\n",
      ">> Epoch 11 finished \tANN training loss nan\n",
      ">> Epoch 12 finished \tANN training loss nan\n",
      ">> Epoch 13 finished \tANN training loss nan\n",
      ">> Epoch 14 finished \tANN training loss nan\n",
      ">> Epoch 15 finished \tANN training loss nan\n",
      ">> Epoch 16 finished \tANN training loss nan\n",
      ">> Epoch 17 finished \tANN training loss nan\n",
      ">> Epoch 18 finished \tANN training loss nan\n",
      ">> Epoch 19 finished \tANN training loss nan\n",
      ">> Epoch 20 finished \tANN training loss nan\n",
      ">> Epoch 21 finished \tANN training loss nan\n",
      ">> Epoch 22 finished \tANN training loss nan\n",
      ">> Epoch 23 finished \tANN training loss nan\n",
      ">> Epoch 24 finished \tANN training loss nan\n",
      ">> Epoch 25 finished \tANN training loss nan\n",
      ">> Epoch 26 finished \tANN training loss nan\n",
      ">> Epoch 27 finished \tANN training loss nan\n",
      ">> Epoch 28 finished \tANN training loss nan\n",
      ">> Epoch 29 finished \tANN training loss nan\n",
      ">> Epoch 30 finished \tANN training loss nan\n",
      ">> Epoch 31 finished \tANN training loss nan\n",
      ">> Epoch 32 finished \tANN training loss nan\n",
      ">> Epoch 33 finished \tANN training loss nan\n",
      ">> Epoch 34 finished \tANN training loss nan\n",
      ">> Epoch 35 finished \tANN training loss nan\n",
      ">> Epoch 36 finished \tANN training loss nan\n",
      ">> Epoch 37 finished \tANN training loss nan\n",
      ">> Epoch 38 finished \tANN training loss nan\n",
      ">> Epoch 39 finished \tANN training loss nan\n",
      ">> Epoch 40 finished \tANN training loss nan\n",
      ">> Epoch 41 finished \tANN training loss nan\n",
      ">> Epoch 42 finished \tANN training loss nan\n",
      ">> Epoch 43 finished \tANN training loss nan\n",
      ">> Epoch 44 finished \tANN training loss nan\n",
      ">> Epoch 45 finished \tANN training loss nan\n",
      ">> Epoch 46 finished \tANN training loss nan\n",
      ">> Epoch 47 finished \tANN training loss nan\n",
      ">> Epoch 48 finished \tANN training loss nan\n",
      ">> Epoch 49 finished \tANN training loss nan\n",
      ">> Epoch 50 finished \tANN training loss nan\n",
      ">> Epoch 51 finished \tANN training loss nan\n",
      ">> Epoch 52 finished \tANN training loss nan\n",
      ">> Epoch 53 finished \tANN training loss nan\n",
      ">> Epoch 54 finished \tANN training loss nan\n",
      ">> Epoch 55 finished \tANN training loss nan\n",
      ">> Epoch 56 finished \tANN training loss nan\n",
      ">> Epoch 57 finished \tANN training loss nan\n",
      ">> Epoch 58 finished \tANN training loss nan\n",
      ">> Epoch 59 finished \tANN training loss nan\n",
      ">> Epoch 60 finished \tANN training loss nan\n",
      ">> Epoch 61 finished \tANN training loss nan\n",
      ">> Epoch 62 finished \tANN training loss nan\n",
      ">> Epoch 63 finished \tANN training loss nan\n",
      ">> Epoch 64 finished \tANN training loss nan\n",
      ">> Epoch 65 finished \tANN training loss nan\n",
      ">> Epoch 66 finished \tANN training loss nan\n",
      ">> Epoch 67 finished \tANN training loss nan\n",
      ">> Epoch 68 finished \tANN training loss nan\n",
      ">> Epoch 69 finished \tANN training loss nan\n",
      ">> Epoch 70 finished \tANN training loss nan\n",
      ">> Epoch 71 finished \tANN training loss nan\n",
      ">> Epoch 72 finished \tANN training loss nan\n",
      ">> Epoch 73 finished \tANN training loss nan\n",
      ">> Epoch 74 finished \tANN training loss nan\n",
      ">> Epoch 75 finished \tANN training loss nan\n",
      ">> Epoch 76 finished \tANN training loss nan\n",
      ">> Epoch 77 finished \tANN training loss nan\n",
      ">> Epoch 78 finished \tANN training loss nan\n",
      ">> Epoch 79 finished \tANN training loss nan\n",
      ">> Epoch 80 finished \tANN training loss nan\n",
      ">> Epoch 81 finished \tANN training loss nan\n",
      ">> Epoch 82 finished \tANN training loss nan\n",
      ">> Epoch 83 finished \tANN training loss nan\n",
      ">> Epoch 84 finished \tANN training loss nan\n",
      ">> Epoch 85 finished \tANN training loss nan\n",
      ">> Epoch 86 finished \tANN training loss nan\n",
      ">> Epoch 87 finished \tANN training loss nan\n",
      ">> Epoch 88 finished \tANN training loss nan\n",
      ">> Epoch 89 finished \tANN training loss nan\n",
      ">> Epoch 90 finished \tANN training loss nan\n",
      ">> Epoch 91 finished \tANN training loss nan\n",
      ">> Epoch 92 finished \tANN training loss nan\n",
      ">> Epoch 93 finished \tANN training loss nan\n",
      ">> Epoch 94 finished \tANN training loss nan\n",
      ">> Epoch 95 finished \tANN training loss nan\n",
      ">> Epoch 96 finished \tANN training loss nan\n",
      ">> Epoch 97 finished \tANN training loss nan\n",
      ">> Epoch 98 finished \tANN training loss nan\n",
      ">> Epoch 99 finished \tANN training loss nan\n",
      ">> Epoch 100 finished \tANN training loss nan\n",
      ">> Epoch 101 finished \tANN training loss nan\n",
      ">> Epoch 102 finished \tANN training loss nan\n",
      ">> Epoch 103 finished \tANN training loss nan\n",
      ">> Epoch 104 finished \tANN training loss nan\n",
      ">> Epoch 105 finished \tANN training loss nan\n",
      ">> Epoch 106 finished \tANN training loss nan\n",
      ">> Epoch 107 finished \tANN training loss nan\n",
      ">> Epoch 108 finished \tANN training loss nan\n",
      ">> Epoch 109 finished \tANN training loss nan\n",
      ">> Epoch 110 finished \tANN training loss nan\n",
      ">> Epoch 111 finished \tANN training loss nan\n",
      ">> Epoch 112 finished \tANN training loss nan\n",
      ">> Epoch 113 finished \tANN training loss nan\n",
      ">> Epoch 114 finished \tANN training loss nan\n",
      ">> Epoch 115 finished \tANN training loss nan\n",
      ">> Epoch 116 finished \tANN training loss nan\n",
      ">> Epoch 117 finished \tANN training loss nan\n",
      ">> Epoch 118 finished \tANN training loss nan\n",
      ">> Epoch 119 finished \tANN training loss nan\n",
      ">> Epoch 120 finished \tANN training loss nan\n",
      ">> Epoch 121 finished \tANN training loss nan\n",
      ">> Epoch 122 finished \tANN training loss nan\n",
      ">> Epoch 123 finished \tANN training loss nan\n",
      ">> Epoch 124 finished \tANN training loss nan\n",
      ">> Epoch 125 finished \tANN training loss nan\n",
      ">> Epoch 126 finished \tANN training loss nan\n",
      ">> Epoch 127 finished \tANN training loss nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 128 finished \tANN training loss nan\n",
      ">> Epoch 129 finished \tANN training loss nan\n",
      ">> Epoch 130 finished \tANN training loss nan\n",
      ">> Epoch 131 finished \tANN training loss nan\n",
      ">> Epoch 132 finished \tANN training loss nan\n",
      ">> Epoch 133 finished \tANN training loss nan\n",
      ">> Epoch 134 finished \tANN training loss nan\n",
      ">> Epoch 135 finished \tANN training loss nan\n",
      ">> Epoch 136 finished \tANN training loss nan\n",
      ">> Epoch 137 finished \tANN training loss nan\n",
      ">> Epoch 138 finished \tANN training loss nan\n",
      ">> Epoch 139 finished \tANN training loss nan\n",
      ">> Epoch 140 finished \tANN training loss nan\n",
      ">> Epoch 141 finished \tANN training loss nan\n",
      ">> Epoch 142 finished \tANN training loss nan\n",
      ">> Epoch 143 finished \tANN training loss nan\n",
      ">> Epoch 144 finished \tANN training loss nan\n",
      ">> Epoch 145 finished \tANN training loss nan\n",
      ">> Epoch 146 finished \tANN training loss nan\n",
      ">> Epoch 147 finished \tANN training loss nan\n",
      ">> Epoch 148 finished \tANN training loss nan\n",
      ">> Epoch 149 finished \tANN training loss nan\n",
      ">> Epoch 150 finished \tANN training loss nan\n",
      ">> Epoch 151 finished \tANN training loss nan\n",
      ">> Epoch 152 finished \tANN training loss nan\n",
      ">> Epoch 153 finished \tANN training loss nan\n",
      ">> Epoch 154 finished \tANN training loss nan\n",
      ">> Epoch 155 finished \tANN training loss nan\n",
      ">> Epoch 156 finished \tANN training loss nan\n",
      ">> Epoch 157 finished \tANN training loss nan\n",
      ">> Epoch 158 finished \tANN training loss nan\n",
      ">> Epoch 159 finished \tANN training loss nan\n",
      ">> Epoch 160 finished \tANN training loss nan\n",
      ">> Epoch 161 finished \tANN training loss nan\n",
      ">> Epoch 162 finished \tANN training loss nan\n",
      ">> Epoch 163 finished \tANN training loss nan\n",
      ">> Epoch 164 finished \tANN training loss nan\n",
      ">> Epoch 165 finished \tANN training loss nan\n",
      ">> Epoch 166 finished \tANN training loss nan\n",
      ">> Epoch 167 finished \tANN training loss nan\n",
      ">> Epoch 168 finished \tANN training loss nan\n",
      ">> Epoch 169 finished \tANN training loss nan\n",
      ">> Epoch 170 finished \tANN training loss nan\n",
      ">> Epoch 171 finished \tANN training loss nan\n",
      ">> Epoch 172 finished \tANN training loss nan\n",
      ">> Epoch 173 finished \tANN training loss nan\n",
      ">> Epoch 174 finished \tANN training loss nan\n",
      ">> Epoch 175 finished \tANN training loss nan\n",
      ">> Epoch 176 finished \tANN training loss nan\n",
      ">> Epoch 177 finished \tANN training loss nan\n",
      ">> Epoch 178 finished \tANN training loss nan\n",
      ">> Epoch 179 finished \tANN training loss nan\n",
      ">> Epoch 180 finished \tANN training loss nan\n",
      ">> Epoch 181 finished \tANN training loss nan\n",
      ">> Epoch 182 finished \tANN training loss nan\n",
      ">> Epoch 183 finished \tANN training loss nan\n",
      ">> Epoch 184 finished \tANN training loss nan\n",
      ">> Epoch 185 finished \tANN training loss nan\n",
      ">> Epoch 186 finished \tANN training loss nan\n",
      ">> Epoch 187 finished \tANN training loss nan\n",
      ">> Epoch 188 finished \tANN training loss nan\n",
      ">> Epoch 189 finished \tANN training loss nan\n",
      ">> Epoch 190 finished \tANN training loss nan\n",
      ">> Epoch 191 finished \tANN training loss nan\n",
      ">> Epoch 192 finished \tANN training loss nan\n",
      ">> Epoch 193 finished \tANN training loss nan\n",
      ">> Epoch 194 finished \tANN training loss nan\n",
      ">> Epoch 195 finished \tANN training loss nan\n",
      ">> Epoch 196 finished \tANN training loss nan\n",
      ">> Epoch 197 finished \tANN training loss nan\n",
      ">> Epoch 198 finished \tANN training loss nan\n",
      ">> Epoch 199 finished \tANN training loss nan\n",
      ">> Epoch 200 finished \tANN training loss nan\n",
      ">> Epoch 201 finished \tANN training loss nan\n",
      ">> Epoch 202 finished \tANN training loss nan\n",
      ">> Epoch 203 finished \tANN training loss nan\n",
      ">> Epoch 204 finished \tANN training loss nan\n",
      ">> Epoch 205 finished \tANN training loss nan\n",
      ">> Epoch 206 finished \tANN training loss nan\n",
      ">> Epoch 207 finished \tANN training loss nan\n",
      ">> Epoch 208 finished \tANN training loss nan\n",
      ">> Epoch 209 finished \tANN training loss nan\n",
      ">> Epoch 210 finished \tANN training loss nan\n",
      ">> Epoch 211 finished \tANN training loss nan\n",
      ">> Epoch 212 finished \tANN training loss nan\n",
      ">> Epoch 213 finished \tANN training loss nan\n",
      ">> Epoch 214 finished \tANN training loss nan\n",
      ">> Epoch 215 finished \tANN training loss nan\n",
      ">> Epoch 216 finished \tANN training loss nan\n",
      ">> Epoch 217 finished \tANN training loss nan\n",
      ">> Epoch 218 finished \tANN training loss nan\n",
      ">> Epoch 219 finished \tANN training loss nan\n",
      ">> Epoch 220 finished \tANN training loss nan\n",
      ">> Epoch 221 finished \tANN training loss nan\n",
      ">> Epoch 222 finished \tANN training loss nan\n",
      ">> Epoch 223 finished \tANN training loss nan\n",
      ">> Epoch 224 finished \tANN training loss nan\n",
      ">> Epoch 225 finished \tANN training loss nan\n",
      ">> Epoch 226 finished \tANN training loss nan\n",
      ">> Epoch 227 finished \tANN training loss nan\n",
      ">> Epoch 228 finished \tANN training loss nan\n",
      ">> Epoch 229 finished \tANN training loss nan\n",
      ">> Epoch 230 finished \tANN training loss nan\n",
      ">> Epoch 231 finished \tANN training loss nan\n",
      ">> Epoch 232 finished \tANN training loss nan\n",
      ">> Epoch 233 finished \tANN training loss nan\n",
      ">> Epoch 234 finished \tANN training loss nan\n",
      ">> Epoch 235 finished \tANN training loss nan\n",
      ">> Epoch 236 finished \tANN training loss nan\n",
      ">> Epoch 237 finished \tANN training loss nan\n",
      ">> Epoch 238 finished \tANN training loss nan\n",
      ">> Epoch 239 finished \tANN training loss nan\n",
      ">> Epoch 240 finished \tANN training loss nan\n",
      ">> Epoch 241 finished \tANN training loss nan\n",
      ">> Epoch 242 finished \tANN training loss nan\n",
      ">> Epoch 243 finished \tANN training loss nan\n",
      ">> Epoch 244 finished \tANN training loss nan\n",
      ">> Epoch 245 finished \tANN training loss nan\n",
      ">> Epoch 246 finished \tANN training loss nan\n",
      ">> Epoch 247 finished \tANN training loss nan\n",
      ">> Epoch 248 finished \tANN training loss nan\n",
      ">> Epoch 249 finished \tANN training loss nan\n",
      ">> Epoch 250 finished \tANN training loss nan\n",
      ">> Epoch 251 finished \tANN training loss nan\n",
      ">> Epoch 252 finished \tANN training loss nan\n",
      ">> Epoch 253 finished \tANN training loss nan\n",
      ">> Epoch 254 finished \tANN training loss nan\n",
      ">> Epoch 255 finished \tANN training loss nan\n",
      ">> Epoch 256 finished \tANN training loss nan\n",
      ">> Epoch 257 finished \tANN training loss nan\n",
      ">> Epoch 258 finished \tANN training loss nan\n",
      ">> Epoch 259 finished \tANN training loss nan\n",
      ">> Epoch 260 finished \tANN training loss nan\n",
      ">> Epoch 261 finished \tANN training loss nan\n",
      ">> Epoch 262 finished \tANN training loss nan\n",
      ">> Epoch 263 finished \tANN training loss nan\n",
      ">> Epoch 264 finished \tANN training loss nan\n",
      ">> Epoch 265 finished \tANN training loss nan\n",
      ">> Epoch 266 finished \tANN training loss nan\n",
      ">> Epoch 267 finished \tANN training loss nan\n",
      ">> Epoch 268 finished \tANN training loss nan\n",
      ">> Epoch 269 finished \tANN training loss nan\n",
      ">> Epoch 270 finished \tANN training loss nan\n",
      ">> Epoch 271 finished \tANN training loss nan\n",
      ">> Epoch 272 finished \tANN training loss nan\n",
      ">> Epoch 273 finished \tANN training loss nan\n",
      ">> Epoch 274 finished \tANN training loss nan\n",
      ">> Epoch 275 finished \tANN training loss nan\n",
      ">> Epoch 276 finished \tANN training loss nan\n",
      ">> Epoch 277 finished \tANN training loss nan\n",
      ">> Epoch 278 finished \tANN training loss nan\n",
      ">> Epoch 279 finished \tANN training loss nan\n",
      ">> Epoch 280 finished \tANN training loss nan\n",
      ">> Epoch 281 finished \tANN training loss nan\n",
      ">> Epoch 282 finished \tANN training loss nan\n",
      ">> Epoch 283 finished \tANN training loss nan\n",
      ">> Epoch 284 finished \tANN training loss nan\n",
      ">> Epoch 285 finished \tANN training loss nan\n",
      ">> Epoch 286 finished \tANN training loss nan\n",
      ">> Epoch 287 finished \tANN training loss nan\n",
      ">> Epoch 288 finished \tANN training loss nan\n",
      ">> Epoch 289 finished \tANN training loss nan\n",
      ">> Epoch 290 finished \tANN training loss nan\n",
      ">> Epoch 291 finished \tANN training loss nan\n",
      ">> Epoch 292 finished \tANN training loss nan\n",
      ">> Epoch 293 finished \tANN training loss nan\n",
      ">> Epoch 294 finished \tANN training loss nan\n",
      ">> Epoch 295 finished \tANN training loss nan\n",
      ">> Epoch 296 finished \tANN training loss nan\n",
      ">> Epoch 297 finished \tANN training loss nan\n",
      ">> Epoch 298 finished \tANN training loss nan\n",
      ">> Epoch 299 finished \tANN training loss nan\n",
      ">> Epoch 300 finished \tANN training loss nan\n",
      ">> Epoch 301 finished \tANN training loss nan\n",
      ">> Epoch 302 finished \tANN training loss nan\n",
      ">> Epoch 303 finished \tANN training loss nan\n",
      ">> Epoch 304 finished \tANN training loss nan\n",
      ">> Epoch 305 finished \tANN training loss nan\n",
      ">> Epoch 306 finished \tANN training loss nan\n",
      ">> Epoch 307 finished \tANN training loss nan\n",
      ">> Epoch 308 finished \tANN training loss nan\n",
      ">> Epoch 309 finished \tANN training loss nan\n",
      ">> Epoch 310 finished \tANN training loss nan\n",
      ">> Epoch 311 finished \tANN training loss nan\n",
      ">> Epoch 312 finished \tANN training loss nan\n",
      ">> Epoch 313 finished \tANN training loss nan\n",
      ">> Epoch 314 finished \tANN training loss nan\n",
      ">> Epoch 315 finished \tANN training loss nan\n",
      ">> Epoch 316 finished \tANN training loss nan\n",
      ">> Epoch 317 finished \tANN training loss nan\n",
      ">> Epoch 318 finished \tANN training loss nan\n",
      ">> Epoch 319 finished \tANN training loss nan\n",
      ">> Epoch 320 finished \tANN training loss nan\n",
      ">> Epoch 321 finished \tANN training loss nan\n",
      ">> Epoch 322 finished \tANN training loss nan\n",
      ">> Epoch 323 finished \tANN training loss nan\n",
      ">> Epoch 324 finished \tANN training loss nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 325 finished \tANN training loss nan\n",
      ">> Epoch 326 finished \tANN training loss nan\n",
      ">> Epoch 327 finished \tANN training loss nan\n",
      ">> Epoch 328 finished \tANN training loss nan\n",
      ">> Epoch 329 finished \tANN training loss nan\n",
      ">> Epoch 330 finished \tANN training loss nan\n",
      ">> Epoch 331 finished \tANN training loss nan\n",
      ">> Epoch 332 finished \tANN training loss nan\n",
      ">> Epoch 333 finished \tANN training loss nan\n",
      ">> Epoch 334 finished \tANN training loss nan\n",
      ">> Epoch 335 finished \tANN training loss nan\n",
      ">> Epoch 336 finished \tANN training loss nan\n",
      ">> Epoch 337 finished \tANN training loss nan\n",
      ">> Epoch 338 finished \tANN training loss nan\n",
      ">> Epoch 339 finished \tANN training loss nan\n",
      ">> Epoch 340 finished \tANN training loss nan\n",
      ">> Epoch 341 finished \tANN training loss nan\n",
      ">> Epoch 342 finished \tANN training loss nan\n",
      ">> Epoch 343 finished \tANN training loss nan\n",
      ">> Epoch 344 finished \tANN training loss nan\n",
      ">> Epoch 345 finished \tANN training loss nan\n",
      ">> Epoch 346 finished \tANN training loss nan\n",
      ">> Epoch 347 finished \tANN training loss nan\n",
      ">> Epoch 348 finished \tANN training loss nan\n",
      ">> Epoch 349 finished \tANN training loss nan\n",
      ">> Epoch 350 finished \tANN training loss nan\n",
      ">> Epoch 351 finished \tANN training loss nan\n",
      ">> Epoch 352 finished \tANN training loss nan\n",
      ">> Epoch 353 finished \tANN training loss nan\n",
      ">> Epoch 354 finished \tANN training loss nan\n",
      ">> Epoch 355 finished \tANN training loss nan\n",
      ">> Epoch 356 finished \tANN training loss nan\n",
      ">> Epoch 357 finished \tANN training loss nan\n",
      ">> Epoch 358 finished \tANN training loss nan\n",
      ">> Epoch 359 finished \tANN training loss nan\n",
      ">> Epoch 360 finished \tANN training loss nan\n",
      ">> Epoch 361 finished \tANN training loss nan\n",
      ">> Epoch 362 finished \tANN training loss nan\n",
      ">> Epoch 363 finished \tANN training loss nan\n",
      ">> Epoch 364 finished \tANN training loss nan\n",
      ">> Epoch 365 finished \tANN training loss nan\n",
      ">> Epoch 366 finished \tANN training loss nan\n",
      ">> Epoch 367 finished \tANN training loss nan\n",
      ">> Epoch 368 finished \tANN training loss nan\n",
      ">> Epoch 369 finished \tANN training loss nan\n",
      ">> Epoch 370 finished \tANN training loss nan\n",
      ">> Epoch 371 finished \tANN training loss nan\n",
      ">> Epoch 372 finished \tANN training loss nan\n",
      ">> Epoch 373 finished \tANN training loss nan\n",
      ">> Epoch 374 finished \tANN training loss nan\n",
      ">> Epoch 375 finished \tANN training loss nan\n",
      ">> Epoch 376 finished \tANN training loss nan\n",
      ">> Epoch 377 finished \tANN training loss nan\n",
      ">> Epoch 378 finished \tANN training loss nan\n",
      ">> Epoch 379 finished \tANN training loss nan\n",
      ">> Epoch 380 finished \tANN training loss nan\n",
      ">> Epoch 381 finished \tANN training loss nan\n",
      ">> Epoch 382 finished \tANN training loss nan\n",
      ">> Epoch 383 finished \tANN training loss nan\n",
      ">> Epoch 384 finished \tANN training loss nan\n",
      ">> Epoch 385 finished \tANN training loss nan\n",
      ">> Epoch 386 finished \tANN training loss nan\n",
      ">> Epoch 387 finished \tANN training loss nan\n",
      ">> Epoch 388 finished \tANN training loss nan\n",
      ">> Epoch 389 finished \tANN training loss nan\n",
      ">> Epoch 390 finished \tANN training loss nan\n",
      ">> Epoch 391 finished \tANN training loss nan\n",
      ">> Epoch 392 finished \tANN training loss nan\n",
      ">> Epoch 393 finished \tANN training loss nan\n",
      ">> Epoch 394 finished \tANN training loss nan\n",
      ">> Epoch 395 finished \tANN training loss nan\n",
      ">> Epoch 396 finished \tANN training loss nan\n",
      ">> Epoch 397 finished \tANN training loss nan\n",
      ">> Epoch 398 finished \tANN training loss nan\n",
      ">> Epoch 399 finished \tANN training loss nan\n",
      ">> Epoch 400 finished \tANN training loss nan\n",
      ">> Epoch 401 finished \tANN training loss nan\n",
      ">> Epoch 402 finished \tANN training loss nan\n",
      ">> Epoch 403 finished \tANN training loss nan\n",
      ">> Epoch 404 finished \tANN training loss nan\n",
      ">> Epoch 405 finished \tANN training loss nan\n",
      ">> Epoch 406 finished \tANN training loss nan\n",
      ">> Epoch 407 finished \tANN training loss nan\n",
      ">> Epoch 408 finished \tANN training loss nan\n",
      ">> Epoch 409 finished \tANN training loss nan\n",
      ">> Epoch 410 finished \tANN training loss nan\n",
      ">> Epoch 411 finished \tANN training loss nan\n",
      ">> Epoch 412 finished \tANN training loss nan\n",
      ">> Epoch 413 finished \tANN training loss nan\n",
      ">> Epoch 414 finished \tANN training loss nan\n",
      ">> Epoch 415 finished \tANN training loss nan\n",
      ">> Epoch 416 finished \tANN training loss nan\n",
      ">> Epoch 417 finished \tANN training loss nan\n",
      ">> Epoch 418 finished \tANN training loss nan\n",
      ">> Epoch 419 finished \tANN training loss nan\n",
      ">> Epoch 420 finished \tANN training loss nan\n",
      ">> Epoch 421 finished \tANN training loss nan\n",
      ">> Epoch 422 finished \tANN training loss nan\n",
      ">> Epoch 423 finished \tANN training loss nan\n",
      ">> Epoch 424 finished \tANN training loss nan\n",
      ">> Epoch 425 finished \tANN training loss nan\n",
      ">> Epoch 426 finished \tANN training loss nan\n",
      ">> Epoch 427 finished \tANN training loss nan\n",
      ">> Epoch 428 finished \tANN training loss nan\n",
      ">> Epoch 429 finished \tANN training loss nan\n",
      ">> Epoch 430 finished \tANN training loss nan\n",
      ">> Epoch 431 finished \tANN training loss nan\n",
      ">> Epoch 432 finished \tANN training loss nan\n",
      ">> Epoch 433 finished \tANN training loss nan\n",
      ">> Epoch 434 finished \tANN training loss nan\n",
      ">> Epoch 435 finished \tANN training loss nan\n",
      ">> Epoch 436 finished \tANN training loss nan\n",
      ">> Epoch 437 finished \tANN training loss nan\n",
      ">> Epoch 438 finished \tANN training loss nan\n",
      ">> Epoch 439 finished \tANN training loss nan\n",
      ">> Epoch 440 finished \tANN training loss nan\n",
      ">> Epoch 441 finished \tANN training loss nan\n",
      ">> Epoch 442 finished \tANN training loss nan\n",
      ">> Epoch 443 finished \tANN training loss nan\n",
      ">> Epoch 444 finished \tANN training loss nan\n",
      ">> Epoch 445 finished \tANN training loss nan\n",
      ">> Epoch 446 finished \tANN training loss nan\n",
      ">> Epoch 447 finished \tANN training loss nan\n",
      ">> Epoch 448 finished \tANN training loss nan\n",
      ">> Epoch 449 finished \tANN training loss nan\n",
      ">> Epoch 450 finished \tANN training loss nan\n",
      ">> Epoch 451 finished \tANN training loss nan\n",
      ">> Epoch 452 finished \tANN training loss nan\n",
      ">> Epoch 453 finished \tANN training loss nan\n",
      ">> Epoch 454 finished \tANN training loss nan\n",
      ">> Epoch 455 finished \tANN training loss nan\n",
      ">> Epoch 456 finished \tANN training loss nan\n",
      ">> Epoch 457 finished \tANN training loss nan\n",
      ">> Epoch 458 finished \tANN training loss nan\n",
      ">> Epoch 459 finished \tANN training loss nan\n",
      ">> Epoch 460 finished \tANN training loss nan\n",
      ">> Epoch 461 finished \tANN training loss nan\n",
      ">> Epoch 462 finished \tANN training loss nan\n",
      ">> Epoch 463 finished \tANN training loss nan\n",
      ">> Epoch 464 finished \tANN training loss nan\n",
      ">> Epoch 465 finished \tANN training loss nan\n",
      ">> Epoch 466 finished \tANN training loss nan\n",
      ">> Epoch 467 finished \tANN training loss nan\n",
      ">> Epoch 468 finished \tANN training loss nan\n",
      ">> Epoch 469 finished \tANN training loss nan\n",
      ">> Epoch 470 finished \tANN training loss nan\n",
      ">> Epoch 471 finished \tANN training loss nan\n",
      ">> Epoch 472 finished \tANN training loss nan\n",
      ">> Epoch 473 finished \tANN training loss nan\n",
      ">> Epoch 474 finished \tANN training loss nan\n",
      ">> Epoch 475 finished \tANN training loss nan\n",
      ">> Epoch 476 finished \tANN training loss nan\n",
      ">> Epoch 477 finished \tANN training loss nan\n",
      ">> Epoch 478 finished \tANN training loss nan\n",
      ">> Epoch 479 finished \tANN training loss nan\n",
      ">> Epoch 480 finished \tANN training loss nan\n",
      ">> Epoch 481 finished \tANN training loss nan\n",
      ">> Epoch 482 finished \tANN training loss nan\n",
      ">> Epoch 483 finished \tANN training loss nan\n",
      ">> Epoch 484 finished \tANN training loss nan\n",
      ">> Epoch 485 finished \tANN training loss nan\n",
      ">> Epoch 486 finished \tANN training loss nan\n",
      ">> Epoch 487 finished \tANN training loss nan\n",
      ">> Epoch 488 finished \tANN training loss nan\n",
      ">> Epoch 489 finished \tANN training loss nan\n",
      ">> Epoch 490 finished \tANN training loss nan\n",
      ">> Epoch 491 finished \tANN training loss nan\n",
      ">> Epoch 492 finished \tANN training loss nan\n",
      ">> Epoch 493 finished \tANN training loss nan\n",
      ">> Epoch 494 finished \tANN training loss nan\n",
      ">> Epoch 495 finished \tANN training loss nan\n",
      ">> Epoch 496 finished \tANN training loss nan\n",
      ">> Epoch 497 finished \tANN training loss nan\n",
      ">> Epoch 498 finished \tANN training loss nan\n",
      ">> Epoch 499 finished \tANN training loss nan\n",
      ">> Epoch 500 finished \tANN training loss nan\n",
      ">> Epoch 501 finished \tANN training loss nan\n",
      ">> Epoch 502 finished \tANN training loss nan\n",
      ">> Epoch 503 finished \tANN training loss nan\n",
      ">> Epoch 504 finished \tANN training loss nan\n",
      ">> Epoch 505 finished \tANN training loss nan\n",
      ">> Epoch 506 finished \tANN training loss nan\n",
      ">> Epoch 507 finished \tANN training loss nan\n",
      ">> Epoch 508 finished \tANN training loss nan\n",
      ">> Epoch 509 finished \tANN training loss nan\n",
      ">> Epoch 510 finished \tANN training loss nan\n",
      ">> Epoch 511 finished \tANN training loss nan\n",
      ">> Epoch 512 finished \tANN training loss nan\n",
      ">> Epoch 513 finished \tANN training loss nan\n",
      ">> Epoch 514 finished \tANN training loss nan\n",
      ">> Epoch 515 finished \tANN training loss nan\n",
      ">> Epoch 516 finished \tANN training loss nan\n",
      ">> Epoch 517 finished \tANN training loss nan\n",
      ">> Epoch 518 finished \tANN training loss nan\n",
      ">> Epoch 519 finished \tANN training loss nan\n",
      ">> Epoch 520 finished \tANN training loss nan\n",
      ">> Epoch 521 finished \tANN training loss nan\n",
      ">> Epoch 522 finished \tANN training loss nan\n",
      ">> Epoch 523 finished \tANN training loss nan\n",
      ">> Epoch 524 finished \tANN training loss nan\n",
      ">> Epoch 525 finished \tANN training loss nan\n",
      ">> Epoch 526 finished \tANN training loss nan\n",
      ">> Epoch 527 finished \tANN training loss nan\n",
      ">> Epoch 528 finished \tANN training loss nan\n",
      ">> Epoch 529 finished \tANN training loss nan\n",
      ">> Epoch 530 finished \tANN training loss nan\n",
      ">> Epoch 531 finished \tANN training loss nan\n",
      ">> Epoch 532 finished \tANN training loss nan\n",
      ">> Epoch 533 finished \tANN training loss nan\n",
      ">> Epoch 534 finished \tANN training loss nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 535 finished \tANN training loss nan\n",
      ">> Epoch 536 finished \tANN training loss nan\n",
      ">> Epoch 537 finished \tANN training loss nan\n",
      ">> Epoch 538 finished \tANN training loss nan\n",
      ">> Epoch 539 finished \tANN training loss nan\n",
      ">> Epoch 540 finished \tANN training loss nan\n",
      ">> Epoch 541 finished \tANN training loss nan\n",
      ">> Epoch 542 finished \tANN training loss nan\n",
      ">> Epoch 543 finished \tANN training loss nan\n",
      ">> Epoch 544 finished \tANN training loss nan\n",
      ">> Epoch 545 finished \tANN training loss nan\n",
      ">> Epoch 546 finished \tANN training loss nan\n",
      ">> Epoch 547 finished \tANN training loss nan\n",
      ">> Epoch 548 finished \tANN training loss nan\n",
      ">> Epoch 549 finished \tANN training loss nan\n",
      ">> Epoch 550 finished \tANN training loss nan\n",
      ">> Epoch 551 finished \tANN training loss nan\n",
      ">> Epoch 552 finished \tANN training loss nan\n",
      ">> Epoch 553 finished \tANN training loss nan\n",
      ">> Epoch 554 finished \tANN training loss nan\n",
      ">> Epoch 555 finished \tANN training loss nan\n",
      ">> Epoch 556 finished \tANN training loss nan\n",
      ">> Epoch 557 finished \tANN training loss nan\n",
      ">> Epoch 558 finished \tANN training loss nan\n",
      ">> Epoch 559 finished \tANN training loss nan\n",
      ">> Epoch 560 finished \tANN training loss nan\n",
      ">> Epoch 561 finished \tANN training loss nan\n",
      ">> Epoch 562 finished \tANN training loss nan\n",
      ">> Epoch 563 finished \tANN training loss nan\n",
      ">> Epoch 564 finished \tANN training loss nan\n",
      ">> Epoch 565 finished \tANN training loss nan\n",
      ">> Epoch 566 finished \tANN training loss nan\n",
      ">> Epoch 567 finished \tANN training loss nan\n",
      ">> Epoch 568 finished \tANN training loss nan\n",
      ">> Epoch 569 finished \tANN training loss nan\n",
      ">> Epoch 570 finished \tANN training loss nan\n",
      ">> Epoch 571 finished \tANN training loss nan\n",
      ">> Epoch 572 finished \tANN training loss nan\n",
      ">> Epoch 573 finished \tANN training loss nan\n",
      ">> Epoch 574 finished \tANN training loss nan\n",
      ">> Epoch 575 finished \tANN training loss nan\n",
      ">> Epoch 576 finished \tANN training loss nan\n",
      ">> Epoch 577 finished \tANN training loss nan\n",
      ">> Epoch 578 finished \tANN training loss nan\n",
      ">> Epoch 579 finished \tANN training loss nan\n",
      ">> Epoch 580 finished \tANN training loss nan\n",
      ">> Epoch 581 finished \tANN training loss nan\n",
      ">> Epoch 582 finished \tANN training loss nan\n",
      ">> Epoch 583 finished \tANN training loss nan\n",
      ">> Epoch 584 finished \tANN training loss nan\n",
      ">> Epoch 585 finished \tANN training loss nan\n",
      ">> Epoch 586 finished \tANN training loss nan\n",
      ">> Epoch 587 finished \tANN training loss nan\n",
      ">> Epoch 588 finished \tANN training loss nan\n",
      ">> Epoch 589 finished \tANN training loss nan\n",
      ">> Epoch 590 finished \tANN training loss nan\n",
      ">> Epoch 591 finished \tANN training loss nan\n",
      ">> Epoch 592 finished \tANN training loss nan\n",
      ">> Epoch 593 finished \tANN training loss nan\n",
      ">> Epoch 594 finished \tANN training loss nan\n",
      ">> Epoch 595 finished \tANN training loss nan\n",
      ">> Epoch 596 finished \tANN training loss nan\n",
      ">> Epoch 597 finished \tANN training loss nan\n",
      ">> Epoch 598 finished \tANN training loss nan\n",
      ">> Epoch 599 finished \tANN training loss nan\n",
      ">> Epoch 600 finished \tANN training loss nan\n",
      ">> Epoch 601 finished \tANN training loss nan\n",
      ">> Epoch 602 finished \tANN training loss nan\n",
      ">> Epoch 603 finished \tANN training loss nan\n",
      ">> Epoch 604 finished \tANN training loss nan\n",
      ">> Epoch 605 finished \tANN training loss nan\n",
      ">> Epoch 606 finished \tANN training loss nan\n",
      ">> Epoch 607 finished \tANN training loss nan\n",
      ">> Epoch 608 finished \tANN training loss nan\n",
      ">> Epoch 609 finished \tANN training loss nan\n",
      ">> Epoch 610 finished \tANN training loss nan\n",
      ">> Epoch 611 finished \tANN training loss nan\n",
      ">> Epoch 612 finished \tANN training loss nan\n",
      ">> Epoch 613 finished \tANN training loss nan\n",
      ">> Epoch 614 finished \tANN training loss nan\n",
      ">> Epoch 615 finished \tANN training loss nan\n",
      ">> Epoch 616 finished \tANN training loss nan\n",
      ">> Epoch 617 finished \tANN training loss nan\n",
      ">> Epoch 618 finished \tANN training loss nan\n",
      ">> Epoch 619 finished \tANN training loss nan\n",
      ">> Epoch 620 finished \tANN training loss nan\n",
      ">> Epoch 621 finished \tANN training loss nan\n",
      ">> Epoch 622 finished \tANN training loss nan\n",
      ">> Epoch 623 finished \tANN training loss nan\n",
      ">> Epoch 624 finished \tANN training loss nan\n",
      ">> Epoch 625 finished \tANN training loss nan\n",
      ">> Epoch 626 finished \tANN training loss nan\n",
      ">> Epoch 627 finished \tANN training loss nan\n",
      ">> Epoch 628 finished \tANN training loss nan\n",
      ">> Epoch 629 finished \tANN training loss nan\n",
      ">> Epoch 630 finished \tANN training loss nan\n",
      ">> Epoch 631 finished \tANN training loss nan\n",
      ">> Epoch 632 finished \tANN training loss nan\n",
      ">> Epoch 633 finished \tANN training loss nan\n",
      ">> Epoch 634 finished \tANN training loss nan\n",
      ">> Epoch 635 finished \tANN training loss nan\n",
      ">> Epoch 636 finished \tANN training loss nan\n",
      ">> Epoch 637 finished \tANN training loss nan\n",
      ">> Epoch 638 finished \tANN training loss nan\n",
      ">> Epoch 639 finished \tANN training loss nan\n",
      ">> Epoch 640 finished \tANN training loss nan\n",
      ">> Epoch 641 finished \tANN training loss nan\n",
      ">> Epoch 642 finished \tANN training loss nan\n",
      ">> Epoch 643 finished \tANN training loss nan\n",
      ">> Epoch 644 finished \tANN training loss nan\n",
      ">> Epoch 645 finished \tANN training loss nan\n",
      ">> Epoch 646 finished \tANN training loss nan\n",
      ">> Epoch 647 finished \tANN training loss nan\n",
      ">> Epoch 648 finished \tANN training loss nan\n",
      ">> Epoch 649 finished \tANN training loss nan\n",
      ">> Epoch 650 finished \tANN training loss nan\n",
      ">> Epoch 651 finished \tANN training loss nan\n",
      ">> Epoch 652 finished \tANN training loss nan\n",
      ">> Epoch 653 finished \tANN training loss nan\n",
      ">> Epoch 654 finished \tANN training loss nan\n",
      ">> Epoch 655 finished \tANN training loss nan\n",
      ">> Epoch 656 finished \tANN training loss nan\n",
      ">> Epoch 657 finished \tANN training loss nan\n",
      ">> Epoch 658 finished \tANN training loss nan\n",
      ">> Epoch 659 finished \tANN training loss nan\n",
      ">> Epoch 660 finished \tANN training loss nan\n",
      ">> Epoch 661 finished \tANN training loss nan\n",
      ">> Epoch 662 finished \tANN training loss nan\n",
      ">> Epoch 663 finished \tANN training loss nan\n",
      ">> Epoch 664 finished \tANN training loss nan\n",
      ">> Epoch 665 finished \tANN training loss nan\n",
      ">> Epoch 666 finished \tANN training loss nan\n",
      ">> Epoch 667 finished \tANN training loss nan\n",
      ">> Epoch 668 finished \tANN training loss nan\n",
      ">> Epoch 669 finished \tANN training loss nan\n",
      ">> Epoch 670 finished \tANN training loss nan\n",
      ">> Epoch 671 finished \tANN training loss nan\n",
      ">> Epoch 672 finished \tANN training loss nan\n",
      ">> Epoch 673 finished \tANN training loss nan\n",
      ">> Epoch 674 finished \tANN training loss nan\n",
      ">> Epoch 675 finished \tANN training loss nan\n",
      ">> Epoch 676 finished \tANN training loss nan\n",
      ">> Epoch 677 finished \tANN training loss nan\n",
      ">> Epoch 678 finished \tANN training loss nan\n",
      ">> Epoch 679 finished \tANN training loss nan\n",
      ">> Epoch 680 finished \tANN training loss nan\n",
      ">> Epoch 681 finished \tANN training loss nan\n",
      ">> Epoch 682 finished \tANN training loss nan\n",
      ">> Epoch 683 finished \tANN training loss nan\n",
      ">> Epoch 684 finished \tANN training loss nan\n",
      ">> Epoch 685 finished \tANN training loss nan\n",
      ">> Epoch 686 finished \tANN training loss nan\n",
      ">> Epoch 687 finished \tANN training loss nan\n",
      ">> Epoch 688 finished \tANN training loss nan\n",
      ">> Epoch 689 finished \tANN training loss nan\n",
      ">> Epoch 690 finished \tANN training loss nan\n",
      ">> Epoch 691 finished \tANN training loss nan\n",
      ">> Epoch 692 finished \tANN training loss nan\n",
      ">> Epoch 693 finished \tANN training loss nan\n",
      ">> Epoch 694 finished \tANN training loss nan\n",
      ">> Epoch 695 finished \tANN training loss nan\n",
      ">> Epoch 696 finished \tANN training loss nan\n",
      ">> Epoch 697 finished \tANN training loss nan\n",
      ">> Epoch 698 finished \tANN training loss nan\n",
      ">> Epoch 699 finished \tANN training loss nan\n",
      ">> Epoch 700 finished \tANN training loss nan\n",
      ">> Epoch 701 finished \tANN training loss nan\n",
      ">> Epoch 702 finished \tANN training loss nan\n",
      ">> Epoch 703 finished \tANN training loss nan\n",
      ">> Epoch 704 finished \tANN training loss nan\n",
      ">> Epoch 705 finished \tANN training loss nan\n",
      ">> Epoch 706 finished \tANN training loss nan\n",
      ">> Epoch 707 finished \tANN training loss nan\n",
      ">> Epoch 708 finished \tANN training loss nan\n",
      ">> Epoch 709 finished \tANN training loss nan\n",
      ">> Epoch 710 finished \tANN training loss nan\n",
      ">> Epoch 711 finished \tANN training loss nan\n",
      ">> Epoch 712 finished \tANN training loss nan\n",
      ">> Epoch 713 finished \tANN training loss nan\n",
      ">> Epoch 714 finished \tANN training loss nan\n",
      ">> Epoch 715 finished \tANN training loss nan\n",
      ">> Epoch 716 finished \tANN training loss nan\n",
      ">> Epoch 717 finished \tANN training loss nan\n",
      ">> Epoch 718 finished \tANN training loss nan\n",
      ">> Epoch 719 finished \tANN training loss nan\n",
      ">> Epoch 720 finished \tANN training loss nan\n",
      ">> Epoch 721 finished \tANN training loss nan\n",
      ">> Epoch 722 finished \tANN training loss nan\n",
      ">> Epoch 723 finished \tANN training loss nan\n",
      ">> Epoch 724 finished \tANN training loss nan\n",
      ">> Epoch 725 finished \tANN training loss nan\n",
      ">> Epoch 726 finished \tANN training loss nan\n",
      ">> Epoch 727 finished \tANN training loss nan\n",
      ">> Epoch 728 finished \tANN training loss nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 729 finished \tANN training loss nan\n",
      ">> Epoch 730 finished \tANN training loss nan\n",
      ">> Epoch 731 finished \tANN training loss nan\n",
      ">> Epoch 732 finished \tANN training loss nan\n",
      ">> Epoch 733 finished \tANN training loss nan\n",
      ">> Epoch 734 finished \tANN training loss nan\n",
      ">> Epoch 735 finished \tANN training loss nan\n",
      ">> Epoch 736 finished \tANN training loss nan\n",
      ">> Epoch 737 finished \tANN training loss nan\n",
      ">> Epoch 738 finished \tANN training loss nan\n",
      ">> Epoch 739 finished \tANN training loss nan\n",
      ">> Epoch 740 finished \tANN training loss nan\n",
      ">> Epoch 741 finished \tANN training loss nan\n",
      ">> Epoch 742 finished \tANN training loss nan\n",
      ">> Epoch 743 finished \tANN training loss nan\n",
      ">> Epoch 744 finished \tANN training loss nan\n",
      ">> Epoch 745 finished \tANN training loss nan\n",
      ">> Epoch 746 finished \tANN training loss nan\n",
      ">> Epoch 747 finished \tANN training loss nan\n",
      ">> Epoch 748 finished \tANN training loss nan\n",
      ">> Epoch 749 finished \tANN training loss nan\n",
      ">> Epoch 750 finished \tANN training loss nan\n",
      ">> Epoch 751 finished \tANN training loss nan\n",
      ">> Epoch 752 finished \tANN training loss nan\n",
      ">> Epoch 753 finished \tANN training loss nan\n",
      ">> Epoch 754 finished \tANN training loss nan\n",
      ">> Epoch 755 finished \tANN training loss nan\n",
      ">> Epoch 756 finished \tANN training loss nan\n",
      ">> Epoch 757 finished \tANN training loss nan\n",
      ">> Epoch 758 finished \tANN training loss nan\n",
      ">> Epoch 759 finished \tANN training loss nan\n",
      ">> Epoch 760 finished \tANN training loss nan\n",
      ">> Epoch 761 finished \tANN training loss nan\n",
      ">> Epoch 762 finished \tANN training loss nan\n",
      ">> Epoch 763 finished \tANN training loss nan\n",
      ">> Epoch 764 finished \tANN training loss nan\n",
      ">> Epoch 765 finished \tANN training loss nan\n",
      ">> Epoch 766 finished \tANN training loss nan\n",
      ">> Epoch 767 finished \tANN training loss nan\n",
      ">> Epoch 768 finished \tANN training loss nan\n",
      ">> Epoch 769 finished \tANN training loss nan\n",
      ">> Epoch 770 finished \tANN training loss nan\n",
      ">> Epoch 771 finished \tANN training loss nan\n",
      ">> Epoch 772 finished \tANN training loss nan\n",
      ">> Epoch 773 finished \tANN training loss nan\n",
      ">> Epoch 774 finished \tANN training loss nan\n",
      ">> Epoch 775 finished \tANN training loss nan\n",
      ">> Epoch 776 finished \tANN training loss nan\n",
      ">> Epoch 777 finished \tANN training loss nan\n",
      ">> Epoch 778 finished \tANN training loss nan\n",
      ">> Epoch 779 finished \tANN training loss nan\n",
      ">> Epoch 780 finished \tANN training loss nan\n",
      ">> Epoch 781 finished \tANN training loss nan\n",
      ">> Epoch 782 finished \tANN training loss nan\n",
      ">> Epoch 783 finished \tANN training loss nan\n",
      ">> Epoch 784 finished \tANN training loss nan\n",
      ">> Epoch 785 finished \tANN training loss nan\n",
      ">> Epoch 786 finished \tANN training loss nan\n",
      ">> Epoch 787 finished \tANN training loss nan\n",
      ">> Epoch 788 finished \tANN training loss nan\n",
      ">> Epoch 789 finished \tANN training loss nan\n",
      ">> Epoch 790 finished \tANN training loss nan\n",
      ">> Epoch 791 finished \tANN training loss nan\n",
      ">> Epoch 792 finished \tANN training loss nan\n",
      ">> Epoch 793 finished \tANN training loss nan\n",
      ">> Epoch 794 finished \tANN training loss nan\n",
      ">> Epoch 795 finished \tANN training loss nan\n",
      ">> Epoch 796 finished \tANN training loss nan\n",
      ">> Epoch 797 finished \tANN training loss nan\n",
      ">> Epoch 798 finished \tANN training loss nan\n",
      ">> Epoch 799 finished \tANN training loss nan\n",
      ">> Epoch 800 finished \tANN training loss nan\n",
      ">> Epoch 801 finished \tANN training loss nan\n",
      ">> Epoch 802 finished \tANN training loss nan\n",
      ">> Epoch 803 finished \tANN training loss nan\n",
      ">> Epoch 804 finished \tANN training loss nan\n",
      ">> Epoch 805 finished \tANN training loss nan\n",
      ">> Epoch 806 finished \tANN training loss nan\n",
      ">> Epoch 807 finished \tANN training loss nan\n",
      ">> Epoch 808 finished \tANN training loss nan\n",
      ">> Epoch 809 finished \tANN training loss nan\n",
      ">> Epoch 810 finished \tANN training loss nan\n",
      ">> Epoch 811 finished \tANN training loss nan\n",
      ">> Epoch 812 finished \tANN training loss nan\n",
      ">> Epoch 813 finished \tANN training loss nan\n",
      ">> Epoch 814 finished \tANN training loss nan\n",
      ">> Epoch 815 finished \tANN training loss nan\n",
      ">> Epoch 816 finished \tANN training loss nan\n",
      ">> Epoch 817 finished \tANN training loss nan\n",
      ">> Epoch 818 finished \tANN training loss nan\n",
      ">> Epoch 819 finished \tANN training loss nan\n",
      ">> Epoch 820 finished \tANN training loss nan\n",
      ">> Epoch 821 finished \tANN training loss nan\n",
      ">> Epoch 822 finished \tANN training loss nan\n",
      ">> Epoch 823 finished \tANN training loss nan\n",
      ">> Epoch 824 finished \tANN training loss nan\n",
      ">> Epoch 825 finished \tANN training loss nan\n",
      ">> Epoch 826 finished \tANN training loss nan\n",
      ">> Epoch 827 finished \tANN training loss nan\n",
      ">> Epoch 828 finished \tANN training loss nan\n",
      ">> Epoch 829 finished \tANN training loss nan\n",
      ">> Epoch 830 finished \tANN training loss nan\n",
      ">> Epoch 831 finished \tANN training loss nan\n",
      ">> Epoch 832 finished \tANN training loss nan\n",
      ">> Epoch 833 finished \tANN training loss nan\n",
      ">> Epoch 834 finished \tANN training loss nan\n",
      ">> Epoch 835 finished \tANN training loss nan\n",
      ">> Epoch 836 finished \tANN training loss nan\n",
      ">> Epoch 837 finished \tANN training loss nan\n",
      ">> Epoch 838 finished \tANN training loss nan\n",
      ">> Epoch 839 finished \tANN training loss nan\n",
      ">> Epoch 840 finished \tANN training loss nan\n",
      ">> Epoch 841 finished \tANN training loss nan\n",
      ">> Epoch 842 finished \tANN training loss nan\n",
      ">> Epoch 843 finished \tANN training loss nan\n",
      ">> Epoch 844 finished \tANN training loss nan\n",
      ">> Epoch 845 finished \tANN training loss nan\n",
      ">> Epoch 846 finished \tANN training loss nan\n",
      ">> Epoch 847 finished \tANN training loss nan\n",
      ">> Epoch 848 finished \tANN training loss nan\n",
      ">> Epoch 849 finished \tANN training loss nan\n",
      ">> Epoch 850 finished \tANN training loss nan\n",
      ">> Epoch 851 finished \tANN training loss nan\n",
      ">> Epoch 852 finished \tANN training loss nan\n",
      ">> Epoch 853 finished \tANN training loss nan\n",
      ">> Epoch 854 finished \tANN training loss nan\n",
      ">> Epoch 855 finished \tANN training loss nan\n",
      ">> Epoch 856 finished \tANN training loss nan\n",
      ">> Epoch 857 finished \tANN training loss nan\n",
      ">> Epoch 858 finished \tANN training loss nan\n",
      ">> Epoch 859 finished \tANN training loss nan\n",
      ">> Epoch 860 finished \tANN training loss nan\n",
      ">> Epoch 861 finished \tANN training loss nan\n",
      ">> Epoch 862 finished \tANN training loss nan\n",
      ">> Epoch 863 finished \tANN training loss nan\n",
      ">> Epoch 864 finished \tANN training loss nan\n",
      ">> Epoch 865 finished \tANN training loss nan\n",
      ">> Epoch 866 finished \tANN training loss nan\n",
      ">> Epoch 867 finished \tANN training loss nan\n",
      ">> Epoch 868 finished \tANN training loss nan\n",
      ">> Epoch 869 finished \tANN training loss nan\n",
      ">> Epoch 870 finished \tANN training loss nan\n",
      ">> Epoch 871 finished \tANN training loss nan\n",
      ">> Epoch 872 finished \tANN training loss nan\n",
      ">> Epoch 873 finished \tANN training loss nan\n",
      ">> Epoch 874 finished \tANN training loss nan\n",
      ">> Epoch 875 finished \tANN training loss nan\n",
      ">> Epoch 876 finished \tANN training loss nan\n",
      ">> Epoch 877 finished \tANN training loss nan\n",
      ">> Epoch 878 finished \tANN training loss nan\n",
      ">> Epoch 879 finished \tANN training loss nan\n",
      ">> Epoch 880 finished \tANN training loss nan\n",
      ">> Epoch 881 finished \tANN training loss nan\n",
      ">> Epoch 882 finished \tANN training loss nan\n",
      ">> Epoch 883 finished \tANN training loss nan\n",
      ">> Epoch 884 finished \tANN training loss nan\n",
      ">> Epoch 885 finished \tANN training loss nan\n",
      ">> Epoch 886 finished \tANN training loss nan\n",
      ">> Epoch 887 finished \tANN training loss nan\n",
      ">> Epoch 888 finished \tANN training loss nan\n",
      ">> Epoch 889 finished \tANN training loss nan\n",
      ">> Epoch 890 finished \tANN training loss nan\n",
      ">> Epoch 891 finished \tANN training loss nan\n",
      ">> Epoch 892 finished \tANN training loss nan\n",
      ">> Epoch 893 finished \tANN training loss nan\n",
      ">> Epoch 894 finished \tANN training loss nan\n",
      ">> Epoch 895 finished \tANN training loss nan\n",
      ">> Epoch 896 finished \tANN training loss nan\n",
      ">> Epoch 897 finished \tANN training loss nan\n",
      ">> Epoch 898 finished \tANN training loss nan\n",
      ">> Epoch 899 finished \tANN training loss nan\n",
      ">> Epoch 900 finished \tANN training loss nan\n",
      ">> Epoch 901 finished \tANN training loss nan\n",
      ">> Epoch 902 finished \tANN training loss nan\n",
      ">> Epoch 903 finished \tANN training loss nan\n",
      ">> Epoch 904 finished \tANN training loss nan\n",
      ">> Epoch 905 finished \tANN training loss nan\n",
      ">> Epoch 906 finished \tANN training loss nan\n",
      ">> Epoch 907 finished \tANN training loss nan\n",
      ">> Epoch 908 finished \tANN training loss nan\n",
      ">> Epoch 909 finished \tANN training loss nan\n",
      ">> Epoch 910 finished \tANN training loss nan\n",
      ">> Epoch 911 finished \tANN training loss nan\n",
      ">> Epoch 912 finished \tANN training loss nan\n",
      ">> Epoch 913 finished \tANN training loss nan\n",
      ">> Epoch 914 finished \tANN training loss nan\n",
      ">> Epoch 915 finished \tANN training loss nan\n",
      ">> Epoch 916 finished \tANN training loss nan\n",
      ">> Epoch 917 finished \tANN training loss nan\n",
      ">> Epoch 918 finished \tANN training loss nan\n",
      ">> Epoch 919 finished \tANN training loss nan\n",
      ">> Epoch 920 finished \tANN training loss nan\n",
      ">> Epoch 921 finished \tANN training loss nan\n",
      ">> Epoch 922 finished \tANN training loss nan\n",
      ">> Epoch 923 finished \tANN training loss nan\n",
      ">> Epoch 924 finished \tANN training loss nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 925 finished \tANN training loss nan\n",
      ">> Epoch 926 finished \tANN training loss nan\n",
      ">> Epoch 927 finished \tANN training loss nan\n",
      ">> Epoch 928 finished \tANN training loss nan\n",
      ">> Epoch 929 finished \tANN training loss nan\n",
      ">> Epoch 930 finished \tANN training loss nan\n",
      ">> Epoch 931 finished \tANN training loss nan\n",
      ">> Epoch 932 finished \tANN training loss nan\n",
      ">> Epoch 933 finished \tANN training loss nan\n",
      ">> Epoch 934 finished \tANN training loss nan\n",
      ">> Epoch 935 finished \tANN training loss nan\n",
      ">> Epoch 936 finished \tANN training loss nan\n",
      ">> Epoch 937 finished \tANN training loss nan\n",
      ">> Epoch 938 finished \tANN training loss nan\n",
      ">> Epoch 939 finished \tANN training loss nan\n",
      ">> Epoch 940 finished \tANN training loss nan\n",
      ">> Epoch 941 finished \tANN training loss nan\n",
      ">> Epoch 942 finished \tANN training loss nan\n",
      ">> Epoch 943 finished \tANN training loss nan\n",
      ">> Epoch 944 finished \tANN training loss nan\n",
      ">> Epoch 945 finished \tANN training loss nan\n",
      ">> Epoch 946 finished \tANN training loss nan\n",
      ">> Epoch 947 finished \tANN training loss nan\n",
      ">> Epoch 948 finished \tANN training loss nan\n",
      ">> Epoch 949 finished \tANN training loss nan\n",
      ">> Epoch 950 finished \tANN training loss nan\n",
      ">> Epoch 951 finished \tANN training loss nan\n",
      ">> Epoch 952 finished \tANN training loss nan\n",
      ">> Epoch 953 finished \tANN training loss nan\n",
      ">> Epoch 954 finished \tANN training loss nan\n",
      ">> Epoch 955 finished \tANN training loss nan\n",
      ">> Epoch 956 finished \tANN training loss nan\n",
      ">> Epoch 957 finished \tANN training loss nan\n",
      ">> Epoch 958 finished \tANN training loss nan\n",
      ">> Epoch 959 finished \tANN training loss nan\n",
      ">> Epoch 960 finished \tANN training loss nan\n",
      ">> Epoch 961 finished \tANN training loss nan\n",
      ">> Epoch 962 finished \tANN training loss nan\n",
      ">> Epoch 963 finished \tANN training loss nan\n",
      ">> Epoch 964 finished \tANN training loss nan\n",
      ">> Epoch 965 finished \tANN training loss nan\n",
      ">> Epoch 966 finished \tANN training loss nan\n",
      ">> Epoch 967 finished \tANN training loss nan\n",
      ">> Epoch 968 finished \tANN training loss nan\n",
      ">> Epoch 969 finished \tANN training loss nan\n",
      ">> Epoch 970 finished \tANN training loss nan\n",
      ">> Epoch 971 finished \tANN training loss nan\n",
      ">> Epoch 972 finished \tANN training loss nan\n",
      ">> Epoch 973 finished \tANN training loss nan\n",
      ">> Epoch 974 finished \tANN training loss nan\n",
      ">> Epoch 975 finished \tANN training loss nan\n",
      ">> Epoch 976 finished \tANN training loss nan\n",
      ">> Epoch 977 finished \tANN training loss nan\n",
      ">> Epoch 978 finished \tANN training loss nan\n",
      ">> Epoch 979 finished \tANN training loss nan\n",
      ">> Epoch 980 finished \tANN training loss nan\n",
      ">> Epoch 981 finished \tANN training loss nan\n",
      ">> Epoch 982 finished \tANN training loss nan\n",
      ">> Epoch 983 finished \tANN training loss nan\n",
      ">> Epoch 984 finished \tANN training loss nan\n",
      ">> Epoch 985 finished \tANN training loss nan\n",
      ">> Epoch 986 finished \tANN training loss nan\n",
      ">> Epoch 987 finished \tANN training loss nan\n",
      ">> Epoch 988 finished \tANN training loss nan\n",
      ">> Epoch 989 finished \tANN training loss nan\n",
      ">> Epoch 990 finished \tANN training loss nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 991 finished \tANN training loss nan\n",
      ">> Epoch 992 finished \tANN training loss nan\n",
      ">> Epoch 993 finished \tANN training loss nan\n",
      ">> Epoch 994 finished \tANN training loss nan\n",
      ">> Epoch 995 finished \tANN training loss nan\n",
      ">> Epoch 996 finished \tANN training loss nan\n",
      ">> Epoch 997 finished \tANN training loss nan\n",
      ">> Epoch 998 finished \tANN training loss nan\n",
      ">> Epoch 999 finished \tANN training loss nan\n",
      "[END] Fine tuning step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-8-99b9c4f13b7b>\", line 15, in <module>\n",
      "    dbn_r2 = r2_score(Y_test.tolist(), dnb_y_pred)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 677, in r2_score\n",
      "    y_true, y_pred, multioutput)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 90, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 664, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 106, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ValueError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-8-99b9c4f13b7b>\", line 15, in <module>\n",
      "    dbn_r2 = r2_score(Y_test.tolist(), dnb_y_pred)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 677, in r2_score\n",
      "    y_true, y_pred, multioutput)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 90, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 664, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 106, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ValueError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3360, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1193, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-8-99b9c4f13b7b>\", line 15, in <module>\n",
      "    dbn_r2 = r2_score(Y_test.tolist(), dnb_y_pred)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 677, in r2_score\n",
      "    y_true, y_pred, multioutput)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 90, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 664, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 106, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ValueError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3360, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1193, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2895, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3072, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3282, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1211, in structured_traceback\n",
      "    chained_exceptions_tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n"
     ]
    }
   ],
   "source": [
    "regress = SupervisedDBNRegression(hidden_layers_structure = [32, 16],\n",
    "learning_rate_rbm=0.05,\n",
    "learning_rate=0.03,\n",
    "n_epochs_rbm=10,\n",
    "n_iter_backprop=1000,\n",
    "batch_size=50,\n",
    "activation_function='relu',\n",
    "dropout_p=0.1)\n",
    "\n",
    "for i in range(trials):\n",
    "    print(\"TRIAL: \", i+1)\n",
    "    print(\"\\n\")\n",
    "    regress.fit(X_train, Y_train)\n",
    "    dnb_y_pred = regress.predict(X_test)\n",
    "    dbn_r2 = r2_score(Y_test.tolist(), dnb_y_pred)\n",
    "    dbn_mse = mean_squared_error(Y_test.tolist(), dnb_y_pred)\n",
    "    dbn_ave_r2.append(dbn_r2)\n",
    "    dbn_ave_mse.append(dbn_mse)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b2223b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7193807233088634,\n",
       " -0.0007203698598818864,\n",
       " 0.5871067933066494,\n",
       " -0.003178974089325548,\n",
       " 0.7750143275474013,\n",
       " -8.705056264091127e-05,\n",
       " -0.00046350318162047266,\n",
       " 0.49654980491061196]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbn_r2_new = ave(dbn_ave_r2)\n",
    "dbn_mse_new = ave(dbn_ave_mse)\n",
    "dbn_ave_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6cc396",
   "metadata": {},
   "source": [
    "## RECURRENT NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cb884cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 1922\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import Sequential, layers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bd14f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310, 6) (310, 1)\n",
      "(61, 6) (61, 1)\n"
     ]
    }
   ],
   "source": [
    "Y = Y.reshape(-1,1)\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tss = TimeSeriesSplit(n_splits = 5)\n",
    "for train_index, test_index in tss.split(X):\n",
    "    X_train, X_test = X[train_index, :], X[test_index,:]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "dXtrain = pd.DataFrame(X_train)\n",
    "dXtrain.columns = ['a','b','c','d','e','f', 'g']\n",
    "X_train = np.array(dXtrain.drop(['g'], axis = 1).reset_index(drop=True))\n",
    "\n",
    "dXtest = pd.DataFrame(X_test)\n",
    "dXtest.columns = ['a','b','c','d','e','f', 'g']\n",
    "X_test = np.array(dXtest.drop(['g'], axis = 1).reset_index(drop=True))\n",
    "\n",
    "\n",
    "print (X_train.shape, y_train.shape)\n",
    "print (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cbf7560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(model, test_forecast, gwl, steps_ahead):\n",
    "    y_pred = []\n",
    "    current_output = gwl\n",
    "    for step in range(steps_ahead):\n",
    "        #print(test_forecast[step])\n",
    "        input_test = np.concatenate([test_forecast[step][:-1], [current_output]])\n",
    "        #input_test = [test_forecast[0][0][:-1], gwl]\n",
    "        #print(input_test)\n",
    "        pred = model.predict(input_test.reshape(1,1,6))\n",
    "        y_pred.append(pred[0][0])\n",
    "        #print(pred)\n",
    "        current_output = pred[0][0]\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2b2fb2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.21085379531656"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gwl = y_test[0][0]\n",
    "steps_ahead = X_test.shape[0]\n",
    "gwl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b892b34f",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dd8fb10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running trial 1\n",
      "(310, 6)\n",
      "WARNING:tensorflow:From C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.0218 - mean_squared_error: 1.0218\n",
      "Epoch 00001: val_loss improved from inf to 0.69149, saving model to model.h5\n",
      "248/248 [==============================] - 1s 3ms/sample - loss: 1.1138 - mean_squared_error: 1.1138 - val_loss: 0.6915 - val_mean_squared_error: 0.6915\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.1472 - mean_squared_error: 1.1472\n",
      "Epoch 00002: val_loss improved from 0.69149 to 0.66006, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.9898 - mean_squared_error: 0.9898 - val_loss: 0.6601 - val_mean_squared_error: 0.6601\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9815 - mean_squared_error: 0.9815\n",
      "Epoch 00003: val_loss improved from 0.66006 to 0.63231, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.8993 - mean_squared_error: 0.8993 - val_loss: 0.6323 - val_mean_squared_error: 0.6323\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9672 - mean_squared_error: 0.9672\n",
      "Epoch 00004: val_loss improved from 0.63231 to 0.60567, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.8364 - mean_squared_error: 0.8364 - val_loss: 0.6057 - val_mean_squared_error: 0.6057\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7684 - mean_squared_error: 0.7684\n",
      "Epoch 00005: val_loss improved from 0.60567 to 0.57825, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.7724 - mean_squared_error: 0.7724 - val_loss: 0.5782 - val_mean_squared_error: 0.5782\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8063 - mean_squared_error: 0.8063\n",
      "Epoch 00006: val_loss improved from 0.57825 to 0.55293, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.7115 - mean_squared_error: 0.7115 - val_loss: 0.5529 - val_mean_squared_error: 0.5529\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6324 - mean_squared_error: 0.6324\n",
      "Epoch 00007: val_loss improved from 0.55293 to 0.53107, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.6599 - mean_squared_error: 0.6599 - val_loss: 0.5311 - val_mean_squared_error: 0.5311\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4855 - mean_squared_error: 0.4855\n",
      "Epoch 00008: val_loss improved from 0.53107 to 0.50763, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.6179 - mean_squared_error: 0.6179 - val_loss: 0.5076 - val_mean_squared_error: 0.5076\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5864 - mean_squared_error: 0.5864\n",
      "Epoch 00009: val_loss improved from 0.50763 to 0.48511, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.5550 - mean_squared_error: 0.5550 - val_loss: 0.4851 - val_mean_squared_error: 0.4851\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5575 - mean_squared_error: 0.5575\n",
      "Epoch 00010: val_loss improved from 0.48511 to 0.46279, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.5124 - mean_squared_error: 0.5124 - val_loss: 0.4628 - val_mean_squared_error: 0.4628\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4579 - mean_squared_error: 0.4579\n",
      "Epoch 00011: val_loss improved from 0.46279 to 0.43990, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.4662 - mean_squared_error: 0.4662 - val_loss: 0.4399 - val_mean_squared_error: 0.4399\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4760 - mean_squared_error: 0.4760\n",
      "Epoch 00012: val_loss improved from 0.43990 to 0.41696, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.4247 - mean_squared_error: 0.4247 - val_loss: 0.4170 - val_mean_squared_error: 0.4170\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4421 - mean_squared_error: 0.4421\n",
      "Epoch 00013: val_loss improved from 0.41696 to 0.39235, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.3869 - mean_squared_error: 0.3869 - val_loss: 0.3923 - val_mean_squared_error: 0.3923\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3652 - mean_squared_error: 0.3652\n",
      "Epoch 00014: val_loss improved from 0.39235 to 0.36924, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.3621 - mean_squared_error: 0.3621 - val_loss: 0.3692 - val_mean_squared_error: 0.3692\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3285 - mean_squared_error: 0.3285\n",
      "Epoch 00015: val_loss improved from 0.36924 to 0.34713, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.3321 - mean_squared_error: 0.3321 - val_loss: 0.3471 - val_mean_squared_error: 0.3471\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2615 - mean_squared_error: 0.2615\n",
      "Epoch 00016: val_loss improved from 0.34713 to 0.32478, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.2904 - mean_squared_error: 0.2904 - val_loss: 0.3248 - val_mean_squared_error: 0.3248\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2877 - mean_squared_error: 0.2877\n",
      "Epoch 00017: val_loss improved from 0.32478 to 0.30101, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.2625 - mean_squared_error: 0.2625 - val_loss: 0.3010 - val_mean_squared_error: 0.3010\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2760 - mean_squared_error: 0.2760\n",
      "Epoch 00018: val_loss improved from 0.30101 to 0.27984, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.2357 - mean_squared_error: 0.2357 - val_loss: 0.2798 - val_mean_squared_error: 0.2798\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2504 - mean_squared_error: 0.2504\n",
      "Epoch 00019: val_loss improved from 0.27984 to 0.25886, saving model to model.h5\n",
      "248/248 [==============================] - 0s 113us/sample - loss: 0.2130 - mean_squared_error: 0.2130 - val_loss: 0.2589 - val_mean_squared_error: 0.2589\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1841 - mean_squared_error: 0.1841\n",
      "Epoch 00020: val_loss improved from 0.25886 to 0.23951, saving model to model.h5\n",
      "248/248 [==============================] - 0s 113us/sample - loss: 0.1978 - mean_squared_error: 0.1978 - val_loss: 0.2395 - val_mean_squared_error: 0.2395\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1522 - mean_squared_error: 0.1522\n",
      "Epoch 00021: val_loss improved from 0.23951 to 0.21969, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.1781 - mean_squared_error: 0.1781 - val_loss: 0.2197 - val_mean_squared_error: 0.2197\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1579 - mean_squared_error: 0.1579\n",
      "Epoch 00022: val_loss improved from 0.21969 to 0.20058, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.1585 - mean_squared_error: 0.1585 - val_loss: 0.2006 - val_mean_squared_error: 0.2006\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1585 - mean_squared_error: 0.1585\n",
      "Epoch 00023: val_loss improved from 0.20058 to 0.18355, saving model to model.h5\n",
      "248/248 [==============================] - 0s 109us/sample - loss: 0.1441 - mean_squared_error: 0.1441 - val_loss: 0.1835 - val_mean_squared_error: 0.1835\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1395 - mean_squared_error: 0.1395\n",
      "Epoch 00024: val_loss improved from 0.18355 to 0.16747, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.1311 - mean_squared_error: 0.1311 - val_loss: 0.1675 - val_mean_squared_error: 0.1675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1291 - mean_squared_error: 0.1291\n",
      "Epoch 00025: val_loss improved from 0.16747 to 0.15166, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.1118 - mean_squared_error: 0.1118 - val_loss: 0.1517 - val_mean_squared_error: 0.1517\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1077 - mean_squared_error: 0.1077\n",
      "Epoch 00026: val_loss improved from 0.15166 to 0.13819, saving model to model.h5\n",
      "248/248 [==============================] - 0s 113us/sample - loss: 0.1040 - mean_squared_error: 0.1040 - val_loss: 0.1382 - val_mean_squared_error: 0.1382\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0924 - mean_squared_error: 0.0924\n",
      "Epoch 00027: val_loss improved from 0.13819 to 0.12647, saving model to model.h5\n",
      "248/248 [==============================] - 0s 106us/sample - loss: 0.0886 - mean_squared_error: 0.0886 - val_loss: 0.1265 - val_mean_squared_error: 0.1265\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0597 - mean_squared_error: 0.0597\n",
      "Epoch 00028: val_loss improved from 0.12647 to 0.11397, saving model to model.h5\n",
      "248/248 [==============================] - 0s 106us/sample - loss: 0.0765 - mean_squared_error: 0.0765 - val_loss: 0.1140 - val_mean_squared_error: 0.1140\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0782 - mean_squared_error: 0.0782\n",
      "Epoch 00029: val_loss improved from 0.11397 to 0.10412, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.0713 - mean_squared_error: 0.0713 - val_loss: 0.1041 - val_mean_squared_error: 0.1041\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0717 - mean_squared_error: 0.0717\n",
      "Epoch 00030: val_loss improved from 0.10412 to 0.09419, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0712 - mean_squared_error: 0.0712 - val_loss: 0.0942 - val_mean_squared_error: 0.0942\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0683 - mean_squared_error: 0.0683\n",
      "Epoch 00031: val_loss improved from 0.09419 to 0.08563, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0661 - mean_squared_error: 0.0661 - val_loss: 0.0856 - val_mean_squared_error: 0.0856\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0626 - mean_squared_error: 0.0626\n",
      "Epoch 00032: val_loss improved from 0.08563 to 0.07846, saving model to model.h5\n",
      "248/248 [==============================] - 0s 127us/sample - loss: 0.0598 - mean_squared_error: 0.0598 - val_loss: 0.0785 - val_mean_squared_error: 0.0785\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0485 - mean_squared_error: 0.0485\n",
      "Epoch 00033: val_loss improved from 0.07846 to 0.07226, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0539 - mean_squared_error: 0.0539 - val_loss: 0.0723 - val_mean_squared_error: 0.0723\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0516 - mean_squared_error: 0.0516\n",
      "Epoch 00034: val_loss improved from 0.07226 to 0.06587, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0501 - mean_squared_error: 0.0501 - val_loss: 0.0659 - val_mean_squared_error: 0.0659\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0499 - mean_squared_error: 0.0499\n",
      "Epoch 00035: val_loss improved from 0.06587 to 0.06019, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0494 - mean_squared_error: 0.0494 - val_loss: 0.0602 - val_mean_squared_error: 0.0602\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0460 - mean_squared_error: 0.0460\n",
      "Epoch 00036: val_loss improved from 0.06019 to 0.05673, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0418 - mean_squared_error: 0.0418 - val_loss: 0.0567 - val_mean_squared_error: 0.0567\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0524 - mean_squared_error: 0.0524\n",
      "Epoch 00037: val_loss improved from 0.05673 to 0.05378, saving model to model.h5\n",
      "248/248 [==============================] - 0s 151us/sample - loss: 0.0450 - mean_squared_error: 0.0450 - val_loss: 0.0538 - val_mean_squared_error: 0.0538\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0438 - mean_squared_error: 0.0438\n",
      "Epoch 00038: val_loss improved from 0.05378 to 0.05132, saving model to model.h5\n",
      "248/248 [==============================] - 0s 120us/sample - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0513 - val_mean_squared_error: 0.0513\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0383 - mean_squared_error: 0.0383\n",
      "Epoch 00039: val_loss improved from 0.05132 to 0.04948, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.0392 - mean_squared_error: 0.0392 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00040: val_loss improved from 0.04948 to 0.04873, saving model to model.h5\n",
      "248/248 [==============================] - 0s 109us/sample - loss: 0.0394 - mean_squared_error: 0.0394 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0383 - mean_squared_error: 0.0383\n",
      "Epoch 00041: val_loss improved from 0.04873 to 0.04606, saving model to model.h5\n",
      "248/248 [==============================] - 0s 106us/sample - loss: 0.0394 - mean_squared_error: 0.0394 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0509 - mean_squared_error: 0.0509\n",
      "Epoch 00042: val_loss improved from 0.04606 to 0.04458, saving model to model.h5\n",
      "248/248 [==============================] - 0s 112us/sample - loss: 0.0374 - mean_squared_error: 0.0374 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0394 - mean_squared_error: 0.0394\n",
      "Epoch 00043: val_loss improved from 0.04458 to 0.04383, saving model to model.h5\n",
      "248/248 [==============================] - 0s 101us/sample - loss: 0.0378 - mean_squared_error: 0.0378 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0367 - mean_squared_error: 0.0367\n",
      "Epoch 00044: val_loss did not improve from 0.04383\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0381 - mean_squared_error: 0.0381 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00045: val_loss improved from 0.04383 to 0.04240, saving model to model.h5\n",
      "248/248 [==============================] - 0s 111us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00046: val_loss improved from 0.04240 to 0.04184, saving model to model.h5\n",
      "248/248 [==============================] - 0s 103us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00047: val_loss improved from 0.04184 to 0.03966, saving model to model.h5\n",
      "248/248 [==============================] - 0s 109us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00048: val_loss improved from 0.03966 to 0.03918, saving model to model.h5\n",
      "248/248 [==============================] - 0s 127us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00049: val_loss did not improve from 0.03918\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0426 - mean_squared_error: 0.0426\n",
      "Epoch 00050: val_loss did not improve from 0.03918\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00051: val_loss improved from 0.03918 to 0.03863, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00052: val_loss did not improve from 0.03863\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00053: val_loss did not improve from 0.03863\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Epoch 00054: val_loss did not improve from 0.03863\n",
      "248/248 [==============================] - 0s 42us/sample - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00055: val_loss did not improve from 0.03863\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00056: val_loss improved from 0.03863 to 0.03842, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00057: val_loss did not improve from 0.03842\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00058: val_loss did not improve from 0.03842\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0382 - mean_squared_error: 0.0382\n",
      "Epoch 00059: val_loss improved from 0.03842 to 0.03738, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Epoch 00060: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00061: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0167 - mean_squared_error: 0.0167\n",
      "Epoch 00062: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Epoch 00063: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00064: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00065: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00066: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00067: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0345 - mean_squared_error: 0.0345\n",
      "Epoch 00068: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0161 - mean_squared_error: 0.0161\n",
      "Epoch 00069: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Epoch 00070: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00071: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00072: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0383 - val_mean_squared_error: 0.0383\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00073: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00074: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00075: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00076: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0383 - val_mean_squared_error: 0.0383\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00077: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00078: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00079: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00080: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0295 - mean_squared_error: 0.0295\n",
      "Epoch 00081: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Epoch 00082: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00083: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00084: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00085: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Epoch 00086: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00087: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00088: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00089: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00090: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00091: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00092: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0131 - mean_squared_error: 0.0131\n",
      "Epoch 00093: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00094: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0123 - mean_squared_error: 0.0123\n",
      "Epoch 00095: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00096: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00097: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Epoch 00098: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00099: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00100: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 101/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0105 - mean_squared_error: 0.0105\n",
      "Epoch 00101: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0099 - mean_squared_error: 0.0099\n",
      "Epoch 00102: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00103: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0133 - mean_squared_error: 0.0133\n",
      "Epoch 00104: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0383 - val_mean_squared_error: 0.0383\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00105: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00106: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00107: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00108: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Epoch 00109: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0109 - mean_squared_error: 0.0109\n",
      "Epoch 00110: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00111: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0136 - mean_squared_error: 0.0136\n",
      "Epoch 00112: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0161 - mean_squared_error: 0.0161\n",
      "Epoch 00113: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00114: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 00115: val_loss did not improve from 0.03738\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00116: val_loss improved from 0.03738 to 0.03732, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00117: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00118: val_loss improved from 0.03732 to 0.03665, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0367 - val_mean_squared_error: 0.0367\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00119: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 47us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0368 - mean_squared_error: 0.0368\n",
      "Epoch 00120: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00121: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00122: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00123: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00124: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00125: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00126: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 43us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00127: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0141 - mean_squared_error: 0.0141\n",
      "Epoch 00128: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0158 - mean_squared_error: 0.0158\n",
      "Epoch 00129: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 130/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00130: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00131: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 132/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00132: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 133/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00133: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 134/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00134: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 135/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00135: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 136/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0177 - mean_squared_error: 0.0177\n",
      "Epoch 00136: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 137/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00137: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 138/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00138: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 139/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00139: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 140/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0187 - mean_squared_error: 0.0187\n",
      "Epoch 00140: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 141/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0160 - mean_squared_error: 0.0160\n",
      "Epoch 00141: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 142/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.0172\n",
      "Epoch 00142: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 143/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0156 - mean_squared_error: 0.0156\n",
      "Epoch 00143: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 37us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 144/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00144: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 145/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00145: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 146/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00146: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 147/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00147: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 148/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0132 - mean_squared_error: 0.0132\n",
      "Epoch 00148: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 149/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0158 - mean_squared_error: 0.0158\n",
      "Epoch 00149: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 150/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00150: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 151/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00151: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 152/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00152: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 153/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00153: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 154/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00154: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 155/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00155: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 156/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00156: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 157/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00157: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 158/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00158: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 159/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0160 - mean_squared_error: 0.0160\n",
      "Epoch 00159: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 160/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0136 - mean_squared_error: 0.0136\n",
      "Epoch 00160: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 161/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00161: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 42us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 162/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0143 - mean_squared_error: 0.0143\n",
      "Epoch 00162: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 163/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0094 - mean_squared_error: 0.0094\n",
      "Epoch 00163: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 164/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00164: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 165/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00165: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 166/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0155 - mean_squared_error: 0.0155\n",
      "Epoch 00166: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 167/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00167: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 168/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00168: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 169/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00169: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 170/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Epoch 00170: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 171/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00171: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 172/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00172: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 173/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0181 - mean_squared_error: 0.0181\n",
      "Epoch 00173: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 174/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00174: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 175/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00175: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 176/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0149 - mean_squared_error: 0.0149\n",
      "Epoch 00176: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 177/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00177: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 178/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Epoch 00178: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 179/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00179: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 180/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00180: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 181/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00181: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 182/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00182: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 183/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0140 - mean_squared_error: 0.0140\n",
      "Epoch 00183: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 184/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00184: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 185/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00185: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 186/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 00186: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 187/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0116 - mean_squared_error: 0.0116\n",
      "Epoch 00187: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 188/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0155 - mean_squared_error: 0.0155\n",
      "Epoch 00188: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 189/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00189: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 190/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00190: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 191/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0126 - mean_squared_error: 0.0126\n",
      "Epoch 00191: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 192/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0161 - mean_squared_error: 0.0161\n",
      "Epoch 00192: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 193/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0153 - mean_squared_error: 0.0153\n",
      "Epoch 00193: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 194/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00194: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 195/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0083 - mean_squared_error: 0.0083\n",
      "Epoch 00195: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 196/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00196: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0172 - mean_squared_error: 0.0172 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 197/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00197: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 198/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0158 - mean_squared_error: 0.0158\n",
      "Epoch 00198: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 199/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00199: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 200/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0114 - mean_squared_error: 0.0114\n",
      "Epoch 00200: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 201/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00201: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 202/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00202: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 203/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00203: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 204/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00204: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 205/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0149 - mean_squared_error: 0.0149\n",
      "Epoch 00205: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 206/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00206: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 207/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0171 - mean_squared_error: 0.0171\n",
      "Epoch 00207: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 208/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00208: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 209/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0227 - mean_squared_error: 0.0227\n",
      "Epoch 00209: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 210/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00210: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 211/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0187 - mean_squared_error: 0.0187\n",
      "Epoch 00211: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0157 - mean_squared_error: 0.0157 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 212/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0088 - mean_squared_error: 0.0088\n",
      "Epoch 00212: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 213/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0231 - mean_squared_error: 0.0231\n",
      "Epoch 00213: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 214/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0152 - mean_squared_error: 0.0152\n",
      "Epoch 00214: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 215/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00215: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 216/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0410 - mean_squared_error: 0.0410\n",
      "Epoch 00216: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 217/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00217: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 218/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0094 - mean_squared_error: 0.0094\n",
      "Epoch 00218: val_loss did not improve from 0.03665\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Running trial 2\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.1401 - mean_squared_error: 1.1401\n",
      "Epoch 00001: val_loss improved from inf to 0.80886, saving model to model.h5\n",
      "248/248 [==============================] - 1s 4ms/sample - loss: 1.2726 - mean_squared_error: 1.2726 - val_loss: 0.8089 - val_mean_squared_error: 0.8089\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.3224 - mean_squared_error: 1.3224\n",
      "Epoch 00002: val_loss improved from 0.80886 to 0.76417, saving model to model.h5\n",
      "248/248 [==============================] - 0s 155us/sample - loss: 1.1096 - mean_squared_error: 1.1096 - val_loss: 0.7642 - val_mean_squared_error: 0.7642\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9822 - mean_squared_error: 0.9822\n",
      "Epoch 00003: val_loss improved from 0.76417 to 0.72687, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 1.0170 - mean_squared_error: 1.0170 - val_loss: 0.7269 - val_mean_squared_error: 0.7269\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9979 - mean_squared_error: 0.9979\n",
      "Epoch 00004: val_loss improved from 0.72687 to 0.69525, saving model to model.h5\n",
      "248/248 [==============================] - 0s 183us/sample - loss: 0.9234 - mean_squared_error: 0.9234 - val_loss: 0.6953 - val_mean_squared_error: 0.6953\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9449 - mean_squared_error: 0.9449\n",
      "Epoch 00005: val_loss improved from 0.69525 to 0.66499, saving model to model.h5\n",
      "248/248 [==============================] - 0s 169us/sample - loss: 0.8408 - mean_squared_error: 0.8408 - val_loss: 0.6650 - val_mean_squared_error: 0.6650\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6584 - mean_squared_error: 0.6584\n",
      "Epoch 00006: val_loss improved from 0.66499 to 0.63638, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.7867 - mean_squared_error: 0.7867 - val_loss: 0.6364 - val_mean_squared_error: 0.6364\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6273 - mean_squared_error: 0.6273\n",
      "Epoch 00007: val_loss improved from 0.63638 to 0.60852, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.7075 - mean_squared_error: 0.7075 - val_loss: 0.6085 - val_mean_squared_error: 0.6085\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6194 - mean_squared_error: 0.6194\n",
      "Epoch 00008: val_loss improved from 0.60852 to 0.58108, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.6471 - mean_squared_error: 0.6471 - val_loss: 0.5811 - val_mean_squared_error: 0.5811\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6440 - mean_squared_error: 0.6440\n",
      "Epoch 00009: val_loss improved from 0.58108 to 0.55246, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.5906 - mean_squared_error: 0.5906 - val_loss: 0.5525 - val_mean_squared_error: 0.5525\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4169 - mean_squared_error: 0.4169\n",
      "Epoch 00010: val_loss improved from 0.55246 to 0.52475, saving model to model.h5\n",
      "248/248 [==============================] - 0s 169us/sample - loss: 0.5398 - mean_squared_error: 0.5398 - val_loss: 0.5248 - val_mean_squared_error: 0.5248\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5722 - mean_squared_error: 0.5722\n",
      "Epoch 00011: val_loss improved from 0.52475 to 0.49581, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.4761 - mean_squared_error: 0.4761 - val_loss: 0.4958 - val_mean_squared_error: 0.4958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4325 - mean_squared_error: 0.4325\n",
      "Epoch 00012: val_loss improved from 0.49581 to 0.46862, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.4510 - mean_squared_error: 0.4510 - val_loss: 0.4686 - val_mean_squared_error: 0.4686\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4293 - mean_squared_error: 0.4293\n",
      "Epoch 00013: val_loss improved from 0.46862 to 0.44010, saving model to model.h5\n",
      "248/248 [==============================] - 0s 269us/sample - loss: 0.4061 - mean_squared_error: 0.4061 - val_loss: 0.4401 - val_mean_squared_error: 0.4401\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4163 - mean_squared_error: 0.4163\n",
      "Epoch 00014: val_loss improved from 0.44010 to 0.41386, saving model to model.h5\n",
      "248/248 [==============================] - 0s 209us/sample - loss: 0.3630 - mean_squared_error: 0.3630 - val_loss: 0.4139 - val_mean_squared_error: 0.4139\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3539 - mean_squared_error: 0.3539\n",
      "Epoch 00015: val_loss improved from 0.41386 to 0.38579, saving model to model.h5\n",
      "248/248 [==============================] - 0s 225us/sample - loss: 0.3280 - mean_squared_error: 0.3280 - val_loss: 0.3858 - val_mean_squared_error: 0.3858\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2635 - mean_squared_error: 0.2635\n",
      "Epoch 00016: val_loss improved from 0.38579 to 0.35958, saving model to model.h5\n",
      "248/248 [==============================] - 0s 286us/sample - loss: 0.2925 - mean_squared_error: 0.2925 - val_loss: 0.3596 - val_mean_squared_error: 0.3596\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2663 - mean_squared_error: 0.2663\n",
      "Epoch 00017: val_loss improved from 0.35958 to 0.33194, saving model to model.h5\n",
      "248/248 [==============================] - 0s 282us/sample - loss: 0.2673 - mean_squared_error: 0.2673 - val_loss: 0.3319 - val_mean_squared_error: 0.3319\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2163 - mean_squared_error: 0.2163\n",
      "Epoch 00018: val_loss improved from 0.33194 to 0.30668, saving model to model.h5\n",
      "248/248 [==============================] - 0s 322us/sample - loss: 0.2409 - mean_squared_error: 0.2409 - val_loss: 0.3067 - val_mean_squared_error: 0.3067\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2140 - mean_squared_error: 0.2140\n",
      "Epoch 00019: val_loss improved from 0.30668 to 0.28234, saving model to model.h5\n",
      "248/248 [==============================] - 0s 383us/sample - loss: 0.2150 - mean_squared_error: 0.2150 - val_loss: 0.2823 - val_mean_squared_error: 0.2823\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1984 - mean_squared_error: 0.1984\n",
      "Epoch 00020: val_loss improved from 0.28234 to 0.26107, saving model to model.h5\n",
      "248/248 [==============================] - 0s 296us/sample - loss: 0.1874 - mean_squared_error: 0.1874 - val_loss: 0.2611 - val_mean_squared_error: 0.2611\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1766 - mean_squared_error: 0.1766\n",
      "Epoch 00021: val_loss improved from 0.26107 to 0.23981, saving model to model.h5\n",
      "248/248 [==============================] - 0s 298us/sample - loss: 0.1709 - mean_squared_error: 0.1709 - val_loss: 0.2398 - val_mean_squared_error: 0.2398\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1523 - mean_squared_error: 0.1523\n",
      "Epoch 00022: val_loss improved from 0.23981 to 0.21938, saving model to model.h5\n",
      "248/248 [==============================] - 0s 285us/sample - loss: 0.1501 - mean_squared_error: 0.1501 - val_loss: 0.2194 - val_mean_squared_error: 0.2194\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1185 - mean_squared_error: 0.1185\n",
      "Epoch 00023: val_loss improved from 0.21938 to 0.19957, saving model to model.h5\n",
      "248/248 [==============================] - 0s 300us/sample - loss: 0.1315 - mean_squared_error: 0.1315 - val_loss: 0.1996 - val_mean_squared_error: 0.1996\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1237 - mean_squared_error: 0.1237\n",
      "Epoch 00024: val_loss improved from 0.19957 to 0.18285, saving model to model.h5\n",
      "248/248 [==============================] - 0s 362us/sample - loss: 0.1179 - mean_squared_error: 0.1179 - val_loss: 0.1828 - val_mean_squared_error: 0.1828\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1173 - mean_squared_error: 0.1173\n",
      "Epoch 00025: val_loss improved from 0.18285 to 0.16595, saving model to model.h5\n",
      "248/248 [==============================] - 0s 309us/sample - loss: 0.1126 - mean_squared_error: 0.1126 - val_loss: 0.1659 - val_mean_squared_error: 0.1659\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0848 - mean_squared_error: 0.0848\n",
      "Epoch 00026: val_loss improved from 0.16595 to 0.15244, saving model to model.h5\n",
      "248/248 [==============================] - 0s 256us/sample - loss: 0.0964 - mean_squared_error: 0.0964 - val_loss: 0.1524 - val_mean_squared_error: 0.1524\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1190 - mean_squared_error: 0.1190\n",
      "Epoch 00027: val_loss improved from 0.15244 to 0.13907, saving model to model.h5\n",
      "248/248 [==============================] - 0s 383us/sample - loss: 0.0862 - mean_squared_error: 0.0862 - val_loss: 0.1391 - val_mean_squared_error: 0.1391\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0778 - mean_squared_error: 0.0778\n",
      "Epoch 00028: val_loss improved from 0.13907 to 0.12755, saving model to model.h5\n",
      "248/248 [==============================] - 0s 303us/sample - loss: 0.0791 - mean_squared_error: 0.0791 - val_loss: 0.1276 - val_mean_squared_error: 0.1276\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0616 - mean_squared_error: 0.0616\n",
      "Epoch 00029: val_loss improved from 0.12755 to 0.11635, saving model to model.h5\n",
      "248/248 [==============================] - 0s 289us/sample - loss: 0.0705 - mean_squared_error: 0.0705 - val_loss: 0.1163 - val_mean_squared_error: 0.1163\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0727 - mean_squared_error: 0.0727\n",
      "Epoch 00030: val_loss improved from 0.11635 to 0.10621, saving model to model.h5\n",
      "248/248 [==============================] - 0s 223us/sample - loss: 0.0630 - mean_squared_error: 0.0630 - val_loss: 0.1062 - val_mean_squared_error: 0.1062\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0693 - mean_squared_error: 0.0693\n",
      "Epoch 00031: val_loss improved from 0.10621 to 0.09693, saving model to model.h5\n",
      "248/248 [==============================] - 0s 282us/sample - loss: 0.0582 - mean_squared_error: 0.0582 - val_loss: 0.0969 - val_mean_squared_error: 0.0969\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0620 - mean_squared_error: 0.0620\n",
      "Epoch 00032: val_loss improved from 0.09693 to 0.08759, saving model to model.h5\n",
      "248/248 [==============================] - 0s 212us/sample - loss: 0.0563 - mean_squared_error: 0.0563 - val_loss: 0.0876 - val_mean_squared_error: 0.0876\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0575 - mean_squared_error: 0.0575\n",
      "Epoch 00033: val_loss improved from 0.08759 to 0.08174, saving model to model.h5\n",
      "248/248 [==============================] - 0s 192us/sample - loss: 0.0539 - mean_squared_error: 0.0539 - val_loss: 0.0817 - val_mean_squared_error: 0.0817\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0470 - mean_squared_error: 0.0470\n",
      "Epoch 00034: val_loss improved from 0.08174 to 0.07524, saving model to model.h5\n",
      "248/248 [==============================] - 0s 225us/sample - loss: 0.0492 - mean_squared_error: 0.0492 - val_loss: 0.0752 - val_mean_squared_error: 0.0752\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0410 - mean_squared_error: 0.0410\n",
      "Epoch 00035: val_loss improved from 0.07524 to 0.06904, saving model to model.h5\n",
      "248/248 [==============================] - 0s 229us/sample - loss: 0.0464 - mean_squared_error: 0.0464 - val_loss: 0.0690 - val_mean_squared_error: 0.0690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0456 - mean_squared_error: 0.0456\n",
      "Epoch 00036: val_loss improved from 0.06904 to 0.06540, saving model to model.h5\n",
      "248/248 [==============================] - 0s 245us/sample - loss: 0.0455 - mean_squared_error: 0.0455 - val_loss: 0.0654 - val_mean_squared_error: 0.0654\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0576 - mean_squared_error: 0.0576\n",
      "Epoch 00037: val_loss improved from 0.06540 to 0.05898, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 0.0436 - mean_squared_error: 0.0436 - val_loss: 0.0590 - val_mean_squared_error: 0.0590\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0368 - mean_squared_error: 0.0368\n",
      "Epoch 00038: val_loss improved from 0.05898 to 0.05555, saving model to model.h5\n",
      "248/248 [==============================] - 0s 223us/sample - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0555 - val_mean_squared_error: 0.0555\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0440 - mean_squared_error: 0.0440\n",
      "Epoch 00039: val_loss improved from 0.05555 to 0.05248, saving model to model.h5\n",
      "248/248 [==============================] - 0s 229us/sample - loss: 0.0397 - mean_squared_error: 0.0397 - val_loss: 0.0525 - val_mean_squared_error: 0.0525\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0390 - mean_squared_error: 0.0390\n",
      "Epoch 00040: val_loss improved from 0.05248 to 0.05080, saving model to model.h5\n",
      "248/248 [==============================] - 0s 302us/sample - loss: 0.0388 - mean_squared_error: 0.0388 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0447 - mean_squared_error: 0.0447\n",
      "Epoch 00041: val_loss improved from 0.05080 to 0.04980, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.0395 - mean_squared_error: 0.0395 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0406 - mean_squared_error: 0.0406\n",
      "Epoch 00042: val_loss improved from 0.04980 to 0.04852, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00043: val_loss improved from 0.04852 to 0.04838, saving model to model.h5\n",
      "248/248 [==============================] - 0s 173us/sample - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00044: val_loss improved from 0.04838 to 0.04627, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0403 - mean_squared_error: 0.0403 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00045: val_loss improved from 0.04627 to 0.04585, saving model to model.h5\n",
      "248/248 [==============================] - 0s 185us/sample - loss: 0.0394 - mean_squared_error: 0.0394 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0411 - mean_squared_error: 0.0411\n",
      "Epoch 00046: val_loss improved from 0.04585 to 0.04443, saving model to model.h5\n",
      "248/248 [==============================] - 0s 169us/sample - loss: 0.0379 - mean_squared_error: 0.0379 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0537 - mean_squared_error: 0.0537\n",
      "Epoch 00047: val_loss improved from 0.04443 to 0.04224, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00048: val_loss improved from 0.04224 to 0.04213, saving model to model.h5\n",
      "248/248 [==============================] - 0s 138us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 00049: val_loss improved from 0.04213 to 0.04127, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0320 - mean_squared_error: 0.0320 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00050: val_loss did not improve from 0.04127\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0320 - mean_squared_error: 0.0320 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00051: val_loss did not improve from 0.04127\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0400 - mean_squared_error: 0.0400\n",
      "Epoch 00052: val_loss improved from 0.04127 to 0.03991, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0319 - mean_squared_error: 0.0319 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0173 - mean_squared_error: 0.0173\n",
      "Epoch 00053: val_loss did not improve from 0.03991\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0343 - mean_squared_error: 0.0343\n",
      "Epoch 00054: val_loss did not improve from 0.03991\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0513 - mean_squared_error: 0.0513\n",
      "Epoch 00055: val_loss improved from 0.03991 to 0.03815, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00056: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00057: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0377 - mean_squared_error: 0.0377\n",
      "Epoch 00058: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0427 - mean_squared_error: 0.0427\n",
      "Epoch 00059: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0362 - mean_squared_error: 0.0362\n",
      "Epoch 00060: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0171 - mean_squared_error: 0.0171\n",
      "Epoch 00061: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00062: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00063: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0366 - mean_squared_error: 0.0366\n",
      "Epoch 00064: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0469 - mean_squared_error: 0.0469\n",
      "Epoch 00065: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00066: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Epoch 00067: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00068: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00069: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Epoch 00070: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00071: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0135 - mean_squared_error: 0.0135\n",
      "Epoch 00072: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00073: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00074: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Epoch 00075: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0231 - mean_squared_error: 0.0231\n",
      "Epoch 00076: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00077: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00078: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00079: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 00080: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0097 - mean_squared_error: 0.0097\n",
      "Epoch 00081: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00082: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00083: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0152 - mean_squared_error: 0.0152\n",
      "Epoch 00084: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0159 - mean_squared_error: 0.0159\n",
      "Epoch 00085: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00086: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 87/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0382 - mean_squared_error: 0.0382\n",
      "Epoch 00087: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 49us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00088: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00089: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00090: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00091: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00092: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 00093: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00094: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00095: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00096: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0137 - mean_squared_error: 0.0137\n",
      "Epoch 00097: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0184 - mean_squared_error: 0.0184\n",
      "Epoch 00098: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00099: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00100: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00101: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00102: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00103: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00104: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0132 - mean_squared_error: 0.0132\n",
      "Epoch 00105: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00106: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00107: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00108: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0181 - mean_squared_error: 0.0181\n",
      "Epoch 00109: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0106 - mean_squared_error: 0.0106\n",
      "Epoch 00110: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0420 - mean_squared_error: 0.0420\n",
      "Epoch 00111: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0184 - mean_squared_error: 0.0184\n",
      "Epoch 00112: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 113/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0117 - mean_squared_error: 0.0117\n",
      "Epoch 00113: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00114: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00115: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Epoch 00116: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00117: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00118: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0367 - mean_squared_error: 0.0367\n",
      "Epoch 00119: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00120: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 57us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0400 - mean_squared_error: 0.0400\n",
      "Epoch 00121: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00122: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00123: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00124: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00125: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00126: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00127: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0181 - mean_squared_error: 0.0181\n",
      "Epoch 00128: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00129: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 130/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00130: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0100 - mean_squared_error: 0.0100\n",
      "Epoch 00131: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 132/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00132: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 133/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00133: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 134/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00134: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 135/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00135: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 136/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00136: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 137/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 00137: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 138/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0125 - mean_squared_error: 0.0125\n",
      "Epoch 00138: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 139/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0155 - mean_squared_error: 0.0155\n",
      "Epoch 00139: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 140/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0108 - mean_squared_error: 0.0108\n",
      "Epoch 00140: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 141/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00141: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 142/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00142: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 143/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00143: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 144/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00144: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 145/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0124 - mean_squared_error: 0.0124\n",
      "Epoch 00145: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 146/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.0185\n",
      "Epoch 00146: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 147/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00147: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 148/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00148: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 149/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00149: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 150/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00150: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 151/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00151: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 152/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00152: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 153/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00153: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 154/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00154: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 155/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00155: val_loss did not improve from 0.03815\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Running trial 3\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.3385 - mean_squared_error: 1.3385\n",
      "Epoch 00001: val_loss improved from inf to 0.77200, saving model to model.h5\n",
      "248/248 [==============================] - 1s 4ms/sample - loss: 1.1758 - mean_squared_error: 1.1758 - val_loss: 0.7720 - val_mean_squared_error: 0.7720\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.0090 - mean_squared_error: 1.0090\n",
      "Epoch 00002: val_loss improved from 0.77200 to 0.73825, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 1.0352 - mean_squared_error: 1.0352 - val_loss: 0.7382 - val_mean_squared_error: 0.7382\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.1054 - mean_squared_error: 1.1054\n",
      "Epoch 00003: val_loss improved from 0.73825 to 0.70614, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.9435 - mean_squared_error: 0.9435 - val_loss: 0.7061 - val_mean_squared_error: 0.7061\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9242 - mean_squared_error: 0.9242\n",
      "Epoch 00004: val_loss improved from 0.70614 to 0.67735, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.8649 - mean_squared_error: 0.8649 - val_loss: 0.6774 - val_mean_squared_error: 0.6774\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8434 - mean_squared_error: 0.8434\n",
      "Epoch 00005: val_loss improved from 0.67735 to 0.64923, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.7925 - mean_squared_error: 0.7925 - val_loss: 0.6492 - val_mean_squared_error: 0.6492\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6298 - mean_squared_error: 0.6298\n",
      "Epoch 00006: val_loss improved from 0.64923 to 0.62518, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.7307 - mean_squared_error: 0.7307 - val_loss: 0.6252 - val_mean_squared_error: 0.6252\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6994 - mean_squared_error: 0.6994\n",
      "Epoch 00007: val_loss improved from 0.62518 to 0.60225, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.6756 - mean_squared_error: 0.6756 - val_loss: 0.6023 - val_mean_squared_error: 0.6023\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6311 - mean_squared_error: 0.6311\n",
      "Epoch 00008: val_loss improved from 0.60225 to 0.57883, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.6290 - mean_squared_error: 0.6290 - val_loss: 0.5788 - val_mean_squared_error: 0.5788\n",
      "Epoch 9/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6228 - mean_squared_error: 0.6228\n",
      "Epoch 00009: val_loss improved from 0.57883 to 0.55567, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.5740 - mean_squared_error: 0.5740 - val_loss: 0.5557 - val_mean_squared_error: 0.5557\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4706 - mean_squared_error: 0.4706\n",
      "Epoch 00010: val_loss improved from 0.55567 to 0.53157, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.5286 - mean_squared_error: 0.5286 - val_loss: 0.5316 - val_mean_squared_error: 0.5316\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5255 - mean_squared_error: 0.5255\n",
      "Epoch 00011: val_loss improved from 0.53157 to 0.51081, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.4834 - mean_squared_error: 0.4834 - val_loss: 0.5108 - val_mean_squared_error: 0.5108\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4353 - mean_squared_error: 0.4353\n",
      "Epoch 00012: val_loss improved from 0.51081 to 0.48751, saving model to model.h5\n",
      "248/248 [==============================] - 0s 120us/sample - loss: 0.4410 - mean_squared_error: 0.4410 - val_loss: 0.4875 - val_mean_squared_error: 0.4875\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4174 - mean_squared_error: 0.4174\n",
      "Epoch 00013: val_loss improved from 0.48751 to 0.46326, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.4073 - mean_squared_error: 0.4073 - val_loss: 0.4633 - val_mean_squared_error: 0.4633\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3860 - mean_squared_error: 0.3860\n",
      "Epoch 00014: val_loss improved from 0.46326 to 0.43983, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.3692 - mean_squared_error: 0.3692 - val_loss: 0.4398 - val_mean_squared_error: 0.4398\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3466 - mean_squared_error: 0.3466\n",
      "Epoch 00015: val_loss improved from 0.43983 to 0.41674, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.3392 - mean_squared_error: 0.3392 - val_loss: 0.4167 - val_mean_squared_error: 0.4167\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2923 - mean_squared_error: 0.2923\n",
      "Epoch 00016: val_loss improved from 0.41674 to 0.39236, saving model to model.h5\n",
      "248/248 [==============================] - 0s 131us/sample - loss: 0.3028 - mean_squared_error: 0.3028 - val_loss: 0.3924 - val_mean_squared_error: 0.3924\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2583 - mean_squared_error: 0.2583\n",
      "Epoch 00017: val_loss improved from 0.39236 to 0.36996, saving model to model.h5\n",
      "248/248 [==============================] - 0s 119us/sample - loss: 0.2829 - mean_squared_error: 0.2829 - val_loss: 0.3700 - val_mean_squared_error: 0.3700\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2518 - mean_squared_error: 0.2518\n",
      "Epoch 00018: val_loss improved from 0.36996 to 0.34550, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.2566 - mean_squared_error: 0.2566 - val_loss: 0.3455 - val_mean_squared_error: 0.3455\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2651 - mean_squared_error: 0.2651\n",
      "Epoch 00019: val_loss improved from 0.34550 to 0.32269, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.2307 - mean_squared_error: 0.2307 - val_loss: 0.3227 - val_mean_squared_error: 0.3227\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2059 - mean_squared_error: 0.2059\n",
      "Epoch 00020: val_loss improved from 0.32269 to 0.30059, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.2031 - mean_squared_error: 0.2031 - val_loss: 0.3006 - val_mean_squared_error: 0.3006\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1677 - mean_squared_error: 0.1677\n",
      "Epoch 00021: val_loss improved from 0.30059 to 0.27975, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.1818 - mean_squared_error: 0.1818 - val_loss: 0.2797 - val_mean_squared_error: 0.2797\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1638 - mean_squared_error: 0.1638\n",
      "Epoch 00022: val_loss improved from 0.27975 to 0.25939, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.1707 - mean_squared_error: 0.1707 - val_loss: 0.2594 - val_mean_squared_error: 0.2594\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1500 - mean_squared_error: 0.1500\n",
      "Epoch 00023: val_loss improved from 0.25939 to 0.24013, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.1488 - mean_squared_error: 0.1488 - val_loss: 0.2401 - val_mean_squared_error: 0.2401\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1378 - mean_squared_error: 0.1378\n",
      "Epoch 00024: val_loss improved from 0.24013 to 0.22100, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.1369 - mean_squared_error: 0.1369 - val_loss: 0.2210 - val_mean_squared_error: 0.2210\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1246 - mean_squared_error: 0.1246\n",
      "Epoch 00025: val_loss improved from 0.22100 to 0.20393, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.1219 - mean_squared_error: 0.1219 - val_loss: 0.2039 - val_mean_squared_error: 0.2039\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1158 - mean_squared_error: 0.1158\n",
      "Epoch 00026: val_loss improved from 0.20393 to 0.18828, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.1118 - mean_squared_error: 0.1118 - val_loss: 0.1883 - val_mean_squared_error: 0.1883\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1094 - mean_squared_error: 0.1094\n",
      "Epoch 00027: val_loss improved from 0.18828 to 0.17232, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0960 - mean_squared_error: 0.0960 - val_loss: 0.1723 - val_mean_squared_error: 0.1723\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0994 - mean_squared_error: 0.0994\n",
      "Epoch 00028: val_loss improved from 0.17232 to 0.15819, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0913 - mean_squared_error: 0.0913 - val_loss: 0.1582 - val_mean_squared_error: 0.1582\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1023 - mean_squared_error: 0.1023\n",
      "Epoch 00029: val_loss improved from 0.15819 to 0.14526, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0793 - mean_squared_error: 0.0793 - val_loss: 0.1453 - val_mean_squared_error: 0.1453\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0840 - mean_squared_error: 0.0840\n",
      "Epoch 00030: val_loss improved from 0.14526 to 0.13412, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0778 - mean_squared_error: 0.0778 - val_loss: 0.1341 - val_mean_squared_error: 0.1341\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0615 - mean_squared_error: 0.0615\n",
      "Epoch 00031: val_loss improved from 0.13412 to 0.12298, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0680 - mean_squared_error: 0.0680 - val_loss: 0.1230 - val_mean_squared_error: 0.1230\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0412 - mean_squared_error: 0.0412\n",
      "Epoch 00032: val_loss improved from 0.12298 to 0.11269, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0585 - mean_squared_error: 0.0585 - val_loss: 0.1127 - val_mean_squared_error: 0.1127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0538 - mean_squared_error: 0.0538\n",
      "Epoch 00033: val_loss improved from 0.11269 to 0.10253, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0589 - mean_squared_error: 0.0589 - val_loss: 0.1025 - val_mean_squared_error: 0.1025\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0600 - mean_squared_error: 0.0600\n",
      "Epoch 00034: val_loss improved from 0.10253 to 0.09330, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0571 - mean_squared_error: 0.0571 - val_loss: 0.0933 - val_mean_squared_error: 0.0933\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0528 - mean_squared_error: 0.0528\n",
      "Epoch 00035: val_loss improved from 0.09330 to 0.08676, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0484 - mean_squared_error: 0.0484 - val_loss: 0.0868 - val_mean_squared_error: 0.0868\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0449 - mean_squared_error: 0.0449\n",
      "Epoch 00036: val_loss improved from 0.08676 to 0.08217, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0473 - mean_squared_error: 0.0473 - val_loss: 0.0822 - val_mean_squared_error: 0.0822\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0573 - mean_squared_error: 0.0573\n",
      "Epoch 00037: val_loss improved from 0.08217 to 0.07584, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0499 - mean_squared_error: 0.0499 - val_loss: 0.0758 - val_mean_squared_error: 0.0758\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0428 - mean_squared_error: 0.0428\n",
      "Epoch 00038: val_loss improved from 0.07584 to 0.07216, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0452 - mean_squared_error: 0.0452 - val_loss: 0.0722 - val_mean_squared_error: 0.0722\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0502 - mean_squared_error: 0.0502\n",
      "Epoch 00039: val_loss improved from 0.07216 to 0.06883, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0424 - mean_squared_error: 0.0424 - val_loss: 0.0688 - val_mean_squared_error: 0.0688\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Epoch 00040: val_loss improved from 0.06883 to 0.06622, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0434 - mean_squared_error: 0.0434 - val_loss: 0.0662 - val_mean_squared_error: 0.0662\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00041: val_loss improved from 0.06622 to 0.06349, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0421 - mean_squared_error: 0.0421 - val_loss: 0.0635 - val_mean_squared_error: 0.0635\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00042: val_loss improved from 0.06349 to 0.06192, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0467 - mean_squared_error: 0.0467 - val_loss: 0.0619 - val_mean_squared_error: 0.0619\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0451 - mean_squared_error: 0.0451\n",
      "Epoch 00043: val_loss improved from 0.06192 to 0.06120, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0425 - mean_squared_error: 0.0425 - val_loss: 0.0612 - val_mean_squared_error: 0.0612\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Epoch 00044: val_loss improved from 0.06120 to 0.05749, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0575 - val_mean_squared_error: 0.0575\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0295 - mean_squared_error: 0.0295\n",
      "Epoch 00045: val_loss improved from 0.05749 to 0.05614, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0378 - mean_squared_error: 0.0378 - val_loss: 0.0561 - val_mean_squared_error: 0.0561\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0492 - mean_squared_error: 0.0492\n",
      "Epoch 00046: val_loss improved from 0.05614 to 0.05360, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0536 - val_mean_squared_error: 0.0536\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00047: val_loss did not improve from 0.05360\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00048: val_loss improved from 0.05360 to 0.05349, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0535 - val_mean_squared_error: 0.0535\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0383 - mean_squared_error: 0.0383\n",
      "Epoch 00049: val_loss improved from 0.05349 to 0.05034, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0333 - mean_squared_error: 0.0333 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00050: val_loss did not improve from 0.05034\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0516 - val_mean_squared_error: 0.0516\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00051: val_loss did not improve from 0.05034\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0330 - mean_squared_error: 0.0330 - val_loss: 0.0534 - val_mean_squared_error: 0.0534\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00052: val_loss did not improve from 0.05034\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0396 - mean_squared_error: 0.0396\n",
      "Epoch 00053: val_loss improved from 0.05034 to 0.04870, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0413 - mean_squared_error: 0.0413\n",
      "Epoch 00054: val_loss improved from 0.04870 to 0.04835, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Epoch 00055: val_loss improved from 0.04835 to 0.04826, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0473 - mean_squared_error: 0.0473\n",
      "Epoch 00056: val_loss improved from 0.04826 to 0.04747, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 57/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0510 - mean_squared_error: 0.0510\n",
      "Epoch 00057: val_loss improved from 0.04747 to 0.04729, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0375 - mean_squared_error: 0.0375 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00058: val_loss improved from 0.04729 to 0.04560, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00059: val_loss did not improve from 0.04560\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00060: val_loss did not improve from 0.04560\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00061: val_loss did not improve from 0.04560\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Epoch 00062: val_loss did not improve from 0.04560\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00063: val_loss did not improve from 0.04560\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Epoch 00064: val_loss did not improve from 0.04560\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Epoch 00065: val_loss did not improve from 0.04560\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00066: val_loss did not improve from 0.04560\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0395 - mean_squared_error: 0.0395\n",
      "Epoch 00067: val_loss did not improve from 0.04560\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0319 - mean_squared_error: 0.0319 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00068: val_loss did not improve from 0.04560\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00069: val_loss did not improve from 0.04560\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00070: val_loss improved from 0.04560 to 0.04464, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00071: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Epoch 00072: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00073: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00074: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0419 - mean_squared_error: 0.0419\n",
      "Epoch 00075: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00076: val_loss improved from 0.04464 to 0.04417, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00077: val_loss did not improve from 0.04417\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00078: val_loss did not improve from 0.04417\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00079: val_loss did not improve from 0.04417\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0153 - mean_squared_error: 0.0153\n",
      "Epoch 00080: val_loss did not improve from 0.04417\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 00081: val_loss did not improve from 0.04417\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0479 - mean_squared_error: 0.0479\n",
      "Epoch 00082: val_loss did not improve from 0.04417\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00083: val_loss improved from 0.04417 to 0.04328, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00084: val_loss did not improve from 0.04328\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00085: val_loss improved from 0.04328 to 0.04285, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0170 - mean_squared_error: 0.0170\n",
      "Epoch 00086: val_loss did not improve from 0.04285\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00087: val_loss did not improve from 0.04285\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Epoch 00088: val_loss did not improve from 0.04285\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00089: val_loss did not improve from 0.04285\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Epoch 00090: val_loss did not improve from 0.04285\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00091: val_loss did not improve from 0.04285\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00092: val_loss did not improve from 0.04285\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00093: val_loss did not improve from 0.04285\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00094: val_loss improved from 0.04285 to 0.04259, saving model to model.h5\n",
      "248/248 [==============================] - 0s 177us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00095: val_loss improved from 0.04259 to 0.04189, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00096: val_loss improved from 0.04189 to 0.04113, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00097: val_loss did not improve from 0.04113\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00098: val_loss did not improve from 0.04113\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00099: val_loss did not improve from 0.04113\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00100: val_loss did not improve from 0.04113\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0159 - mean_squared_error: 0.0159\n",
      "Epoch 00101: val_loss did not improve from 0.04113\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00102: val_loss improved from 0.04113 to 0.04112, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00103: val_loss improved from 0.04112 to 0.04069, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00104: val_loss did not improve from 0.04069\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00105: val_loss did not improve from 0.04069\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00106: val_loss did not improve from 0.04069\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00107: val_loss did not improve from 0.04069\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00108: val_loss did not improve from 0.04069\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0158 - mean_squared_error: 0.0158\n",
      "Epoch 00109: val_loss did not improve from 0.04069\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0294 - mean_squared_error: 0.0294\n",
      "Epoch 00110: val_loss did not improve from 0.04069\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00111: val_loss did not improve from 0.04069\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00112: val_loss improved from 0.04069 to 0.04056, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Epoch 00113: val_loss did not improve from 0.04056\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00114: val_loss did not improve from 0.04056\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00115: val_loss did not improve from 0.04056\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00116: val_loss did not improve from 0.04056\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 00117: val_loss did not improve from 0.04056\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00118: val_loss did not improve from 0.04056\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00119: val_loss did not improve from 0.04056\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.0185\n",
      "Epoch 00120: val_loss did not improve from 0.04056\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00121: val_loss did not improve from 0.04056\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.0185\n",
      "Epoch 00122: val_loss did not improve from 0.04056\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0148 - mean_squared_error: 0.0148\n",
      "Epoch 00123: val_loss did not improve from 0.04056\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00124: val_loss did not improve from 0.04056\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00125: val_loss did not improve from 0.04056\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00126: val_loss did not improve from 0.04056\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0295 - mean_squared_error: 0.0295\n",
      "Epoch 00127: val_loss did not improve from 0.04056\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00128: val_loss did not improve from 0.04056\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00129: val_loss did not improve from 0.04056\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 130/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0345 - mean_squared_error: 0.0345\n",
      "Epoch 00130: val_loss did not improve from 0.04056\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00131: val_loss did not improve from 0.04056\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 132/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00132: val_loss did not improve from 0.04056\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 133/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00133: val_loss did not improve from 0.04056\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 134/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00134: val_loss did not improve from 0.04056\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00135: val_loss improved from 0.04056 to 0.04023, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 136/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Epoch 00136: val_loss did not improve from 0.04023\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 137/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00137: val_loss did not improve from 0.04023\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 138/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00138: val_loss did not improve from 0.04023\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 139/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00139: val_loss did not improve from 0.04023\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 140/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Epoch 00140: val_loss did not improve from 0.04023\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 141/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0110 - mean_squared_error: 0.0110\n",
      "Epoch 00141: val_loss did not improve from 0.04023\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 142/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0176 - mean_squared_error: 0.0176\n",
      "Epoch 00142: val_loss did not improve from 0.04023\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 143/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0112 - mean_squared_error: 0.0112\n",
      "Epoch 00143: val_loss did not improve from 0.04023\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 144/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00144: val_loss did not improve from 0.04023\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 145/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00145: val_loss did not improve from 0.04023\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 146/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00146: val_loss improved from 0.04023 to 0.04001, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 147/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00147: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 148/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00148: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 149/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0134 - mean_squared_error: 0.0134\n",
      "Epoch 00149: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 150/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0307 - mean_squared_error: 0.0307\n",
      "Epoch 00150: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 151/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0187 - mean_squared_error: 0.0187\n",
      "Epoch 00151: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 47us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 152/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00152: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 153/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0072 - mean_squared_error: 0.0072\n",
      "Epoch 00153: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 154/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00154: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 155/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00155: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 156/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0167 - mean_squared_error: 0.0167\n",
      "Epoch 00156: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 157/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0135 - mean_squared_error: 0.0135\n",
      "Epoch 00157: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 158/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0147 - mean_squared_error: 0.0147\n",
      "Epoch 00158: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 159/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00159: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 160/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 00160: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00161: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 162/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0088 - mean_squared_error: 0.0088\n",
      "Epoch 00162: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 163/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00163: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 164/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00164: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 165/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00165: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 166/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0167 - mean_squared_error: 0.0167\n",
      "Epoch 00166: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 167/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00167: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 168/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0176 - mean_squared_error: 0.0176\n",
      "Epoch 00168: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 169/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00169: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 170/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00170: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 171/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00171: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 172/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.0185\n",
      "Epoch 00172: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 173/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00173: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 38us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 174/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.0172\n",
      "Epoch 00174: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 175/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00175: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 176/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00176: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 177/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00177: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 178/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0177 - mean_squared_error: 0.0177\n",
      "Epoch 00178: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 179/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.0172\n",
      "Epoch 00179: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 180/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00180: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 181/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00181: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 182/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0157 - mean_squared_error: 0.0157\n",
      "Epoch 00182: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 183/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00183: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 184/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00184: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 185/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0114 - mean_squared_error: 0.0114\n",
      "Epoch 00185: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 186/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00186: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 58us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 187/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00187: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 188/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0161 - mean_squared_error: 0.0161\n",
      "Epoch 00188: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 189/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0131 - mean_squared_error: 0.0131\n",
      "Epoch 00189: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 190/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00190: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 191/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0128 - mean_squared_error: 0.0128\n",
      "Epoch 00191: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 192/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00192: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 58us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 193/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0104 - mean_squared_error: 0.0104\n",
      "Epoch 00193: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 194/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0133 - mean_squared_error: 0.0133\n",
      "Epoch 00194: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 195/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00195: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 196/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00196: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 197/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0127 - mean_squared_error: 0.0127\n",
      "Epoch 00197: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 198/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00198: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 199/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00199: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 200/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0090 - mean_squared_error: 0.0090\n",
      "Epoch 00200: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 201/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00201: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 202/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0369 - mean_squared_error: 0.0369\n",
      "Epoch 00202: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 203/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00203: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 204/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0143 - mean_squared_error: 0.0143\n",
      "Epoch 00204: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 205/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00205: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 206/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0178 - mean_squared_error: 0.0178\n",
      "Epoch 00206: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 207/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00207: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 208/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0093 - mean_squared_error: 0.0093\n",
      "Epoch 00208: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 209/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00209: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 210/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00210: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 211/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00211: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 212/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0187 - mean_squared_error: 0.0187\n",
      "Epoch 00212: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 213/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00213: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 214/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0125 - mean_squared_error: 0.0125\n",
      "Epoch 00214: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 215/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0137 - mean_squared_error: 0.0137\n",
      "Epoch 00215: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 216/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00216: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 217/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00217: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 218/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00218: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 219/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00219: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 220/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00220: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 221/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 00221: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0513 - val_mean_squared_error: 0.0513\n",
      "Epoch 222/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0134 - mean_squared_error: 0.0134\n",
      "Epoch 00222: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0530 - val_mean_squared_error: 0.0530\n",
      "Epoch 223/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00223: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 224/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00224: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 225/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0096 - mean_squared_error: 0.0096\n",
      "Epoch 00225: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 226/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0096 - mean_squared_error: 0.0096\n",
      "Epoch 00226: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 227/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0111 - mean_squared_error: 0.0111\n",
      "Epoch 00227: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0482 - val_mean_squared_error: 0.0482\n",
      "Epoch 228/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00228: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 229/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00229: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0175 - mean_squared_error: 0.0175 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 230/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00230: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 231/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00231: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 232/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00232: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 233/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00233: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 234/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00234: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 235/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0123 - mean_squared_error: 0.0123\n",
      "Epoch 00235: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 236/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00236: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 237/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0177 - mean_squared_error: 0.0177\n",
      "Epoch 00237: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0530 - val_mean_squared_error: 0.0530\n",
      "Epoch 238/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00238: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0513 - val_mean_squared_error: 0.0513\n",
      "Epoch 239/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00239: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 240/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0142 - mean_squared_error: 0.0142\n",
      "Epoch 00240: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 241/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00241: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 242/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00242: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 243/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00243: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 244/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00244: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 245/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0176 - mean_squared_error: 0.0176\n",
      "Epoch 00245: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 246/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00246: val_loss did not improve from 0.04001\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Running trial 4\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.0081 - mean_squared_error: 1.0081\n",
      "Epoch 00001: val_loss improved from inf to 0.72603, saving model to model.h5\n",
      "248/248 [==============================] - 1s 4ms/sample - loss: 1.1696 - mean_squared_error: 1.1696 - val_loss: 0.7260 - val_mean_squared_error: 0.7260\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.0416 - mean_squared_error: 1.0416\n",
      "Epoch 00002: val_loss improved from 0.72603 to 0.69118, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 1.0320 - mean_squared_error: 1.0320 - val_loss: 0.6912 - val_mean_squared_error: 0.6912\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9969 - mean_squared_error: 0.9969\n",
      "Epoch 00003: val_loss improved from 0.69118 to 0.65817, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.9484 - mean_squared_error: 0.9484 - val_loss: 0.6582 - val_mean_squared_error: 0.6582\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9369 - mean_squared_error: 0.9369\n",
      "Epoch 00004: val_loss improved from 0.65817 to 0.62952, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.8616 - mean_squared_error: 0.8616 - val_loss: 0.6295 - val_mean_squared_error: 0.6295\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7481 - mean_squared_error: 0.7481\n",
      "Epoch 00005: val_loss improved from 0.62952 to 0.60510, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.7989 - mean_squared_error: 0.7989 - val_loss: 0.6051 - val_mean_squared_error: 0.6051\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8598 - mean_squared_error: 0.8598\n",
      "Epoch 00006: val_loss improved from 0.60510 to 0.58000, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.7396 - mean_squared_error: 0.7396 - val_loss: 0.5800 - val_mean_squared_error: 0.5800\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5999 - mean_squared_error: 0.5999\n",
      "Epoch 00007: val_loss improved from 0.58000 to 0.55559, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.6804 - mean_squared_error: 0.6804 - val_loss: 0.5556 - val_mean_squared_error: 0.5556\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5810 - mean_squared_error: 0.5810\n",
      "Epoch 00008: val_loss improved from 0.55559 to 0.53175, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.6270 - mean_squared_error: 0.6270 - val_loss: 0.5318 - val_mean_squared_error: 0.5318\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6051 - mean_squared_error: 0.6051\n",
      "Epoch 00009: val_loss improved from 0.53175 to 0.50848, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.5692 - mean_squared_error: 0.5692 - val_loss: 0.5085 - val_mean_squared_error: 0.5085\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4711 - mean_squared_error: 0.4711\n",
      "Epoch 00010: val_loss improved from 0.50848 to 0.48521, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.5222 - mean_squared_error: 0.5222 - val_loss: 0.4852 - val_mean_squared_error: 0.4852\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4128 - mean_squared_error: 0.4128\n",
      "Epoch 00011: val_loss improved from 0.48521 to 0.46240, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.4750 - mean_squared_error: 0.4750 - val_loss: 0.4624 - val_mean_squared_error: 0.4624\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4491 - mean_squared_error: 0.4491\n",
      "Epoch 00012: val_loss improved from 0.46240 to 0.43900, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.4359 - mean_squared_error: 0.4359 - val_loss: 0.4390 - val_mean_squared_error: 0.4390\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3508 - mean_squared_error: 0.3508\n",
      "Epoch 00013: val_loss improved from 0.43900 to 0.41682, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.3930 - mean_squared_error: 0.3930 - val_loss: 0.4168 - val_mean_squared_error: 0.4168\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3940 - mean_squared_error: 0.3940\n",
      "Epoch 00014: val_loss improved from 0.41682 to 0.39459, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.3655 - mean_squared_error: 0.3655 - val_loss: 0.3946 - val_mean_squared_error: 0.3946\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2673 - mean_squared_error: 0.2673\n",
      "Epoch 00015: val_loss improved from 0.39459 to 0.37052, saving model to model.h5\n",
      "248/248 [==============================] - 0s 181us/sample - loss: 0.3222 - mean_squared_error: 0.3222 - val_loss: 0.3705 - val_mean_squared_error: 0.3705\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2829 - mean_squared_error: 0.2829\n",
      "Epoch 00016: val_loss improved from 0.37052 to 0.34494, saving model to model.h5\n",
      "248/248 [==============================] - 0s 169us/sample - loss: 0.2968 - mean_squared_error: 0.2968 - val_loss: 0.3449 - val_mean_squared_error: 0.3449\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2656 - mean_squared_error: 0.2656\n",
      "Epoch 00017: val_loss improved from 0.34494 to 0.32024, saving model to model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 169us/sample - loss: 0.2605 - mean_squared_error: 0.2605 - val_loss: 0.3202 - val_mean_squared_error: 0.3202\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2444 - mean_squared_error: 0.2444\n",
      "Epoch 00018: val_loss improved from 0.32024 to 0.29729, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.2392 - mean_squared_error: 0.2392 - val_loss: 0.2973 - val_mean_squared_error: 0.2973\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2153 - mean_squared_error: 0.2153\n",
      "Epoch 00019: val_loss improved from 0.29729 to 0.27468, saving model to model.h5\n",
      "248/248 [==============================] - 0s 181us/sample - loss: 0.2141 - mean_squared_error: 0.2141 - val_loss: 0.2747 - val_mean_squared_error: 0.2747\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1888 - mean_squared_error: 0.1888\n",
      "Epoch 00020: val_loss improved from 0.27468 to 0.25227, saving model to model.h5\n",
      "248/248 [==============================] - 0s 181us/sample - loss: 0.1870 - mean_squared_error: 0.1870 - val_loss: 0.2523 - val_mean_squared_error: 0.2523\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1933 - mean_squared_error: 0.1933\n",
      "Epoch 00021: val_loss improved from 0.25227 to 0.23271, saving model to model.h5\n",
      "248/248 [==============================] - 0s 177us/sample - loss: 0.1691 - mean_squared_error: 0.1691 - val_loss: 0.2327 - val_mean_squared_error: 0.2327\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1497 - mean_squared_error: 0.1497\n",
      "Epoch 00022: val_loss improved from 0.23271 to 0.21457, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.1538 - mean_squared_error: 0.1538 - val_loss: 0.2146 - val_mean_squared_error: 0.2146\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1296 - mean_squared_error: 0.1296\n",
      "Epoch 00023: val_loss improved from 0.21457 to 0.19694, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.1359 - mean_squared_error: 0.1359 - val_loss: 0.1969 - val_mean_squared_error: 0.1969\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1296 - mean_squared_error: 0.1296\n",
      "Epoch 00024: val_loss improved from 0.19694 to 0.17919, saving model to model.h5\n",
      "248/248 [==============================] - 0s 169us/sample - loss: 0.1282 - mean_squared_error: 0.1282 - val_loss: 0.1792 - val_mean_squared_error: 0.1792\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1067 - mean_squared_error: 0.1067\n",
      "Epoch 00025: val_loss improved from 0.17919 to 0.16360, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.1093 - mean_squared_error: 0.1093 - val_loss: 0.1636 - val_mean_squared_error: 0.1636\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0984 - mean_squared_error: 0.0984\n",
      "Epoch 00026: val_loss improved from 0.16360 to 0.14999, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.1001 - mean_squared_error: 0.1001 - val_loss: 0.1500 - val_mean_squared_error: 0.1500\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0851 - mean_squared_error: 0.0851\n",
      "Epoch 00027: val_loss improved from 0.14999 to 0.13731, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0872 - mean_squared_error: 0.0872 - val_loss: 0.1373 - val_mean_squared_error: 0.1373\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0717 - mean_squared_error: 0.0717\n",
      "Epoch 00028: val_loss improved from 0.13731 to 0.12592, saving model to model.h5\n",
      "248/248 [==============================] - 0s 181us/sample - loss: 0.0796 - mean_squared_error: 0.0796 - val_loss: 0.1259 - val_mean_squared_error: 0.1259\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0737 - mean_squared_error: 0.0737\n",
      "Epoch 00029: val_loss improved from 0.12592 to 0.11470, saving model to model.h5\n",
      "248/248 [==============================] - 0s 163us/sample - loss: 0.0738 - mean_squared_error: 0.0738 - val_loss: 0.1147 - val_mean_squared_error: 0.1147\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0675 - mean_squared_error: 0.0675\n",
      "Epoch 00030: val_loss improved from 0.11470 to 0.10446, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0659 - mean_squared_error: 0.0659 - val_loss: 0.1045 - val_mean_squared_error: 0.1045\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0684 - mean_squared_error: 0.0684\n",
      "Epoch 00031: val_loss improved from 0.10446 to 0.09558, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0627 - mean_squared_error: 0.0627 - val_loss: 0.0956 - val_mean_squared_error: 0.0956\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0779 - mean_squared_error: 0.0779\n",
      "Epoch 00032: val_loss improved from 0.09558 to 0.08669, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0597 - mean_squared_error: 0.0597 - val_loss: 0.0867 - val_mean_squared_error: 0.0867\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0563 - mean_squared_error: 0.0563\n",
      "Epoch 00033: val_loss improved from 0.08669 to 0.07922, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.0563 - mean_squared_error: 0.0563 - val_loss: 0.0792 - val_mean_squared_error: 0.0792\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0667 - mean_squared_error: 0.0667\n",
      "Epoch 00034: val_loss improved from 0.07922 to 0.07318, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0506 - mean_squared_error: 0.0506 - val_loss: 0.0732 - val_mean_squared_error: 0.0732\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0627 - mean_squared_error: 0.0627\n",
      "Epoch 00035: val_loss improved from 0.07318 to 0.06863, saving model to model.h5\n",
      "248/248 [==============================] - 0s 177us/sample - loss: 0.0475 - mean_squared_error: 0.0475 - val_loss: 0.0686 - val_mean_squared_error: 0.0686\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0483 - mean_squared_error: 0.0483\n",
      "Epoch 00036: val_loss improved from 0.06863 to 0.06390, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0457 - mean_squared_error: 0.0457 - val_loss: 0.0639 - val_mean_squared_error: 0.0639\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0386 - mean_squared_error: 0.0386\n",
      "Epoch 00037: val_loss improved from 0.06390 to 0.06024, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0431 - mean_squared_error: 0.0431 - val_loss: 0.0602 - val_mean_squared_error: 0.0602\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00038: val_loss improved from 0.06024 to 0.05735, saving model to model.h5\n",
      "248/248 [==============================] - 0s 169us/sample - loss: 0.0425 - mean_squared_error: 0.0425 - val_loss: 0.0574 - val_mean_squared_error: 0.0574\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0541 - mean_squared_error: 0.0541\n",
      "Epoch 00039: val_loss improved from 0.05735 to 0.05559, saving model to model.h5\n",
      "248/248 [==============================] - 0s 173us/sample - loss: 0.0398 - mean_squared_error: 0.0398 - val_loss: 0.0556 - val_mean_squared_error: 0.0556\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0471 - mean_squared_error: 0.0471\n",
      "Epoch 00040: val_loss improved from 0.05559 to 0.05289, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 41/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00041: val_loss improved from 0.05289 to 0.05079, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0366 - mean_squared_error: 0.0366\n",
      "Epoch 00042: val_loss improved from 0.05079 to 0.05054, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0375 - mean_squared_error: 0.0375 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0410 - mean_squared_error: 0.0410\n",
      "Epoch 00043: val_loss improved from 0.05054 to 0.04723, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0468 - mean_squared_error: 0.0468\n",
      "Epoch 00044: val_loss did not improve from 0.04723\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00045: val_loss did not improve from 0.04723\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00046: val_loss improved from 0.04723 to 0.04579, saving model to model.h5\n",
      "248/248 [==============================] - 0s 173us/sample - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00047: val_loss did not improve from 0.04579\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0403 - mean_squared_error: 0.0403\n",
      "Epoch 00048: val_loss did not improve from 0.04579\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0339 - mean_squared_error: 0.0339\n",
      "Epoch 00049: val_loss did not improve from 0.04579\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00050: val_loss improved from 0.04579 to 0.04522, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00051: val_loss improved from 0.04522 to 0.04347, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0407 - mean_squared_error: 0.0407\n",
      "Epoch 00052: val_loss did not improve from 0.04347\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00053: val_loss improved from 0.04347 to 0.04197, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00054: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00055: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0330 - mean_squared_error: 0.0330 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0360 - mean_squared_error: 0.0360\n",
      "Epoch 00056: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0308 - mean_squared_error: 0.0308 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00057: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00058: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00059: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0393 - mean_squared_error: 0.0393\n",
      "Epoch 00060: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0428 - mean_squared_error: 0.0428\n",
      "Epoch 00061: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00062: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00063: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00064: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00065: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00066: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00067: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00068: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00069: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0411 - mean_squared_error: 0.0411\n",
      "Epoch 00070: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00071: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00072: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00073: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00074: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0383 - mean_squared_error: 0.0383\n",
      "Epoch 00075: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00076: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00077: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00078: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00079: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00080: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0177 - mean_squared_error: 0.0177\n",
      "Epoch 00081: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00082: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0371 - mean_squared_error: 0.0371\n",
      "Epoch 00083: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00084: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00085: val_loss did not improve from 0.04197\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Epoch 00086: val_loss improved from 0.04197 to 0.04185, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0446 - mean_squared_error: 0.0446\n",
      "Epoch 00087: val_loss did not improve from 0.04185\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00088: val_loss did not improve from 0.04185\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00089: val_loss did not improve from 0.04185\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00090: val_loss did not improve from 0.04185\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0105 - mean_squared_error: 0.0105\n",
      "Epoch 00091: val_loss did not improve from 0.04185\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0231 - mean_squared_error: 0.0231\n",
      "Epoch 00092: val_loss did not improve from 0.04185\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00093: val_loss did not improve from 0.04185\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00094: val_loss did not improve from 0.04185\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00095: val_loss did not improve from 0.04185\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00096: val_loss did not improve from 0.04185\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00097: val_loss did not improve from 0.04185\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00098: val_loss did not improve from 0.04185\n",
      "248/248 [==============================] - 0s 50us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00099: val_loss improved from 0.04185 to 0.04148, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00100: val_loss did not improve from 0.04148\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0372 - mean_squared_error: 0.0372\n",
      "Epoch 00101: val_loss did not improve from 0.04148\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0131 - mean_squared_error: 0.0131\n",
      "Epoch 00102: val_loss did not improve from 0.04148\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0159 - mean_squared_error: 0.0159\n",
      "Epoch 00103: val_loss did not improve from 0.04148\n",
      "248/248 [==============================] - 0s 58us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Epoch 00104: val_loss did not improve from 0.04148\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0429 - mean_squared_error: 0.0429\n",
      "Epoch 00105: val_loss did not improve from 0.04148\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00106: val_loss did not improve from 0.04148\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00107: val_loss did not improve from 0.04148\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00108: val_loss improved from 0.04148 to 0.04108, saving model to model.h5\n",
      "248/248 [==============================] - 0s 177us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0365 - mean_squared_error: 0.0365\n",
      "Epoch 00109: val_loss improved from 0.04108 to 0.03977, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00110: val_loss did not improve from 0.03977\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Epoch 00111: val_loss did not improve from 0.03977\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Epoch 00112: val_loss did not improve from 0.03977\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.0172\n",
      "Epoch 00113: val_loss improved from 0.03977 to 0.03976, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0126 - mean_squared_error: 0.0126\n",
      "Epoch 00114: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0140 - mean_squared_error: 0.0140\n",
      "Epoch 00115: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00116: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00117: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00118: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00119: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0345 - mean_squared_error: 0.0345\n",
      "Epoch 00120: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00121: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00122: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00123: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00124: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00125: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0174 - mean_squared_error: 0.0174\n",
      "Epoch 00126: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00127: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00128: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00129: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 130/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00130: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00131: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 132/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00132: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 133/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00133: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 134/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 00134: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 135/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00135: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 136/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00136: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 137/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0157 - mean_squared_error: 0.0157\n",
      "Epoch 00137: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 138/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0227 - mean_squared_error: 0.0227\n",
      "Epoch 00138: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 139/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00139: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 140/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00140: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 141/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0133 - mean_squared_error: 0.0133\n",
      "Epoch 00141: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 142/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00142: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 143/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00143: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 144/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00144: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 145/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Epoch 00145: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 146/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00146: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 147/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00147: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 148/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0141 - mean_squared_error: 0.0141\n",
      "Epoch 00148: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 149/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00149: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 150/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0156 - mean_squared_error: 0.0156\n",
      "Epoch 00150: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 151/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0118 - mean_squared_error: 0.0118\n",
      "Epoch 00151: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 152/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0152 - mean_squared_error: 0.0152\n",
      "Epoch 00152: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 153/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0188 - mean_squared_error: 0.0188\n",
      "Epoch 00153: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 154/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0368 - mean_squared_error: 0.0368\n",
      "Epoch 00154: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 155/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00155: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 156/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00156: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 58us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 157/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00157: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 158/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00158: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 159/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00159: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 160/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0170 - mean_squared_error: 0.0170\n",
      "Epoch 00160: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 161/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00161: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 162/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00162: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 163/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00163: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 164/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00164: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 165/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00165: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 166/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00166: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 167/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00167: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 168/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00168: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 169/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00169: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 170/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0150 - mean_squared_error: 0.0150\n",
      "Epoch 00170: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 171/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00171: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 172/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0114 - mean_squared_error: 0.0114\n",
      "Epoch 00172: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 173/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00173: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 174/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00174: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 175/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00175: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 176/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00176: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 177/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0152 - mean_squared_error: 0.0152\n",
      "Epoch 00177: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 178/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00178: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 179/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00179: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 180/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00180: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 181/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Epoch 00181: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 182/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00182: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 183/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00183: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 184/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00184: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 185/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0187 - mean_squared_error: 0.0187\n",
      "Epoch 00185: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 97us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 186/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00186: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 95us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 187/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0144 - mean_squared_error: 0.0144\n",
      "Epoch 00187: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 188/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00188: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 189/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00189: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 190/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00190: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 191/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00191: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 192/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0117 - mean_squared_error: 0.0117\n",
      "Epoch 00192: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 97us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 193/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00193: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 92us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 194/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00194: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 83us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 195/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00195: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 196/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0100 - mean_squared_error: 0.0100\n",
      "Epoch 00196: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 197/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00197: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 198/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00198: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 199/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00199: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 200/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0138 - mean_squared_error: 0.0138\n",
      "Epoch 00200: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 201/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0106 - mean_squared_error: 0.0106\n",
      "Epoch 00201: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 202/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0231 - mean_squared_error: 0.0231\n",
      "Epoch 00202: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 203/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00203: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 204/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0184 - mean_squared_error: 0.0184\n",
      "Epoch 00204: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 205/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00205: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 206/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0124 - mean_squared_error: 0.0124\n",
      "Epoch 00206: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 207/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00207: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 208/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.0172\n",
      "Epoch 00208: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 209/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00209: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 210/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00210: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 211/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0126 - mean_squared_error: 0.0126\n",
      "Epoch 00211: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 212/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0125 - mean_squared_error: 0.0125\n",
      "Epoch 00212: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 213/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00213: val_loss did not improve from 0.03976\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Running trial 5\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9102 - mean_squared_error: 0.9102\n",
      "Epoch 00001: val_loss improved from inf to 0.72085, saving model to model.h5\n",
      "248/248 [==============================] - 1s 5ms/sample - loss: 1.0080 - mean_squared_error: 1.0080 - val_loss: 0.7208 - val_mean_squared_error: 0.7208\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8884 - mean_squared_error: 0.8884\n",
      "Epoch 00002: val_loss improved from 0.72085 to 0.68243, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.8646 - mean_squared_error: 0.8646 - val_loss: 0.6824 - val_mean_squared_error: 0.6824\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7359 - mean_squared_error: 0.7359\n",
      "Epoch 00003: val_loss improved from 0.68243 to 0.64957, saving model to model.h5\n",
      "248/248 [==============================] - 0s 219us/sample - loss: 0.7766 - mean_squared_error: 0.7766 - val_loss: 0.6496 - val_mean_squared_error: 0.6496\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8449 - mean_squared_error: 0.8449\n",
      "Epoch 00004: val_loss improved from 0.64957 to 0.61918, saving model to model.h5\n",
      "248/248 [==============================] - 0s 209us/sample - loss: 0.7167 - mean_squared_error: 0.7167 - val_loss: 0.6192 - val_mean_squared_error: 0.6192\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6751 - mean_squared_error: 0.6751\n",
      "Epoch 00005: val_loss improved from 0.61918 to 0.59152, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.6459 - mean_squared_error: 0.6459 - val_loss: 0.5915 - val_mean_squared_error: 0.5915\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5479 - mean_squared_error: 0.5479\n",
      "Epoch 00006: val_loss improved from 0.59152 to 0.56372, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.5826 - mean_squared_error: 0.5826 - val_loss: 0.5637 - val_mean_squared_error: 0.5637\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4942 - mean_squared_error: 0.4942\n",
      "Epoch 00007: val_loss improved from 0.56372 to 0.53968, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.5340 - mean_squared_error: 0.5340 - val_loss: 0.5397 - val_mean_squared_error: 0.5397\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5832 - mean_squared_error: 0.5832\n",
      "Epoch 00008: val_loss improved from 0.53968 to 0.51343, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.4880 - mean_squared_error: 0.4880 - val_loss: 0.5134 - val_mean_squared_error: 0.5134\n",
      "Epoch 9/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4180 - mean_squared_error: 0.4180\n",
      "Epoch 00009: val_loss improved from 0.51343 to 0.48822, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.4425 - mean_squared_error: 0.4425 - val_loss: 0.4882 - val_mean_squared_error: 0.4882\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4480 - mean_squared_error: 0.4480\n",
      "Epoch 00010: val_loss improved from 0.48822 to 0.46326, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.4087 - mean_squared_error: 0.4087 - val_loss: 0.4633 - val_mean_squared_error: 0.4633\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4187 - mean_squared_error: 0.4187\n",
      "Epoch 00011: val_loss improved from 0.46326 to 0.44010, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.3656 - mean_squared_error: 0.3656 - val_loss: 0.4401 - val_mean_squared_error: 0.4401\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3325 - mean_squared_error: 0.3325\n",
      "Epoch 00012: val_loss improved from 0.44010 to 0.41418, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.3310 - mean_squared_error: 0.3310 - val_loss: 0.4142 - val_mean_squared_error: 0.4142\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2572 - mean_squared_error: 0.2572\n",
      "Epoch 00013: val_loss improved from 0.41418 to 0.38916, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.2947 - mean_squared_error: 0.2947 - val_loss: 0.3892 - val_mean_squared_error: 0.3892\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2803 - mean_squared_error: 0.2803\n",
      "Epoch 00014: val_loss improved from 0.38916 to 0.36714, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.2634 - mean_squared_error: 0.2634 - val_loss: 0.3671 - val_mean_squared_error: 0.3671\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2918 - mean_squared_error: 0.2918\n",
      "Epoch 00015: val_loss improved from 0.36714 to 0.34401, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.2582 - mean_squared_error: 0.2582 - val_loss: 0.3440 - val_mean_squared_error: 0.3440\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2224 - mean_squared_error: 0.2224\n",
      "Epoch 00016: val_loss improved from 0.34401 to 0.32165, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.2256 - mean_squared_error: 0.2256 - val_loss: 0.3217 - val_mean_squared_error: 0.3217\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1946 - mean_squared_error: 0.1946\n",
      "Epoch 00017: val_loss improved from 0.32165 to 0.29823, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.2016 - mean_squared_error: 0.2016 - val_loss: 0.2982 - val_mean_squared_error: 0.2982\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1954 - mean_squared_error: 0.1954\n",
      "Epoch 00018: val_loss improved from 0.29823 to 0.27614, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.1758 - mean_squared_error: 0.1758 - val_loss: 0.2761 - val_mean_squared_error: 0.2761\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1405 - mean_squared_error: 0.1405\n",
      "Epoch 00019: val_loss improved from 0.27614 to 0.25557, saving model to model.h5\n",
      "248/248 [==============================] - 0s 181us/sample - loss: 0.1625 - mean_squared_error: 0.1625 - val_loss: 0.2556 - val_mean_squared_error: 0.2556\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1614 - mean_squared_error: 0.1614\n",
      "Epoch 00020: val_loss improved from 0.25557 to 0.23599, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.1483 - mean_squared_error: 0.1483 - val_loss: 0.2360 - val_mean_squared_error: 0.2360\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1247 - mean_squared_error: 0.1247\n",
      "Epoch 00021: val_loss improved from 0.23599 to 0.21748, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.1299 - mean_squared_error: 0.1299 - val_loss: 0.2175 - val_mean_squared_error: 0.2175\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1160 - mean_squared_error: 0.1160\n",
      "Epoch 00022: val_loss improved from 0.21748 to 0.20057, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.1166 - mean_squared_error: 0.1166 - val_loss: 0.2006 - val_mean_squared_error: 0.2006\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1157 - mean_squared_error: 0.1157\n",
      "Epoch 00023: val_loss improved from 0.20057 to 0.18485, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.1059 - mean_squared_error: 0.1059 - val_loss: 0.1849 - val_mean_squared_error: 0.1849\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0955 - mean_squared_error: 0.0955\n",
      "Epoch 00024: val_loss improved from 0.18485 to 0.17060, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0959 - mean_squared_error: 0.0959 - val_loss: 0.1706 - val_mean_squared_error: 0.1706\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0762 - mean_squared_error: 0.0762\n",
      "Epoch 00025: val_loss improved from 0.17060 to 0.15633, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0826 - mean_squared_error: 0.0826 - val_loss: 0.1563 - val_mean_squared_error: 0.1563\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0876 - mean_squared_error: 0.0876\n",
      "Epoch 00026: val_loss improved from 0.15633 to 0.14427, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0765 - mean_squared_error: 0.0765 - val_loss: 0.1443 - val_mean_squared_error: 0.1443\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0634 - mean_squared_error: 0.0634\n",
      "Epoch 00027: val_loss improved from 0.14427 to 0.13261, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0705 - mean_squared_error: 0.0705 - val_loss: 0.1326 - val_mean_squared_error: 0.1326\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0664 - mean_squared_error: 0.0664\n",
      "Epoch 00028: val_loss improved from 0.13261 to 0.12008, saving model to model.h5\n",
      "248/248 [==============================] - 0s 143us/sample - loss: 0.0639 - mean_squared_error: 0.0639 - val_loss: 0.1201 - val_mean_squared_error: 0.1201\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0710 - mean_squared_error: 0.0710\n",
      "Epoch 00029: val_loss improved from 0.12008 to 0.10997, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0647 - mean_squared_error: 0.0647 - val_loss: 0.1100 - val_mean_squared_error: 0.1100\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0485 - mean_squared_error: 0.0485\n",
      "Epoch 00030: val_loss improved from 0.10997 to 0.10195, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0582 - mean_squared_error: 0.0582 - val_loss: 0.1020 - val_mean_squared_error: 0.1020\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0559 - mean_squared_error: 0.0559\n",
      "Epoch 00031: val_loss improved from 0.10195 to 0.09348, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0557 - mean_squared_error: 0.0557 - val_loss: 0.0935 - val_mean_squared_error: 0.0935\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0514 - mean_squared_error: 0.0514\n",
      "Epoch 00032: val_loss improved from 0.09348 to 0.08730, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0546 - mean_squared_error: 0.0546 - val_loss: 0.0873 - val_mean_squared_error: 0.0873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0554 - mean_squared_error: 0.0554\n",
      "Epoch 00033: val_loss improved from 0.08730 to 0.08021, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.0523 - mean_squared_error: 0.0523 - val_loss: 0.0802 - val_mean_squared_error: 0.0802\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0499 - mean_squared_error: 0.0499\n",
      "Epoch 00034: val_loss improved from 0.08021 to 0.07488, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0474 - mean_squared_error: 0.0474 - val_loss: 0.0749 - val_mean_squared_error: 0.0749\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0435 - mean_squared_error: 0.0435\n",
      "Epoch 00035: val_loss improved from 0.07488 to 0.06912, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0432 - mean_squared_error: 0.0432 - val_loss: 0.0691 - val_mean_squared_error: 0.0691\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0514 - mean_squared_error: 0.0514\n",
      "Epoch 00036: val_loss improved from 0.06912 to 0.06434, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0420 - mean_squared_error: 0.0420 - val_loss: 0.0643 - val_mean_squared_error: 0.0643\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00037: val_loss improved from 0.06434 to 0.06256, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0626 - val_mean_squared_error: 0.0626\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Epoch 00038: val_loss improved from 0.06256 to 0.06115, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0400 - mean_squared_error: 0.0400 - val_loss: 0.0612 - val_mean_squared_error: 0.0612\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00039: val_loss improved from 0.06115 to 0.06050, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.0402 - mean_squared_error: 0.0402 - val_loss: 0.0605 - val_mean_squared_error: 0.0605\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0521 - mean_squared_error: 0.0521\n",
      "Epoch 00040: val_loss improved from 0.06050 to 0.05900, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0417 - mean_squared_error: 0.0417 - val_loss: 0.0590 - val_mean_squared_error: 0.0590\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00041: val_loss improved from 0.05900 to 0.05876, saving model to model.h5\n",
      "248/248 [==============================] - 0s 139us/sample - loss: 0.0394 - mean_squared_error: 0.0394 - val_loss: 0.0588 - val_mean_squared_error: 0.0588\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0339 - mean_squared_error: 0.0339\n",
      "Epoch 00042: val_loss improved from 0.05876 to 0.05400, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00043: val_loss improved from 0.05400 to 0.05288, saving model to model.h5\n",
      "248/248 [==============================] - 0s 147us/sample - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Epoch 00044: val_loss improved from 0.05288 to 0.05245, saving model to model.h5\n",
      "248/248 [==============================] - 0s 181us/sample - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0412 - mean_squared_error: 0.0412\n",
      "Epoch 00045: val_loss improved from 0.05245 to 0.05100, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0382 - mean_squared_error: 0.0382 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0447 - mean_squared_error: 0.0447\n",
      "Epoch 00046: val_loss improved from 0.05100 to 0.04982, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00047: val_loss did not improve from 0.04982\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0384 - mean_squared_error: 0.0384\n",
      "Epoch 00048: val_loss did not improve from 0.04982\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0526 - val_mean_squared_error: 0.0526\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00049: val_loss did not improve from 0.04982\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0520 - val_mean_squared_error: 0.0520\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00050: val_loss did not improve from 0.04982\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0538 - val_mean_squared_error: 0.0538\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0386 - mean_squared_error: 0.0386\n",
      "Epoch 00051: val_loss did not improve from 0.04982\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0539 - val_mean_squared_error: 0.0539\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00052: val_loss improved from 0.04982 to 0.04973, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00053: val_loss improved from 0.04973 to 0.04968, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00054: val_loss improved from 0.04968 to 0.04921, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00055: val_loss improved from 0.04921 to 0.04839, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00056: val_loss did not improve from 0.04839\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 57/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00057: val_loss improved from 0.04839 to 0.04746, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0385 - mean_squared_error: 0.0385\n",
      "Epoch 00058: val_loss improved from 0.04746 to 0.04720, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0472 - mean_squared_error: 0.0472\n",
      "Epoch 00059: val_loss did not improve from 0.04720\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00060: val_loss did not improve from 0.04720\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00061: val_loss improved from 0.04720 to 0.04659, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00062: val_loss improved from 0.04659 to 0.04651, saving model to model.h5\n",
      "248/248 [==============================] - 0s 169us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00063: val_loss did not improve from 0.04651\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00064: val_loss improved from 0.04651 to 0.04579, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00065: val_loss improved from 0.04579 to 0.04510, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0428 - mean_squared_error: 0.0428\n",
      "Epoch 00066: val_loss improved from 0.04510 to 0.04464, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0387 - mean_squared_error: 0.0387\n",
      "Epoch 00067: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00068: val_loss improved from 0.04464 to 0.04328, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00069: val_loss did not improve from 0.04328\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00070: val_loss did not improve from 0.04328\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0384 - mean_squared_error: 0.0384\n",
      "Epoch 00071: val_loss did not improve from 0.04328\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00072: val_loss improved from 0.04328 to 0.04283, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00073: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00074: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0125 - mean_squared_error: 0.0125\n",
      "Epoch 00075: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00076: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00077: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00078: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00079: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Epoch 00080: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00081: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 82/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00082: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00083: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00084: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 00085: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00086: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 58us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00087: val_loss improved from 0.04283 to 0.04273, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00088: val_loss improved from 0.04273 to 0.04207, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00089: val_loss did not improve from 0.04207\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00090: val_loss did not improve from 0.04207\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Epoch 00091: val_loss did not improve from 0.04207\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0450 - mean_squared_error: 0.0450\n",
      "Epoch 00092: val_loss did not improve from 0.04207\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0177 - mean_squared_error: 0.0177\n",
      "Epoch 00093: val_loss did not improve from 0.04207\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0149 - mean_squared_error: 0.0149\n",
      "Epoch 00094: val_loss did not improve from 0.04207\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0333 - mean_squared_error: 0.0333\n",
      "Epoch 00095: val_loss did not improve from 0.04207\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00096: val_loss did not improve from 0.04207\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00097: val_loss did not improve from 0.04207\n",
      "248/248 [==============================] - 0s 59us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00098: val_loss did not improve from 0.04207\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00099: val_loss did not improve from 0.04207\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00100: val_loss did not improve from 0.04207\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00101: val_loss did not improve from 0.04207\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00102: val_loss did not improve from 0.04207\n",
      "248/248 [==============================] - 0s 92us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00103: val_loss improved from 0.04207 to 0.04091, saving model to model.h5\n",
      "248/248 [==============================] - 0s 185us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00104: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0174 - mean_squared_error: 0.0174\n",
      "Epoch 00105: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 00106: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00107: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00108: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0294 - mean_squared_error: 0.0294\n",
      "Epoch 00109: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0150 - mean_squared_error: 0.0150\n",
      "Epoch 00110: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00111: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0137 - mean_squared_error: 0.0137\n",
      "Epoch 00112: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00113: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00114: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00115: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00116: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00117: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00118: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00119: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00120: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00121: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00122: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00123: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00124: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0107 - mean_squared_error: 0.0107\n",
      "Epoch 00125: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00126: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00127: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00128: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 95us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00129: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 130/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0341 - mean_squared_error: 0.0341\n",
      "Epoch 00130: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00131: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 132/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00132: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 133/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00133: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 134/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00134: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 135/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0181 - mean_squared_error: 0.0181\n",
      "Epoch 00135: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 136/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Epoch 00136: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 137/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00137: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 138/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0139 - mean_squared_error: 0.0139\n",
      "Epoch 00138: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 139/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0376 - mean_squared_error: 0.0376\n",
      "Epoch 00139: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 140/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0170 - mean_squared_error: 0.0170\n",
      "Epoch 00140: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 141/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0132 - mean_squared_error: 0.0132\n",
      "Epoch 00141: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 142/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00142: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 143/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0136 - mean_squared_error: 0.0136\n",
      "Epoch 00143: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 144/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00144: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 145/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00145: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 146/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00146: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 147/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0147 - mean_squared_error: 0.0147\n",
      "Epoch 00147: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 148/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00148: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 149/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0126 - mean_squared_error: 0.0126\n",
      "Epoch 00149: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 150/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0159 - mean_squared_error: 0.0159\n",
      "Epoch 00150: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 151/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0142 - mean_squared_error: 0.0142\n",
      "Epoch 00151: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 152/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00152: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 153/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00153: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 154/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0116 - mean_squared_error: 0.0116\n",
      "Epoch 00154: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 155/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00155: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 156/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0307 - mean_squared_error: 0.0307\n",
      "Epoch 00156: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 157/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00157: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 158/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00158: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 159/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0145 - mean_squared_error: 0.0145\n",
      "Epoch 00159: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 160/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0087 - mean_squared_error: 0.0087\n",
      "Epoch 00160: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 161/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0307 - mean_squared_error: 0.0307\n",
      "Epoch 00161: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 162/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00162: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 163/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Epoch 00163: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 164/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0132 - mean_squared_error: 0.0132\n",
      "Epoch 00164: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 165/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00165: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 166/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00166: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 167/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0153 - mean_squared_error: 0.0153\n",
      "Epoch 00167: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 168/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0124 - mean_squared_error: 0.0124\n",
      "Epoch 00168: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 169/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00169: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 170/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0170 - mean_squared_error: 0.0170\n",
      "Epoch 00170: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 171/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00171: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 172/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00172: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 173/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0142 - mean_squared_error: 0.0142\n",
      "Epoch 00173: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 174/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00174: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0168 - mean_squared_error: 0.0168 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 175/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00175: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 176/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00176: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 177/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0170 - mean_squared_error: 0.0170\n",
      "Epoch 00177: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 178/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00178: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 179/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00179: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 180/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00180: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 181/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0110 - mean_squared_error: 0.0110\n",
      "Epoch 00181: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 182/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00182: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 183/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00183: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 184/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00184: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 185/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00185: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 186/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0132 - mean_squared_error: 0.0132\n",
      "Epoch 00186: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 187/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.0172\n",
      "Epoch 00187: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 188/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0151 - mean_squared_error: 0.0151\n",
      "Epoch 00188: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 189/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00189: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 190/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0170 - mean_squared_error: 0.0170\n",
      "Epoch 00190: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 191/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00191: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 192/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00192: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 193/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00193: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 194/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Epoch 00194: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 195/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0153 - mean_squared_error: 0.0153\n",
      "Epoch 00195: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 196/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00196: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 197/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0136 - mean_squared_error: 0.0136\n",
      "Epoch 00197: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 198/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0106 - mean_squared_error: 0.0106\n",
      "Epoch 00198: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 199/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0145 - mean_squared_error: 0.0145\n",
      "Epoch 00199: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0516 - val_mean_squared_error: 0.0516\n",
      "Epoch 200/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0111 - mean_squared_error: 0.0111\n",
      "Epoch 00200: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 201/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0113 - mean_squared_error: 0.0113\n",
      "Epoch 00201: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0534 - val_mean_squared_error: 0.0534\n",
      "Epoch 202/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00202: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0176 - mean_squared_error: 0.0176 - val_loss: 0.0509 - val_mean_squared_error: 0.0509\n",
      "Epoch 203/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0145 - mean_squared_error: 0.0145\n",
      "Epoch 00203: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0520 - val_mean_squared_error: 0.0520\n",
      "Running trial 6\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.2397 - mean_squared_error: 1.2397\n",
      "Epoch 00001: val_loss improved from inf to 0.79225, saving model to model.h5\n",
      "248/248 [==============================] - 1s 5ms/sample - loss: 1.1828 - mean_squared_error: 1.1828 - val_loss: 0.7922 - val_mean_squared_error: 0.7922\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9152 - mean_squared_error: 0.9152\n",
      "Epoch 00002: val_loss improved from 0.79225 to 0.75699, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 1.0538 - mean_squared_error: 1.0538 - val_loss: 0.7570 - val_mean_squared_error: 0.7570\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.1118 - mean_squared_error: 1.1118\n",
      "Epoch 00003: val_loss improved from 0.75699 to 0.72698, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.9726 - mean_squared_error: 0.9726 - val_loss: 0.7270 - val_mean_squared_error: 0.7270\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9533 - mean_squared_error: 0.9533\n",
      "Epoch 00004: val_loss improved from 0.72698 to 0.69916, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.8978 - mean_squared_error: 0.8978 - val_loss: 0.6992 - val_mean_squared_error: 0.6992\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8123 - mean_squared_error: 0.8123\n",
      "Epoch 00005: val_loss improved from 0.69916 to 0.67124, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 0.8342 - mean_squared_error: 0.8342 - val_loss: 0.6712 - val_mean_squared_error: 0.6712\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7572 - mean_squared_error: 0.7572\n",
      "Epoch 00006: val_loss improved from 0.67124 to 0.64620, saving model to model.h5\n",
      "248/248 [==============================] - 0s 177us/sample - loss: 0.7710 - mean_squared_error: 0.7710 - val_loss: 0.6462 - val_mean_squared_error: 0.6462\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7707 - mean_squared_error: 0.7707\n",
      "Epoch 00007: val_loss improved from 0.64620 to 0.61991, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.7191 - mean_squared_error: 0.7191 - val_loss: 0.6199 - val_mean_squared_error: 0.6199\n",
      "Epoch 8/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7825 - mean_squared_error: 0.7825\n",
      "Epoch 00008: val_loss improved from 0.61991 to 0.59714, saving model to model.h5\n",
      "248/248 [==============================] - 0s 211us/sample - loss: 0.6652 - mean_squared_error: 0.6652 - val_loss: 0.5971 - val_mean_squared_error: 0.5971\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6483 - mean_squared_error: 0.6483\n",
      "Epoch 00009: val_loss improved from 0.59714 to 0.57205, saving model to model.h5\n",
      "248/248 [==============================] - 0s 177us/sample - loss: 0.6112 - mean_squared_error: 0.6112 - val_loss: 0.5721 - val_mean_squared_error: 0.5721\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7678 - mean_squared_error: 0.7678\n",
      "Epoch 00010: val_loss improved from 0.57205 to 0.54749, saving model to model.h5\n",
      "248/248 [==============================] - 0s 177us/sample - loss: 0.5807 - mean_squared_error: 0.5807 - val_loss: 0.5475 - val_mean_squared_error: 0.5475\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5403 - mean_squared_error: 0.5403\n",
      "Epoch 00011: val_loss improved from 0.54749 to 0.52186, saving model to model.h5\n",
      "248/248 [==============================] - 0s 185us/sample - loss: 0.5267 - mean_squared_error: 0.5267 - val_loss: 0.5219 - val_mean_squared_error: 0.5219\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4756 - mean_squared_error: 0.4756\n",
      "Epoch 00012: val_loss improved from 0.52186 to 0.49848, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.4885 - mean_squared_error: 0.4885 - val_loss: 0.4985 - val_mean_squared_error: 0.4985\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4314 - mean_squared_error: 0.4314\n",
      "Epoch 00013: val_loss improved from 0.49848 to 0.47364, saving model to model.h5\n",
      "248/248 [==============================] - 0s 181us/sample - loss: 0.4435 - mean_squared_error: 0.4435 - val_loss: 0.4736 - val_mean_squared_error: 0.4736\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4366 - mean_squared_error: 0.4366\n",
      "Epoch 00014: val_loss improved from 0.47364 to 0.44785, saving model to model.h5\n",
      "248/248 [==============================] - 0s 173us/sample - loss: 0.4050 - mean_squared_error: 0.4050 - val_loss: 0.4478 - val_mean_squared_error: 0.4478\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4061 - mean_squared_error: 0.4061\n",
      "Epoch 00015: val_loss improved from 0.44785 to 0.42468, saving model to model.h5\n",
      "248/248 [==============================] - 0s 181us/sample - loss: 0.3727 - mean_squared_error: 0.3727 - val_loss: 0.4247 - val_mean_squared_error: 0.4247\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3739 - mean_squared_error: 0.3739\n",
      "Epoch 00016: val_loss improved from 0.42468 to 0.40032, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.3413 - mean_squared_error: 0.3413 - val_loss: 0.4003 - val_mean_squared_error: 0.4003\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3080 - mean_squared_error: 0.3080\n",
      "Epoch 00017: val_loss improved from 0.40032 to 0.37676, saving model to model.h5\n",
      "248/248 [==============================] - 0s 169us/sample - loss: 0.2991 - mean_squared_error: 0.2991 - val_loss: 0.3768 - val_mean_squared_error: 0.3768\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2908 - mean_squared_error: 0.2908\n",
      "Epoch 00018: val_loss improved from 0.37676 to 0.35084, saving model to model.h5\n",
      "248/248 [==============================] - 0s 169us/sample - loss: 0.2744 - mean_squared_error: 0.2744 - val_loss: 0.3508 - val_mean_squared_error: 0.3508\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2677 - mean_squared_error: 0.2677\n",
      "Epoch 00019: val_loss improved from 0.35084 to 0.32822, saving model to model.h5\n",
      "248/248 [==============================] - 0s 181us/sample - loss: 0.2603 - mean_squared_error: 0.2603 - val_loss: 0.3282 - val_mean_squared_error: 0.3282\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2258 - mean_squared_error: 0.2258\n",
      "Epoch 00020: val_loss improved from 0.32822 to 0.30526, saving model to model.h5\n",
      "248/248 [==============================] - 0s 173us/sample - loss: 0.2204 - mean_squared_error: 0.2204 - val_loss: 0.3053 - val_mean_squared_error: 0.3053\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2132 - mean_squared_error: 0.2132\n",
      "Epoch 00021: val_loss improved from 0.30526 to 0.28394, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.1997 - mean_squared_error: 0.1997 - val_loss: 0.2839 - val_mean_squared_error: 0.2839\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1605 - mean_squared_error: 0.1605\n",
      "Epoch 00022: val_loss improved from 0.28394 to 0.26201, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.1795 - mean_squared_error: 0.1795 - val_loss: 0.2620 - val_mean_squared_error: 0.2620\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1596 - mean_squared_error: 0.1596\n",
      "Epoch 00023: val_loss improved from 0.26201 to 0.24212, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.1573 - mean_squared_error: 0.1573 - val_loss: 0.2421 - val_mean_squared_error: 0.2421\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1743 - mean_squared_error: 0.1743\n",
      "Epoch 00024: val_loss improved from 0.24212 to 0.22340, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.1494 - mean_squared_error: 0.1494 - val_loss: 0.2234 - val_mean_squared_error: 0.2234\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1351 - mean_squared_error: 0.1351\n",
      "Epoch 00025: val_loss improved from 0.22340 to 0.20508, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.1285 - mean_squared_error: 0.1285 - val_loss: 0.2051 - val_mean_squared_error: 0.2051\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1129 - mean_squared_error: 0.1129\n",
      "Epoch 00026: val_loss improved from 0.20508 to 0.18858, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.1234 - mean_squared_error: 0.1234 - val_loss: 0.1886 - val_mean_squared_error: 0.1886\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1150 - mean_squared_error: 0.1150\n",
      "Epoch 00027: val_loss improved from 0.18858 to 0.17274, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.1054 - mean_squared_error: 0.1054 - val_loss: 0.1727 - val_mean_squared_error: 0.1727\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0935 - mean_squared_error: 0.0935\n",
      "Epoch 00028: val_loss improved from 0.17274 to 0.15777, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0954 - mean_squared_error: 0.0954 - val_loss: 0.1578 - val_mean_squared_error: 0.1578\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0872 - mean_squared_error: 0.0872\n",
      "Epoch 00029: val_loss improved from 0.15777 to 0.14433, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0852 - mean_squared_error: 0.0852 - val_loss: 0.1443 - val_mean_squared_error: 0.1443\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0661 - mean_squared_error: 0.0661\n",
      "Epoch 00030: val_loss improved from 0.14433 to 0.13168, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0743 - mean_squared_error: 0.0743 - val_loss: 0.1317 - val_mean_squared_error: 0.1317\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0575 - mean_squared_error: 0.0575\n",
      "Epoch 00031: val_loss improved from 0.13168 to 0.12170, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0656 - mean_squared_error: 0.0656 - val_loss: 0.1217 - val_mean_squared_error: 0.1217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0641 - mean_squared_error: 0.0641\n",
      "Epoch 00032: val_loss improved from 0.12170 to 0.11188, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0675 - mean_squared_error: 0.0675 - val_loss: 0.1119 - val_mean_squared_error: 0.1119\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0430 - mean_squared_error: 0.0430\n",
      "Epoch 00033: val_loss improved from 0.11188 to 0.10150, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0598 - mean_squared_error: 0.0598 - val_loss: 0.1015 - val_mean_squared_error: 0.1015\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0539 - mean_squared_error: 0.0539\n",
      "Epoch 00034: val_loss improved from 0.10150 to 0.09256, saving model to model.h5\n",
      "248/248 [==============================] - 0s 177us/sample - loss: 0.0551 - mean_squared_error: 0.0551 - val_loss: 0.0926 - val_mean_squared_error: 0.0926\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0628 - mean_squared_error: 0.0628\n",
      "Epoch 00035: val_loss improved from 0.09256 to 0.08563, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0511 - mean_squared_error: 0.0511 - val_loss: 0.0856 - val_mean_squared_error: 0.0856\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0583 - mean_squared_error: 0.0583\n",
      "Epoch 00036: val_loss improved from 0.08563 to 0.07869, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0463 - mean_squared_error: 0.0463 - val_loss: 0.0787 - val_mean_squared_error: 0.0787\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0471 - mean_squared_error: 0.0471\n",
      "Epoch 00037: val_loss improved from 0.07869 to 0.07373, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0489 - mean_squared_error: 0.0489 - val_loss: 0.0737 - val_mean_squared_error: 0.0737\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0539 - mean_squared_error: 0.0539\n",
      "Epoch 00038: val_loss improved from 0.07373 to 0.07030, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0456 - mean_squared_error: 0.0456 - val_loss: 0.0703 - val_mean_squared_error: 0.0703\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0444 - mean_squared_error: 0.0444\n",
      "Epoch 00039: val_loss improved from 0.07030 to 0.06575, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0402 - mean_squared_error: 0.0402 - val_loss: 0.0657 - val_mean_squared_error: 0.0657\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0474 - mean_squared_error: 0.0474\n",
      "Epoch 00040: val_loss improved from 0.06575 to 0.06189, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0619 - val_mean_squared_error: 0.0619\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0441 - mean_squared_error: 0.0441\n",
      "Epoch 00041: val_loss improved from 0.06189 to 0.05961, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0392 - mean_squared_error: 0.0392 - val_loss: 0.0596 - val_mean_squared_error: 0.0596\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0417 - mean_squared_error: 0.0417\n",
      "Epoch 00042: val_loss improved from 0.05961 to 0.05675, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0422 - mean_squared_error: 0.0422 - val_loss: 0.0568 - val_mean_squared_error: 0.0568\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0424 - mean_squared_error: 0.0424\n",
      "Epoch 00043: val_loss improved from 0.05675 to 0.05522, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0437 - mean_squared_error: 0.0437 - val_loss: 0.0552 - val_mean_squared_error: 0.0552\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0390 - mean_squared_error: 0.0390\n",
      "Epoch 00044: val_loss did not improve from 0.05522\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0394 - mean_squared_error: 0.0394 - val_loss: 0.0557 - val_mean_squared_error: 0.0557\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0394 - mean_squared_error: 0.0394\n",
      "Epoch 00045: val_loss improved from 0.05522 to 0.05315, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.0532 - val_mean_squared_error: 0.0532\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0333 - mean_squared_error: 0.0333\n",
      "Epoch 00046: val_loss improved from 0.05315 to 0.05215, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0366 - mean_squared_error: 0.0366 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00047: val_loss improved from 0.05215 to 0.05014, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0460 - mean_squared_error: 0.0460\n",
      "Epoch 00048: val_loss did not improve from 0.05014\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0384 - mean_squared_error: 0.0384 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0423 - mean_squared_error: 0.0423\n",
      "Epoch 00049: val_loss did not improve from 0.05014\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0517 - val_mean_squared_error: 0.0517\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0345 - mean_squared_error: 0.0345\n",
      "Epoch 00050: val_loss improved from 0.05014 to 0.04981, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0370 - mean_squared_error: 0.0370 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00051: val_loss improved from 0.04981 to 0.04889, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Epoch 00052: val_loss improved from 0.04889 to 0.04801, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00053: val_loss improved from 0.04801 to 0.04684, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00054: val_loss improved from 0.04684 to 0.04525, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00055: val_loss did not improve from 0.04525\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0330 - mean_squared_error: 0.0330 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 56/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Epoch 00056: val_loss improved from 0.04525 to 0.04424, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00057: val_loss improved from 0.04424 to 0.04335, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0397 - mean_squared_error: 0.0397\n",
      "Epoch 00058: val_loss did not improve from 0.04335\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0294 - mean_squared_error: 0.0294\n",
      "Epoch 00059: val_loss did not improve from 0.04335\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0330 - mean_squared_error: 0.0330 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0394 - mean_squared_error: 0.0394\n",
      "Epoch 00060: val_loss improved from 0.04335 to 0.04303, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00061: val_loss did not improve from 0.04303\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00062: val_loss did not improve from 0.04303\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00063: val_loss did not improve from 0.04303\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00064: val_loss did not improve from 0.04303\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0405 - mean_squared_error: 0.0405\n",
      "Epoch 00065: val_loss did not improve from 0.04303\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0383 - mean_squared_error: 0.0383\n",
      "Epoch 00066: val_loss did not improve from 0.04303\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0372 - mean_squared_error: 0.0372\n",
      "Epoch 00067: val_loss did not improve from 0.04303\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00068: val_loss did not improve from 0.04303\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00069: val_loss did not improve from 0.04303\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00070: val_loss did not improve from 0.04303\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 00071: val_loss improved from 0.04303 to 0.04292, saving model to model.h5\n",
      "248/248 [==============================] - 0s 169us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00072: val_loss did not improve from 0.04292\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Epoch 00073: val_loss did not improve from 0.04292\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00074: val_loss did not improve from 0.04292\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0393 - mean_squared_error: 0.0393\n",
      "Epoch 00075: val_loss did not improve from 0.04292\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00076: val_loss did not improve from 0.04292\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00077: val_loss improved from 0.04292 to 0.04270, saving model to model.h5\n",
      "248/248 [==============================] - 0s 177us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00078: val_loss did not improve from 0.04270\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0406 - mean_squared_error: 0.0406\n",
      "Epoch 00079: val_loss did not improve from 0.04270\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Epoch 00080: val_loss did not improve from 0.04270\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00081: val_loss did not improve from 0.04270\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00082: val_loss did not improve from 0.04270\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00083: val_loss improved from 0.04270 to 0.04263, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00084: val_loss did not improve from 0.04263\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Epoch 00085: val_loss did not improve from 0.04263\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00086: val_loss did not improve from 0.04263\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00087: val_loss improved from 0.04263 to 0.04176, saving model to model.h5\n",
      "248/248 [==============================] - 0s 143us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0188 - mean_squared_error: 0.0188\n",
      "Epoch 00088: val_loss did not improve from 0.04176\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00089: val_loss did not improve from 0.04176\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00090: val_loss did not improve from 0.04176\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0231 - mean_squared_error: 0.0231\n",
      "Epoch 00091: val_loss improved from 0.04176 to 0.03962, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00092: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Epoch 00093: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0400 - mean_squared_error: 0.0400\n",
      "Epoch 00094: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00095: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0106 - mean_squared_error: 0.0106\n",
      "Epoch 00096: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00097: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0227 - mean_squared_error: 0.0227\n",
      "Epoch 00098: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00099: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0339 - mean_squared_error: 0.0339\n",
      "Epoch 00100: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00101: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00102: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0387 - mean_squared_error: 0.0387\n",
      "Epoch 00103: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00104: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00105: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 53us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00106: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0385 - mean_squared_error: 0.0385\n",
      "Epoch 00107: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00108: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00109: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00110: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00111: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00112: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00113: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0177 - mean_squared_error: 0.0177\n",
      "Epoch 00114: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00115: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 00116: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00117: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00118: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00119: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00120: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00121: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00122: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00123: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00124: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00125: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00126: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00127: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00128: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00129: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 130/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00130: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0178 - mean_squared_error: 0.0178\n",
      "Epoch 00131: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 132/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00132: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 133/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0152 - mean_squared_error: 0.0152\n",
      "Epoch 00133: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 134/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0158 - mean_squared_error: 0.0158\n",
      "Epoch 00134: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 135/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00135: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 136/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00136: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 137/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00137: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 138/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0339 - mean_squared_error: 0.0339\n",
      "Epoch 00138: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 139/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Epoch 00139: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 140/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00140: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 141/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00141: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 142/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00142: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 143/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00143: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 144/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Epoch 00144: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 145/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00145: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 146/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Epoch 00146: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 147/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00147: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 148/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00148: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 149/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00149: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 150/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0100 - mean_squared_error: 0.0100\n",
      "Epoch 00150: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 151/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00151: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 152/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00152: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 153/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00153: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 154/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00154: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 155/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0113 - mean_squared_error: 0.0113\n",
      "Epoch 00155: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 156/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0177 - mean_squared_error: 0.0177\n",
      "Epoch 00156: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 157/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00157: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 158/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00158: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 159/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00159: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 160/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00160: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 161/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0259 - mean_squared_error: 0.0259\n",
      "Epoch 00161: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 162/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00162: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 163/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00163: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 164/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0134 - mean_squared_error: 0.0134\n",
      "Epoch 00164: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 165/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00165: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 166/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00166: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 167/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0141 - mean_squared_error: 0.0141\n",
      "Epoch 00167: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 168/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00168: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 169/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00169: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 170/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00170: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 171/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0150 - mean_squared_error: 0.0150\n",
      "Epoch 00171: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 172/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0367 - mean_squared_error: 0.0367\n",
      "Epoch 00172: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 173/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00173: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 174/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00174: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 175/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0128 - mean_squared_error: 0.0128\n",
      "Epoch 00175: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 176/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00176: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 177/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0121 - mean_squared_error: 0.0121\n",
      "Epoch 00177: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 178/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00178: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 179/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00179: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 180/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.0172\n",
      "Epoch 00180: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 181/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0156 - mean_squared_error: 0.0156\n",
      "Epoch 00181: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 182/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00182: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 183/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0184 - mean_squared_error: 0.0184\n",
      "Epoch 00183: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 184/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00184: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 185/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0113 - mean_squared_error: 0.0113\n",
      "Epoch 00185: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 186/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0158 - mean_squared_error: 0.0158\n",
      "Epoch 00186: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 187/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00187: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 188/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00188: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 189/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00189: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 190/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00190: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 191/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0137 - mean_squared_error: 0.0137\n",
      "Epoch 00191: val_loss did not improve from 0.03962\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Running trial 7\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9503 - mean_squared_error: 0.9503\n",
      "Epoch 00001: val_loss improved from inf to 0.65934, saving model to model.h5\n",
      "248/248 [==============================] - 1s 6ms/sample - loss: 0.9769 - mean_squared_error: 0.9769 - val_loss: 0.6593 - val_mean_squared_error: 0.6593\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8304 - mean_squared_error: 0.8304\n",
      "Epoch 00002: val_loss improved from 0.65934 to 0.62446, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.8409 - mean_squared_error: 0.8409 - val_loss: 0.6245 - val_mean_squared_error: 0.6245\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7134 - mean_squared_error: 0.7134\n",
      "Epoch 00003: val_loss improved from 0.62446 to 0.59278, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.7564 - mean_squared_error: 0.7564 - val_loss: 0.5928 - val_mean_squared_error: 0.5928\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7313 - mean_squared_error: 0.7313\n",
      "Epoch 00004: val_loss improved from 0.59278 to 0.56511, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.6730 - mean_squared_error: 0.6730 - val_loss: 0.5651 - val_mean_squared_error: 0.5651\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6910 - mean_squared_error: 0.6910\n",
      "Epoch 00005: val_loss improved from 0.56511 to 0.54146, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.6092 - mean_squared_error: 0.6092 - val_loss: 0.5415 - val_mean_squared_error: 0.5415\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6258 - mean_squared_error: 0.6258\n",
      "Epoch 00006: val_loss improved from 0.54146 to 0.51675, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.5492 - mean_squared_error: 0.5492 - val_loss: 0.5168 - val_mean_squared_error: 0.5168\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5092 - mean_squared_error: 0.5092\n",
      "Epoch 00007: val_loss improved from 0.51675 to 0.49192, saving model to model.h5\n",
      "248/248 [==============================] - 0s 177us/sample - loss: 0.4954 - mean_squared_error: 0.4954 - val_loss: 0.4919 - val_mean_squared_error: 0.4919\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4506 - mean_squared_error: 0.4506\n",
      "Epoch 00008: val_loss improved from 0.49192 to 0.46949, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.4525 - mean_squared_error: 0.4525 - val_loss: 0.4695 - val_mean_squared_error: 0.4695\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3952 - mean_squared_error: 0.3952\n",
      "Epoch 00009: val_loss improved from 0.46949 to 0.44721, saving model to model.h5\n",
      "248/248 [==============================] - 0s 173us/sample - loss: 0.4068 - mean_squared_error: 0.4068 - val_loss: 0.4472 - val_mean_squared_error: 0.4472\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3498 - mean_squared_error: 0.3498\n",
      "Epoch 00010: val_loss improved from 0.44721 to 0.42279, saving model to model.h5\n",
      "248/248 [==============================] - 0s 173us/sample - loss: 0.3771 - mean_squared_error: 0.3771 - val_loss: 0.4228 - val_mean_squared_error: 0.4228\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3252 - mean_squared_error: 0.3252\n",
      "Epoch 00011: val_loss improved from 0.42279 to 0.39995, saving model to model.h5\n",
      "248/248 [==============================] - 0s 185us/sample - loss: 0.3349 - mean_squared_error: 0.3349 - val_loss: 0.3999 - val_mean_squared_error: 0.3999\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3232 - mean_squared_error: 0.3232\n",
      "Epoch 00012: val_loss improved from 0.39995 to 0.37922, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.3053 - mean_squared_error: 0.3053 - val_loss: 0.3792 - val_mean_squared_error: 0.3792\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2523 - mean_squared_error: 0.2523\n",
      "Epoch 00013: val_loss improved from 0.37922 to 0.35571, saving model to model.h5\n",
      "248/248 [==============================] - 0s 298us/sample - loss: 0.2678 - mean_squared_error: 0.2678 - val_loss: 0.3557 - val_mean_squared_error: 0.3557\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2282 - mean_squared_error: 0.2282\n",
      "Epoch 00014: val_loss improved from 0.35571 to 0.33272, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.2483 - mean_squared_error: 0.2483 - val_loss: 0.3327 - val_mean_squared_error: 0.3327\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2045 - mean_squared_error: 0.2045\n",
      "Epoch 00015: val_loss improved from 0.33272 to 0.31128, saving model to model.h5\n",
      "248/248 [==============================] - 0s 177us/sample - loss: 0.2159 - mean_squared_error: 0.2159 - val_loss: 0.3113 - val_mean_squared_error: 0.3113\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2000 - mean_squared_error: 0.2000\n",
      "Epoch 00016: val_loss improved from 0.31128 to 0.28998, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.1995 - mean_squared_error: 0.1995 - val_loss: 0.2900 - val_mean_squared_error: 0.2900\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1864 - mean_squared_error: 0.1864\n",
      "Epoch 00017: val_loss improved from 0.28998 to 0.26970, saving model to model.h5\n",
      "248/248 [==============================] - 0s 150us/sample - loss: 0.1735 - mean_squared_error: 0.1735 - val_loss: 0.2697 - val_mean_squared_error: 0.2697\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1552 - mean_squared_error: 0.1552\n",
      "Epoch 00018: val_loss improved from 0.26970 to 0.25224, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.1551 - mean_squared_error: 0.1551 - val_loss: 0.2522 - val_mean_squared_error: 0.2522\n",
      "Epoch 19/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1529 - mean_squared_error: 0.1529\n",
      "Epoch 00019: val_loss improved from 0.25224 to 0.23268, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.1463 - mean_squared_error: 0.1463 - val_loss: 0.2327 - val_mean_squared_error: 0.2327\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1046 - mean_squared_error: 0.1046\n",
      "Epoch 00020: val_loss improved from 0.23268 to 0.21457, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.1278 - mean_squared_error: 0.1278 - val_loss: 0.2146 - val_mean_squared_error: 0.2146\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1191 - mean_squared_error: 0.1191\n",
      "Epoch 00021: val_loss improved from 0.21457 to 0.19830, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.1230 - mean_squared_error: 0.1230 - val_loss: 0.1983 - val_mean_squared_error: 0.1983\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1326 - mean_squared_error: 0.1326\n",
      "Epoch 00022: val_loss improved from 0.19830 to 0.18166, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.1076 - mean_squared_error: 0.1076 - val_loss: 0.1817 - val_mean_squared_error: 0.1817\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1134 - mean_squared_error: 0.1134\n",
      "Epoch 00023: val_loss improved from 0.18166 to 0.16811, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0931 - mean_squared_error: 0.0931 - val_loss: 0.1681 - val_mean_squared_error: 0.1681\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0895 - mean_squared_error: 0.0895\n",
      "Epoch 00024: val_loss improved from 0.16811 to 0.15534, saving model to model.h5\n",
      "248/248 [==============================] - 0s 181us/sample - loss: 0.0899 - mean_squared_error: 0.0899 - val_loss: 0.1553 - val_mean_squared_error: 0.1553\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0932 - mean_squared_error: 0.0932\n",
      "Epoch 00025: val_loss improved from 0.15534 to 0.14321, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.0789 - mean_squared_error: 0.0789 - val_loss: 0.1432 - val_mean_squared_error: 0.1432\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0769 - mean_squared_error: 0.0769\n",
      "Epoch 00026: val_loss improved from 0.14321 to 0.13234, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.0719 - mean_squared_error: 0.0719 - val_loss: 0.1323 - val_mean_squared_error: 0.1323\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0633 - mean_squared_error: 0.0633\n",
      "Epoch 00027: val_loss improved from 0.13234 to 0.12042, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.0678 - mean_squared_error: 0.0678 - val_loss: 0.1204 - val_mean_squared_error: 0.1204\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0802 - mean_squared_error: 0.0802\n",
      "Epoch 00028: val_loss improved from 0.12042 to 0.11087, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0651 - mean_squared_error: 0.0651 - val_loss: 0.1109 - val_mean_squared_error: 0.1109\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0473 - mean_squared_error: 0.0473\n",
      "Epoch 00029: val_loss improved from 0.11087 to 0.10341, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0611 - mean_squared_error: 0.0611 - val_loss: 0.1034 - val_mean_squared_error: 0.1034\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0503 - mean_squared_error: 0.0503\n",
      "Epoch 00030: val_loss improved from 0.10341 to 0.09609, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0510 - mean_squared_error: 0.0510 - val_loss: 0.0961 - val_mean_squared_error: 0.0961\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0498 - mean_squared_error: 0.0498\n",
      "Epoch 00031: val_loss improved from 0.09609 to 0.09036, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0539 - mean_squared_error: 0.0539 - val_loss: 0.0904 - val_mean_squared_error: 0.0904\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0608 - mean_squared_error: 0.0608\n",
      "Epoch 00032: val_loss improved from 0.09036 to 0.08366, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0514 - mean_squared_error: 0.0514 - val_loss: 0.0837 - val_mean_squared_error: 0.0837\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0531 - mean_squared_error: 0.0531\n",
      "Epoch 00033: val_loss improved from 0.08366 to 0.07924, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0527 - mean_squared_error: 0.0527 - val_loss: 0.0792 - val_mean_squared_error: 0.0792\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0504 - mean_squared_error: 0.0504\n",
      "Epoch 00034: val_loss improved from 0.07924 to 0.07571, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0534 - mean_squared_error: 0.0534 - val_loss: 0.0757 - val_mean_squared_error: 0.0757\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Epoch 00035: val_loss improved from 0.07571 to 0.07298, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0448 - mean_squared_error: 0.0448 - val_loss: 0.0730 - val_mean_squared_error: 0.0730\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0529 - mean_squared_error: 0.0529\n",
      "Epoch 00036: val_loss improved from 0.07298 to 0.06950, saving model to model.h5\n",
      "248/248 [==============================] - 0s 181us/sample - loss: 0.0447 - mean_squared_error: 0.0447 - val_loss: 0.0695 - val_mean_squared_error: 0.0695\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00037: val_loss improved from 0.06950 to 0.06612, saving model to model.h5\n",
      "248/248 [==============================] - 0s 302us/sample - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0661 - val_mean_squared_error: 0.0661\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0467 - mean_squared_error: 0.0467\n",
      "Epoch 00038: val_loss improved from 0.06612 to 0.06584, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0658 - val_mean_squared_error: 0.0658\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0395 - mean_squared_error: 0.0395\n",
      "Epoch 00039: val_loss improved from 0.06584 to 0.06343, saving model to model.h5\n",
      "248/248 [==============================] - 0s 221us/sample - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0634 - val_mean_squared_error: 0.0634\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00040: val_loss improved from 0.06343 to 0.06183, saving model to model.h5\n",
      "248/248 [==============================] - 0s 281us/sample - loss: 0.0391 - mean_squared_error: 0.0391 - val_loss: 0.0618 - val_mean_squared_error: 0.0618\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0388 - mean_squared_error: 0.0388\n",
      "Epoch 00041: val_loss improved from 0.06183 to 0.06146, saving model to model.h5\n",
      "248/248 [==============================] - 0s 273us/sample - loss: 0.0365 - mean_squared_error: 0.0365 - val_loss: 0.0615 - val_mean_squared_error: 0.0615\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0400 - mean_squared_error: 0.0400\n",
      "Epoch 00042: val_loss improved from 0.06146 to 0.06033, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.0386 - mean_squared_error: 0.0386 - val_loss: 0.0603 - val_mean_squared_error: 0.0603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00043: val_loss improved from 0.06033 to 0.05811, saving model to model.h5\n",
      "248/248 [==============================] - 0s 269us/sample - loss: 0.0366 - mean_squared_error: 0.0366 - val_loss: 0.0581 - val_mean_squared_error: 0.0581\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0448 - mean_squared_error: 0.0448\n",
      "Epoch 00044: val_loss improved from 0.05811 to 0.05556, saving model to model.h5\n",
      "248/248 [==============================] - 0s 282us/sample - loss: 0.0401 - mean_squared_error: 0.0401 - val_loss: 0.0556 - val_mean_squared_error: 0.0556\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Epoch 00045: val_loss did not improve from 0.05556\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0378 - mean_squared_error: 0.0378 - val_loss: 0.0556 - val_mean_squared_error: 0.0556\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0362 - mean_squared_error: 0.0362\n",
      "Epoch 00046: val_loss improved from 0.05556 to 0.05417, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.0367 - mean_squared_error: 0.0367 - val_loss: 0.0542 - val_mean_squared_error: 0.0542\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00047: val_loss did not improve from 0.05417\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0331 - mean_squared_error: 0.0331 - val_loss: 0.0555 - val_mean_squared_error: 0.0555\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00048: val_loss improved from 0.05417 to 0.05291, saving model to model.h5\n",
      "248/248 [==============================] - 0s 181us/sample - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00049: val_loss did not improve from 0.05291\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0541 - val_mean_squared_error: 0.0541\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0429 - mean_squared_error: 0.0429\n",
      "Epoch 00050: val_loss did not improve from 0.05291\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0535 - val_mean_squared_error: 0.0535\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00051: val_loss improved from 0.05291 to 0.05246, saving model to model.h5\n",
      "248/248 [==============================] - 0s 185us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0525 - val_mean_squared_error: 0.0525\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0371 - mean_squared_error: 0.0371\n",
      "Epoch 00052: val_loss improved from 0.05246 to 0.05244, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00053: val_loss did not improve from 0.05244\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0538 - val_mean_squared_error: 0.0538\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00054: val_loss did not improve from 0.05244\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0562 - val_mean_squared_error: 0.0562\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Epoch 00055: val_loss did not improve from 0.05244\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 00056: val_loss did not improve from 0.05244\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0532 - val_mean_squared_error: 0.0532\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00057: val_loss improved from 0.05244 to 0.05120, saving model to model.h5\n",
      "248/248 [==============================] - 0s 173us/sample - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0414 - mean_squared_error: 0.0414\n",
      "Epoch 00058: val_loss did not improve from 0.05120\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0341 - mean_squared_error: 0.0341\n",
      "Epoch 00059: val_loss did not improve from 0.05120\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0541 - val_mean_squared_error: 0.0541\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0433 - mean_squared_error: 0.0433\n",
      "Epoch 00060: val_loss did not improve from 0.05120\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00061: val_loss did not improve from 0.05120\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0419 - mean_squared_error: 0.0419\n",
      "Epoch 00062: val_loss did not improve from 0.05120\n",
      "248/248 [==============================] - 0s 54us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0532 - val_mean_squared_error: 0.0532\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0380 - mean_squared_error: 0.0380\n",
      "Epoch 00063: val_loss did not improve from 0.05120\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00064: val_loss did not improve from 0.05120\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0526 - val_mean_squared_error: 0.0526\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Epoch 00065: val_loss did not improve from 0.05120\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0546 - val_mean_squared_error: 0.0546\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00066: val_loss did not improve from 0.05120\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0343 - mean_squared_error: 0.0343\n",
      "Epoch 00067: val_loss did not improve from 0.05120\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Epoch 00068: val_loss improved from 0.05120 to 0.05081, saving model to model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 169us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00069: val_loss did not improve from 0.05081\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0539 - val_mean_squared_error: 0.0539\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00070: val_loss did not improve from 0.05081\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00071: val_loss improved from 0.05081 to 0.04994, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0358 - mean_squared_error: 0.0358\n",
      "Epoch 00072: val_loss did not improve from 0.04994\n",
      "248/248 [==============================] - 0s 43us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00073: val_loss did not improve from 0.04994\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0526 - val_mean_squared_error: 0.0526\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00074: val_loss improved from 0.04994 to 0.04943, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00075: val_loss did not improve from 0.04943\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00076: val_loss improved from 0.04943 to 0.04810, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00077: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0174 - mean_squared_error: 0.0174\n",
      "Epoch 00078: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0542 - val_mean_squared_error: 0.0542\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00079: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0514 - val_mean_squared_error: 0.0514\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00080: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0534 - val_mean_squared_error: 0.0534\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00081: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00082: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0530 - val_mean_squared_error: 0.0530\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00083: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0513 - val_mean_squared_error: 0.0513\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00084: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0514 - val_mean_squared_error: 0.0514\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00085: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0525 - val_mean_squared_error: 0.0525\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00086: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0548 - val_mean_squared_error: 0.0548\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0382 - mean_squared_error: 0.0382\n",
      "Epoch 00087: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0533 - val_mean_squared_error: 0.0533\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Epoch 00088: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0509 - val_mean_squared_error: 0.0509\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00089: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0530 - val_mean_squared_error: 0.0530\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0155 - mean_squared_error: 0.0155\n",
      "Epoch 00090: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Epoch 00091: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00092: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0141 - mean_squared_error: 0.0141\n",
      "Epoch 00093: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 94/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00094: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00095: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0157 - mean_squared_error: 0.0157\n",
      "Epoch 00096: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0535 - val_mean_squared_error: 0.0535\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00097: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00098: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0509 - val_mean_squared_error: 0.0509\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00099: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0145 - mean_squared_error: 0.0145\n",
      "Epoch 00100: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00101: val_loss did not improve from 0.04810\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00102: val_loss improved from 0.04810 to 0.04796, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00103: val_loss did not improve from 0.04796\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00104: val_loss did not improve from 0.04796\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00105: val_loss did not improve from 0.04796\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0482 - val_mean_squared_error: 0.0482\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00106: val_loss did not improve from 0.04796\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00107: val_loss improved from 0.04796 to 0.04756, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00108: val_loss improved from 0.04756 to 0.04714, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00109: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00110: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0482 - val_mean_squared_error: 0.0482\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00111: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0136 - mean_squared_error: 0.0136\n",
      "Epoch 00112: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00113: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0122 - mean_squared_error: 0.0122\n",
      "Epoch 00114: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00115: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00116: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Epoch 00117: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00118: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00119: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00120: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00121: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0122 - mean_squared_error: 0.0122\n",
      "Epoch 00122: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00123: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00124: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0184 - mean_squared_error: 0.0184\n",
      "Epoch 00125: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0121 - mean_squared_error: 0.0121\n",
      "Epoch 00126: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00127: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00128: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00129: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 130/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00130: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0326 - mean_squared_error: 0.0326\n",
      "Epoch 00131: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 132/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0132 - mean_squared_error: 0.0132\n",
      "Epoch 00132: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 133/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0160 - mean_squared_error: 0.0160\n",
      "Epoch 00133: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 134/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00134: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 135/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00135: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 136/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00136: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0532 - val_mean_squared_error: 0.0532\n",
      "Epoch 137/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00137: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 138/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00138: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 139/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00139: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 140/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00140: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0482 - val_mean_squared_error: 0.0482\n",
      "Epoch 141/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00141: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 142/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0170 - mean_squared_error: 0.0170\n",
      "Epoch 00142: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0539 - val_mean_squared_error: 0.0539\n",
      "Epoch 143/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0170 - mean_squared_error: 0.0170\n",
      "Epoch 00143: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "Epoch 144/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0149 - mean_squared_error: 0.0149\n",
      "Epoch 00144: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0575 - val_mean_squared_error: 0.0575\n",
      "Epoch 145/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0227 - mean_squared_error: 0.0227\n",
      "Epoch 00145: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0543 - val_mean_squared_error: 0.0543\n",
      "Epoch 146/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00146: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0541 - val_mean_squared_error: 0.0541\n",
      "Epoch 147/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0160 - mean_squared_error: 0.0160\n",
      "Epoch 00147: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0546 - val_mean_squared_error: 0.0546\n",
      "Epoch 148/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00148: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0534 - val_mean_squared_error: 0.0534\n",
      "Epoch 149/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0116 - mean_squared_error: 0.0116\n",
      "Epoch 00149: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0530 - val_mean_squared_error: 0.0530\n",
      "Epoch 150/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00150: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 151/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0161 - mean_squared_error: 0.0161\n",
      "Epoch 00151: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 152/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0158 - mean_squared_error: 0.0158\n",
      "Epoch 00152: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 153/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00153: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0516 - val_mean_squared_error: 0.0516\n",
      "Epoch 154/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00154: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 155/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00155: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 156/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0130 - mean_squared_error: 0.0130\n",
      "Epoch 00156: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0572 - val_mean_squared_error: 0.0572\n",
      "Epoch 157/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00157: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0541 - val_mean_squared_error: 0.0541\n",
      "Epoch 158/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00158: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0531 - val_mean_squared_error: 0.0531\n",
      "Epoch 159/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0127 - mean_squared_error: 0.0127\n",
      "Epoch 00159: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 160/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00160: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0517 - val_mean_squared_error: 0.0517\n",
      "Epoch 161/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00161: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 162/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00162: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 163/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00163: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0544 - val_mean_squared_error: 0.0544\n",
      "Epoch 164/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00164: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 45us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0546 - val_mean_squared_error: 0.0546\n",
      "Epoch 165/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0188 - mean_squared_error: 0.0188\n",
      "Epoch 00165: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 166/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00166: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0535 - val_mean_squared_error: 0.0535\n",
      "Epoch 167/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0087 - mean_squared_error: 0.0087\n",
      "Epoch 00167: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 168/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00168: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 169/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0151 - mean_squared_error: 0.0151\n",
      "Epoch 00169: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 170/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00170: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0535 - val_mean_squared_error: 0.0535\n",
      "Epoch 171/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0091 - mean_squared_error: 0.0091\n",
      "Epoch 00171: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0541 - val_mean_squared_error: 0.0541\n",
      "Epoch 172/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00172: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n",
      "Epoch 173/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00173: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0538 - val_mean_squared_error: 0.0538\n",
      "Epoch 174/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Epoch 00174: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0553 - val_mean_squared_error: 0.0553\n",
      "Epoch 175/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00175: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0547 - val_mean_squared_error: 0.0547\n",
      "Epoch 176/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00176: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0532 - val_mean_squared_error: 0.0532\n",
      "Epoch 177/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00177: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0558 - val_mean_squared_error: 0.0558\n",
      "Epoch 178/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0115 - mean_squared_error: 0.0115\n",
      "Epoch 00178: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0547 - val_mean_squared_error: 0.0547\n",
      "Epoch 179/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00179: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 37us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0547 - val_mean_squared_error: 0.0547\n",
      "Epoch 180/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0158 - mean_squared_error: 0.0158\n",
      "Epoch 00180: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0576 - val_mean_squared_error: 0.0576\n",
      "Epoch 181/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00181: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0535 - val_mean_squared_error: 0.0535\n",
      "Epoch 182/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00182: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0569 - val_mean_squared_error: 0.0569\n",
      "Epoch 183/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0344 - mean_squared_error: 0.0344\n",
      "Epoch 00183: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 184/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00184: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 185/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00185: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 186/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00186: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0556 - val_mean_squared_error: 0.0556\n",
      "Epoch 187/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00187: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0573 - val_mean_squared_error: 0.0573\n",
      "Epoch 188/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00188: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0558 - val_mean_squared_error: 0.0558\n",
      "Epoch 189/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0100 - mean_squared_error: 0.0100\n",
      "Epoch 00189: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 190/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00190: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0550 - val_mean_squared_error: 0.0550\n",
      "Epoch 191/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00191: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0547 - val_mean_squared_error: 0.0547\n",
      "Epoch 192/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00192: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 193/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00193: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0577 - val_mean_squared_error: 0.0577\n",
      "Epoch 194/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00194: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0544 - val_mean_squared_error: 0.0544\n",
      "Epoch 195/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00195: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0570 - val_mean_squared_error: 0.0570\n",
      "Epoch 196/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00196: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0564 - val_mean_squared_error: 0.0564\n",
      "Epoch 197/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00197: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0578 - val_mean_squared_error: 0.0578\n",
      "Epoch 198/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0100 - mean_squared_error: 0.0100\n",
      "Epoch 00198: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n",
      "Epoch 199/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0133 - mean_squared_error: 0.0133\n",
      "Epoch 00199: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0552 - val_mean_squared_error: 0.0552\n",
      "Epoch 200/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0187 - mean_squared_error: 0.0187\n",
      "Epoch 00200: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0563 - val_mean_squared_error: 0.0563\n",
      "Epoch 201/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0123 - mean_squared_error: 0.0123\n",
      "Epoch 00201: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0561 - val_mean_squared_error: 0.0561\n",
      "Epoch 202/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00202: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 203/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0143 - mean_squared_error: 0.0143\n",
      "Epoch 00203: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0588 - val_mean_squared_error: 0.0588\n",
      "Epoch 204/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00204: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0565 - val_mean_squared_error: 0.0565\n",
      "Epoch 205/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00205: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n",
      "Epoch 206/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00206: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0571 - val_mean_squared_error: 0.0571\n",
      "Epoch 207/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00207: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0577 - val_mean_squared_error: 0.0577\n",
      "Epoch 208/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0151 - mean_squared_error: 0.0151\n",
      "Epoch 00208: val_loss did not improve from 0.04714\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0166 - mean_squared_error: 0.0166 - val_loss: 0.0542 - val_mean_squared_error: 0.0542\n",
      "Running trial 8\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.0685 - mean_squared_error: 1.0685\n",
      "Epoch 00001: val_loss improved from inf to 0.72924, saving model to model.h5\n",
      "248/248 [==============================] - 1s 6ms/sample - loss: 1.0555 - mean_squared_error: 1.0555 - val_loss: 0.7292 - val_mean_squared_error: 0.7292\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9298 - mean_squared_error: 0.9298\n",
      "Epoch 00002: val_loss improved from 0.72924 to 0.69362, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.9238 - mean_squared_error: 0.9238 - val_loss: 0.6936 - val_mean_squared_error: 0.6936\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9416 - mean_squared_error: 0.9416\n",
      "Epoch 00003: val_loss improved from 0.69362 to 0.66367, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.8213 - mean_squared_error: 0.8213 - val_loss: 0.6637 - val_mean_squared_error: 0.6637\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8730 - mean_squared_error: 0.8730\n",
      "Epoch 00004: val_loss improved from 0.66367 to 0.63586, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.7511 - mean_squared_error: 0.7511 - val_loss: 0.6359 - val_mean_squared_error: 0.6359\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7116 - mean_squared_error: 0.7116\n",
      "Epoch 00005: val_loss improved from 0.63586 to 0.60682, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.6943 - mean_squared_error: 0.6943 - val_loss: 0.6068 - val_mean_squared_error: 0.6068\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6099 - mean_squared_error: 0.6099\n",
      "Epoch 00006: val_loss improved from 0.60682 to 0.58180, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.6400 - mean_squared_error: 0.6400 - val_loss: 0.5818 - val_mean_squared_error: 0.5818\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6407 - mean_squared_error: 0.6407\n",
      "Epoch 00007: val_loss improved from 0.58180 to 0.55588, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.5754 - mean_squared_error: 0.5754 - val_loss: 0.5559 - val_mean_squared_error: 0.5559\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4428 - mean_squared_error: 0.4428\n",
      "Epoch 00008: val_loss improved from 0.55588 to 0.53035, saving model to model.h5\n",
      "248/248 [==============================] - 0s 169us/sample - loss: 0.5373 - mean_squared_error: 0.5373 - val_loss: 0.5304 - val_mean_squared_error: 0.5304\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5022 - mean_squared_error: 0.5022\n",
      "Epoch 00009: val_loss improved from 0.53035 to 0.50531, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.4844 - mean_squared_error: 0.4844 - val_loss: 0.5053 - val_mean_squared_error: 0.5053\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5772 - mean_squared_error: 0.5772\n",
      "Epoch 00010: val_loss improved from 0.50531 to 0.48259, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.4463 - mean_squared_error: 0.4463 - val_loss: 0.4826 - val_mean_squared_error: 0.4826\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4300 - mean_squared_error: 0.4300\n",
      "Epoch 00011: val_loss improved from 0.48259 to 0.45698, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.4102 - mean_squared_error: 0.4102 - val_loss: 0.4570 - val_mean_squared_error: 0.4570\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3216 - mean_squared_error: 0.3216\n",
      "Epoch 00012: val_loss improved from 0.45698 to 0.43309, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.3770 - mean_squared_error: 0.3770 - val_loss: 0.4331 - val_mean_squared_error: 0.4331\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3270 - mean_squared_error: 0.3270\n",
      "Epoch 00013: val_loss improved from 0.43309 to 0.40633, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.3360 - mean_squared_error: 0.3360 - val_loss: 0.4063 - val_mean_squared_error: 0.4063\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2643 - mean_squared_error: 0.2643\n",
      "Epoch 00014: val_loss improved from 0.40633 to 0.38275, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.2988 - mean_squared_error: 0.2988 - val_loss: 0.3827 - val_mean_squared_error: 0.3827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2702 - mean_squared_error: 0.2702\n",
      "Epoch 00015: val_loss improved from 0.38275 to 0.35875, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.2763 - mean_squared_error: 0.2763 - val_loss: 0.3588 - val_mean_squared_error: 0.3588\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2111 - mean_squared_error: 0.2111\n",
      "Epoch 00016: val_loss improved from 0.35875 to 0.33329, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.2530 - mean_squared_error: 0.2530 - val_loss: 0.3333 - val_mean_squared_error: 0.3333\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2347 - mean_squared_error: 0.2347\n",
      "Epoch 00017: val_loss improved from 0.33329 to 0.31185, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.2255 - mean_squared_error: 0.2255 - val_loss: 0.3118 - val_mean_squared_error: 0.3118\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2237 - mean_squared_error: 0.2237\n",
      "Epoch 00018: val_loss improved from 0.31185 to 0.28979, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.2039 - mean_squared_error: 0.2039 - val_loss: 0.2898 - val_mean_squared_error: 0.2898\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2200 - mean_squared_error: 0.2200\n",
      "Epoch 00019: val_loss improved from 0.28979 to 0.26968, saving model to model.h5\n",
      "248/248 [==============================] - 0s 128us/sample - loss: 0.1848 - mean_squared_error: 0.1848 - val_loss: 0.2697 - val_mean_squared_error: 0.2697\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1453 - mean_squared_error: 0.1453\n",
      "Epoch 00020: val_loss improved from 0.26968 to 0.24979, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.1653 - mean_squared_error: 0.1653 - val_loss: 0.2498 - val_mean_squared_error: 0.2498\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1587 - mean_squared_error: 0.1587\n",
      "Epoch 00021: val_loss improved from 0.24979 to 0.22953, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.1487 - mean_squared_error: 0.1487 - val_loss: 0.2295 - val_mean_squared_error: 0.2295\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1330 - mean_squared_error: 0.1330\n",
      "Epoch 00022: val_loss improved from 0.22953 to 0.21185, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.1369 - mean_squared_error: 0.1369 - val_loss: 0.2118 - val_mean_squared_error: 0.2118\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1578 - mean_squared_error: 0.1578\n",
      "Epoch 00023: val_loss improved from 0.21185 to 0.19497, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.1256 - mean_squared_error: 0.1256 - val_loss: 0.1950 - val_mean_squared_error: 0.1950\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1286 - mean_squared_error: 0.1286\n",
      "Epoch 00024: val_loss improved from 0.19497 to 0.17985, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.1079 - mean_squared_error: 0.1079 - val_loss: 0.1799 - val_mean_squared_error: 0.1799\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0931 - mean_squared_error: 0.0931\n",
      "Epoch 00025: val_loss improved from 0.17985 to 0.16570, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.1036 - mean_squared_error: 0.1036 - val_loss: 0.1657 - val_mean_squared_error: 0.1657\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1164 - mean_squared_error: 0.1164\n",
      "Epoch 00026: val_loss improved from 0.16570 to 0.15252, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0956 - mean_squared_error: 0.0956 - val_loss: 0.1525 - val_mean_squared_error: 0.1525\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0955 - mean_squared_error: 0.0955\n",
      "Epoch 00027: val_loss improved from 0.15252 to 0.14043, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0870 - mean_squared_error: 0.0870 - val_loss: 0.1404 - val_mean_squared_error: 0.1404\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0761 - mean_squared_error: 0.0761\n",
      "Epoch 00028: val_loss improved from 0.14043 to 0.12975, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0742 - mean_squared_error: 0.0742 - val_loss: 0.1298 - val_mean_squared_error: 0.1298\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0860 - mean_squared_error: 0.0860\n",
      "Epoch 00029: val_loss improved from 0.12975 to 0.11920, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0744 - mean_squared_error: 0.0744 - val_loss: 0.1192 - val_mean_squared_error: 0.1192\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0612 - mean_squared_error: 0.0612\n",
      "Epoch 00030: val_loss improved from 0.11920 to 0.10977, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0666 - mean_squared_error: 0.0666 - val_loss: 0.1098 - val_mean_squared_error: 0.1098\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0562 - mean_squared_error: 0.0562\n",
      "Epoch 00031: val_loss improved from 0.10977 to 0.10115, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0640 - mean_squared_error: 0.0640 - val_loss: 0.1011 - val_mean_squared_error: 0.1011\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0527 - mean_squared_error: 0.0527\n",
      "Epoch 00032: val_loss improved from 0.10115 to 0.09510, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0583 - mean_squared_error: 0.0583 - val_loss: 0.0951 - val_mean_squared_error: 0.0951\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0654 - mean_squared_error: 0.0654\n",
      "Epoch 00033: val_loss improved from 0.09510 to 0.08802, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0537 - mean_squared_error: 0.0537 - val_loss: 0.0880 - val_mean_squared_error: 0.0880\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0561 - mean_squared_error: 0.0561\n",
      "Epoch 00034: val_loss improved from 0.08802 to 0.08307, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0517 - mean_squared_error: 0.0517 - val_loss: 0.0831 - val_mean_squared_error: 0.0831\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0600 - mean_squared_error: 0.0600\n",
      "Epoch 00035: val_loss improved from 0.08307 to 0.07879, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0514 - mean_squared_error: 0.0514 - val_loss: 0.0788 - val_mean_squared_error: 0.0788\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0543 - mean_squared_error: 0.0543\n",
      "Epoch 00036: val_loss improved from 0.07879 to 0.07506, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0466 - mean_squared_error: 0.0466 - val_loss: 0.0751 - val_mean_squared_error: 0.0751\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0570 - mean_squared_error: 0.0570\n",
      "Epoch 00037: val_loss improved from 0.07506 to 0.07146, saving model to model.h5\n",
      "248/248 [==============================] - 0s 135us/sample - loss: 0.0505 - mean_squared_error: 0.0505 - val_loss: 0.0715 - val_mean_squared_error: 0.0715\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00038: val_loss improved from 0.07146 to 0.06914, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0436 - mean_squared_error: 0.0436 - val_loss: 0.0691 - val_mean_squared_error: 0.0691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0434 - mean_squared_error: 0.0434\n",
      "Epoch 00039: val_loss improved from 0.06914 to 0.06622, saving model to model.h5\n",
      "248/248 [==============================] - 0s 187us/sample - loss: 0.0468 - mean_squared_error: 0.0468 - val_loss: 0.0662 - val_mean_squared_error: 0.0662\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0603 - mean_squared_error: 0.0603\n",
      "Epoch 00040: val_loss improved from 0.06622 to 0.06297, saving model to model.h5\n",
      "248/248 [==============================] - 0s 173us/sample - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0630 - val_mean_squared_error: 0.0630\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0507 - mean_squared_error: 0.0507\n",
      "Epoch 00041: val_loss improved from 0.06297 to 0.06280, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0417 - mean_squared_error: 0.0417 - val_loss: 0.0628 - val_mean_squared_error: 0.0628\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0365 - mean_squared_error: 0.0365\n",
      "Epoch 00042: val_loss improved from 0.06280 to 0.06017, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0389 - mean_squared_error: 0.0389 - val_loss: 0.0602 - val_mean_squared_error: 0.0602\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0366 - mean_squared_error: 0.0366\n",
      "Epoch 00043: val_loss improved from 0.06017 to 0.05915, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0385 - mean_squared_error: 0.0385 - val_loss: 0.0591 - val_mean_squared_error: 0.0591\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0358 - mean_squared_error: 0.0358\n",
      "Epoch 00044: val_loss improved from 0.05915 to 0.05819, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0400 - mean_squared_error: 0.0400 - val_loss: 0.0582 - val_mean_squared_error: 0.0582\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0425 - mean_squared_error: 0.0425\n",
      "Epoch 00045: val_loss improved from 0.05819 to 0.05782, saving model to model.h5\n",
      "248/248 [==============================] - 0s 169us/sample - loss: 0.0403 - mean_squared_error: 0.0403 - val_loss: 0.0578 - val_mean_squared_error: 0.0578\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0435 - mean_squared_error: 0.0435\n",
      "Epoch 00046: val_loss improved from 0.05782 to 0.05487, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0375 - mean_squared_error: 0.0375 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0481 - mean_squared_error: 0.0481\n",
      "Epoch 00047: val_loss improved from 0.05487 to 0.05388, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0386 - mean_squared_error: 0.0386 - val_loss: 0.0539 - val_mean_squared_error: 0.0539\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00048: val_loss improved from 0.05388 to 0.05220, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00049: val_loss did not improve from 0.05220\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0378 - mean_squared_error: 0.0378 - val_loss: 0.0541 - val_mean_squared_error: 0.0541\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00050: val_loss did not improve from 0.05220\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 00051: val_loss improved from 0.05220 to 0.05043, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00052: val_loss improved from 0.05043 to 0.05009, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Epoch 00053: val_loss improved from 0.05009 to 0.04868, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0370 - mean_squared_error: 0.0370 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0458 - mean_squared_error: 0.0458\n",
      "Epoch 00054: val_loss did not improve from 0.04868\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00055: val_loss improved from 0.04868 to 0.04678, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00056: val_loss did not improve from 0.04678\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Epoch 00057: val_loss did not improve from 0.04678\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00058: val_loss did not improve from 0.04678\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0319 - mean_squared_error: 0.0319 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00059: val_loss did not improve from 0.04678\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0387 - mean_squared_error: 0.0387\n",
      "Epoch 00060: val_loss did not improve from 0.04678\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00061: val_loss did not improve from 0.04678\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00062: val_loss did not improve from 0.04678\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0365 - mean_squared_error: 0.0365\n",
      "Epoch 00063: val_loss did not improve from 0.04678\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00064: val_loss improved from 0.04678 to 0.04601, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0320 - mean_squared_error: 0.0320 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00065: val_loss did not improve from 0.04601\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Epoch 00066: val_loss did not improve from 0.04601\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0423 - mean_squared_error: 0.0423\n",
      "Epoch 00067: val_loss improved from 0.04601 to 0.04556, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0432 - mean_squared_error: 0.0432\n",
      "Epoch 00068: val_loss improved from 0.04556 to 0.04461, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0320 - mean_squared_error: 0.0320 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00069: val_loss did not improve from 0.04461\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00070: val_loss did not improve from 0.04461\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00071: val_loss did not improve from 0.04461\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0333 - mean_squared_error: 0.0333 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00072: val_loss did not improve from 0.04461\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0188 - mean_squared_error: 0.0188\n",
      "Epoch 00073: val_loss did not improve from 0.04461\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0402 - mean_squared_error: 0.0402\n",
      "Epoch 00074: val_loss did not improve from 0.04461\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Epoch 00075: val_loss did not improve from 0.04461\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00076: val_loss improved from 0.04461 to 0.04435, saving model to model.h5\n",
      "248/248 [==============================] - 0s 181us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00077: val_loss did not improve from 0.04435\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00078: val_loss did not improve from 0.04435\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0343 - mean_squared_error: 0.0343\n",
      "Epoch 00079: val_loss did not improve from 0.04435\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0150 - mean_squared_error: 0.0150\n",
      "Epoch 00080: val_loss did not improve from 0.04435\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00081: val_loss did not improve from 0.04435\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00082: val_loss did not improve from 0.04435\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00083: val_loss improved from 0.04435 to 0.04353, saving model to model.h5\n",
      "248/248 [==============================] - 0s 216us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00084: val_loss did not improve from 0.04353\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00085: val_loss improved from 0.04353 to 0.04323, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00086: val_loss improved from 0.04323 to 0.04309, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Epoch 00087: val_loss improved from 0.04309 to 0.03988, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00088: val_loss did not improve from 0.03988\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 89/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00089: val_loss did not improve from 0.03988\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0173 - mean_squared_error: 0.0173\n",
      "Epoch 00090: val_loss did not improve from 0.03988\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00091: val_loss did not improve from 0.03988\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00092: val_loss did not improve from 0.03988\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00093: val_loss did not improve from 0.03988\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 00094: val_loss did not improve from 0.03988\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00095: val_loss did not improve from 0.03988\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00096: val_loss did not improve from 0.03988\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00097: val_loss did not improve from 0.03988\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00098: val_loss did not improve from 0.03988\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Epoch 00099: val_loss did not improve from 0.03988\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00100: val_loss did not improve from 0.03988\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00101: val_loss did not improve from 0.03988\n",
      "248/248 [==============================] - 0s 51us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00102: val_loss improved from 0.03988 to 0.03974, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00103: val_loss did not improve from 0.03974\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Epoch 00104: val_loss improved from 0.03974 to 0.03822, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0143 - mean_squared_error: 0.0143\n",
      "Epoch 00105: val_loss did not improve from 0.03822\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00106: val_loss did not improve from 0.03822\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00107: val_loss did not improve from 0.03822\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0128 - mean_squared_error: 0.0128\n",
      "Epoch 00108: val_loss did not improve from 0.03822\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0145 - mean_squared_error: 0.0145\n",
      "Epoch 00109: val_loss did not improve from 0.03822\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00110: val_loss did not improve from 0.03822\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00111: val_loss did not improve from 0.03822\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00112: val_loss did not improve from 0.03822\n",
      "248/248 [==============================] - 0s 38us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Epoch 00113: val_loss did not improve from 0.03822\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00114: val_loss did not improve from 0.03822\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00115: val_loss did not improve from 0.03822\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Epoch 00116: val_loss improved from 0.03822 to 0.03672, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0367 - val_mean_squared_error: 0.0367\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0294 - mean_squared_error: 0.0294\n",
      "Epoch 00117: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Epoch 00118: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00119: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 50us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0227 - mean_squared_error: 0.0227\n",
      "Epoch 00120: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00121: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00122: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00123: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0443 - mean_squared_error: 0.0443\n",
      "Epoch 00124: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00125: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0124 - mean_squared_error: 0.0124\n",
      "Epoch 00126: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 42us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00127: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0362 - mean_squared_error: 0.0362\n",
      "Epoch 00128: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0188 - mean_squared_error: 0.0188\n",
      "Epoch 00129: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 130/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00130: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0119 - mean_squared_error: 0.0119\n",
      "Epoch 00131: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 132/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Epoch 00132: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 133/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0161 - mean_squared_error: 0.0161\n",
      "Epoch 00133: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 134/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00134: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 135/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0084 - mean_squared_error: 0.0084\n",
      "Epoch 00135: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 136/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00136: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 137/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00137: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 138/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00138: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 139/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00139: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 140/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0137 - mean_squared_error: 0.0137\n",
      "Epoch 00140: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00141: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 142/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0088 - mean_squared_error: 0.0088\n",
      "Epoch 00142: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 143/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00143: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 144/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0365 - mean_squared_error: 0.0365\n",
      "Epoch 00144: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 145/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0160 - mean_squared_error: 0.0160\n",
      "Epoch 00145: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 146/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00146: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 147/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00147: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 148/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00148: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 149/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00149: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 150/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00150: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 151/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00151: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 152/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0187 - mean_squared_error: 0.0187\n",
      "Epoch 00152: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 153/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0159 - mean_squared_error: 0.0159\n",
      "Epoch 00153: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 154/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0159 - mean_squared_error: 0.0159\n",
      "Epoch 00154: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 155/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00155: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 156/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00156: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 157/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00157: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 158/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00158: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 159/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00159: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 160/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00160: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 161/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0294 - mean_squared_error: 0.0294\n",
      "Epoch 00161: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 162/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00162: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 92us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 163/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00163: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 164/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00164: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 165/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0147 - mean_squared_error: 0.0147\n",
      "Epoch 00165: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 166/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00166: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 167/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00167: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 168/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00168: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 169/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0227 - mean_squared_error: 0.0227\n",
      "Epoch 00169: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 170/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0147 - mean_squared_error: 0.0147\n",
      "Epoch 00170: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 171/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00171: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 172/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0144 - mean_squared_error: 0.0144\n",
      "Epoch 00172: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 173/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00173: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 174/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00174: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 175/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0139 - mean_squared_error: 0.0139\n",
      "Epoch 00175: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 176/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0157 - mean_squared_error: 0.0157\n",
      "Epoch 00176: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 177/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00177: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 178/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00178: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 179/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00179: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 180/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00180: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 181/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0176 - mean_squared_error: 0.0176\n",
      "Epoch 00181: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 182/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00182: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 183/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0097 - mean_squared_error: 0.0097\n",
      "Epoch 00183: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 184/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0121 - mean_squared_error: 0.0121\n",
      "Epoch 00184: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 185/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0146 - mean_squared_error: 0.0146\n",
      "Epoch 00185: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 186/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0170 - mean_squared_error: 0.0170\n",
      "Epoch 00186: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 187/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00187: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 188/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00188: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 189/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00189: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 190/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00190: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 191/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00191: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 192/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00192: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 193/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00193: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 194/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00194: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 195/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00195: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 196/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Epoch 00196: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 197/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00197: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 198/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00198: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 199/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0132 - mean_squared_error: 0.0132\n",
      "Epoch 00199: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 200/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00200: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 201/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0130 - mean_squared_error: 0.0130\n",
      "Epoch 00201: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 202/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00202: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 203/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00203: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 204/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0115 - mean_squared_error: 0.0115\n",
      "Epoch 00204: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 205/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00205: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 206/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00206: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 207/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0135 - mean_squared_error: 0.0135\n",
      "Epoch 00207: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 208/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00208: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 209/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00209: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 210/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0173 - mean_squared_error: 0.0173\n",
      "Epoch 00210: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 211/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0145 - mean_squared_error: 0.0145\n",
      "Epoch 00211: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 212/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00212: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 213/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0155 - mean_squared_error: 0.0155\n",
      "Epoch 00213: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 214/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00214: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 215/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00215: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 216/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00216: val_loss did not improve from 0.03672\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Running trial 9\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7311 - mean_squared_error: 0.7311\n",
      "Epoch 00001: val_loss improved from inf to 0.70871, saving model to model.h5\n",
      "248/248 [==============================] - 1s 6ms/sample - loss: 1.0983 - mean_squared_error: 1.0983 - val_loss: 0.7087 - val_mean_squared_error: 0.7087\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7560 - mean_squared_error: 0.7560\n",
      "Epoch 00002: val_loss improved from 0.70871 to 0.67008, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.9515 - mean_squared_error: 0.9515 - val_loss: 0.6701 - val_mean_squared_error: 0.6701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7291 - mean_squared_error: 0.7291\n",
      "Epoch 00003: val_loss improved from 0.67008 to 0.64007, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.8584 - mean_squared_error: 0.8584 - val_loss: 0.6401 - val_mean_squared_error: 0.6401\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7658 - mean_squared_error: 0.7658\n",
      "Epoch 00004: val_loss improved from 0.64007 to 0.61050, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.7964 - mean_squared_error: 0.7964 - val_loss: 0.6105 - val_mean_squared_error: 0.6105\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7991 - mean_squared_error: 0.7991\n",
      "Epoch 00005: val_loss improved from 0.61050 to 0.58251, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.7292 - mean_squared_error: 0.7292 - val_loss: 0.5825 - val_mean_squared_error: 0.5825\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7012 - mean_squared_error: 0.7012\n",
      "Epoch 00006: val_loss improved from 0.58251 to 0.55766, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.6624 - mean_squared_error: 0.6624 - val_loss: 0.5577 - val_mean_squared_error: 0.5577\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5842 - mean_squared_error: 0.5842\n",
      "Epoch 00007: val_loss improved from 0.55766 to 0.53264, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.6156 - mean_squared_error: 0.6156 - val_loss: 0.5326 - val_mean_squared_error: 0.5326\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5463 - mean_squared_error: 0.5463\n",
      "Epoch 00008: val_loss improved from 0.53264 to 0.50812, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.5496 - mean_squared_error: 0.5496 - val_loss: 0.5081 - val_mean_squared_error: 0.5081\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4834 - mean_squared_error: 0.4834\n",
      "Epoch 00009: val_loss improved from 0.50812 to 0.48341, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.5011 - mean_squared_error: 0.5011 - val_loss: 0.4834 - val_mean_squared_error: 0.4834\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5783 - mean_squared_error: 0.5783\n",
      "Epoch 00010: val_loss improved from 0.48341 to 0.45992, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.4565 - mean_squared_error: 0.4565 - val_loss: 0.4599 - val_mean_squared_error: 0.4599\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4805 - mean_squared_error: 0.4805\n",
      "Epoch 00011: val_loss improved from 0.45992 to 0.43684, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.4160 - mean_squared_error: 0.4160 - val_loss: 0.4368 - val_mean_squared_error: 0.4368\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3866 - mean_squared_error: 0.3866\n",
      "Epoch 00012: val_loss improved from 0.43684 to 0.41208, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.3770 - mean_squared_error: 0.3770 - val_loss: 0.4121 - val_mean_squared_error: 0.4121\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4088 - mean_squared_error: 0.4088\n",
      "Epoch 00013: val_loss improved from 0.41208 to 0.38869, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.3365 - mean_squared_error: 0.3365 - val_loss: 0.3887 - val_mean_squared_error: 0.3887\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3117 - mean_squared_error: 0.3117\n",
      "Epoch 00014: val_loss improved from 0.38869 to 0.36517, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.2972 - mean_squared_error: 0.2972 - val_loss: 0.3652 - val_mean_squared_error: 0.3652\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2801 - mean_squared_error: 0.2801\n",
      "Epoch 00015: val_loss improved from 0.36517 to 0.34285, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.2646 - mean_squared_error: 0.2646 - val_loss: 0.3429 - val_mean_squared_error: 0.3429\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2565 - mean_squared_error: 0.2565\n",
      "Epoch 00016: val_loss improved from 0.34285 to 0.31966, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.2399 - mean_squared_error: 0.2399 - val_loss: 0.3197 - val_mean_squared_error: 0.3197\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2164 - mean_squared_error: 0.2164\n",
      "Epoch 00017: val_loss improved from 0.31966 to 0.29725, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.2120 - mean_squared_error: 0.2120 - val_loss: 0.2973 - val_mean_squared_error: 0.2973\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2092 - mean_squared_error: 0.2092\n",
      "Epoch 00018: val_loss improved from 0.29725 to 0.27427, saving model to model.h5\n",
      "248/248 [==============================] - 0s 203us/sample - loss: 0.1976 - mean_squared_error: 0.1976 - val_loss: 0.2743 - val_mean_squared_error: 0.2743\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1651 - mean_squared_error: 0.1651\n",
      "Epoch 00019: val_loss improved from 0.27427 to 0.25419, saving model to model.h5\n",
      "248/248 [==============================] - 0s 169us/sample - loss: 0.1731 - mean_squared_error: 0.1731 - val_loss: 0.2542 - val_mean_squared_error: 0.2542\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1304 - mean_squared_error: 0.1304\n",
      "Epoch 00020: val_loss improved from 0.25419 to 0.23301, saving model to model.h5\n",
      "248/248 [==============================] - 0s 177us/sample - loss: 0.1457 - mean_squared_error: 0.1457 - val_loss: 0.2330 - val_mean_squared_error: 0.2330\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1247 - mean_squared_error: 0.1247\n",
      "Epoch 00021: val_loss improved from 0.23301 to 0.21430, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.1326 - mean_squared_error: 0.1326 - val_loss: 0.2143 - val_mean_squared_error: 0.2143\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1532 - mean_squared_error: 0.1532\n",
      "Epoch 00022: val_loss improved from 0.21430 to 0.19548, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.1208 - mean_squared_error: 0.1208 - val_loss: 0.1955 - val_mean_squared_error: 0.1955\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1182 - mean_squared_error: 0.1182\n",
      "Epoch 00023: val_loss improved from 0.19548 to 0.17976, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.1040 - mean_squared_error: 0.1040 - val_loss: 0.1798 - val_mean_squared_error: 0.1798\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1010 - mean_squared_error: 0.1010\n",
      "Epoch 00024: val_loss improved from 0.17976 to 0.16514, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0954 - mean_squared_error: 0.0954 - val_loss: 0.1651 - val_mean_squared_error: 0.1651\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0759 - mean_squared_error: 0.0759\n",
      "Epoch 00025: val_loss improved from 0.16514 to 0.15016, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0871 - mean_squared_error: 0.0871 - val_loss: 0.1502 - val_mean_squared_error: 0.1502\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0816 - mean_squared_error: 0.0816\n",
      "Epoch 00026: val_loss improved from 0.15016 to 0.13849, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0787 - mean_squared_error: 0.0787 - val_loss: 0.1385 - val_mean_squared_error: 0.1385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0865 - mean_squared_error: 0.0865\n",
      "Epoch 00027: val_loss improved from 0.13849 to 0.12605, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.0735 - mean_squared_error: 0.0735 - val_loss: 0.1261 - val_mean_squared_error: 0.1261\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0685 - mean_squared_error: 0.0685\n",
      "Epoch 00028: val_loss improved from 0.12605 to 0.11436, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.0646 - mean_squared_error: 0.0646 - val_loss: 0.1144 - val_mean_squared_error: 0.1144\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0505 - mean_squared_error: 0.0505\n",
      "Epoch 00029: val_loss improved from 0.11436 to 0.10615, saving model to model.h5\n",
      "248/248 [==============================] - 0s 123us/sample - loss: 0.0578 - mean_squared_error: 0.0578 - val_loss: 0.1062 - val_mean_squared_error: 0.1062\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0377 - mean_squared_error: 0.0377\n",
      "Epoch 00030: val_loss improved from 0.10615 to 0.09714, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0577 - mean_squared_error: 0.0577 - val_loss: 0.0971 - val_mean_squared_error: 0.0971\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0512 - mean_squared_error: 0.0512\n",
      "Epoch 00031: val_loss improved from 0.09714 to 0.08902, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0525 - mean_squared_error: 0.0525 - val_loss: 0.0890 - val_mean_squared_error: 0.0890\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0484 - mean_squared_error: 0.0484\n",
      "Epoch 00032: val_loss improved from 0.08902 to 0.08096, saving model to model.h5\n",
      "248/248 [==============================] - 0s 176us/sample - loss: 0.0491 - mean_squared_error: 0.0491 - val_loss: 0.0810 - val_mean_squared_error: 0.0810\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0428 - mean_squared_error: 0.0428\n",
      "Epoch 00033: val_loss improved from 0.08096 to 0.07652, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0445 - mean_squared_error: 0.0445 - val_loss: 0.0765 - val_mean_squared_error: 0.0765\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0344 - mean_squared_error: 0.0344\n",
      "Epoch 00034: val_loss improved from 0.07652 to 0.07080, saving model to model.h5\n",
      "248/248 [==============================] - 0s 185us/sample - loss: 0.0434 - mean_squared_error: 0.0434 - val_loss: 0.0708 - val_mean_squared_error: 0.0708\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0455 - mean_squared_error: 0.0455\n",
      "Epoch 00035: val_loss improved from 0.07080 to 0.06527, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0653 - val_mean_squared_error: 0.0653\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0469 - mean_squared_error: 0.0469\n",
      "Epoch 00036: val_loss improved from 0.06527 to 0.06233, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0417 - mean_squared_error: 0.0417 - val_loss: 0.0623 - val_mean_squared_error: 0.0623\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00037: val_loss improved from 0.06233 to 0.05904, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0374 - mean_squared_error: 0.0374 - val_loss: 0.0590 - val_mean_squared_error: 0.0590\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0519 - mean_squared_error: 0.0519\n",
      "Epoch 00038: val_loss improved from 0.05904 to 0.05577, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0397 - mean_squared_error: 0.0397 - val_loss: 0.0558 - val_mean_squared_error: 0.0558\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0409 - mean_squared_error: 0.0409\n",
      "Epoch 00039: val_loss improved from 0.05577 to 0.05520, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0375 - mean_squared_error: 0.0375 - val_loss: 0.0552 - val_mean_squared_error: 0.0552\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00040: val_loss improved from 0.05520 to 0.05238, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0405 - mean_squared_error: 0.0405\n",
      "Epoch 00041: val_loss improved from 0.05238 to 0.05209, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0372 - mean_squared_error: 0.0372 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00042: val_loss improved from 0.05209 to 0.05101, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0344 - mean_squared_error: 0.0344\n",
      "Epoch 00043: val_loss improved from 0.05101 to 0.04984, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0370 - mean_squared_error: 0.0370 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0404 - mean_squared_error: 0.0404\n",
      "Epoch 00044: val_loss improved from 0.04984 to 0.04748, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0378 - mean_squared_error: 0.0378\n",
      "Epoch 00045: val_loss did not improve from 0.04748\n",
      "248/248 [==============================] - 0s 97us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0424 - mean_squared_error: 0.0424\n",
      "Epoch 00046: val_loss did not improve from 0.04748\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0462 - mean_squared_error: 0.0462\n",
      "Epoch 00047: val_loss did not improve from 0.04748\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Epoch 00048: val_loss did not improve from 0.04748\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0598 - mean_squared_error: 0.0598\n",
      "Epoch 00049: val_loss improved from 0.04748 to 0.04699, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0386 - mean_squared_error: 0.0386 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00050: val_loss did not improve from 0.04699\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 51/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00051: val_loss improved from 0.04699 to 0.04652, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00052: val_loss did not improve from 0.04652\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Epoch 00053: val_loss did not improve from 0.04652\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0414 - mean_squared_error: 0.0414\n",
      "Epoch 00054: val_loss improved from 0.04652 to 0.04578, saving model to model.h5\n",
      "248/248 [==============================] - 0s 120us/sample - loss: 0.0331 - mean_squared_error: 0.0331 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00055: val_loss did not improve from 0.04578\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00056: val_loss did not improve from 0.04578\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00057: val_loss did not improve from 0.04578\n",
      "248/248 [==============================] - 0s 38us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0334 - mean_squared_error: 0.0334\n",
      "Epoch 00058: val_loss did not improve from 0.04578\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0388 - mean_squared_error: 0.0388\n",
      "Epoch 00059: val_loss did not improve from 0.04578\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.0185\n",
      "Epoch 00060: val_loss did not improve from 0.04578\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00061: val_loss did not improve from 0.04578\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00062: val_loss did not improve from 0.04578\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00063: val_loss improved from 0.04578 to 0.04534, saving model to model.h5\n",
      "248/248 [==============================] - 0s 128us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00064: val_loss did not improve from 0.04534\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00065: val_loss did not improve from 0.04534\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0484 - mean_squared_error: 0.0484\n",
      "Epoch 00066: val_loss did not improve from 0.04534\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00067: val_loss improved from 0.04534 to 0.04414, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00068: val_loss did not improve from 0.04414\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00069: val_loss did not improve from 0.04414\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00070: val_loss did not improve from 0.04414\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00071: val_loss did not improve from 0.04414\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00072: val_loss did not improve from 0.04414\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00073: val_loss did not improve from 0.04414\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0144 - mean_squared_error: 0.0144\n",
      "Epoch 00074: val_loss did not improve from 0.04414\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00075: val_loss did not improve from 0.04414\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00076: val_loss did not improve from 0.04414\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0133 - mean_squared_error: 0.0133\n",
      "Epoch 00077: val_loss improved from 0.04414 to 0.04403, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Epoch 00078: val_loss improved from 0.04403 to 0.04343, saving model to model.h5\n",
      "248/248 [==============================] - 0s 185us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0231 - mean_squared_error: 0.0231\n",
      "Epoch 00079: val_loss did not improve from 0.04343\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0411 - mean_squared_error: 0.0411\n",
      "Epoch 00080: val_loss did not improve from 0.04343\n",
      "248/248 [==============================] - 0s 97us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00081: val_loss did not improve from 0.04343\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00082: val_loss did not improve from 0.04343\n",
      "248/248 [==============================] - 0s 105us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00083: val_loss did not improve from 0.04343\n",
      "248/248 [==============================] - 0s 97us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00084: val_loss did not improve from 0.04343\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00085: val_loss improved from 0.04343 to 0.04311, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00086: val_loss did not improve from 0.04311\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 00087: val_loss did not improve from 0.04311\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00088: val_loss did not improve from 0.04311\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00089: val_loss did not improve from 0.04311\n",
      "248/248 [==============================] - 0s 38us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00090: val_loss improved from 0.04311 to 0.04265, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00091: val_loss improved from 0.04265 to 0.04177, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0155 - mean_squared_error: 0.0155\n",
      "Epoch 00092: val_loss improved from 0.04177 to 0.04107, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00093: val_loss did not improve from 0.04107\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00094: val_loss did not improve from 0.04107\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00095: val_loss did not improve from 0.04107\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00096: val_loss did not improve from 0.04107\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00097: val_loss improved from 0.04107 to 0.04010, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00098: val_loss did not improve from 0.04010\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Epoch 00099: val_loss did not improve from 0.04010\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00100: val_loss did not improve from 0.04010\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00101: val_loss did not improve from 0.04010\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00102: val_loss did not improve from 0.04010\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00103: val_loss did not improve from 0.04010\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0126 - mean_squared_error: 0.0126\n",
      "Epoch 00104: val_loss did not improve from 0.04010\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00105: val_loss did not improve from 0.04010\n",
      "248/248 [==============================] - 0s 39us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00106: val_loss did not improve from 0.04010\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0458 - mean_squared_error: 0.0458\n",
      "Epoch 00107: val_loss did not improve from 0.04010\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00108: val_loss did not improve from 0.04010\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00109: val_loss improved from 0.04010 to 0.04008, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00110: val_loss did not improve from 0.04008\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00111: val_loss did not improve from 0.04008\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00112: val_loss did not improve from 0.04008\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00113: val_loss did not improve from 0.04008\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00114: val_loss did not improve from 0.04008\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0109 - mean_squared_error: 0.0109\n",
      "Epoch 00115: val_loss improved from 0.04008 to 0.03938, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00116: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00117: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0153 - mean_squared_error: 0.0153\n",
      "Epoch 00118: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00119: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00120: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00121: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00122: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00123: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00124: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00125: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00126: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00127: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0167 - mean_squared_error: 0.0167\n",
      "Epoch 00128: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00129: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 130/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00130: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00131: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 132/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00132: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 133/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00133: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 134/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00134: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 135/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00135: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 136/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00136: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 137/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00137: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 138/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00138: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 139/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0188 - mean_squared_error: 0.0188\n",
      "Epoch 00139: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 140/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00140: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 141/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00141: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 142/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0133 - mean_squared_error: 0.0133\n",
      "Epoch 00142: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 143/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00143: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 144/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0106 - mean_squared_error: 0.0106\n",
      "Epoch 00144: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 145/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00145: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 146/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00146: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 147/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00147: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 148/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00148: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 149/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0184 - mean_squared_error: 0.0184\n",
      "Epoch 00149: val_loss did not improve from 0.03938\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 150/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00150: val_loss improved from 0.03938 to 0.03879, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 151/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0112 - mean_squared_error: 0.0112\n",
      "Epoch 00151: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 152/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00152: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 153/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00153: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 154/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00154: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00155: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 156/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00156: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 157/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00157: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 158/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0092 - mean_squared_error: 0.0092\n",
      "Epoch 00158: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 159/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0184 - mean_squared_error: 0.0184\n",
      "Epoch 00159: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 160/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0152 - mean_squared_error: 0.0152\n",
      "Epoch 00160: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 161/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00161: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 162/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00162: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 163/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 00163: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 164/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00164: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 165/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00165: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 166/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00166: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 167/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0136 - mean_squared_error: 0.0136\n",
      "Epoch 00167: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 168/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Epoch 00168: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 169/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00169: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 170/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 00170: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 171/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0173 - mean_squared_error: 0.0173\n",
      "Epoch 00171: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 172/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00172: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 173/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00173: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 174/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0418 - mean_squared_error: 0.0418\n",
      "Epoch 00174: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 175/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0126 - mean_squared_error: 0.0126\n",
      "Epoch 00175: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 176/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00176: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 177/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00177: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 178/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0147 - mean_squared_error: 0.0147\n",
      "Epoch 00178: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 179/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00179: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 180/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0097 - mean_squared_error: 0.0097\n",
      "Epoch 00180: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 181/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00181: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 182/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00182: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 183/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00183: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 41us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 184/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0107 - mean_squared_error: 0.0107\n",
      "Epoch 00184: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 185/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0135 - mean_squared_error: 0.0135\n",
      "Epoch 00185: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 186/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0148 - mean_squared_error: 0.0148\n",
      "Epoch 00186: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 187/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0117 - mean_squared_error: 0.0117\n",
      "Epoch 00187: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 188/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00188: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 189/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00189: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 190/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00190: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 191/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Epoch 00191: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 41us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 192/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00192: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 193/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00193: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 194/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0326 - mean_squared_error: 0.0326\n",
      "Epoch 00194: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 195/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00195: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 196/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00196: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 197/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0116 - mean_squared_error: 0.0116\n",
      "Epoch 00197: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 198/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00198: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 199/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0150 - mean_squared_error: 0.0150\n",
      "Epoch 00199: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 200/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00200: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 201/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00201: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 202/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00202: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 203/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00203: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 204/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00204: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 205/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00205: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 206/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00206: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 207/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00207: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 42us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 208/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0131 - mean_squared_error: 0.0131\n",
      "Epoch 00208: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 209/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00209: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 210/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00210: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 211/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00211: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 212/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00212: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 213/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00213: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 214/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00214: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 215/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0121 - mean_squared_error: 0.0121\n",
      "Epoch 00215: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0175 - mean_squared_error: 0.0175 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 216/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00216: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 217/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0147 - mean_squared_error: 0.0147\n",
      "Epoch 00217: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 218/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0112 - mean_squared_error: 0.0112\n",
      "Epoch 00218: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 219/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0094 - mean_squared_error: 0.0094\n",
      "Epoch 00219: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 220/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00220: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 221/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00221: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 222/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0112 - mean_squared_error: 0.0112\n",
      "Epoch 00222: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 223/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00223: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 224/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00224: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 225/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0118 - mean_squared_error: 0.0118\n",
      "Epoch 00225: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0168 - mean_squared_error: 0.0168 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 226/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00226: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 227/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00227: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 228/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.0185\n",
      "Epoch 00228: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 229/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00229: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 47us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 230/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0160 - mean_squared_error: 0.0160\n",
      "Epoch 00230: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 231/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00231: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 232/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0138 - mean_squared_error: 0.0138\n",
      "Epoch 00232: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 233/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0139 - mean_squared_error: 0.0139\n",
      "Epoch 00233: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 234/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0082 - mean_squared_error: 0.0082\n",
      "Epoch 00234: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 235/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00235: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0166 - mean_squared_error: 0.0166 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 236/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0102 - mean_squared_error: 0.0102\n",
      "Epoch 00236: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 237/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00237: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 238/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0142 - mean_squared_error: 0.0142\n",
      "Epoch 00238: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 239/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0128 - mean_squared_error: 0.0128\n",
      "Epoch 00239: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 240/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0098 - mean_squared_error: 0.0098\n",
      "Epoch 00240: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0172 - mean_squared_error: 0.0172 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 241/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00241: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 242/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00242: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 243/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00243: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 244/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00244: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 245/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00245: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 246/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00246: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 247/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00247: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 248/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00248: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 249/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0156 - mean_squared_error: 0.0156\n",
      "Epoch 00249: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 250/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0339 - mean_squared_error: 0.0339\n",
      "Epoch 00250: val_loss did not improve from 0.03879\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Running trial 10\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.1448 - mean_squared_error: 1.1448\n",
      "Epoch 00001: val_loss improved from inf to 0.66786, saving model to model.h5\n",
      "248/248 [==============================] - 2s 6ms/sample - loss: 1.0311 - mean_squared_error: 1.0311 - val_loss: 0.6679 - val_mean_squared_error: 0.6679\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.0879 - mean_squared_error: 1.0879\n",
      "Epoch 00002: val_loss improved from 0.66786 to 0.63230, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.9173 - mean_squared_error: 0.9173 - val_loss: 0.6323 - val_mean_squared_error: 0.6323\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9469 - mean_squared_error: 0.9469\n",
      "Epoch 00003: val_loss improved from 0.63230 to 0.60461, saving model to model.h5\n",
      "248/248 [==============================] - 0s 118us/sample - loss: 0.8274 - mean_squared_error: 0.8274 - val_loss: 0.6046 - val_mean_squared_error: 0.6046\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7842 - mean_squared_error: 0.7842\n",
      "Epoch 00004: val_loss improved from 0.60461 to 0.57899, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.7658 - mean_squared_error: 0.7658 - val_loss: 0.5790 - val_mean_squared_error: 0.5790\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7341 - mean_squared_error: 0.7341\n",
      "Epoch 00005: val_loss improved from 0.57899 to 0.55384, saving model to model.h5\n",
      "248/248 [==============================] - 0s 118us/sample - loss: 0.7106 - mean_squared_error: 0.7106 - val_loss: 0.5538 - val_mean_squared_error: 0.5538\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8358 - mean_squared_error: 0.8358\n",
      "Epoch 00006: val_loss improved from 0.55384 to 0.53046, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.6435 - mean_squared_error: 0.6435 - val_loss: 0.5305 - val_mean_squared_error: 0.5305\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6490 - mean_squared_error: 0.6490\n",
      "Epoch 00007: val_loss improved from 0.53046 to 0.50744, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.6067 - mean_squared_error: 0.6067 - val_loss: 0.5074 - val_mean_squared_error: 0.5074\n",
      "Epoch 8/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5446 - mean_squared_error: 0.5446\n",
      "Epoch 00008: val_loss improved from 0.50744 to 0.48455, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.5569 - mean_squared_error: 0.5569 - val_loss: 0.4845 - val_mean_squared_error: 0.4845\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5146 - mean_squared_error: 0.5146\n",
      "Epoch 00009: val_loss improved from 0.48455 to 0.46392, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.5069 - mean_squared_error: 0.5069 - val_loss: 0.4639 - val_mean_squared_error: 0.4639\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4859 - mean_squared_error: 0.4859\n",
      "Epoch 00010: val_loss improved from 0.46392 to 0.44355, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.4688 - mean_squared_error: 0.4688 - val_loss: 0.4436 - val_mean_squared_error: 0.4436\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4545 - mean_squared_error: 0.4545\n",
      "Epoch 00011: val_loss improved from 0.44355 to 0.42208, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.4356 - mean_squared_error: 0.4356 - val_loss: 0.4221 - val_mean_squared_error: 0.4221\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4580 - mean_squared_error: 0.4580\n",
      "Epoch 00012: val_loss improved from 0.42208 to 0.39886, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.3927 - mean_squared_error: 0.3927 - val_loss: 0.3989 - val_mean_squared_error: 0.3989\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3970 - mean_squared_error: 0.3970\n",
      "Epoch 00013: val_loss improved from 0.39886 to 0.37739, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.3572 - mean_squared_error: 0.3572 - val_loss: 0.3774 - val_mean_squared_error: 0.3774\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3127 - mean_squared_error: 0.3127\n",
      "Epoch 00014: val_loss improved from 0.37739 to 0.35516, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.3318 - mean_squared_error: 0.3318 - val_loss: 0.3552 - val_mean_squared_error: 0.3552\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2901 - mean_squared_error: 0.2901\n",
      "Epoch 00015: val_loss improved from 0.35516 to 0.33303, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.3041 - mean_squared_error: 0.3041 - val_loss: 0.3330 - val_mean_squared_error: 0.3330\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3113 - mean_squared_error: 0.3113\n",
      "Epoch 00016: val_loss improved from 0.33303 to 0.31089, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.2739 - mean_squared_error: 0.2739 - val_loss: 0.3109 - val_mean_squared_error: 0.3109\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2662 - mean_squared_error: 0.2662\n",
      "Epoch 00017: val_loss improved from 0.31089 to 0.28928, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.2463 - mean_squared_error: 0.2463 - val_loss: 0.2893 - val_mean_squared_error: 0.2893\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2443 - mean_squared_error: 0.2443\n",
      "Epoch 00018: val_loss improved from 0.28928 to 0.26959, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.2231 - mean_squared_error: 0.2231 - val_loss: 0.2696 - val_mean_squared_error: 0.2696\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1795 - mean_squared_error: 0.1795\n",
      "Epoch 00019: val_loss improved from 0.26959 to 0.24899, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.2098 - mean_squared_error: 0.2098 - val_loss: 0.2490 - val_mean_squared_error: 0.2490\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1773 - mean_squared_error: 0.1773\n",
      "Epoch 00020: val_loss improved from 0.24899 to 0.23183, saving model to model.h5\n",
      "248/248 [==============================] - 0s 118us/sample - loss: 0.1845 - mean_squared_error: 0.1845 - val_loss: 0.2318 - val_mean_squared_error: 0.2318\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1467 - mean_squared_error: 0.1467\n",
      "Epoch 00021: val_loss improved from 0.23183 to 0.21328, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.1537 - mean_squared_error: 0.1537 - val_loss: 0.2133 - val_mean_squared_error: 0.2133\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1527 - mean_squared_error: 0.1527\n",
      "Epoch 00022: val_loss improved from 0.21328 to 0.19556, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.1491 - mean_squared_error: 0.1491 - val_loss: 0.1956 - val_mean_squared_error: 0.1956\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1301 - mean_squared_error: 0.1301\n",
      "Epoch 00023: val_loss improved from 0.19556 to 0.17891, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.1309 - mean_squared_error: 0.1309 - val_loss: 0.1789 - val_mean_squared_error: 0.1789\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1206 - mean_squared_error: 0.1206\n",
      "Epoch 00024: val_loss improved from 0.17891 to 0.16286, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.1156 - mean_squared_error: 0.1156 - val_loss: 0.1629 - val_mean_squared_error: 0.1629\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1011 - mean_squared_error: 0.1011\n",
      "Epoch 00025: val_loss improved from 0.16286 to 0.14850, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.1049 - mean_squared_error: 0.1049 - val_loss: 0.1485 - val_mean_squared_error: 0.1485\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1051 - mean_squared_error: 0.1051\n",
      "Epoch 00026: val_loss improved from 0.14850 to 0.13506, saving model to model.h5\n",
      "248/248 [==============================] - 0s 122us/sample - loss: 0.1033 - mean_squared_error: 0.1033 - val_loss: 0.1351 - val_mean_squared_error: 0.1351\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0928 - mean_squared_error: 0.0928\n",
      "Epoch 00027: val_loss improved from 0.13506 to 0.12357, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0854 - mean_squared_error: 0.0854 - val_loss: 0.1236 - val_mean_squared_error: 0.1236\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0783 - mean_squared_error: 0.0783\n",
      "Epoch 00028: val_loss improved from 0.12357 to 0.11163, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0771 - mean_squared_error: 0.0771 - val_loss: 0.1116 - val_mean_squared_error: 0.1116\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0834 - mean_squared_error: 0.0834\n",
      "Epoch 00029: val_loss improved from 0.11163 to 0.10115, saving model to model.h5\n",
      "248/248 [==============================] - 0s 130us/sample - loss: 0.0735 - mean_squared_error: 0.0735 - val_loss: 0.1012 - val_mean_squared_error: 0.1012\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0710 - mean_squared_error: 0.0710\n",
      "Epoch 00030: val_loss improved from 0.10115 to 0.09193, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0665 - mean_squared_error: 0.0665 - val_loss: 0.0919 - val_mean_squared_error: 0.0919\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0684 - mean_squared_error: 0.0684\n",
      "Epoch 00031: val_loss improved from 0.09193 to 0.08382, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0658 - mean_squared_error: 0.0658 - val_loss: 0.0838 - val_mean_squared_error: 0.0838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0568 - mean_squared_error: 0.0568\n",
      "Epoch 00032: val_loss improved from 0.08382 to 0.07643, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0569 - mean_squared_error: 0.0569 - val_loss: 0.0764 - val_mean_squared_error: 0.0764\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0608 - mean_squared_error: 0.0608\n",
      "Epoch 00033: val_loss improved from 0.07643 to 0.06965, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0538 - mean_squared_error: 0.0538 - val_loss: 0.0696 - val_mean_squared_error: 0.0696\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0558 - mean_squared_error: 0.0558\n",
      "Epoch 00034: val_loss improved from 0.06965 to 0.06407, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0549 - mean_squared_error: 0.0549 - val_loss: 0.0641 - val_mean_squared_error: 0.0641\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0604 - mean_squared_error: 0.0604\n",
      "Epoch 00035: val_loss improved from 0.06407 to 0.05953, saving model to model.h5\n",
      "248/248 [==============================] - 0s 143us/sample - loss: 0.0487 - mean_squared_error: 0.0487 - val_loss: 0.0595 - val_mean_squared_error: 0.0595\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0502 - mean_squared_error: 0.0502\n",
      "Epoch 00036: val_loss improved from 0.05953 to 0.05618, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0456 - mean_squared_error: 0.0456 - val_loss: 0.0562 - val_mean_squared_error: 0.0562\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0454 - mean_squared_error: 0.0454\n",
      "Epoch 00037: val_loss improved from 0.05618 to 0.05250, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0425 - mean_squared_error: 0.0425 - val_loss: 0.0525 - val_mean_squared_error: 0.0525\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0398 - mean_squared_error: 0.0398\n",
      "Epoch 00038: val_loss improved from 0.05250 to 0.04987, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Epoch 00039: val_loss improved from 0.04987 to 0.04813, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0399 - mean_squared_error: 0.0399 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0407 - mean_squared_error: 0.0407\n",
      "Epoch 00040: val_loss improved from 0.04813 to 0.04771, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0388 - mean_squared_error: 0.0388 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0399 - mean_squared_error: 0.0399\n",
      "Epoch 00041: val_loss improved from 0.04771 to 0.04382, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0334 - mean_squared_error: 0.0334\n",
      "Epoch 00042: val_loss improved from 0.04382 to 0.04303, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00043: val_loss did not improve from 0.04303\n",
      "248/248 [==============================] - 0s 47us/sample - loss: 0.0386 - mean_squared_error: 0.0386 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00044: val_loss improved from 0.04303 to 0.04209, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0363 - mean_squared_error: 0.0363 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0425 - mean_squared_error: 0.0425\n",
      "Epoch 00045: val_loss improved from 0.04209 to 0.04058, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0295 - mean_squared_error: 0.0295\n",
      "Epoch 00046: val_loss improved from 0.04058 to 0.04040, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0368 - mean_squared_error: 0.0368\n",
      "Epoch 00047: val_loss did not improve from 0.04040\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00048: val_loss improved from 0.04040 to 0.03974, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0370 - mean_squared_error: 0.0370\n",
      "Epoch 00049: val_loss did not improve from 0.03974\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0377 - mean_squared_error: 0.0377\n",
      "Epoch 00050: val_loss improved from 0.03974 to 0.03966, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0366 - mean_squared_error: 0.0366\n",
      "Epoch 00051: val_loss improved from 0.03966 to 0.03762, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00052: val_loss did not improve from 0.03762\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0404 - mean_squared_error: 0.0404\n",
      "Epoch 00053: val_loss did not improve from 0.03762\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00054: val_loss did not improve from 0.03762\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00055: val_loss did not improve from 0.03762\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0323 - mean_squared_error: 0.0323 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0294 - mean_squared_error: 0.0294\n",
      "Epoch 00056: val_loss did not improve from 0.03762\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0259 - mean_squared_error: 0.0259\n",
      "Epoch 00057: val_loss did not improve from 0.03762\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Epoch 00058: val_loss improved from 0.03762 to 0.03734, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00059: val_loss did not improve from 0.03734\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00060: val_loss did not improve from 0.03734\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00061: val_loss did not improve from 0.03734\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00062: val_loss did not improve from 0.03734\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00063: val_loss did not improve from 0.03734\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00064: val_loss did not improve from 0.03734\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00065: val_loss did not improve from 0.03734\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00066: val_loss did not improve from 0.03734\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0383 - val_mean_squared_error: 0.0383\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00067: val_loss did not improve from 0.03734\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00068: val_loss did not improve from 0.03734\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Epoch 00069: val_loss improved from 0.03734 to 0.03686, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00070: val_loss did not improve from 0.03686\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00071: val_loss did not improve from 0.03686\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0407 - mean_squared_error: 0.0407\n",
      "Epoch 00072: val_loss did not improve from 0.03686\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00073: val_loss improved from 0.03686 to 0.03637, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00074: val_loss did not improve from 0.03637\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0374 - mean_squared_error: 0.0374\n",
      "Epoch 00075: val_loss did not improve from 0.03637\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00076: val_loss did not improve from 0.03637\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00077: val_loss did not improve from 0.03637\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00078: val_loss did not improve from 0.03637\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0383 - val_mean_squared_error: 0.0383\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Epoch 00079: val_loss did not improve from 0.03637\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00080: val_loss did not improve from 0.03637\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00081: val_loss did not improve from 0.03637\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0107 - mean_squared_error: 0.0107\n",
      "Epoch 00082: val_loss did not improve from 0.03637\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0441 - mean_squared_error: 0.0441\n",
      "Epoch 00083: val_loss did not improve from 0.03637\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00084: val_loss did not improve from 0.03637\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00085: val_loss did not improve from 0.03637\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00086: val_loss did not improve from 0.03637\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00087: val_loss did not improve from 0.03637\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00088: val_loss improved from 0.03637 to 0.03586, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0359 - val_mean_squared_error: 0.0359\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00089: val_loss improved from 0.03586 to 0.03555, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0355 - val_mean_squared_error: 0.0355\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00090: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0365 - val_mean_squared_error: 0.0365\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00091: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00092: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00093: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0360 - val_mean_squared_error: 0.0360\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0187 - mean_squared_error: 0.0187\n",
      "Epoch 00094: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00095: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0359 - val_mean_squared_error: 0.0359\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00096: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0360 - val_mean_squared_error: 0.0360\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00097: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0361 - val_mean_squared_error: 0.0361\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00098: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00099: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Epoch 00100: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0367 - val_mean_squared_error: 0.0367\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00101: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00102: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00103: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00104: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0176 - mean_squared_error: 0.0176\n",
      "Epoch 00105: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00106: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0361 - val_mean_squared_error: 0.0361\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0341 - mean_squared_error: 0.0341\n",
      "Epoch 00107: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00108: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0170 - mean_squared_error: 0.0170\n",
      "Epoch 00109: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0383 - val_mean_squared_error: 0.0383\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00110: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00111: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00112: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0363 - val_mean_squared_error: 0.0363\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00113: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0366 - val_mean_squared_error: 0.0366\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0144 - mean_squared_error: 0.0144\n",
      "Epoch 00114: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00115: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0134 - mean_squared_error: 0.0134\n",
      "Epoch 00116: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00117: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0382 - mean_squared_error: 0.0382\n",
      "Epoch 00118: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00119: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00120: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0367 - val_mean_squared_error: 0.0367\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00121: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0152 - mean_squared_error: 0.0152\n",
      "Epoch 00122: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0367 - val_mean_squared_error: 0.0367\n",
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00123: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00124: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0363 - val_mean_squared_error: 0.0363\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00125: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0366 - val_mean_squared_error: 0.0366\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00126: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0360 - val_mean_squared_error: 0.0360\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0144 - mean_squared_error: 0.0144\n",
      "Epoch 00127: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00128: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0148 - mean_squared_error: 0.0148\n",
      "Epoch 00129: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0361 - val_mean_squared_error: 0.0361\n",
      "Epoch 130/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00130: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0365 - val_mean_squared_error: 0.0365\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00131: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0361 - val_mean_squared_error: 0.0361\n",
      "Epoch 132/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00132: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0359 - val_mean_squared_error: 0.0359\n",
      "Epoch 133/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00133: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0366 - val_mean_squared_error: 0.0366\n",
      "Epoch 134/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00134: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 135/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00135: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 136/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00136: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 137/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00137: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0362 - val_mean_squared_error: 0.0362\n",
      "Epoch 138/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00138: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 47us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0358 - val_mean_squared_error: 0.0358\n",
      "Epoch 139/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00139: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 140/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00140: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0366 - val_mean_squared_error: 0.0366\n",
      "Epoch 141/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00141: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0359 - val_mean_squared_error: 0.0359\n",
      "Epoch 142/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00142: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 143/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00143: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0362 - val_mean_squared_error: 0.0362\n",
      "Epoch 144/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0108 - mean_squared_error: 0.0108\n",
      "Epoch 00144: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0366 - val_mean_squared_error: 0.0366\n",
      "Epoch 145/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00145: val_loss did not improve from 0.03555\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0358 - val_mean_squared_error: 0.0358\n",
      "Epoch 146/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00146: val_loss improved from 0.03555 to 0.03525, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0353 - val_mean_squared_error: 0.0353\n",
      "Epoch 147/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00147: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 148/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00148: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0359 - val_mean_squared_error: 0.0359\n",
      "Epoch 149/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00149: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 150/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00150: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 151/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00151: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 152/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00152: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 153/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00153: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 154/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00154: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 155/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00155: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 156/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0154 - mean_squared_error: 0.0154\n",
      "Epoch 00156: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0367 - val_mean_squared_error: 0.0367\n",
      "Epoch 157/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0171 - mean_squared_error: 0.0171\n",
      "Epoch 00157: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 158/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00158: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 159/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00159: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 160/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00160: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0107 - mean_squared_error: 0.0107\n",
      "Epoch 00161: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 162/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0171 - mean_squared_error: 0.0171\n",
      "Epoch 00162: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 163/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0128 - mean_squared_error: 0.0128\n",
      "Epoch 00163: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 164/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00164: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 165/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00165: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 166/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00166: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0383 - val_mean_squared_error: 0.0383\n",
      "Epoch 167/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0333 - mean_squared_error: 0.0333\n",
      "Epoch 00167: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 168/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00168: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 169/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 00169: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 170/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00170: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 171/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00171: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 172/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00172: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 173/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00173: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 174/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00174: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 175/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0097 - mean_squared_error: 0.0097\n",
      "Epoch 00175: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 176/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0112 - mean_squared_error: 0.0112\n",
      "Epoch 00176: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 177/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00177: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 178/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00178: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 179/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0156 - mean_squared_error: 0.0156\n",
      "Epoch 00179: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 180/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00180: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 181/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00181: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 182/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00182: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 183/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0116 - mean_squared_error: 0.0116\n",
      "Epoch 00183: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 184/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0130 - mean_squared_error: 0.0130\n",
      "Epoch 00184: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 185/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0108 - mean_squared_error: 0.0108\n",
      "Epoch 00185: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 186/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00186: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 187/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0127 - mean_squared_error: 0.0127\n",
      "Epoch 00187: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 188/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00188: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 189/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00189: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 190/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0358 - mean_squared_error: 0.0358\n",
      "Epoch 00190: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 191/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0065 - mean_squared_error: 0.0065\n",
      "Epoch 00191: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 192/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00192: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 193/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0130 - mean_squared_error: 0.0130\n",
      "Epoch 00193: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 194/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0174 - mean_squared_error: 0.0174\n",
      "Epoch 00194: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 195/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00195: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 196/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00196: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 197/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0141 - mean_squared_error: 0.0141\n",
      "Epoch 00197: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 198/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0159 - mean_squared_error: 0.0159\n",
      "Epoch 00198: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 199/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00199: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 200/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00200: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 201/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00201: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0383 - val_mean_squared_error: 0.0383\n",
      "Epoch 202/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0129 - mean_squared_error: 0.0129\n",
      "Epoch 00202: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 203/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0326 - mean_squared_error: 0.0326\n",
      "Epoch 00203: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 204/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00204: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0172 - mean_squared_error: 0.0172 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 205/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00205: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 206/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0150 - mean_squared_error: 0.0150\n",
      "Epoch 00206: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 207/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00207: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 208/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00208: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 209/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0148 - mean_squared_error: 0.0148\n",
      "Epoch 00209: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 210/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0122 - mean_squared_error: 0.0122\n",
      "Epoch 00210: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 211/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0151 - mean_squared_error: 0.0151\n",
      "Epoch 00211: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 92us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 212/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00212: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 213/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00213: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 214/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00214: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 215/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0115 - mean_squared_error: 0.0115\n",
      "Epoch 00215: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0169 - mean_squared_error: 0.0169 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 216/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00216: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 217/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00217: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0176 - mean_squared_error: 0.0176 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 218/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00218: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 219/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00219: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 220/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00220: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 221/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00221: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 222/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00222: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 223/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00223: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 224/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.0185\n",
      "Epoch 00224: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 225/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0365 - mean_squared_error: 0.0365\n",
      "Epoch 00225: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 226/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0142 - mean_squared_error: 0.0142\n",
      "Epoch 00226: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 227/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00227: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 228/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 00228: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 51us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 229/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0139 - mean_squared_error: 0.0139\n",
      "Epoch 00229: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 230/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00230: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0173 - mean_squared_error: 0.0173 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 231/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0124 - mean_squared_error: 0.0124\n",
      "Epoch 00231: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0169 - mean_squared_error: 0.0169 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 232/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00232: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 233/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0161 - mean_squared_error: 0.0161\n",
      "Epoch 00233: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 234/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00234: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 235/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0150 - mean_squared_error: 0.0150\n",
      "Epoch 00235: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 236/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00236: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 237/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00237: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 238/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00238: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0174 - mean_squared_error: 0.0174 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 239/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00239: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 240/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0107 - mean_squared_error: 0.0107\n",
      "Epoch 00240: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0169 - mean_squared_error: 0.0169 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 241/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00241: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 242/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00242: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 243/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0187 - mean_squared_error: 0.0187\n",
      "Epoch 00243: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 244/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0104 - mean_squared_error: 0.0104\n",
      "Epoch 00244: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 245/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00245: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 246/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00246: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "import csv  \n",
    "\n",
    "# rounding off\n",
    "Round = 3      \n",
    "\n",
    "test_pred_lstm  = [] \n",
    "\n",
    "for fold in range(trials):\n",
    "    print(f'Running trial {fold+1}')\n",
    "    print(X_train.shape)\n",
    "    \n",
    "    X_t_reshaped   = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_val_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "\n",
    "    check = ModelCheckpoint('model.h5', verbose=1, save_best_only=True)\n",
    "\n",
    "    early = EarlyStopping(patience=100)\n",
    "\n",
    "    optimizer = Adam(lr=0.00001)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(1, 6)))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(loss='mse', optimizer='rmsprop', metrics=['mse'])\n",
    "\n",
    "    X_t_reshaped   = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_val_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "\n",
    "    history = model.fit(X_t_reshaped, \n",
    "                    y_train, \n",
    "                    validation_split = 0.2,\n",
    "                    epochs=1000, \n",
    "                    batch_size=96, \n",
    "                    verbose=1, callbacks=[check, early])\n",
    "    X_val_reshaped = X_val_reshaped.reshape(X_test.shape[0], 6)\n",
    "\n",
    "    #running function\n",
    "    test_forecast = X_test\n",
    "    y_lstm = forecast(model, test_forecast, gwl, steps_ahead)\n",
    "    y_lstm = np.array(y_lstm)\n",
    "    \n",
    "    #metrics for test\n",
    "    test_pred_lstm.append(y_lstm)\n",
    "    mse_lstm = mean_squared_error(y_test, y_lstm)\n",
    "    r2_lstm = r2_score(y_test, y_lstm)\n",
    "    lstm_ave_r2.append(r2_lstm)\n",
    "    lstm_ave_mse.append(mse_lstm)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ccaed5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8198585707785504,\n",
       " 0.8080336496758007,\n",
       " 0.8050464636597415,\n",
       " 0.8206611672486461,\n",
       " 0.7847501755061803,\n",
       " 0.8032901432298686,\n",
       " 0.7822969351967293,\n",
       " 0.8203647233725851,\n",
       " 0.8089392932526223,\n",
       " 0.817146441683096]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_r2_new = ave(lstm_ave_r2)\n",
    "lstm_mse_new = ave(lstm_ave_mse)\n",
    "lstm_ave_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a47e54",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "676bbf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running trial 1\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 1.1482 - mean_squared_error: 1.1482\n",
      "Epoch 00001: val_loss improved from inf to 0.76655, saving model to model.h5\n",
      "248/248 [==============================] - 2s 6ms/sample - loss: 1.1052 - mean_squared_error: 1.1052 - val_loss: 0.7665 - val_mean_squared_error: 0.7665\n",
      "Epoch 2/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.9395 - mean_squared_error: 0.9395\n",
      "Epoch 00002: val_loss improved from 0.76655 to 0.73224, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 1.0078 - mean_squared_error: 1.0078 - val_loss: 0.7322 - val_mean_squared_error: 0.7322\n",
      "Epoch 3/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 1.0186 - mean_squared_error: 1.0186\n",
      "Epoch 00003: val_loss improved from 0.73224 to 0.69882, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.9118 - mean_squared_error: 0.9118 - val_loss: 0.6988 - val_mean_squared_error: 0.6988\n",
      "Epoch 4/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.7515 - mean_squared_error: 0.7515\n",
      "Epoch 00004: val_loss improved from 0.69882 to 0.66415, saving model to model.h5\n",
      "248/248 [==============================] - 0s 154us/sample - loss: 0.8393 - mean_squared_error: 0.8393 - val_loss: 0.6641 - val_mean_squared_error: 0.6641\n",
      "Epoch 5/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.7504 - mean_squared_error: 0.7504\n",
      "Epoch 00005: val_loss improved from 0.66415 to 0.63065, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.7616 - mean_squared_error: 0.7616 - val_loss: 0.6306 - val_mean_squared_error: 0.6306\n",
      "Epoch 6/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.6822 - mean_squared_error: 0.6822\n",
      "Epoch 00006: val_loss improved from 0.63065 to 0.59747, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.7067 - mean_squared_error: 0.7067 - val_loss: 0.5975 - val_mean_squared_error: 0.5975\n",
      "Epoch 7/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.6670 - mean_squared_error: 0.6670\n",
      "Epoch 00007: val_loss improved from 0.59747 to 0.56391, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.6281 - mean_squared_error: 0.6281 - val_loss: 0.5639 - val_mean_squared_error: 0.5639\n",
      "Epoch 8/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.6448 - mean_squared_error: 0.6448\n",
      "Epoch 00008: val_loss improved from 0.56391 to 0.53051, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.5495 - mean_squared_error: 0.5495 - val_loss: 0.5305 - val_mean_squared_error: 0.5305\n",
      "Epoch 9/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.5006 - mean_squared_error: 0.5006\n",
      "Epoch 00009: val_loss improved from 0.53051 to 0.49769, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.4865 - mean_squared_error: 0.4865 - val_loss: 0.4977 - val_mean_squared_error: 0.4977\n",
      "Epoch 10/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.4568 - mean_squared_error: 0.4568\n",
      "Epoch 00010: val_loss improved from 0.49769 to 0.46522, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.4392 - mean_squared_error: 0.4392 - val_loss: 0.4652 - val_mean_squared_error: 0.4652\n",
      "Epoch 11/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3886 - mean_squared_error: 0.3886\n",
      "Epoch 00011: val_loss improved from 0.46522 to 0.43331, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.3855 - mean_squared_error: 0.3855 - val_loss: 0.4333 - val_mean_squared_error: 0.4333\n",
      "Epoch 12/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.4000 - mean_squared_error: 0.4000\n",
      "Epoch 00012: val_loss improved from 0.43331 to 0.40157, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.3467 - mean_squared_error: 0.3467 - val_loss: 0.4016 - val_mean_squared_error: 0.4016\n",
      "Epoch 13/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3042 - mean_squared_error: 0.3042\n",
      "Epoch 00013: val_loss improved from 0.40157 to 0.37032, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.3049 - mean_squared_error: 0.3049 - val_loss: 0.3703 - val_mean_squared_error: 0.3703\n",
      "Epoch 14/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2878 - mean_squared_error: 0.2878\n",
      "Epoch 00014: val_loss improved from 0.37032 to 0.34039, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.2663 - mean_squared_error: 0.2663 - val_loss: 0.3404 - val_mean_squared_error: 0.3404\n",
      "Epoch 15/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2508 - mean_squared_error: 0.2508\n",
      "Epoch 00015: val_loss improved from 0.34039 to 0.31145, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.2354 - mean_squared_error: 0.2354 - val_loss: 0.3114 - val_mean_squared_error: 0.3114\n",
      "Epoch 16/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2293 - mean_squared_error: 0.2293\n",
      "Epoch 00016: val_loss improved from 0.31145 to 0.28387, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.2154 - mean_squared_error: 0.2154 - val_loss: 0.2839 - val_mean_squared_error: 0.2839\n",
      "Epoch 17/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1892 - mean_squared_error: 0.1892\n",
      "Epoch 00017: val_loss improved from 0.28387 to 0.25761, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.1924 - mean_squared_error: 0.1924 - val_loss: 0.2576 - val_mean_squared_error: 0.2576\n",
      "Epoch 18/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1423 - mean_squared_error: 0.1423\n",
      "Epoch 00018: val_loss improved from 0.25761 to 0.23281, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.1613 - mean_squared_error: 0.1613 - val_loss: 0.2328 - val_mean_squared_error: 0.2328\n",
      "Epoch 19/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1427 - mean_squared_error: 0.1427\n",
      "Epoch 00019: val_loss improved from 0.23281 to 0.21020, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.1418 - mean_squared_error: 0.1418 - val_loss: 0.2102 - val_mean_squared_error: 0.2102\n",
      "Epoch 20/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1237 - mean_squared_error: 0.1237\n",
      "Epoch 00020: val_loss improved from 0.21020 to 0.18962, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.1333 - mean_squared_error: 0.1333 - val_loss: 0.1896 - val_mean_squared_error: 0.1896\n",
      "Epoch 21/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1182 - mean_squared_error: 0.1182\n",
      "Epoch 00021: val_loss improved from 0.18962 to 0.17075, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.1199 - mean_squared_error: 0.1199 - val_loss: 0.1707 - val_mean_squared_error: 0.1707\n",
      "Epoch 22/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1201 - mean_squared_error: 0.1201\n",
      "Epoch 00022: val_loss improved from 0.17075 to 0.15361, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.1083 - mean_squared_error: 0.1083 - val_loss: 0.1536 - val_mean_squared_error: 0.1536\n",
      "Epoch 23/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1045 - mean_squared_error: 0.1045\n",
      "Epoch 00023: val_loss improved from 0.15361 to 0.13813, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.1065 - mean_squared_error: 0.1065 - val_loss: 0.1381 - val_mean_squared_error: 0.1381\n",
      "Epoch 24/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0892 - mean_squared_error: 0.0892\n",
      "Epoch 00024: val_loss improved from 0.13813 to 0.12433, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0890 - mean_squared_error: 0.0890 - val_loss: 0.1243 - val_mean_squared_error: 0.1243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0761 - mean_squared_error: 0.0761\n",
      "Epoch 00025: val_loss improved from 0.12433 to 0.11230, saving model to model.h5\n",
      "248/248 [==============================] - 0s 253us/sample - loss: 0.0759 - mean_squared_error: 0.0759 - val_loss: 0.1123 - val_mean_squared_error: 0.1123\n",
      "Epoch 26/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0830 - mean_squared_error: 0.0830\n",
      "Epoch 00026: val_loss improved from 0.11230 to 0.10172, saving model to model.h5\n",
      "248/248 [==============================] - 0s 181us/sample - loss: 0.0721 - mean_squared_error: 0.0721 - val_loss: 0.1017 - val_mean_squared_error: 0.1017\n",
      "Epoch 27/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0835 - mean_squared_error: 0.0835\n",
      "Epoch 00027: val_loss improved from 0.10172 to 0.09242, saving model to model.h5\n",
      "248/248 [==============================] - 0s 173us/sample - loss: 0.0732 - mean_squared_error: 0.0732 - val_loss: 0.0924 - val_mean_squared_error: 0.0924\n",
      "Epoch 28/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0448 - mean_squared_error: 0.0448\n",
      "Epoch 00028: val_loss improved from 0.09242 to 0.08452, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.0545 - mean_squared_error: 0.0545 - val_loss: 0.0845 - val_mean_squared_error: 0.0845\n",
      "Epoch 29/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0515 - mean_squared_error: 0.0515\n",
      "Epoch 00029: val_loss improved from 0.08452 to 0.07770, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0624 - mean_squared_error: 0.0624 - val_loss: 0.0777 - val_mean_squared_error: 0.0777\n",
      "Epoch 30/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0508 - mean_squared_error: 0.0508\n",
      "Epoch 00030: val_loss improved from 0.07770 to 0.07190, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0558 - mean_squared_error: 0.0558 - val_loss: 0.0719 - val_mean_squared_error: 0.0719\n",
      "Epoch 31/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0433 - mean_squared_error: 0.0433\n",
      "Epoch 00031: val_loss improved from 0.07190 to 0.06703, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0553 - mean_squared_error: 0.0553 - val_loss: 0.0670 - val_mean_squared_error: 0.0670\n",
      "Epoch 32/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0510 - mean_squared_error: 0.0510\n",
      "Epoch 00032: val_loss improved from 0.06703 to 0.06299, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0503 - mean_squared_error: 0.0503 - val_loss: 0.0630 - val_mean_squared_error: 0.0630\n",
      "Epoch 33/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0458 - mean_squared_error: 0.0458\n",
      "Epoch 00033: val_loss improved from 0.06299 to 0.05972, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0492 - mean_squared_error: 0.0492 - val_loss: 0.0597 - val_mean_squared_error: 0.0597\n",
      "Epoch 34/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0712 - mean_squared_error: 0.0712\n",
      "Epoch 00034: val_loss improved from 0.05972 to 0.05719, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0547 - mean_squared_error: 0.0547 - val_loss: 0.0572 - val_mean_squared_error: 0.0572\n",
      "Epoch 35/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0460 - mean_squared_error: 0.0460\n",
      "Epoch 00035: val_loss improved from 0.05719 to 0.05528, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.0450 - mean_squared_error: 0.0450 - val_loss: 0.0553 - val_mean_squared_error: 0.0553\n",
      "Epoch 36/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0519 - mean_squared_error: 0.0519\n",
      "Epoch 00036: val_loss improved from 0.05528 to 0.05373, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0484 - mean_squared_error: 0.0484 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n",
      "Epoch 37/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Epoch 00037: val_loss improved from 0.05373 to 0.05265, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0445 - mean_squared_error: 0.0445 - val_loss: 0.0526 - val_mean_squared_error: 0.0526\n",
      "Epoch 38/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0427 - mean_squared_error: 0.0427\n",
      "Epoch 00038: val_loss improved from 0.05265 to 0.05187, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "Epoch 39/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0426 - mean_squared_error: 0.0426\n",
      "Epoch 00039: val_loss improved from 0.05187 to 0.05136, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0514 - val_mean_squared_error: 0.0514\n",
      "Epoch 40/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0411 - mean_squared_error: 0.0411\n",
      "Epoch 00040: val_loss improved from 0.05136 to 0.05109, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0437 - mean_squared_error: 0.0437 - val_loss: 0.0511 - val_mean_squared_error: 0.0511\n",
      "Epoch 41/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Epoch 00041: val_loss improved from 0.05109 to 0.05099, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 42/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0405 - mean_squared_error: 0.0405\n",
      "Epoch 00042: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0425 - mean_squared_error: 0.0425 - val_loss: 0.0511 - val_mean_squared_error: 0.0511\n",
      "Epoch 43/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Epoch 00043: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0389 - mean_squared_error: 0.0389 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 44/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 00044: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 45/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0457 - mean_squared_error: 0.0457\n",
      "Epoch 00045: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0443 - mean_squared_error: 0.0443 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "Epoch 46/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00046: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0389 - mean_squared_error: 0.0389 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 47/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0456 - mean_squared_error: 0.0456\n",
      "Epoch 00047: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 47us/sample - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0527 - val_mean_squared_error: 0.0527\n",
      "Epoch 48/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0491 - mean_squared_error: 0.0491\n",
      "Epoch 00048: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0422 - mean_squared_error: 0.0422 - val_loss: 0.0532 - val_mean_squared_error: 0.0532\n",
      "Epoch 49/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0439 - mean_squared_error: 0.0439\n",
      "Epoch 00049: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0432 - mean_squared_error: 0.0432 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0406 - mean_squared_error: 0.0406\n",
      "Epoch 00050: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.0541 - val_mean_squared_error: 0.0541\n",
      "Epoch 51/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0352 - mean_squared_error: 0.0352\n",
      "Epoch 00051: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0542 - val_mean_squared_error: 0.0542\n",
      "Epoch 52/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0449 - mean_squared_error: 0.0449\n",
      "Epoch 00052: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0382 - mean_squared_error: 0.0382 - val_loss: 0.0544 - val_mean_squared_error: 0.0544\n",
      "Epoch 53/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0462 - mean_squared_error: 0.0462\n",
      "Epoch 00053: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0363 - mean_squared_error: 0.0363 - val_loss: 0.0545 - val_mean_squared_error: 0.0545\n",
      "Epoch 54/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00054: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0544 - val_mean_squared_error: 0.0544\n",
      "Epoch 55/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00055: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0545 - val_mean_squared_error: 0.0545\n",
      "Epoch 56/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Epoch 00056: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0545 - val_mean_squared_error: 0.0545\n",
      "Epoch 57/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00057: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0386 - mean_squared_error: 0.0386 - val_loss: 0.0544 - val_mean_squared_error: 0.0544\n",
      "Epoch 58/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0341 - mean_squared_error: 0.0341\n",
      "Epoch 00058: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0543 - val_mean_squared_error: 0.0543\n",
      "Epoch 59/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00059: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0375 - mean_squared_error: 0.0375 - val_loss: 0.0543 - val_mean_squared_error: 0.0543\n",
      "Epoch 60/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00060: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0401 - mean_squared_error: 0.0401 - val_loss: 0.0542 - val_mean_squared_error: 0.0542\n",
      "Epoch 61/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Epoch 00061: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0365 - mean_squared_error: 0.0365 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 62/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Epoch 00062: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0372 - mean_squared_error: 0.0372 - val_loss: 0.0539 - val_mean_squared_error: 0.0539\n",
      "Epoch 63/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0427 - mean_squared_error: 0.0427\n",
      "Epoch 00063: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0539 - val_mean_squared_error: 0.0539\n",
      "Epoch 64/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00064: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0539 - val_mean_squared_error: 0.0539\n",
      "Epoch 65/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Epoch 00065: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0395 - mean_squared_error: 0.0395 - val_loss: 0.0539 - val_mean_squared_error: 0.0539\n",
      "Epoch 66/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0365 - mean_squared_error: 0.0365\n",
      "Epoch 00066: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0539 - val_mean_squared_error: 0.0539\n",
      "Epoch 67/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00067: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0538 - val_mean_squared_error: 0.0538\n",
      "Epoch 68/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0398 - mean_squared_error: 0.0398\n",
      "Epoch 00068: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0539 - val_mean_squared_error: 0.0539\n",
      "Epoch 69/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00069: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0538 - val_mean_squared_error: 0.0538\n",
      "Epoch 70/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00070: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0539 - val_mean_squared_error: 0.0539\n",
      "Epoch 71/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Epoch 00071: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 72/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Epoch 00072: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0380 - mean_squared_error: 0.0380 - val_loss: 0.0538 - val_mean_squared_error: 0.0538\n",
      "Epoch 73/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0391 - mean_squared_error: 0.0391\n",
      "Epoch 00073: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0384 - mean_squared_error: 0.0384 - val_loss: 0.0536 - val_mean_squared_error: 0.0536\n",
      "Epoch 74/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0379 - mean_squared_error: 0.0379\n",
      "Epoch 00074: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0534 - val_mean_squared_error: 0.0534\n",
      "Epoch 75/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00075: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0532 - val_mean_squared_error: 0.0532\n",
      "Epoch 76/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 00076: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0530 - val_mean_squared_error: 0.0530\n",
      "Epoch 77/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0344 - mean_squared_error: 0.0344\n",
      "Epoch 00077: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0527 - val_mean_squared_error: 0.0527\n",
      "Epoch 78/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Epoch 00078: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 79/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00079: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 80/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0488 - mean_squared_error: 0.0488\n",
      "Epoch 00080: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.0520 - val_mean_squared_error: 0.0520\n",
      "Epoch 81/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0358 - mean_squared_error: 0.0358\n",
      "Epoch 00081: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 82/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00082: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0516 - val_mean_squared_error: 0.0516\n",
      "Epoch 83/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00083: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0514 - val_mean_squared_error: 0.0514\n",
      "Epoch 84/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Epoch 00084: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0379 - mean_squared_error: 0.0379 - val_loss: 0.0513 - val_mean_squared_error: 0.0513\n",
      "Epoch 85/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0408 - mean_squared_error: 0.0408\n",
      "Epoch 00085: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0514 - val_mean_squared_error: 0.0514\n",
      "Epoch 86/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00086: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0513 - val_mean_squared_error: 0.0513\n",
      "Epoch 87/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00087: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 88/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Epoch 00088: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0517 - val_mean_squared_error: 0.0517\n",
      "Epoch 89/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0383 - mean_squared_error: 0.0383\n",
      "Epoch 00089: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0401 - mean_squared_error: 0.0401 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "Epoch 90/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00090: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 91/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00091: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0525 - val_mean_squared_error: 0.0525\n",
      "Epoch 92/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0383 - mean_squared_error: 0.0383\n",
      "Epoch 00092: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 93/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Epoch 00093: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0319 - mean_squared_error: 0.0319 - val_loss: 0.0530 - val_mean_squared_error: 0.0530\n",
      "Epoch 94/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00094: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0531 - val_mean_squared_error: 0.0531\n",
      "Epoch 95/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00095: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0333 - mean_squared_error: 0.0333 - val_loss: 0.0533 - val_mean_squared_error: 0.0533\n",
      "Epoch 96/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0344 - mean_squared_error: 0.0344\n",
      "Epoch 00096: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0535 - val_mean_squared_error: 0.0535\n",
      "Epoch 97/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0367 - mean_squared_error: 0.0367\n",
      "Epoch 00097: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0539 - val_mean_squared_error: 0.0539\n",
      "Epoch 98/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00098: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0328 - mean_squared_error: 0.0328 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 99/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Epoch 00099: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 100/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00100: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0539 - val_mean_squared_error: 0.0539\n",
      "Epoch 101/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0326 - mean_squared_error: 0.0326\n",
      "Epoch 00101: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n",
      "Epoch 102/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00102: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0331 - mean_squared_error: 0.0331 - val_loss: 0.0535 - val_mean_squared_error: 0.0535\n",
      "Epoch 103/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00103: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0533 - val_mean_squared_error: 0.0533\n",
      "Epoch 104/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00104: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0530 - val_mean_squared_error: 0.0530\n",
      "Epoch 105/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00105: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0320 - mean_squared_error: 0.0320 - val_loss: 0.0525 - val_mean_squared_error: 0.0525\n",
      "Epoch 106/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00106: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 107/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Epoch 00107: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0520 - val_mean_squared_error: 0.0520\n",
      "Epoch 108/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00108: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "Epoch 109/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00109: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0308 - mean_squared_error: 0.0308 - val_loss: 0.0520 - val_mean_squared_error: 0.0520\n",
      "Epoch 110/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00110: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0520 - val_mean_squared_error: 0.0520\n",
      "Epoch 111/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00111: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 112/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00112: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 113/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0368 - mean_squared_error: 0.0368\n",
      "Epoch 00113: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 114/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00114: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0525 - val_mean_squared_error: 0.0525\n",
      "Epoch 115/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00115: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 116/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Epoch 00116: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0533 - val_mean_squared_error: 0.0533\n",
      "Epoch 117/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0402 - mean_squared_error: 0.0402\n",
      "Epoch 00117: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n",
      "Epoch 118/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00118: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0541 - val_mean_squared_error: 0.0541\n",
      "Epoch 119/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Epoch 00119: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0542 - val_mean_squared_error: 0.0542\n",
      "Epoch 120/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00120: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0542 - val_mean_squared_error: 0.0542\n",
      "Epoch 121/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00121: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0542 - val_mean_squared_error: 0.0542\n",
      "Epoch 122/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00122: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 123/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00123: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0538 - val_mean_squared_error: 0.0538\n",
      "Epoch 124/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00124: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0536 - val_mean_squared_error: 0.0536\n",
      "Epoch 125/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 00125: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0533 - val_mean_squared_error: 0.0533\n",
      "Epoch 126/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Epoch 00126: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0323 - mean_squared_error: 0.0323 - val_loss: 0.0532 - val_mean_squared_error: 0.0532\n",
      "Epoch 127/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00127: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0530 - val_mean_squared_error: 0.0530\n",
      "Epoch 128/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00128: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 129/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0368 - mean_squared_error: 0.0368\n",
      "Epoch 00129: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 130/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00130: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 131/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 00131: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 132/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00132: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0530 - val_mean_squared_error: 0.0530\n",
      "Epoch 133/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0398 - mean_squared_error: 0.0398\n",
      "Epoch 00133: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0533 - val_mean_squared_error: 0.0533\n",
      "Epoch 134/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0378 - mean_squared_error: 0.0378\n",
      "Epoch 00134: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0535 - val_mean_squared_error: 0.0535\n",
      "Epoch 135/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Epoch 00135: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n",
      "Epoch 136/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00136: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0539 - val_mean_squared_error: 0.0539\n",
      "Epoch 137/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00137: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0541 - val_mean_squared_error: 0.0541\n",
      "Epoch 138/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00138: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0543 - val_mean_squared_error: 0.0543\n",
      "Epoch 139/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00139: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0544 - val_mean_squared_error: 0.0544\n",
      "Epoch 140/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00140: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0545 - val_mean_squared_error: 0.0545\n",
      "Epoch 141/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00141: val_loss did not improve from 0.05099\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0546 - val_mean_squared_error: 0.0546\n",
      "Running trial 2\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 1.1410 - mean_squared_error: 1.1410\n",
      "Epoch 00001: val_loss improved from inf to 0.72040, saving model to model.h5\n",
      "248/248 [==============================] - 2s 8ms/sample - loss: 1.0780 - mean_squared_error: 1.0780 - val_loss: 0.7204 - val_mean_squared_error: 0.7204\n",
      "Epoch 2/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.8908 - mean_squared_error: 0.8908\n",
      "Epoch 00002: val_loss improved from 0.72040 to 0.68427, saving model to model.h5\n",
      "248/248 [==============================] - 0s 169us/sample - loss: 0.9938 - mean_squared_error: 0.9938 - val_loss: 0.6843 - val_mean_squared_error: 0.6843\n",
      "Epoch 3/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.9278 - mean_squared_error: 0.9278\n",
      "Epoch 00003: val_loss improved from 0.68427 to 0.64854, saving model to model.h5\n",
      "248/248 [==============================] - 0s 173us/sample - loss: 0.9088 - mean_squared_error: 0.9088 - val_loss: 0.6485 - val_mean_squared_error: 0.6485\n",
      "Epoch 4/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.6231 - mean_squared_error: 0.6231\n",
      "Epoch 00004: val_loss improved from 0.64854 to 0.61365, saving model to model.h5\n",
      "248/248 [==============================] - 0s 185us/sample - loss: 0.8204 - mean_squared_error: 0.8204 - val_loss: 0.6137 - val_mean_squared_error: 0.6137\n",
      "Epoch 5/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.8762 - mean_squared_error: 0.8762\n",
      "Epoch 00005: val_loss improved from 0.61365 to 0.58024, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.7784 - mean_squared_error: 0.7784 - val_loss: 0.5802 - val_mean_squared_error: 0.5802\n",
      "Epoch 6/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.6718 - mean_squared_error: 0.6718\n",
      "Epoch 00006: val_loss improved from 0.58024 to 0.54695, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.7171 - mean_squared_error: 0.7171 - val_loss: 0.5469 - val_mean_squared_error: 0.5469\n",
      "Epoch 7/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.6831 - mean_squared_error: 0.6831\n",
      "Epoch 00007: val_loss improved from 0.54695 to 0.51458, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.6429 - mean_squared_error: 0.6429 - val_loss: 0.5146 - val_mean_squared_error: 0.5146\n",
      "Epoch 8/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.6334 - mean_squared_error: 0.6334\n",
      "Epoch 00008: val_loss improved from 0.51458 to 0.48209, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.5836 - mean_squared_error: 0.5836 - val_loss: 0.4821 - val_mean_squared_error: 0.4821\n",
      "Epoch 9/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.5518 - mean_squared_error: 0.5518\n",
      "Epoch 00009: val_loss improved from 0.48209 to 0.45127, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.5190 - mean_squared_error: 0.5190 - val_loss: 0.4513 - val_mean_squared_error: 0.4513\n",
      "Epoch 10/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.5485 - mean_squared_error: 0.5485\n",
      "Epoch 00010: val_loss improved from 0.45127 to 0.42082, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.4879 - mean_squared_error: 0.4879 - val_loss: 0.4208 - val_mean_squared_error: 0.4208\n",
      "Epoch 11/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.4295 - mean_squared_error: 0.4295\n",
      "Epoch 00011: val_loss improved from 0.42082 to 0.39065, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.4322 - mean_squared_error: 0.4322 - val_loss: 0.3907 - val_mean_squared_error: 0.3907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3476 - mean_squared_error: 0.3476\n",
      "Epoch 00012: val_loss improved from 0.39065 to 0.36136, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.3836 - mean_squared_error: 0.3836 - val_loss: 0.3614 - val_mean_squared_error: 0.3614\n",
      "Epoch 13/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3698 - mean_squared_error: 0.3698\n",
      "Epoch 00013: val_loss improved from 0.36136 to 0.33295, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.3566 - mean_squared_error: 0.3566 - val_loss: 0.3330 - val_mean_squared_error: 0.3330\n",
      "Epoch 14/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3328 - mean_squared_error: 0.3328\n",
      "Epoch 00014: val_loss improved from 0.33295 to 0.30596, saving model to model.h5\n",
      "248/248 [==============================] - 0s 173us/sample - loss: 0.3236 - mean_squared_error: 0.3236 - val_loss: 0.3060 - val_mean_squared_error: 0.3060\n",
      "Epoch 15/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2785 - mean_squared_error: 0.2785\n",
      "Epoch 00015: val_loss improved from 0.30596 to 0.27995, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.2818 - mean_squared_error: 0.2818 - val_loss: 0.2799 - val_mean_squared_error: 0.2799\n",
      "Epoch 16/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2307 - mean_squared_error: 0.2307\n",
      "Epoch 00016: val_loss improved from 0.27995 to 0.25545, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.2482 - mean_squared_error: 0.2482 - val_loss: 0.2555 - val_mean_squared_error: 0.2555\n",
      "Epoch 17/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2251 - mean_squared_error: 0.2251\n",
      "Epoch 00017: val_loss improved from 0.25545 to 0.23241, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.2321 - mean_squared_error: 0.2321 - val_loss: 0.2324 - val_mean_squared_error: 0.2324\n",
      "Epoch 18/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2118 - mean_squared_error: 0.2118\n",
      "Epoch 00018: val_loss improved from 0.23241 to 0.21090, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.2060 - mean_squared_error: 0.2060 - val_loss: 0.2109 - val_mean_squared_error: 0.2109\n",
      "Epoch 19/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1862 - mean_squared_error: 0.1862\n",
      "Epoch 00019: val_loss improved from 0.21090 to 0.19073, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.1826 - mean_squared_error: 0.1826 - val_loss: 0.1907 - val_mean_squared_error: 0.1907\n",
      "Epoch 20/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1760 - mean_squared_error: 0.1760\n",
      "Epoch 00020: val_loss improved from 0.19073 to 0.17214, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.1660 - mean_squared_error: 0.1660 - val_loss: 0.1721 - val_mean_squared_error: 0.1721\n",
      "Epoch 21/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1490 - mean_squared_error: 0.1490\n",
      "Epoch 00021: val_loss improved from 0.17214 to 0.15494, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.1571 - mean_squared_error: 0.1571 - val_loss: 0.1549 - val_mean_squared_error: 0.1549\n",
      "Epoch 22/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1364 - mean_squared_error: 0.1364\n",
      "Epoch 00022: val_loss improved from 0.15494 to 0.13953, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.1359 - mean_squared_error: 0.1359 - val_loss: 0.1395 - val_mean_squared_error: 0.1395\n",
      "Epoch 23/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1341 - mean_squared_error: 0.1341\n",
      "Epoch 00023: val_loss improved from 0.13953 to 0.12561, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.1321 - mean_squared_error: 0.1321 - val_loss: 0.1256 - val_mean_squared_error: 0.1256\n",
      "Epoch 24/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1129 - mean_squared_error: 0.1129\n",
      "Epoch 00024: val_loss improved from 0.12561 to 0.11318, saving model to model.h5\n",
      "248/248 [==============================] - 0s 139us/sample - loss: 0.1185 - mean_squared_error: 0.1185 - val_loss: 0.1132 - val_mean_squared_error: 0.1132\n",
      "Epoch 25/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1171 - mean_squared_error: 0.1171\n",
      "Epoch 00025: val_loss improved from 0.11318 to 0.10219, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.1028 - mean_squared_error: 0.1028 - val_loss: 0.1022 - val_mean_squared_error: 0.1022\n",
      "Epoch 26/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0923 - mean_squared_error: 0.0923\n",
      "Epoch 00026: val_loss improved from 0.10219 to 0.09254, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0932 - mean_squared_error: 0.0932 - val_loss: 0.0925 - val_mean_squared_error: 0.0925\n",
      "Epoch 27/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1010 - mean_squared_error: 0.1010\n",
      "Epoch 00027: val_loss improved from 0.09254 to 0.08432, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0878 - mean_squared_error: 0.0878 - val_loss: 0.0843 - val_mean_squared_error: 0.0843\n",
      "Epoch 28/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0940 - mean_squared_error: 0.0940\n",
      "Epoch 00028: val_loss improved from 0.08432 to 0.07704, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0831 - mean_squared_error: 0.0831 - val_loss: 0.0770 - val_mean_squared_error: 0.0770\n",
      "Epoch 29/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0806 - mean_squared_error: 0.0806\n",
      "Epoch 00029: val_loss improved from 0.07704 to 0.07091, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0712 - mean_squared_error: 0.0712 - val_loss: 0.0709 - val_mean_squared_error: 0.0709\n",
      "Epoch 30/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0649 - mean_squared_error: 0.0649\n",
      "Epoch 00030: val_loss improved from 0.07091 to 0.06573, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0650 - mean_squared_error: 0.0650 - val_loss: 0.0657 - val_mean_squared_error: 0.0657\n",
      "Epoch 31/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0632 - mean_squared_error: 0.0632\n",
      "Epoch 00031: val_loss improved from 0.06573 to 0.06131, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0658 - mean_squared_error: 0.0658 - val_loss: 0.0613 - val_mean_squared_error: 0.0613\n",
      "Epoch 32/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0555 - mean_squared_error: 0.0555\n",
      "Epoch 00032: val_loss improved from 0.06131 to 0.05765, saving model to model.h5\n",
      "248/248 [==============================] - 0s 173us/sample - loss: 0.0593 - mean_squared_error: 0.0593 - val_loss: 0.0577 - val_mean_squared_error: 0.0577\n",
      "Epoch 33/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0721 - mean_squared_error: 0.0721\n",
      "Epoch 00033: val_loss improved from 0.05765 to 0.05459, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0584 - mean_squared_error: 0.0584 - val_loss: 0.0546 - val_mean_squared_error: 0.0546\n",
      "Epoch 34/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0558 - mean_squared_error: 0.0558\n",
      "Epoch 00034: val_loss improved from 0.05459 to 0.05200, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0569 - mean_squared_error: 0.0569 - val_loss: 0.0520 - val_mean_squared_error: 0.0520\n",
      "Epoch 35/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0492 - mean_squared_error: 0.0492\n",
      "Epoch 00035: val_loss improved from 0.05200 to 0.05003, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0559 - mean_squared_error: 0.0559 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0532 - mean_squared_error: 0.0532\n",
      "Epoch 00036: val_loss improved from 0.05003 to 0.04850, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0517 - mean_squared_error: 0.0517 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 37/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0568 - mean_squared_error: 0.0568\n",
      "Epoch 00037: val_loss improved from 0.04850 to 0.04724, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0523 - mean_squared_error: 0.0523 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 38/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0566 - mean_squared_error: 0.0566\n",
      "Epoch 00038: val_loss improved from 0.04724 to 0.04636, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0544 - mean_squared_error: 0.0544 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 39/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0535 - mean_squared_error: 0.0535\n",
      "Epoch 00039: val_loss improved from 0.04636 to 0.04582, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0518 - mean_squared_error: 0.0518 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 40/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0534 - mean_squared_error: 0.0534\n",
      "Epoch 00040: val_loss improved from 0.04582 to 0.04552, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.0480 - mean_squared_error: 0.0480 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 41/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0480 - mean_squared_error: 0.0480\n",
      "Epoch 00041: val_loss improved from 0.04552 to 0.04541, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.0487 - mean_squared_error: 0.0487 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 42/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0482 - mean_squared_error: 0.0482\n",
      "Epoch 00042: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 39us/sample - loss: 0.0490 - mean_squared_error: 0.0490 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 43/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0451 - mean_squared_error: 0.0451\n",
      "Epoch 00043: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0476 - mean_squared_error: 0.0476 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 44/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0476 - mean_squared_error: 0.0476\n",
      "Epoch 00044: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0462 - mean_squared_error: 0.0462 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 45/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0482 - mean_squared_error: 0.0482\n",
      "Epoch 00045: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0465 - mean_squared_error: 0.0465 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 46/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0402 - mean_squared_error: 0.0402\n",
      "Epoch 00046: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0447 - mean_squared_error: 0.0447 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 47/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0524 - mean_squared_error: 0.0524\n",
      "Epoch 00047: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0445 - mean_squared_error: 0.0445 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 48/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0489 - mean_squared_error: 0.0489\n",
      "Epoch 00048: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0463 - mean_squared_error: 0.0463 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 49/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0440 - mean_squared_error: 0.0440\n",
      "Epoch 00049: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 50/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0464 - mean_squared_error: 0.0464\n",
      "Epoch 00050: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0430 - mean_squared_error: 0.0430 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 51/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0407 - mean_squared_error: 0.0407\n",
      "Epoch 00051: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 52/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0532 - mean_squared_error: 0.0532\n",
      "Epoch 00052: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0429 - mean_squared_error: 0.0429 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 53/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0447 - mean_squared_error: 0.0447\n",
      "Epoch 00053: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 54/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0431 - mean_squared_error: 0.0431\n",
      "Epoch 00054: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0425 - mean_squared_error: 0.0425 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 55/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0496 - mean_squared_error: 0.0496\n",
      "Epoch 00055: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0462 - mean_squared_error: 0.0462 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 56/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0398 - mean_squared_error: 0.0398\n",
      "Epoch 00056: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0400 - mean_squared_error: 0.0400 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 57/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0369 - mean_squared_error: 0.0369\n",
      "Epoch 00057: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0394 - mean_squared_error: 0.0394 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 58/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0417 - mean_squared_error: 0.0417\n",
      "Epoch 00058: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0397 - mean_squared_error: 0.0397 - val_loss: 0.0517 - val_mean_squared_error: 0.0517\n",
      "Epoch 59/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0358 - mean_squared_error: 0.0358\n",
      "Epoch 00059: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "Epoch 60/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0400 - mean_squared_error: 0.0400\n",
      "Epoch 00060: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0379 - mean_squared_error: 0.0379 - val_loss: 0.0520 - val_mean_squared_error: 0.0520\n",
      "Epoch 61/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0324 - mean_squared_error: 0.0324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00061: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 62/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00062: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n",
      "Epoch 63/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0484 - mean_squared_error: 0.0484\n",
      "Epoch 00063: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0525 - val_mean_squared_error: 0.0525\n",
      "Epoch 64/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0482 - mean_squared_error: 0.0482\n",
      "Epoch 00064: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0400 - mean_squared_error: 0.0400 - val_loss: 0.0527 - val_mean_squared_error: 0.0527\n",
      "Epoch 65/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0380 - mean_squared_error: 0.0380\n",
      "Epoch 00065: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0527 - val_mean_squared_error: 0.0527\n",
      "Epoch 66/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Epoch 00066: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0371 - mean_squared_error: 0.0371 - val_loss: 0.0527 - val_mean_squared_error: 0.0527\n",
      "Epoch 67/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0391 - mean_squared_error: 0.0391\n",
      "Epoch 00067: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0400 - mean_squared_error: 0.0400 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 68/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0407 - mean_squared_error: 0.0407\n",
      "Epoch 00068: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0379 - mean_squared_error: 0.0379 - val_loss: 0.0530 - val_mean_squared_error: 0.0530\n",
      "Epoch 69/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00069: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0383 - mean_squared_error: 0.0383 - val_loss: 0.0531 - val_mean_squared_error: 0.0531\n",
      "Epoch 70/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 00070: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0532 - val_mean_squared_error: 0.0532\n",
      "Epoch 71/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0400 - mean_squared_error: 0.0400\n",
      "Epoch 00071: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0381 - mean_squared_error: 0.0381 - val_loss: 0.0532 - val_mean_squared_error: 0.0532\n",
      "Epoch 72/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0518 - mean_squared_error: 0.0518\n",
      "Epoch 00072: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0431 - mean_squared_error: 0.0431 - val_loss: 0.0533 - val_mean_squared_error: 0.0533\n",
      "Epoch 73/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0374 - mean_squared_error: 0.0374\n",
      "Epoch 00073: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0399 - mean_squared_error: 0.0399 - val_loss: 0.0532 - val_mean_squared_error: 0.0532\n",
      "Epoch 74/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0362 - mean_squared_error: 0.0362\n",
      "Epoch 00074: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0532 - val_mean_squared_error: 0.0532\n",
      "Epoch 75/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0413 - mean_squared_error: 0.0413\n",
      "Epoch 00075: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0533 - val_mean_squared_error: 0.0533\n",
      "Epoch 76/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0372 - mean_squared_error: 0.0372\n",
      "Epoch 00076: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.0534 - val_mean_squared_error: 0.0534\n",
      "Epoch 77/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00077: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0533 - val_mean_squared_error: 0.0533\n",
      "Epoch 78/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0375 - mean_squared_error: 0.0375\n",
      "Epoch 00078: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0378 - mean_squared_error: 0.0378 - val_loss: 0.0532 - val_mean_squared_error: 0.0532\n",
      "Epoch 79/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0427 - mean_squared_error: 0.0427\n",
      "Epoch 00079: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0371 - mean_squared_error: 0.0371 - val_loss: 0.0533 - val_mean_squared_error: 0.0533\n",
      "Epoch 80/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0458 - mean_squared_error: 0.0458\n",
      "Epoch 00080: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0534 - val_mean_squared_error: 0.0534\n",
      "Epoch 81/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0427 - mean_squared_error: 0.0427\n",
      "Epoch 00081: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0385 - mean_squared_error: 0.0385 - val_loss: 0.0535 - val_mean_squared_error: 0.0535\n",
      "Epoch 82/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0391 - mean_squared_error: 0.0391\n",
      "Epoch 00082: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0396 - mean_squared_error: 0.0396 - val_loss: 0.0536 - val_mean_squared_error: 0.0536\n",
      "Epoch 83/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00083: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0536 - val_mean_squared_error: 0.0536\n",
      "Epoch 84/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0453 - mean_squared_error: 0.0453\n",
      "Epoch 00084: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n",
      "Epoch 85/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0435 - mean_squared_error: 0.0435\n",
      "Epoch 00085: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0538 - val_mean_squared_error: 0.0538\n",
      "Epoch 86/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0294 - mean_squared_error: 0.0294\n",
      "Epoch 00086: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0372 - mean_squared_error: 0.0372 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n",
      "Epoch 87/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0374 - mean_squared_error: 0.0374\n",
      "Epoch 00087: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0386 - mean_squared_error: 0.0386\n",
      "Epoch 00088: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0372 - mean_squared_error: 0.0372 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n",
      "Epoch 89/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Epoch 00089: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n",
      "Epoch 90/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00090: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n",
      "Epoch 91/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0450 - mean_squared_error: 0.0450\n",
      "Epoch 00091: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0381 - mean_squared_error: 0.0381 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n",
      "Epoch 92/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00092: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n",
      "Epoch 93/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0404 - mean_squared_error: 0.0404\n",
      "Epoch 00093: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0539 - val_mean_squared_error: 0.0539\n",
      "Epoch 94/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00094: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0542 - val_mean_squared_error: 0.0542\n",
      "Epoch 95/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00095: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0544 - val_mean_squared_error: 0.0544\n",
      "Epoch 96/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00096: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0547 - val_mean_squared_error: 0.0547\n",
      "Epoch 97/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Epoch 00097: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0320 - mean_squared_error: 0.0320 - val_loss: 0.0547 - val_mean_squared_error: 0.0547\n",
      "Epoch 98/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0409 - mean_squared_error: 0.0409\n",
      "Epoch 00098: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0371 - mean_squared_error: 0.0371 - val_loss: 0.0548 - val_mean_squared_error: 0.0548\n",
      "Epoch 99/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00099: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0548 - val_mean_squared_error: 0.0548\n",
      "Epoch 100/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0334 - mean_squared_error: 0.0334\n",
      "Epoch 00100: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.0547 - val_mean_squared_error: 0.0547\n",
      "Epoch 101/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Epoch 00101: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0547 - val_mean_squared_error: 0.0547\n",
      "Epoch 102/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0372 - mean_squared_error: 0.0372\n",
      "Epoch 00102: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0547 - val_mean_squared_error: 0.0547\n",
      "Epoch 103/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0352 - mean_squared_error: 0.0352\n",
      "Epoch 00103: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0333 - mean_squared_error: 0.0333 - val_loss: 0.0546 - val_mean_squared_error: 0.0546\n",
      "Epoch 104/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00104: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0544 - val_mean_squared_error: 0.0544\n",
      "Epoch 105/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00105: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0370 - mean_squared_error: 0.0370 - val_loss: 0.0541 - val_mean_squared_error: 0.0541\n",
      "Epoch 106/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00106: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 107/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00107: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0538 - val_mean_squared_error: 0.0538\n",
      "Epoch 108/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Epoch 00108: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0539 - val_mean_squared_error: 0.0539\n",
      "Epoch 109/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Epoch 00109: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0330 - mean_squared_error: 0.0330 - val_loss: 0.0541 - val_mean_squared_error: 0.0541\n",
      "Epoch 110/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00110: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0543 - val_mean_squared_error: 0.0543\n",
      "Epoch 111/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Epoch 00111: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0544 - val_mean_squared_error: 0.0544\n",
      "Epoch 112/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0333 - mean_squared_error: 0.0333\n",
      "Epoch 00112: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0547 - val_mean_squared_error: 0.0547\n",
      "Epoch 113/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00113: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 114/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00114: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0551 - val_mean_squared_error: 0.0551\n",
      "Epoch 115/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Epoch 00115: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0553 - val_mean_squared_error: 0.0553\n",
      "Epoch 116/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Epoch 00116: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0554 - val_mean_squared_error: 0.0554\n",
      "Epoch 117/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0392 - mean_squared_error: 0.0392\n",
      "Epoch 00117: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0553 - val_mean_squared_error: 0.0553\n",
      "Epoch 118/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0378 - mean_squared_error: 0.0378\n",
      "Epoch 00118: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0553 - val_mean_squared_error: 0.0553\n",
      "Epoch 119/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Epoch 00119: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0553 - val_mean_squared_error: 0.0553\n",
      "Epoch 120/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00120: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0552 - val_mean_squared_error: 0.0552\n",
      "Epoch 121/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00121: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0550 - val_mean_squared_error: 0.0550\n",
      "Epoch 122/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0371 - mean_squared_error: 0.0371\n",
      "Epoch 00122: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 123/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00123: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0548 - val_mean_squared_error: 0.0548\n",
      "Epoch 124/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00124: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0328 - mean_squared_error: 0.0328 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 125/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Epoch 00125: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0551 - val_mean_squared_error: 0.0551\n",
      "Epoch 126/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00126: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0367 - mean_squared_error: 0.0367 - val_loss: 0.0553 - val_mean_squared_error: 0.0553\n",
      "Epoch 127/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Epoch 00127: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0554 - val_mean_squared_error: 0.0554\n",
      "Epoch 128/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00128: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0323 - mean_squared_error: 0.0323 - val_loss: 0.0554 - val_mean_squared_error: 0.0554\n",
      "Epoch 129/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0345 - mean_squared_error: 0.0345\n",
      "Epoch 00129: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0555 - val_mean_squared_error: 0.0555\n",
      "Epoch 130/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00130: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0556 - val_mean_squared_error: 0.0556\n",
      "Epoch 131/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00131: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0559 - val_mean_squared_error: 0.0559\n",
      "Epoch 132/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00132: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0560 - val_mean_squared_error: 0.0560\n",
      "Epoch 133/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00133: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0560 - val_mean_squared_error: 0.0560\n",
      "Epoch 134/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00134: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0561 - val_mean_squared_error: 0.0561\n",
      "Epoch 135/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00135: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0560 - val_mean_squared_error: 0.0560\n",
      "Epoch 136/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0346 - mean_squared_error: 0.0346\n",
      "Epoch 00136: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0560 - val_mean_squared_error: 0.0560\n",
      "Epoch 137/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00137: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0560 - val_mean_squared_error: 0.0560\n",
      "Epoch 138/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00138: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0559 - val_mean_squared_error: 0.0559\n",
      "Epoch 139/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0339 - mean_squared_error: 0.0339\n",
      "Epoch 00139: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0558 - val_mean_squared_error: 0.0558\n",
      "Epoch 140/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00140: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0555 - val_mean_squared_error: 0.0555\n",
      "Epoch 141/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00141: val_loss did not improve from 0.04541\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0553 - val_mean_squared_error: 0.0553\n",
      "Running trial 3\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 1.0307 - mean_squared_error: 1.0307\n",
      "Epoch 00001: val_loss improved from inf to 0.65982, saving model to model.h5\n",
      "248/248 [==============================] - 2s 10ms/sample - loss: 1.0564 - mean_squared_error: 1.0564 - val_loss: 0.6598 - val_mean_squared_error: 0.6598\n",
      "Epoch 2/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.9434 - mean_squared_error: 0.9434\n",
      "Epoch 00002: val_loss improved from 0.65982 to 0.63407, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.9781 - mean_squared_error: 0.9781 - val_loss: 0.6341 - val_mean_squared_error: 0.6341\n",
      "Epoch 3/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.9315 - mean_squared_error: 0.9315\n",
      "Epoch 00003: val_loss improved from 0.63407 to 0.60879, saving model to model.h5\n",
      "248/248 [==============================] - 0s 139us/sample - loss: 0.9136 - mean_squared_error: 0.9136 - val_loss: 0.6088 - val_mean_squared_error: 0.6088\n",
      "Epoch 4/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.8440 - mean_squared_error: 0.8440\n",
      "Epoch 00004: val_loss improved from 0.60879 to 0.58414, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.8452 - mean_squared_error: 0.8452 - val_loss: 0.5841 - val_mean_squared_error: 0.5841\n",
      "Epoch 5/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.8408 - mean_squared_error: 0.8408\n",
      "Epoch 00005: val_loss improved from 0.58414 to 0.55950, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.7736 - mean_squared_error: 0.7736 - val_loss: 0.5595 - val_mean_squared_error: 0.5595\n",
      "Epoch 6/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.7484 - mean_squared_error: 0.7484\n",
      "Epoch 00006: val_loss improved from 0.55950 to 0.53422, saving model to model.h5\n",
      "248/248 [==============================] - 0s 139us/sample - loss: 0.7176 - mean_squared_error: 0.7176 - val_loss: 0.5342 - val_mean_squared_error: 0.5342\n",
      "Epoch 7/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.6578 - mean_squared_error: 0.6578\n",
      "Epoch 00007: val_loss improved from 0.53422 to 0.50985, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.6513 - mean_squared_error: 0.6513 - val_loss: 0.5098 - val_mean_squared_error: 0.5098\n",
      "Epoch 8/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.5607 - mean_squared_error: 0.5607\n",
      "Epoch 00008: val_loss improved from 0.50985 to 0.48482, saving model to model.h5\n",
      "248/248 [==============================] - 0s 134us/sample - loss: 0.5874 - mean_squared_error: 0.5874 - val_loss: 0.4848 - val_mean_squared_error: 0.4848\n",
      "Epoch 9/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.5292 - mean_squared_error: 0.5292\n",
      "Epoch 00009: val_loss improved from 0.48482 to 0.46000, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.5411 - mean_squared_error: 0.5411 - val_loss: 0.4600 - val_mean_squared_error: 0.4600\n",
      "Epoch 10/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.5521 - mean_squared_error: 0.5521\n",
      "Epoch 00010: val_loss improved from 0.46000 to 0.43591, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.5009 - mean_squared_error: 0.5009 - val_loss: 0.4359 - val_mean_squared_error: 0.4359\n",
      "Epoch 11/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.4625 - mean_squared_error: 0.4625\n",
      "Epoch 00011: val_loss improved from 0.43591 to 0.41171, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.4456 - mean_squared_error: 0.4456 - val_loss: 0.4117 - val_mean_squared_error: 0.4117\n",
      "Epoch 12/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.4378 - mean_squared_error: 0.4378\n",
      "Epoch 00012: val_loss improved from 0.41171 to 0.38734, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.3987 - mean_squared_error: 0.3987 - val_loss: 0.3873 - val_mean_squared_error: 0.3873\n",
      "Epoch 13/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.4293 - mean_squared_error: 0.4293\n",
      "Epoch 00013: val_loss improved from 0.38734 to 0.36302, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.3825 - mean_squared_error: 0.3825 - val_loss: 0.3630 - val_mean_squared_error: 0.3630\n",
      "Epoch 14/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3872 - mean_squared_error: 0.3872\n",
      "Epoch 00014: val_loss improved from 0.36302 to 0.33907, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.3402 - mean_squared_error: 0.3402 - val_loss: 0.3391 - val_mean_squared_error: 0.3391\n",
      "Epoch 15/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3090 - mean_squared_error: 0.3090\n",
      "Epoch 00015: val_loss improved from 0.33907 to 0.31585, saving model to model.h5\n",
      "248/248 [==============================] - 0s 143us/sample - loss: 0.3040 - mean_squared_error: 0.3040 - val_loss: 0.3159 - val_mean_squared_error: 0.3159\n",
      "Epoch 16/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2937 - mean_squared_error: 0.2937\n",
      "Epoch 00016: val_loss improved from 0.31585 to 0.29320, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.2810 - mean_squared_error: 0.2810 - val_loss: 0.2932 - val_mean_squared_error: 0.2932\n",
      "Epoch 17/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2584 - mean_squared_error: 0.2584\n",
      "Epoch 00017: val_loss improved from 0.29320 to 0.27096, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.2456 - mean_squared_error: 0.2456 - val_loss: 0.2710 - val_mean_squared_error: 0.2710\n",
      "Epoch 18/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2361 - mean_squared_error: 0.2361\n",
      "Epoch 00018: val_loss improved from 0.27096 to 0.24967, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.2242 - mean_squared_error: 0.2242 - val_loss: 0.2497 - val_mean_squared_error: 0.2497\n",
      "Epoch 19/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2170 - mean_squared_error: 0.2170\n",
      "Epoch 00019: val_loss improved from 0.24967 to 0.22914, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.2019 - mean_squared_error: 0.2019 - val_loss: 0.2291 - val_mean_squared_error: 0.2291\n",
      "Epoch 20/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1722 - mean_squared_error: 0.1722\n",
      "Epoch 00020: val_loss improved from 0.22914 to 0.20979, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.1826 - mean_squared_error: 0.1826 - val_loss: 0.2098 - val_mean_squared_error: 0.2098\n",
      "Epoch 21/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1740 - mean_squared_error: 0.1740\n",
      "Epoch 00021: val_loss improved from 0.20979 to 0.19132, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.1719 - mean_squared_error: 0.1719 - val_loss: 0.1913 - val_mean_squared_error: 0.1913\n",
      "Epoch 22/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1722 - mean_squared_error: 0.1722\n",
      "Epoch 00022: val_loss improved from 0.19132 to 0.17393, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.1494 - mean_squared_error: 0.1494 - val_loss: 0.1739 - val_mean_squared_error: 0.1739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1485 - mean_squared_error: 0.1485\n",
      "Epoch 00023: val_loss improved from 0.17393 to 0.15794, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.1375 - mean_squared_error: 0.1375 - val_loss: 0.1579 - val_mean_squared_error: 0.1579\n",
      "Epoch 24/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1367 - mean_squared_error: 0.1367\n",
      "Epoch 00024: val_loss improved from 0.15794 to 0.14313, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.1292 - mean_squared_error: 0.1292 - val_loss: 0.1431 - val_mean_squared_error: 0.1431\n",
      "Epoch 25/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1124 - mean_squared_error: 0.1124\n",
      "Epoch 00025: val_loss improved from 0.14313 to 0.12981, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.1134 - mean_squared_error: 0.1134 - val_loss: 0.1298 - val_mean_squared_error: 0.1298\n",
      "Epoch 26/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0895 - mean_squared_error: 0.0895\n",
      "Epoch 00026: val_loss improved from 0.12981 to 0.11754, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0966 - mean_squared_error: 0.0966 - val_loss: 0.1175 - val_mean_squared_error: 0.1175\n",
      "Epoch 27/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0916 - mean_squared_error: 0.0916\n",
      "Epoch 00027: val_loss improved from 0.11754 to 0.10647, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0981 - mean_squared_error: 0.0981 - val_loss: 0.1065 - val_mean_squared_error: 0.1065\n",
      "Epoch 28/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0844 - mean_squared_error: 0.0844\n",
      "Epoch 00028: val_loss improved from 0.10647 to 0.09664, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0881 - mean_squared_error: 0.0881 - val_loss: 0.0966 - val_mean_squared_error: 0.0966\n",
      "Epoch 29/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0741 - mean_squared_error: 0.0741\n",
      "Epoch 00029: val_loss improved from 0.09664 to 0.08803, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0764 - mean_squared_error: 0.0764 - val_loss: 0.0880 - val_mean_squared_error: 0.0880\n",
      "Epoch 30/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0735 - mean_squared_error: 0.0735\n",
      "Epoch 00030: val_loss improved from 0.08803 to 0.08038, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0736 - mean_squared_error: 0.0736 - val_loss: 0.0804 - val_mean_squared_error: 0.0804\n",
      "Epoch 31/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0604 - mean_squared_error: 0.0604\n",
      "Epoch 00031: val_loss improved from 0.08038 to 0.07368, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0658 - mean_squared_error: 0.0658 - val_loss: 0.0737 - val_mean_squared_error: 0.0737\n",
      "Epoch 32/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0598 - mean_squared_error: 0.0598\n",
      "Epoch 00032: val_loss improved from 0.07368 to 0.06790, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0615 - mean_squared_error: 0.0615 - val_loss: 0.0679 - val_mean_squared_error: 0.0679\n",
      "Epoch 33/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0693 - mean_squared_error: 0.0693\n",
      "Epoch 00033: val_loss improved from 0.06790 to 0.06301, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0586 - mean_squared_error: 0.0586 - val_loss: 0.0630 - val_mean_squared_error: 0.0630\n",
      "Epoch 34/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0545 - mean_squared_error: 0.0545\n",
      "Epoch 00034: val_loss improved from 0.06301 to 0.05888, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0495 - mean_squared_error: 0.0495 - val_loss: 0.0589 - val_mean_squared_error: 0.0589\n",
      "Epoch 35/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0607 - mean_squared_error: 0.0607\n",
      "Epoch 00035: val_loss improved from 0.05888 to 0.05539, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0505 - mean_squared_error: 0.0505 - val_loss: 0.0554 - val_mean_squared_error: 0.0554\n",
      "Epoch 36/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0529 - mean_squared_error: 0.0529\n",
      "Epoch 00036: val_loss improved from 0.05539 to 0.05246, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0501 - mean_squared_error: 0.0501 - val_loss: 0.0525 - val_mean_squared_error: 0.0525\n",
      "Epoch 37/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Epoch 00037: val_loss improved from 0.05246 to 0.05009, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0491 - mean_squared_error: 0.0491 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 38/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0427 - mean_squared_error: 0.0427\n",
      "Epoch 00038: val_loss improved from 0.05009 to 0.04827, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0475 - mean_squared_error: 0.0475 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 39/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0468 - mean_squared_error: 0.0468\n",
      "Epoch 00039: val_loss improved from 0.04827 to 0.04683, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0466 - mean_squared_error: 0.0466 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 40/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0413 - mean_squared_error: 0.0413\n",
      "Epoch 00040: val_loss improved from 0.04683 to 0.04579, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.0436 - mean_squared_error: 0.0436 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 41/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0345 - mean_squared_error: 0.0345\n",
      "Epoch 00041: val_loss improved from 0.04579 to 0.04497, saving model to model.h5\n",
      "248/248 [==============================] - 0s 169us/sample - loss: 0.0396 - mean_squared_error: 0.0396 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 42/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0454 - mean_squared_error: 0.0454\n",
      "Epoch 00042: val_loss improved from 0.04497 to 0.04450, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 43/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00043: val_loss improved from 0.04450 to 0.04418, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0398 - mean_squared_error: 0.0398 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 44/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0497 - mean_squared_error: 0.0497\n",
      "Epoch 00044: val_loss improved from 0.04418 to 0.04403, saving model to model.h5\n",
      "248/248 [==============================] - 0s 152us/sample - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 45/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0374 - mean_squared_error: 0.0374\n",
      "Epoch 00045: val_loss improved from 0.04403 to 0.04390, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0392 - mean_squared_error: 0.0392 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 46/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0415 - mean_squared_error: 0.0415\n",
      "Epoch 00046: val_loss improved from 0.04390 to 0.04389, saving model to model.h5\n",
      "248/248 [==============================] - 0s 135us/sample - loss: 0.0384 - mean_squared_error: 0.0384 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0405 - mean_squared_error: 0.0405\n",
      "Epoch 00047: val_loss did not improve from 0.04389\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0381 - mean_squared_error: 0.0381 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 48/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00048: val_loss did not improve from 0.04389\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 49/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00049: val_loss did not improve from 0.04389\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 50/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0466 - mean_squared_error: 0.0466\n",
      "Epoch 00050: val_loss did not improve from 0.04389\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0394 - mean_squared_error: 0.0394 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 51/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0417 - mean_squared_error: 0.0417\n",
      "Epoch 00051: val_loss did not improve from 0.04389\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0391 - mean_squared_error: 0.0391 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 52/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0438 - mean_squared_error: 0.0438\n",
      "Epoch 00052: val_loss did not improve from 0.04389\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0382 - mean_squared_error: 0.0382 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 53/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0419 - mean_squared_error: 0.0419\n",
      "Epoch 00053: val_loss did not improve from 0.04389\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0385 - mean_squared_error: 0.0385 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 54/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Epoch 00054: val_loss did not improve from 0.04389\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 55/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 00055: val_loss improved from 0.04389 to 0.04387, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 56/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0515 - mean_squared_error: 0.0515\n",
      "Epoch 00056: val_loss improved from 0.04387 to 0.04385, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 57/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0467 - mean_squared_error: 0.0467\n",
      "Epoch 00057: val_loss improved from 0.04385 to 0.04374, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 58/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0414 - mean_squared_error: 0.0414\n",
      "Epoch 00058: val_loss improved from 0.04374 to 0.04356, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0379 - mean_squared_error: 0.0379 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 59/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0392 - mean_squared_error: 0.0392\n",
      "Epoch 00059: val_loss improved from 0.04356 to 0.04339, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 60/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0391 - mean_squared_error: 0.0391\n",
      "Epoch 00060: val_loss improved from 0.04339 to 0.04322, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0371 - mean_squared_error: 0.0371 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 61/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0448 - mean_squared_error: 0.0448\n",
      "Epoch 00061: val_loss improved from 0.04322 to 0.04317, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 62/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00062: val_loss improved from 0.04317 to 0.04312, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 63/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0307 - mean_squared_error: 0.0307\n",
      "Epoch 00063: val_loss improved from 0.04312 to 0.04304, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0399 - mean_squared_error: 0.0399 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 64/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00064: val_loss improved from 0.04304 to 0.04290, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0379 - mean_squared_error: 0.0379 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 65/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0385 - mean_squared_error: 0.0385\n",
      "Epoch 00065: val_loss improved from 0.04290 to 0.04277, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 66/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0333 - mean_squared_error: 0.0333\n",
      "Epoch 00066: val_loss improved from 0.04277 to 0.04266, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0386 - mean_squared_error: 0.0386 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 67/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Epoch 00067: val_loss improved from 0.04266 to 0.04255, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0371 - mean_squared_error: 0.0371 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 68/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0408 - mean_squared_error: 0.0408\n",
      "Epoch 00068: val_loss did not improve from 0.04255\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 69/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Epoch 00069: val_loss did not improve from 0.04255\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 70/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Epoch 00070: val_loss improved from 0.04255 to 0.04240, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 71/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Epoch 00071: val_loss improved from 0.04240 to 0.04232, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0400 - mean_squared_error: 0.0400\n",
      "Epoch 00072: val_loss improved from 0.04232 to 0.04227, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 73/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0383 - mean_squared_error: 0.0383\n",
      "Epoch 00073: val_loss did not improve from 0.04227\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0333 - mean_squared_error: 0.0333 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 74/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0432 - mean_squared_error: 0.0432\n",
      "Epoch 00074: val_loss did not improve from 0.04227\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 75/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00075: val_loss did not improve from 0.04227\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 76/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00076: val_loss did not improve from 0.04227\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 77/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00077: val_loss improved from 0.04227 to 0.04224, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 78/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0334 - mean_squared_error: 0.0334\n",
      "Epoch 00078: val_loss improved from 0.04224 to 0.04218, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0382 - mean_squared_error: 0.0382 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 79/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0348 - mean_squared_error: 0.0348\n",
      "Epoch 00079: val_loss improved from 0.04218 to 0.04214, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 80/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00080: val_loss improved from 0.04214 to 0.04210, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0393 - mean_squared_error: 0.0393 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 81/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00081: val_loss did not improve from 0.04210\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 82/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00082: val_loss did not improve from 0.04210\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 83/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0444 - mean_squared_error: 0.0444\n",
      "Epoch 00083: val_loss did not improve from 0.04210\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0319 - mean_squared_error: 0.0319 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 84/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00084: val_loss did not improve from 0.04210\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 85/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0412 - mean_squared_error: 0.0412\n",
      "Epoch 00085: val_loss did not improve from 0.04210\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 86/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00086: val_loss improved from 0.04210 to 0.04206, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 87/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00087: val_loss improved from 0.04206 to 0.04197, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 88/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Epoch 00088: val_loss improved from 0.04197 to 0.04192, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 89/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0456 - mean_squared_error: 0.0456\n",
      "Epoch 00089: val_loss improved from 0.04192 to 0.04190, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0391 - mean_squared_error: 0.0391 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 90/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00090: val_loss improved from 0.04190 to 0.04190, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 91/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Epoch 00091: val_loss did not improve from 0.04190\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0365 - mean_squared_error: 0.0365 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 92/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0365 - mean_squared_error: 0.0365\n",
      "Epoch 00092: val_loss did not improve from 0.04190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0308 - mean_squared_error: 0.0308 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 93/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00093: val_loss did not improve from 0.04190\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 94/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00094: val_loss did not improve from 0.04190\n",
      "248/248 [==============================] - 0s 39us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 95/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0521 - mean_squared_error: 0.0521\n",
      "Epoch 00095: val_loss did not improve from 0.04190\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0385 - mean_squared_error: 0.0385 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 96/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00096: val_loss did not improve from 0.04190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 97/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00097: val_loss did not improve from 0.04190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 98/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0410 - mean_squared_error: 0.0410\n",
      "Epoch 00098: val_loss did not improve from 0.04190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 99/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00099: val_loss did not improve from 0.04190\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0308 - mean_squared_error: 0.0308 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 100/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0399 - mean_squared_error: 0.0399\n",
      "Epoch 00100: val_loss did not improve from 0.04190\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0323 - mean_squared_error: 0.0323 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 101/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00101: val_loss improved from 0.04190 to 0.04186, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 102/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Epoch 00102: val_loss improved from 0.04186 to 0.04169, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 103/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0334 - mean_squared_error: 0.0334\n",
      "Epoch 00103: val_loss improved from 0.04169 to 0.04166, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 104/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Epoch 00104: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0319 - mean_squared_error: 0.0319 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 105/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0390 - mean_squared_error: 0.0390\n",
      "Epoch 00105: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 106/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00106: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 107/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00107: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 108/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00108: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 109/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00109: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 110/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00110: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 111/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00111: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 112/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Epoch 00112: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 113/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00113: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 114/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00114: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 115/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00115: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 116/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00116: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 117/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00117: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 118/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0307 - mean_squared_error: 0.0307\n",
      "Epoch 00118: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 119/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00119: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 120/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00120: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 121/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00121: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 42us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 122/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00122: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00123: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 124/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00124: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 125/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00125: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 126/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00126: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 127/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00127: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 128/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0368 - mean_squared_error: 0.0368\n",
      "Epoch 00128: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 129/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00129: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 130/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00130: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 35us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 131/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00131: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 132/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0384 - mean_squared_error: 0.0384\n",
      "Epoch 00132: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 133/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0294 - mean_squared_error: 0.0294\n",
      "Epoch 00133: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 134/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Epoch 00134: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 135/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00135: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 136/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00136: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 137/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00137: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 138/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00138: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0320 - mean_squared_error: 0.0320 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 139/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00139: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 140/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Epoch 00140: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 141/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00141: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 142/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00142: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 143/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00143: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 144/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0437 - mean_squared_error: 0.0437\n",
      "Epoch 00144: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 145/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0227 - mean_squared_error: 0.0227\n",
      "Epoch 00145: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0319 - mean_squared_error: 0.0319 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 146/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00146: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 147/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00147: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 148/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00148: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 149/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00149: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 150/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00150: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 151/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00151: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 152/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00152: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 153/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00153: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 154/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00154: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 155/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Epoch 00155: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 156/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00156: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 157/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00157: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 158/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00158: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 159/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0326 - mean_squared_error: 0.0326\n",
      "Epoch 00159: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 160/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00160: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 161/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00161: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 162/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00162: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 163/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00163: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 164/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00164: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 165/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0346 - mean_squared_error: 0.0346\n",
      "Epoch 00165: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 166/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00166: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 167/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00167: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 168/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00168: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 169/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00169: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 170/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00170: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 171/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00171: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 172/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00172: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 173/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00173: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 174/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00174: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 175/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00175: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 176/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00176: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 177/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00177: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 178/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00178: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 179/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00179: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 180/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00180: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 181/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00181: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 182/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00182: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 183/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0177 - mean_squared_error: 0.0177\n",
      "Epoch 00183: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 184/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Epoch 00184: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 185/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00185: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 186/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00186: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 187/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00187: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 188/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00188: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 189/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0387 - mean_squared_error: 0.0387\n",
      "Epoch 00189: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 190/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 00190: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 191/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00191: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 192/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00192: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 193/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0419 - mean_squared_error: 0.0419\n",
      "Epoch 00193: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 194/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00194: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 195/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00195: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 196/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Epoch 00196: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 197/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00197: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 198/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00198: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 199/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00199: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 200/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00200: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 201/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00201: val_loss did not improve from 0.04166\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 202/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00202: val_loss improved from 0.04166 to 0.04162, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 203/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00203: val_loss improved from 0.04162 to 0.04159, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 204/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0390 - mean_squared_error: 0.0390\n",
      "Epoch 00204: val_loss did not improve from 0.04159\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 205/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00205: val_loss did not improve from 0.04159\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 206/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Epoch 00206: val_loss did not improve from 0.04159\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 207/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00207: val_loss did not improve from 0.04159\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 208/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Epoch 00208: val_loss did not improve from 0.04159\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 209/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00209: val_loss did not improve from 0.04159\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 210/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0171 - mean_squared_error: 0.0171\n",
      "Epoch 00210: val_loss did not improve from 0.04159\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 211/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00211: val_loss improved from 0.04159 to 0.04143, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 212/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00212: val_loss improved from 0.04143 to 0.04127, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 213/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00213: val_loss improved from 0.04127 to 0.04112, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 214/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Epoch 00214: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 215/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00215: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 216/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0149 - mean_squared_error: 0.0149\n",
      "Epoch 00216: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 217/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00217: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 218/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00218: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 219/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00219: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 220/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00220: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 221/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00221: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 222/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Epoch 00222: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 223/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00223: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 224/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00224: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 225/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00225: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 226/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00226: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 43us/sample - loss: 0.0308 - mean_squared_error: 0.0308 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0231 - mean_squared_error: 0.0231\n",
      "Epoch 00227: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 228/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00228: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 229/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00229: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 230/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00230: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 231/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00231: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 232/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00232: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 233/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Epoch 00233: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 234/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00234: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 235/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00235: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 236/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00236: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 237/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00237: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 238/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00238: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 239/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00239: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 240/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00240: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 241/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00241: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 242/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00242: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 243/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00243: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 244/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00244: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 245/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00245: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 246/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00246: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 247/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00247: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 248/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00248: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 249/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00249: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 250/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00250: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 35us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 251/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 00251: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 252/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00252: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 253/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00253: val_loss did not improve from 0.04112\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 254/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00254: val_loss improved from 0.04112 to 0.04108, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 255/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00255: val_loss improved from 0.04108 to 0.04093, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 256/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00256: val_loss did not improve from 0.04093\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 257/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00257: val_loss did not improve from 0.04093\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 258/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00258: val_loss did not improve from 0.04093\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 259/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00259: val_loss did not improve from 0.04093\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 260/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00260: val_loss did not improve from 0.04093\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 261/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0295 - mean_squared_error: 0.0295\n",
      "Epoch 00261: val_loss did not improve from 0.04093\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 262/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00262: val_loss did not improve from 0.04093\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 263/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00263: val_loss did not improve from 0.04093\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 264/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.0185\n",
      "Epoch 00264: val_loss did not improve from 0.04093\n",
      "248/248 [==============================] - 0s 34us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 265/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00265: val_loss did not improve from 0.04093\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 266/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00266: val_loss did not improve from 0.04093\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 267/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00267: val_loss did not improve from 0.04093\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 268/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00268: val_loss did not improve from 0.04093\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 269/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00269: val_loss did not improve from 0.04093\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 270/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00270: val_loss did not improve from 0.04093\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 271/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00271: val_loss did not improve from 0.04093\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 272/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00272: val_loss did not improve from 0.04093\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 273/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00273: val_loss did not improve from 0.04093\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 274/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00274: val_loss did not improve from 0.04093\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 275/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00275: val_loss did not improve from 0.04093\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 276/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Epoch 00276: val_loss did not improve from 0.04093\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 277/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00277: val_loss did not improve from 0.04093\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 278/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00278: val_loss improved from 0.04093 to 0.04092, saving model to model.h5\n",
      "248/248 [==============================] - 0s 173us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 279/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Epoch 00279: val_loss improved from 0.04092 to 0.04081, saving model to model.h5\n",
      "248/248 [==============================] - 0s 173us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 280/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00280: val_loss improved from 0.04081 to 0.04074, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 281/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0173 - mean_squared_error: 0.0173\n",
      "Epoch 00281: val_loss improved from 0.04074 to 0.04058, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 282/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00282: val_loss improved from 0.04058 to 0.04048, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 283/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00283: val_loss improved from 0.04048 to 0.04040, saving model to model.h5\n",
      "248/248 [==============================] - 0s 169us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 284/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00284: val_loss improved from 0.04040 to 0.04040, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 285/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00285: val_loss did not improve from 0.04040\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 286/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00286: val_loss did not improve from 0.04040\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 287/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00287: val_loss did not improve from 0.04040\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 288/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00288: val_loss did not improve from 0.04040\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 289/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00289: val_loss improved from 0.04040 to 0.04030, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 290/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00290: val_loss improved from 0.04030 to 0.04025, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 291/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00291: val_loss improved from 0.04025 to 0.04022, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 292/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00292: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 293/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00293: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 294/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0139 - mean_squared_error: 0.0139\n",
      "Epoch 00294: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 295/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00295: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 296/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00296: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 297/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00297: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 298/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00298: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 299/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00299: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 300/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00300: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 301/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00301: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 302/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00302: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 303/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00303: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 304/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00304: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 305/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00305: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 306/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0115 - mean_squared_error: 0.0115\n",
      "Epoch 00306: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 307/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00307: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 308/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0307 - mean_squared_error: 0.0307\n",
      "Epoch 00308: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 309/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00309: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 310/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00310: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 311/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00311: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 312/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00312: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 313/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00313: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 314/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00314: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 315/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00315: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 316/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00316: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 317/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00317: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 318/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00318: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 319/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0157 - mean_squared_error: 0.0157\n",
      "Epoch 00319: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 320/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0130 - mean_squared_error: 0.0130\n",
      "Epoch 00320: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 321/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00321: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 322/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00322: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 323/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0159 - mean_squared_error: 0.0159\n",
      "Epoch 00323: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 324/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0346 - mean_squared_error: 0.0346\n",
      "Epoch 00324: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 325/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00325: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 326/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00326: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 327/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00327: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 328/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00328: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 329/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00329: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 330/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00330: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 331/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00331: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 332/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00332: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 333/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00333: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 334/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00334: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 335/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00335: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 336/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00336: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 337/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00337: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 338/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00338: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 339/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00339: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 340/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00340: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 341/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00341: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 342/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00342: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 343/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00343: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 344/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00344: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 345/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00345: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 346/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00346: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 347/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00347: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 348/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00348: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 349/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00349: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 350/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00350: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 351/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0129 - mean_squared_error: 0.0129\n",
      "Epoch 00351: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 352/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0136 - mean_squared_error: 0.0136\n",
      "Epoch 00352: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 353/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00353: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 354/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00354: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 35us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 355/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00355: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 356/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00356: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 357/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00357: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 358/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00358: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 359/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00359: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 360/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00360: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 361/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00361: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 362/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0307 - mean_squared_error: 0.0307\n",
      "Epoch 00362: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 363/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0231 - mean_squared_error: 0.0231\n",
      "Epoch 00363: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 364/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00364: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 42us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 365/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00365: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 366/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00366: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 367/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00367: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 368/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Epoch 00368: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 369/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0177 - mean_squared_error: 0.0177\n",
      "Epoch 00369: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 370/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00370: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 371/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00371: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 372/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0178 - mean_squared_error: 0.0178\n",
      "Epoch 00372: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 373/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00373: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 374/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0161 - mean_squared_error: 0.0161\n",
      "Epoch 00374: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 375/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0153 - mean_squared_error: 0.0153\n",
      "Epoch 00375: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 376/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00376: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 377/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0171 - mean_squared_error: 0.0171\n",
      "Epoch 00377: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 378/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00378: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 379/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00379: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 380/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00380: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 381/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00381: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 382/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00382: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 383/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00383: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 384/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00384: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 385/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00385: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 386/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0167 - mean_squared_error: 0.0167\n",
      "Epoch 00386: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 387/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00387: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 388/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00388: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 389/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00389: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 390/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0176 - mean_squared_error: 0.0176\n",
      "Epoch 00390: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 391/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00391: val_loss did not improve from 0.04022\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Running trial 4\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 1.0619 - mean_squared_error: 1.0619\n",
      "Epoch 00001: val_loss improved from inf to 0.89459, saving model to model.h5\n",
      "248/248 [==============================] - 2s 7ms/sample - loss: 1.2215 - mean_squared_error: 1.2215 - val_loss: 0.8946 - val_mean_squared_error: 0.8946\n",
      "Epoch 2/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 1.1315 - mean_squared_error: 1.1315\n",
      "Epoch 00002: val_loss improved from 0.89459 to 0.85092, saving model to model.h5\n",
      "248/248 [==============================] - 0s 113us/sample - loss: 1.1280 - mean_squared_error: 1.1280 - val_loss: 0.8509 - val_mean_squared_error: 0.8509\n",
      "Epoch 3/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 1.0999 - mean_squared_error: 1.0999\n",
      "Epoch 00003: val_loss improved from 0.85092 to 0.80883, saving model to model.h5\n",
      "248/248 [==============================] - 0s 113us/sample - loss: 1.0342 - mean_squared_error: 1.0342 - val_loss: 0.8088 - val_mean_squared_error: 0.8088\n",
      "Epoch 4/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.9346 - mean_squared_error: 0.9346\n",
      "Epoch 00004: val_loss improved from 0.80883 to 0.76628, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.9161 - mean_squared_error: 0.9161 - val_loss: 0.7663 - val_mean_squared_error: 0.7663\n",
      "Epoch 5/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.7839 - mean_squared_error: 0.7839\n",
      "Epoch 00005: val_loss improved from 0.76628 to 0.72416, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.8273 - mean_squared_error: 0.8273 - val_loss: 0.7242 - val_mean_squared_error: 0.7242\n",
      "Epoch 6/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.8005 - mean_squared_error: 0.8005\n",
      "Epoch 00006: val_loss improved from 0.72416 to 0.68296, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.7591 - mean_squared_error: 0.7591 - val_loss: 0.6830 - val_mean_squared_error: 0.6830\n",
      "Epoch 7/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.7062 - mean_squared_error: 0.7062\n",
      "Epoch 00007: val_loss improved from 0.68296 to 0.64223, saving model to model.h5\n",
      "248/248 [==============================] - 0s 108us/sample - loss: 0.6727 - mean_squared_error: 0.6727 - val_loss: 0.6422 - val_mean_squared_error: 0.6422\n",
      "Epoch 8/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.6179 - mean_squared_error: 0.6179\n",
      "Epoch 00008: val_loss improved from 0.64223 to 0.60244, saving model to model.h5\n",
      "248/248 [==============================] - 0s 112us/sample - loss: 0.6064 - mean_squared_error: 0.6064 - val_loss: 0.6024 - val_mean_squared_error: 0.6024\n",
      "Epoch 9/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.6109 - mean_squared_error: 0.6109\n",
      "Epoch 00009: val_loss improved from 0.60244 to 0.56260, saving model to model.h5\n",
      "248/248 [==============================] - 0s 119us/sample - loss: 0.5524 - mean_squared_error: 0.5524 - val_loss: 0.5626 - val_mean_squared_error: 0.5626\n",
      "Epoch 10/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.4540 - mean_squared_error: 0.4540\n",
      "Epoch 00010: val_loss improved from 0.56260 to 0.52290, saving model to model.h5\n",
      "248/248 [==============================] - 0s 119us/sample - loss: 0.4897 - mean_squared_error: 0.4897 - val_loss: 0.5229 - val_mean_squared_error: 0.5229\n",
      "Epoch 11/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.4344 - mean_squared_error: 0.4344\n",
      "Epoch 00011: val_loss improved from 0.52290 to 0.48442, saving model to model.h5\n",
      "248/248 [==============================] - 0s 111us/sample - loss: 0.4482 - mean_squared_error: 0.4482 - val_loss: 0.4844 - val_mean_squared_error: 0.4844\n",
      "Epoch 12/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.4401 - mean_squared_error: 0.4401\n",
      "Epoch 00012: val_loss improved from 0.48442 to 0.44692, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.3938 - mean_squared_error: 0.3938 - val_loss: 0.4469 - val_mean_squared_error: 0.4469\n",
      "Epoch 13/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3622 - mean_squared_error: 0.3622\n",
      "Epoch 00013: val_loss improved from 0.44692 to 0.41050, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.3267 - mean_squared_error: 0.3267 - val_loss: 0.4105 - val_mean_squared_error: 0.4105\n",
      "Epoch 14/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3679 - mean_squared_error: 0.3679\n",
      "Epoch 00014: val_loss improved from 0.41050 to 0.37552, saving model to model.h5\n",
      "248/248 [==============================] - 0s 112us/sample - loss: 0.3108 - mean_squared_error: 0.3108 - val_loss: 0.3755 - val_mean_squared_error: 0.3755\n",
      "Epoch 15/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2872 - mean_squared_error: 0.2872\n",
      "Epoch 00015: val_loss improved from 0.37552 to 0.34245, saving model to model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 122us/sample - loss: 0.2614 - mean_squared_error: 0.2614 - val_loss: 0.3425 - val_mean_squared_error: 0.3425\n",
      "Epoch 16/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2158 - mean_squared_error: 0.2158\n",
      "Epoch 00016: val_loss improved from 0.34245 to 0.31041, saving model to model.h5\n",
      "248/248 [==============================] - 0s 128us/sample - loss: 0.2341 - mean_squared_error: 0.2341 - val_loss: 0.3104 - val_mean_squared_error: 0.3104\n",
      "Epoch 17/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2328 - mean_squared_error: 0.2328\n",
      "Epoch 00017: val_loss improved from 0.31041 to 0.28003, saving model to model.h5\n",
      "248/248 [==============================] - 0s 191us/sample - loss: 0.2133 - mean_squared_error: 0.2133 - val_loss: 0.2800 - val_mean_squared_error: 0.2800\n",
      "Epoch 18/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1924 - mean_squared_error: 0.1924\n",
      "Epoch 00018: val_loss improved from 0.28003 to 0.25174, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.1875 - mean_squared_error: 0.1875 - val_loss: 0.2517 - val_mean_squared_error: 0.2517\n",
      "Epoch 19/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1925 - mean_squared_error: 0.1925\n",
      "Epoch 00019: val_loss improved from 0.25174 to 0.22552, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.1643 - mean_squared_error: 0.1643 - val_loss: 0.2255 - val_mean_squared_error: 0.2255\n",
      "Epoch 20/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1529 - mean_squared_error: 0.1529\n",
      "Epoch 00020: val_loss improved from 0.22552 to 0.20135, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.1416 - mean_squared_error: 0.1416 - val_loss: 0.2013 - val_mean_squared_error: 0.2013\n",
      "Epoch 21/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1116 - mean_squared_error: 0.1116\n",
      "Epoch 00021: val_loss improved from 0.20135 to 0.17934, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.1252 - mean_squared_error: 0.1252 - val_loss: 0.1793 - val_mean_squared_error: 0.1793\n",
      "Epoch 22/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1074 - mean_squared_error: 0.1074\n",
      "Epoch 00022: val_loss improved from 0.17934 to 0.15940, saving model to model.h5\n",
      "248/248 [==============================] - 0s 123us/sample - loss: 0.1149 - mean_squared_error: 0.1149 - val_loss: 0.1594 - val_mean_squared_error: 0.1594\n",
      "Epoch 23/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1083 - mean_squared_error: 0.1083\n",
      "Epoch 00023: val_loss improved from 0.15940 to 0.14152, saving model to model.h5\n",
      "248/248 [==============================] - 0s 118us/sample - loss: 0.1137 - mean_squared_error: 0.1137 - val_loss: 0.1415 - val_mean_squared_error: 0.1415\n",
      "Epoch 24/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0830 - mean_squared_error: 0.0830\n",
      "Epoch 00024: val_loss improved from 0.14152 to 0.12587, saving model to model.h5\n",
      "248/248 [==============================] - 0s 122us/sample - loss: 0.0872 - mean_squared_error: 0.0872 - val_loss: 0.1259 - val_mean_squared_error: 0.1259\n",
      "Epoch 25/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0906 - mean_squared_error: 0.0906\n",
      "Epoch 00025: val_loss improved from 0.12587 to 0.11208, saving model to model.h5\n",
      "248/248 [==============================] - 0s 114us/sample - loss: 0.0845 - mean_squared_error: 0.0845 - val_loss: 0.1121 - val_mean_squared_error: 0.1121\n",
      "Epoch 26/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0736 - mean_squared_error: 0.0736\n",
      "Epoch 00026: val_loss improved from 0.11208 to 0.10013, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.0777 - mean_squared_error: 0.0777 - val_loss: 0.1001 - val_mean_squared_error: 0.1001\n",
      "Epoch 27/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0776 - mean_squared_error: 0.0776\n",
      "Epoch 00027: val_loss improved from 0.10013 to 0.08983, saving model to model.h5\n",
      "248/248 [==============================] - 0s 122us/sample - loss: 0.0674 - mean_squared_error: 0.0674 - val_loss: 0.0898 - val_mean_squared_error: 0.0898\n",
      "Epoch 28/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0673 - mean_squared_error: 0.0673\n",
      "Epoch 00028: val_loss improved from 0.08983 to 0.08120, saving model to model.h5\n",
      "248/248 [==============================] - 0s 112us/sample - loss: 0.0697 - mean_squared_error: 0.0697 - val_loss: 0.0812 - val_mean_squared_error: 0.0812\n",
      "Epoch 29/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0593 - mean_squared_error: 0.0593\n",
      "Epoch 00029: val_loss improved from 0.08120 to 0.07381, saving model to model.h5\n",
      "248/248 [==============================] - 0s 120us/sample - loss: 0.0630 - mean_squared_error: 0.0630 - val_loss: 0.0738 - val_mean_squared_error: 0.0738\n",
      "Epoch 30/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0545 - mean_squared_error: 0.0545\n",
      "Epoch 00030: val_loss improved from 0.07381 to 0.06758, saving model to model.h5\n",
      "248/248 [==============================] - 0s 123us/sample - loss: 0.0596 - mean_squared_error: 0.0596 - val_loss: 0.0676 - val_mean_squared_error: 0.0676\n",
      "Epoch 31/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0522 - mean_squared_error: 0.0522\n",
      "Epoch 00031: val_loss improved from 0.06758 to 0.06253, saving model to model.h5\n",
      "248/248 [==============================] - 0s 118us/sample - loss: 0.0549 - mean_squared_error: 0.0549 - val_loss: 0.0625 - val_mean_squared_error: 0.0625\n",
      "Epoch 32/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0518 - mean_squared_error: 0.0518\n",
      "Epoch 00032: val_loss improved from 0.06253 to 0.05837, saving model to model.h5\n",
      "248/248 [==============================] - 0s 116us/sample - loss: 0.0585 - mean_squared_error: 0.0585 - val_loss: 0.0584 - val_mean_squared_error: 0.0584\n",
      "Epoch 33/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0558 - mean_squared_error: 0.0558\n",
      "Epoch 00033: val_loss improved from 0.05837 to 0.05491, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0578 - mean_squared_error: 0.0578 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 34/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0512 - mean_squared_error: 0.0512\n",
      "Epoch 00034: val_loss improved from 0.05491 to 0.05214, saving model to model.h5\n",
      "248/248 [==============================] - 0s 120us/sample - loss: 0.0503 - mean_squared_error: 0.0503 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 35/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0479 - mean_squared_error: 0.0479\n",
      "Epoch 00035: val_loss improved from 0.05214 to 0.05002, saving model to model.h5\n",
      "248/248 [==============================] - 0s 119us/sample - loss: 0.0560 - mean_squared_error: 0.0560 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 36/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0398 - mean_squared_error: 0.0398\n",
      "Epoch 00036: val_loss improved from 0.05002 to 0.04826, saving model to model.h5\n",
      "248/248 [==============================] - 0s 109us/sample - loss: 0.0565 - mean_squared_error: 0.0565 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 37/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0534 - mean_squared_error: 0.0534\n",
      "Epoch 00037: val_loss improved from 0.04826 to 0.04690, saving model to model.h5\n",
      "248/248 [==============================] - 0s 116us/sample - loss: 0.0515 - mean_squared_error: 0.0515 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 38/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0564 - mean_squared_error: 0.0564\n",
      "Epoch 00038: val_loss improved from 0.04690 to 0.04587, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0504 - mean_squared_error: 0.0504 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 39/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0473 - mean_squared_error: 0.0473\n",
      "Epoch 00039: val_loss improved from 0.04587 to 0.04521, saving model to model.h5\n",
      "248/248 [==============================] - 0s 113us/sample - loss: 0.0501 - mean_squared_error: 0.0501 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 40/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00040: val_loss improved from 0.04521 to 0.04472, saving model to model.h5\n",
      "248/248 [==============================] - 0s 130us/sample - loss: 0.0417 - mean_squared_error: 0.0417 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 41/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0501 - mean_squared_error: 0.0501\n",
      "Epoch 00041: val_loss improved from 0.04472 to 0.04438, saving model to model.h5\n",
      "248/248 [==============================] - 0s 118us/sample - loss: 0.0501 - mean_squared_error: 0.0501 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 42/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0536 - mean_squared_error: 0.0536\n",
      "Epoch 00042: val_loss improved from 0.04438 to 0.04423, saving model to model.h5\n",
      "248/248 [==============================] - 0s 116us/sample - loss: 0.0431 - mean_squared_error: 0.0431 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 43/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0448 - mean_squared_error: 0.0448\n",
      "Epoch 00043: val_loss improved from 0.04423 to 0.04423, saving model to model.h5\n",
      "248/248 [==============================] - 0s 116us/sample - loss: 0.0460 - mean_squared_error: 0.0460 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 44/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0521 - mean_squared_error: 0.0521\n",
      "Epoch 00044: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0489 - mean_squared_error: 0.0489 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 45/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0475 - mean_squared_error: 0.0475\n",
      "Epoch 00045: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 33us/sample - loss: 0.0456 - mean_squared_error: 0.0456 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 46/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0522 - mean_squared_error: 0.0522\n",
      "Epoch 00046: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0459 - mean_squared_error: 0.0459 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 47/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Epoch 00047: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0465 - mean_squared_error: 0.0465 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 48/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Epoch 00048: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0386 - mean_squared_error: 0.0386 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 49/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0466 - mean_squared_error: 0.0466\n",
      "Epoch 00049: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0430 - mean_squared_error: 0.0430 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 50/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0348 - mean_squared_error: 0.0348\n",
      "Epoch 00050: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0403 - mean_squared_error: 0.0403 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 51/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0420 - mean_squared_error: 0.0420\n",
      "Epoch 00051: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 52/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0478 - mean_squared_error: 0.0478\n",
      "Epoch 00052: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 53/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0334 - mean_squared_error: 0.0334\n",
      "Epoch 00053: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 54/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0378 - mean_squared_error: 0.0378\n",
      "Epoch 00054: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0426 - mean_squared_error: 0.0426 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 55/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0436 - mean_squared_error: 0.0436\n",
      "Epoch 00055: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0398 - mean_squared_error: 0.0398 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 56/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0457 - mean_squared_error: 0.0457\n",
      "Epoch 00056: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0428 - mean_squared_error: 0.0428 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 57/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00057: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 33us/sample - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 58/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0500 - mean_squared_error: 0.0500\n",
      "Epoch 00058: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0434 - mean_squared_error: 0.0434 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 59/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0512 - mean_squared_error: 0.0512\n",
      "Epoch 00059: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0392 - mean_squared_error: 0.0392 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 60/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00060: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0397 - mean_squared_error: 0.0397 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 61/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0441 - mean_squared_error: 0.0441\n",
      "Epoch 00061: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0392 - mean_squared_error: 0.0392 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 62/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00062: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0378 - mean_squared_error: 0.0378 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 63/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0448 - mean_squared_error: 0.0448\n",
      "Epoch 00063: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0437 - mean_squared_error: 0.0437 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 64/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0552 - mean_squared_error: 0.0552\n",
      "Epoch 00064: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0432 - mean_squared_error: 0.0432 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0388 - mean_squared_error: 0.0388\n",
      "Epoch 00065: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0396 - mean_squared_error: 0.0396 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 66/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0449 - mean_squared_error: 0.0449\n",
      "Epoch 00066: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 67/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0411 - mean_squared_error: 0.0411\n",
      "Epoch 00067: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0387 - mean_squared_error: 0.0387 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 68/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0386 - mean_squared_error: 0.0386\n",
      "Epoch 00068: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0418 - mean_squared_error: 0.0418 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 69/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0468 - mean_squared_error: 0.0468\n",
      "Epoch 00069: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0399 - mean_squared_error: 0.0399 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 70/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0402 - mean_squared_error: 0.0402\n",
      "Epoch 00070: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 71/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0409 - mean_squared_error: 0.0409\n",
      "Epoch 00071: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0396 - mean_squared_error: 0.0396 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 72/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 00072: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0363 - mean_squared_error: 0.0363 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 73/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0434 - mean_squared_error: 0.0434\n",
      "Epoch 00073: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 74/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0406 - mean_squared_error: 0.0406\n",
      "Epoch 00074: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 75/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0382 - mean_squared_error: 0.0382\n",
      "Epoch 00075: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0400 - mean_squared_error: 0.0400 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 76/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0398 - mean_squared_error: 0.0398\n",
      "Epoch 00076: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 77/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0461 - mean_squared_error: 0.0461\n",
      "Epoch 00077: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0387 - mean_squared_error: 0.0387 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 78/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0396 - mean_squared_error: 0.0396\n",
      "Epoch 00078: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0428 - mean_squared_error: 0.0428 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 79/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00079: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 80/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00080: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0328 - mean_squared_error: 0.0328 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 81/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0388 - mean_squared_error: 0.0388\n",
      "Epoch 00081: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0393 - mean_squared_error: 0.0393 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 82/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0426 - mean_squared_error: 0.0426\n",
      "Epoch 00082: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 83/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Epoch 00083: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0371 - mean_squared_error: 0.0371 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 84/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0430 - mean_squared_error: 0.0430\n",
      "Epoch 00084: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 33us/sample - loss: 0.0380 - mean_squared_error: 0.0380 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 85/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0426 - mean_squared_error: 0.0426\n",
      "Epoch 00085: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0375 - mean_squared_error: 0.0375 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 86/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0431 - mean_squared_error: 0.0431\n",
      "Epoch 00086: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0389 - mean_squared_error: 0.0389 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 87/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0366 - mean_squared_error: 0.0366\n",
      "Epoch 00087: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 88/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00088: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 89/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00089: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0308 - mean_squared_error: 0.0308 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 90/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00090: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0363 - mean_squared_error: 0.0363 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 91/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Epoch 00091: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 92/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Epoch 00092: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0330 - mean_squared_error: 0.0330 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 93/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0346 - mean_squared_error: 0.0346\n",
      "Epoch 00093: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 94/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00094: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 95/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0443 - mean_squared_error: 0.0443\n",
      "Epoch 00095: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 96/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0406 - mean_squared_error: 0.0406\n",
      "Epoch 00096: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 97/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Epoch 00097: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0386 - mean_squared_error: 0.0386 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 98/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Epoch 00098: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 99/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00099: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 31us/sample - loss: 0.0323 - mean_squared_error: 0.0323 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 100/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00100: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 101/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00101: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 102/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0379 - mean_squared_error: 0.0379\n",
      "Epoch 00102: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 34us/sample - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 103/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0368 - mean_squared_error: 0.0368\n",
      "Epoch 00103: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0370 - mean_squared_error: 0.0370 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 104/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00104: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 105/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0344 - mean_squared_error: 0.0344\n",
      "Epoch 00105: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0333 - mean_squared_error: 0.0333 - val_loss: 0.0509 - val_mean_squared_error: 0.0509\n",
      "Epoch 106/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00106: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0511 - val_mean_squared_error: 0.0511\n",
      "Epoch 107/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0307 - mean_squared_error: 0.0307\n",
      "Epoch 00107: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0513 - val_mean_squared_error: 0.0513\n",
      "Epoch 108/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0368 - mean_squared_error: 0.0368\n",
      "Epoch 00108: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0333 - mean_squared_error: 0.0333 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 109/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0371 - mean_squared_error: 0.0371\n",
      "Epoch 00109: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0517 - val_mean_squared_error: 0.0517\n",
      "Epoch 110/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0392 - mean_squared_error: 0.0392\n",
      "Epoch 00110: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 111/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Epoch 00111: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0516 - val_mean_squared_error: 0.0516\n",
      "Epoch 112/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00112: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0514 - val_mean_squared_error: 0.0514\n",
      "Epoch 113/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00113: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 114/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0409 - mean_squared_error: 0.0409\n",
      "Epoch 00114: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 33us/sample - loss: 0.0328 - mean_squared_error: 0.0328 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 115/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00115: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 116/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0437 - mean_squared_error: 0.0437\n",
      "Epoch 00116: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 117/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0507 - mean_squared_error: 0.0507\n",
      "Epoch 00117: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 118/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00118: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 119/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00119: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 120/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0395 - mean_squared_error: 0.0395\n",
      "Epoch 00120: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0371 - mean_squared_error: 0.0371 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 121/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0387 - mean_squared_error: 0.0387\n",
      "Epoch 00121: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 31us/sample - loss: 0.0308 - mean_squared_error: 0.0308 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 122/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00122: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 123/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 00123: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 124/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0387 - mean_squared_error: 0.0387\n",
      "Epoch 00124: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 125/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00125: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 126/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00126: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 127/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0367 - mean_squared_error: 0.0367\n",
      "Epoch 00127: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 128/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00128: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 129/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Epoch 00129: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 130/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Epoch 00130: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0389 - mean_squared_error: 0.0389 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 131/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00131: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0367 - mean_squared_error: 0.0367 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 132/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0376 - mean_squared_error: 0.0376\n",
      "Epoch 00132: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 133/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0417 - mean_squared_error: 0.0417\n",
      "Epoch 00133: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 134/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00134: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 135/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Epoch 00135: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0331 - mean_squared_error: 0.0331 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 136/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00136: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 137/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00137: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 138/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00138: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0514 - val_mean_squared_error: 0.0514\n",
      "Epoch 139/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0339 - mean_squared_error: 0.0339\n",
      "Epoch 00139: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0516 - val_mean_squared_error: 0.0516\n",
      "Epoch 140/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00140: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "Epoch 141/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Epoch 00141: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "Epoch 142/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0366 - mean_squared_error: 0.0366\n",
      "Epoch 00142: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0363 - mean_squared_error: 0.0363 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "Epoch 143/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00143: val_loss did not improve from 0.04423\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "Running trial 5\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.9206 - mean_squared_error: 0.9206\n",
      "Epoch 00001: val_loss improved from inf to 0.59921, saving model to model.h5\n",
      "248/248 [==============================] - 2s 7ms/sample - loss: 0.9313 - mean_squared_error: 0.9313 - val_loss: 0.5992 - val_mean_squared_error: 0.5992\n",
      "Epoch 2/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 1.0033 - mean_squared_error: 1.0033\n",
      "Epoch 00002: val_loss improved from 0.59921 to 0.57267, saving model to model.h5\n",
      "248/248 [==============================] - 0s 139us/sample - loss: 0.8762 - mean_squared_error: 0.8762 - val_loss: 0.5727 - val_mean_squared_error: 0.5727\n",
      "Epoch 3/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.8259 - mean_squared_error: 0.8259\n",
      "Epoch 00003: val_loss improved from 0.57267 to 0.54595, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.7985 - mean_squared_error: 0.7985 - val_loss: 0.5460 - val_mean_squared_error: 0.5460\n",
      "Epoch 4/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.6278 - mean_squared_error: 0.6278\n",
      "Epoch 00004: val_loss improved from 0.54595 to 0.51900, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.7308 - mean_squared_error: 0.7308 - val_loss: 0.5190 - val_mean_squared_error: 0.5190\n",
      "Epoch 5/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.7069 - mean_squared_error: 0.7069\n",
      "Epoch 00005: val_loss improved from 0.51900 to 0.49338, saving model to model.h5\n",
      "248/248 [==============================] - 0s 118us/sample - loss: 0.6710 - mean_squared_error: 0.6710 - val_loss: 0.4934 - val_mean_squared_error: 0.4934\n",
      "Epoch 6/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.5877 - mean_squared_error: 0.5877\n",
      "Epoch 00006: val_loss improved from 0.49338 to 0.46756, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.6246 - mean_squared_error: 0.6246 - val_loss: 0.4676 - val_mean_squared_error: 0.4676\n",
      "Epoch 7/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.6105 - mean_squared_error: 0.6105\n",
      "Epoch 00007: val_loss improved from 0.46756 to 0.44236, saving model to model.h5\n",
      "248/248 [==============================] - 0s 130us/sample - loss: 0.5740 - mean_squared_error: 0.5740 - val_loss: 0.4424 - val_mean_squared_error: 0.4424\n",
      "Epoch 8/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.5473 - mean_squared_error: 0.5473\n",
      "Epoch 00008: val_loss improved from 0.44236 to 0.41716, saving model to model.h5\n",
      "248/248 [==============================] - 0s 126us/sample - loss: 0.5132 - mean_squared_error: 0.5132 - val_loss: 0.4172 - val_mean_squared_error: 0.4172\n",
      "Epoch 9/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.4138 - mean_squared_error: 0.4138\n",
      "Epoch 00009: val_loss improved from 0.41716 to 0.39222, saving model to model.h5\n",
      "248/248 [==============================] - 0s 122us/sample - loss: 0.4662 - mean_squared_error: 0.4662 - val_loss: 0.3922 - val_mean_squared_error: 0.3922\n",
      "Epoch 10/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.4228 - mean_squared_error: 0.4228\n",
      "Epoch 00010: val_loss improved from 0.39222 to 0.36753, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.4300 - mean_squared_error: 0.4300 - val_loss: 0.3675 - val_mean_squared_error: 0.3675\n",
      "Epoch 11/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3857 - mean_squared_error: 0.3857\n",
      "Epoch 00011: val_loss improved from 0.36753 to 0.34319, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.3805 - mean_squared_error: 0.3805 - val_loss: 0.3432 - val_mean_squared_error: 0.3432\n",
      "Epoch 12/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3703 - mean_squared_error: 0.3703\n",
      "Epoch 00012: val_loss improved from 0.34319 to 0.31921, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.3428 - mean_squared_error: 0.3428 - val_loss: 0.3192 - val_mean_squared_error: 0.3192\n",
      "Epoch 13/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3291 - mean_squared_error: 0.3291\n",
      "Epoch 00013: val_loss improved from 0.31921 to 0.29576, saving model to model.h5\n",
      "248/248 [==============================] - 0s 134us/sample - loss: 0.3073 - mean_squared_error: 0.3073 - val_loss: 0.2958 - val_mean_squared_error: 0.2958\n",
      "Epoch 14/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2987 - mean_squared_error: 0.2987\n",
      "Epoch 00014: val_loss improved from 0.29576 to 0.27291, saving model to model.h5\n",
      "248/248 [==============================] - 0s 138us/sample - loss: 0.2852 - mean_squared_error: 0.2852 - val_loss: 0.2729 - val_mean_squared_error: 0.2729\n",
      "Epoch 15/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2461 - mean_squared_error: 0.2461\n",
      "Epoch 00015: val_loss improved from 0.27291 to 0.25085, saving model to model.h5\n",
      "248/248 [==============================] - 0s 131us/sample - loss: 0.2578 - mean_squared_error: 0.2578 - val_loss: 0.2509 - val_mean_squared_error: 0.2509\n",
      "Epoch 16/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2507 - mean_squared_error: 0.2507\n",
      "Epoch 00016: val_loss improved from 0.25085 to 0.22969, saving model to model.h5\n",
      "248/248 [==============================] - 0s 151us/sample - loss: 0.2251 - mean_squared_error: 0.2251 - val_loss: 0.2297 - val_mean_squared_error: 0.2297\n",
      "Epoch 17/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2543 - mean_squared_error: 0.2543\n",
      "Epoch 00017: val_loss improved from 0.22969 to 0.20982, saving model to model.h5\n",
      "248/248 [==============================] - 0s 131us/sample - loss: 0.2133 - mean_squared_error: 0.2133 - val_loss: 0.2098 - val_mean_squared_error: 0.2098\n",
      "Epoch 18/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1837 - mean_squared_error: 0.1837\n",
      "Epoch 00018: val_loss improved from 0.20982 to 0.19068, saving model to model.h5\n",
      "248/248 [==============================] - 0s 152us/sample - loss: 0.1792 - mean_squared_error: 0.1792 - val_loss: 0.1907 - val_mean_squared_error: 0.1907\n",
      "Epoch 19/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1473 - mean_squared_error: 0.1473\n",
      "Epoch 00019: val_loss improved from 0.19068 to 0.17258, saving model to model.h5\n",
      "248/248 [==============================] - 0s 225us/sample - loss: 0.1694 - mean_squared_error: 0.1694 - val_loss: 0.1726 - val_mean_squared_error: 0.1726\n",
      "Epoch 20/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1455 - mean_squared_error: 0.1455\n",
      "Epoch 00020: val_loss improved from 0.17258 to 0.15548, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.1476 - mean_squared_error: 0.1476 - val_loss: 0.1555 - val_mean_squared_error: 0.1555\n",
      "Epoch 21/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1188 - mean_squared_error: 0.1188\n",
      "Epoch 00021: val_loss improved from 0.15548 to 0.13981, saving model to model.h5\n",
      "248/248 [==============================] - 0s 169us/sample - loss: 0.1269 - mean_squared_error: 0.1269 - val_loss: 0.1398 - val_mean_squared_error: 0.1398\n",
      "Epoch 22/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1387 - mean_squared_error: 0.1387\n",
      "Epoch 00022: val_loss improved from 0.13981 to 0.12557, saving model to model.h5\n",
      "248/248 [==============================] - 0s 169us/sample - loss: 0.1170 - mean_squared_error: 0.1170 - val_loss: 0.1256 - val_mean_squared_error: 0.1256\n",
      "Epoch 23/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0967 - mean_squared_error: 0.0967\n",
      "Epoch 00023: val_loss improved from 0.12557 to 0.11273, saving model to model.h5\n",
      "248/248 [==============================] - 0s 221us/sample - loss: 0.1059 - mean_squared_error: 0.1059 - val_loss: 0.1127 - val_mean_squared_error: 0.1127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1055 - mean_squared_error: 0.1055\n",
      "Epoch 00024: val_loss improved from 0.11273 to 0.10098, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 0.0977 - mean_squared_error: 0.0977 - val_loss: 0.1010 - val_mean_squared_error: 0.1010\n",
      "Epoch 25/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0994 - mean_squared_error: 0.0994\n",
      "Epoch 00025: val_loss improved from 0.10098 to 0.09064, saving model to model.h5\n",
      "248/248 [==============================] - 0s 181us/sample - loss: 0.0881 - mean_squared_error: 0.0881 - val_loss: 0.0906 - val_mean_squared_error: 0.0906\n",
      "Epoch 26/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0837 - mean_squared_error: 0.0837\n",
      "Epoch 00026: val_loss improved from 0.09064 to 0.08163, saving model to model.h5\n",
      "248/248 [==============================] - 0s 181us/sample - loss: 0.0794 - mean_squared_error: 0.0794 - val_loss: 0.0816 - val_mean_squared_error: 0.0816\n",
      "Epoch 27/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0820 - mean_squared_error: 0.0820\n",
      "Epoch 00027: val_loss improved from 0.08163 to 0.07361, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.0714 - mean_squared_error: 0.0714 - val_loss: 0.0736 - val_mean_squared_error: 0.0736\n",
      "Epoch 28/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0730 - mean_squared_error: 0.0730\n",
      "Epoch 00028: val_loss improved from 0.07361 to 0.06672, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0694 - mean_squared_error: 0.0694 - val_loss: 0.0667 - val_mean_squared_error: 0.0667\n",
      "Epoch 29/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0627 - mean_squared_error: 0.0627\n",
      "Epoch 00029: val_loss improved from 0.06672 to 0.06079, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0653 - mean_squared_error: 0.0653 - val_loss: 0.0608 - val_mean_squared_error: 0.0608\n",
      "Epoch 30/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0592 - mean_squared_error: 0.0592\n",
      "Epoch 00030: val_loss improved from 0.06079 to 0.05571, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0566 - mean_squared_error: 0.0566 - val_loss: 0.0557 - val_mean_squared_error: 0.0557\n",
      "Epoch 31/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0654 - mean_squared_error: 0.0654\n",
      "Epoch 00031: val_loss improved from 0.05571 to 0.05147, saving model to model.h5\n",
      "248/248 [==============================] - 0s 169us/sample - loss: 0.0551 - mean_squared_error: 0.0551 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 32/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0604 - mean_squared_error: 0.0604\n",
      "Epoch 00032: val_loss improved from 0.05147 to 0.04797, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0545 - mean_squared_error: 0.0545 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 33/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0507 - mean_squared_error: 0.0507\n",
      "Epoch 00033: val_loss improved from 0.04797 to 0.04508, saving model to model.h5\n",
      "248/248 [==============================] - 0s 181us/sample - loss: 0.0561 - mean_squared_error: 0.0561 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 34/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0494 - mean_squared_error: 0.0494\n",
      "Epoch 00034: val_loss improved from 0.04508 to 0.04268, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.0474 - mean_squared_error: 0.0474 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 35/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0607 - mean_squared_error: 0.0607\n",
      "Epoch 00035: val_loss improved from 0.04268 to 0.04075, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.0447 - mean_squared_error: 0.0447 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 36/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Epoch 00036: val_loss improved from 0.04075 to 0.03921, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0445 - mean_squared_error: 0.0445 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 37/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0495 - mean_squared_error: 0.0495\n",
      "Epoch 00037: val_loss improved from 0.03921 to 0.03800, saving model to model.h5\n",
      "248/248 [==============================] - 0s 169us/sample - loss: 0.0464 - mean_squared_error: 0.0464 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 38/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0446 - mean_squared_error: 0.0446\n",
      "Epoch 00038: val_loss improved from 0.03800 to 0.03706, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0449 - mean_squared_error: 0.0449 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 39/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0485 - mean_squared_error: 0.0485\n",
      "Epoch 00039: val_loss improved from 0.03706 to 0.03637, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0420 - mean_squared_error: 0.0420 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n",
      "Epoch 40/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00040: val_loss improved from 0.03637 to 0.03587, saving model to model.h5\n",
      "248/248 [==============================] - 0s 120us/sample - loss: 0.0427 - mean_squared_error: 0.0427 - val_loss: 0.0359 - val_mean_squared_error: 0.0359\n",
      "Epoch 41/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00041: val_loss improved from 0.03587 to 0.03555, saving model to model.h5\n",
      "248/248 [==============================] - 0s 177us/sample - loss: 0.0418 - mean_squared_error: 0.0418 - val_loss: 0.0355 - val_mean_squared_error: 0.0355\n",
      "Epoch 42/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0498 - mean_squared_error: 0.0498\n",
      "Epoch 00042: val_loss improved from 0.03555 to 0.03537, saving model to model.h5\n",
      "248/248 [==============================] - 0s 177us/sample - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0354 - val_mean_squared_error: 0.0354\n",
      "Epoch 43/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0379 - mean_squared_error: 0.0379\n",
      "Epoch 00043: val_loss improved from 0.03537 to 0.03525, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0391 - mean_squared_error: 0.0391 - val_loss: 0.0353 - val_mean_squared_error: 0.0353\n",
      "Epoch 44/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0399 - mean_squared_error: 0.0399\n",
      "Epoch 00044: val_loss improved from 0.03525 to 0.03524, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0352 - val_mean_squared_error: 0.0352\n",
      "Epoch 45/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0413 - mean_squared_error: 0.0413\n",
      "Epoch 00045: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0392 - mean_squared_error: 0.0392 - val_loss: 0.0353 - val_mean_squared_error: 0.0353\n",
      "Epoch 46/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Epoch 00046: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.0355 - val_mean_squared_error: 0.0355\n",
      "Epoch 47/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Epoch 00047: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0381 - mean_squared_error: 0.0381 - val_loss: 0.0357 - val_mean_squared_error: 0.0357\n",
      "Epoch 48/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0383 - mean_squared_error: 0.0383\n",
      "Epoch 00048: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0393 - mean_squared_error: 0.0393 - val_loss: 0.0358 - val_mean_squared_error: 0.0358\n",
      "Epoch 49/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00049: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0399 - mean_squared_error: 0.0399 - val_loss: 0.0361 - val_mean_squared_error: 0.0361\n",
      "Epoch 50/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0460 - mean_squared_error: 0.0460\n",
      "Epoch 00050: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0392 - mean_squared_error: 0.0392 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n",
      "Epoch 51/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0407 - mean_squared_error: 0.0407\n",
      "Epoch 00051: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0366 - mean_squared_error: 0.0366 - val_loss: 0.0367 - val_mean_squared_error: 0.0367\n",
      "Epoch 52/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0414 - mean_squared_error: 0.0414\n",
      "Epoch 00052: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0383 - mean_squared_error: 0.0383 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 53/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0334 - mean_squared_error: 0.0334\n",
      "Epoch 00053: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 54/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0360 - mean_squared_error: 0.0360\n",
      "Epoch 00054: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 55/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0307 - mean_squared_error: 0.0307\n",
      "Epoch 00055: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0372 - mean_squared_error: 0.0372 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 56/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00056: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0375 - mean_squared_error: 0.0375 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 57/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00057: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0328 - mean_squared_error: 0.0328 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 58/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0435 - mean_squared_error: 0.0435\n",
      "Epoch 00058: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0383 - val_mean_squared_error: 0.0383\n",
      "Epoch 59/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Epoch 00059: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 60/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00060: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 61/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0410 - mean_squared_error: 0.0410\n",
      "Epoch 00061: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 62/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0407 - mean_squared_error: 0.0407\n",
      "Epoch 00062: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0378 - mean_squared_error: 0.0378 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 63/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00063: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 64/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00064: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 65/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Epoch 00065: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 66/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0378 - mean_squared_error: 0.0378\n",
      "Epoch 00066: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 67/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Epoch 00067: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 68/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00068: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 69/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Epoch 00069: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 29us/sample - loss: 0.0361 - mean_squared_error: 0.0361 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 70/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00070: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 71/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00071: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 72/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00072: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 73/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0360 - mean_squared_error: 0.0360\n",
      "Epoch 00073: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 74/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0295 - mean_squared_error: 0.0295\n",
      "Epoch 00074: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 75/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0184 - mean_squared_error: 0.0184\n",
      "Epoch 00075: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 76/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0326 - mean_squared_error: 0.0326\n",
      "Epoch 00076: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 77/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0362 - mean_squared_error: 0.0362\n",
      "Epoch 00077: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 78/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00078: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 79/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0367 - mean_squared_error: 0.0367\n",
      "Epoch 00079: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 80/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00080: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 81/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Epoch 00081: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 82/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00082: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0330 - mean_squared_error: 0.0330 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 83/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0392 - mean_squared_error: 0.0392\n",
      "Epoch 00083: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 84/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0346 - mean_squared_error: 0.0346\n",
      "Epoch 00084: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 85/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00085: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 86/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0440 - mean_squared_error: 0.0440\n",
      "Epoch 00086: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0333 - mean_squared_error: 0.0333 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 87/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Epoch 00087: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 30us/sample - loss: 0.0361 - mean_squared_error: 0.0361 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 88/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00088: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 35us/sample - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 89/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0294 - mean_squared_error: 0.0294\n",
      "Epoch 00089: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 90/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00090: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 91/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00091: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 92/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00092: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 93/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00093: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 94/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00094: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 95/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0412 - mean_squared_error: 0.0412\n",
      "Epoch 00095: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0330 - mean_squared_error: 0.0330 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 96/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0432 - mean_squared_error: 0.0432\n",
      "Epoch 00096: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 97/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0344 - mean_squared_error: 0.0344\n",
      "Epoch 00097: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 98/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00098: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 33us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 99/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0384 - mean_squared_error: 0.0384\n",
      "Epoch 00099: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 100/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00100: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 101/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00101: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 102/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0161 - mean_squared_error: 0.0161\n",
      "Epoch 00102: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 103/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0294 - mean_squared_error: 0.0294\n",
      "Epoch 00103: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 104/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00104: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 105/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00105: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 106/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00106: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 107/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0344 - mean_squared_error: 0.0344\n",
      "Epoch 00107: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 37us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 108/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0339 - mean_squared_error: 0.0339\n",
      "Epoch 00108: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 109/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00109: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 110/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00110: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 111/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Epoch 00111: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0330 - mean_squared_error: 0.0330 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 112/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00112: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 113/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00113: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 114/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Epoch 00114: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 115/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00115: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 116/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00116: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 117/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00117: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 118/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00118: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 119/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00119: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 120/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00120: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 121/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00121: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 122/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 00122: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 123/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00123: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 124/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0366 - mean_squared_error: 0.0366\n",
      "Epoch 00124: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 49us/sample - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 125/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0348 - mean_squared_error: 0.0348\n",
      "Epoch 00125: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 126/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Epoch 00126: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 127/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Epoch 00127: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 45us/sample - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 128/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00128: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 129/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00129: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 130/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0334 - mean_squared_error: 0.0334\n",
      "Epoch 00130: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 131/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0378 - mean_squared_error: 0.0378\n",
      "Epoch 00131: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 132/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0424 - mean_squared_error: 0.0424\n",
      "Epoch 00132: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 133/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Epoch 00133: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 134/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00134: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 135/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00135: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 136/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0383 - mean_squared_error: 0.0383\n",
      "Epoch 00136: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 137/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00137: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 138/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00138: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 139/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0346 - mean_squared_error: 0.0346\n",
      "Epoch 00139: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 140/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00140: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 141/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 00141: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 142/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0343 - mean_squared_error: 0.0343\n",
      "Epoch 00142: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 143/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00143: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 144/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00144: val_loss did not improve from 0.03524\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Running trial 6\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 1.0728 - mean_squared_error: 1.0728\n",
      "Epoch 00001: val_loss improved from inf to 0.72682, saving model to model.h5\n",
      "248/248 [==============================] - 2s 7ms/sample - loss: 1.0421 - mean_squared_error: 1.0421 - val_loss: 0.7268 - val_mean_squared_error: 0.7268\n",
      "Epoch 2/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.9073 - mean_squared_error: 0.9073\n",
      "Epoch 00002: val_loss improved from 0.72682 to 0.68820, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.9558 - mean_squared_error: 0.9558 - val_loss: 0.6882 - val_mean_squared_error: 0.6882\n",
      "Epoch 3/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.9542 - mean_squared_error: 0.9542\n",
      "Epoch 00003: val_loss improved from 0.68820 to 0.65160, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.8814 - mean_squared_error: 0.8814 - val_loss: 0.6516 - val_mean_squared_error: 0.6516\n",
      "Epoch 4/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.6854 - mean_squared_error: 0.6854\n",
      "Epoch 00004: val_loss improved from 0.65160 to 0.61439, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.7747 - mean_squared_error: 0.7747 - val_loss: 0.6144 - val_mean_squared_error: 0.6144\n",
      "Epoch 5/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.8064 - mean_squared_error: 0.8064\n",
      "Epoch 00005: val_loss improved from 0.61439 to 0.57844, saving model to model.h5\n",
      "248/248 [==============================] - 0s 122us/sample - loss: 0.7107 - mean_squared_error: 0.7107 - val_loss: 0.5784 - val_mean_squared_error: 0.5784\n",
      "Epoch 6/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.7262 - mean_squared_error: 0.7262\n",
      "Epoch 00006: val_loss improved from 0.57844 to 0.54405, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.6572 - mean_squared_error: 0.6572 - val_loss: 0.5440 - val_mean_squared_error: 0.5440\n",
      "Epoch 7/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.6154 - mean_squared_error: 0.6154\n",
      "Epoch 00007: val_loss improved from 0.54405 to 0.51043, saving model to model.h5\n",
      "248/248 [==============================] - 0s 126us/sample - loss: 0.5763 - mean_squared_error: 0.5763 - val_loss: 0.5104 - val_mean_squared_error: 0.5104\n",
      "Epoch 8/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.5848 - mean_squared_error: 0.5848\n",
      "Epoch 00008: val_loss improved from 0.51043 to 0.47762, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.5146 - mean_squared_error: 0.5146 - val_loss: 0.4776 - val_mean_squared_error: 0.4776\n",
      "Epoch 9/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.5472 - mean_squared_error: 0.5472\n",
      "Epoch 00009: val_loss improved from 0.47762 to 0.44643, saving model to model.h5\n",
      "248/248 [==============================] - 0s 131us/sample - loss: 0.4552 - mean_squared_error: 0.4552 - val_loss: 0.4464 - val_mean_squared_error: 0.4464\n",
      "Epoch 10/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3960 - mean_squared_error: 0.3960\n",
      "Epoch 00010: val_loss improved from 0.44643 to 0.41527, saving model to model.h5\n",
      "248/248 [==============================] - 0s 126us/sample - loss: 0.4011 - mean_squared_error: 0.4011 - val_loss: 0.4153 - val_mean_squared_error: 0.4153\n",
      "Epoch 11/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3349 - mean_squared_error: 0.3349\n",
      "Epoch 00011: val_loss improved from 0.41527 to 0.38540, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.3683 - mean_squared_error: 0.3683 - val_loss: 0.3854 - val_mean_squared_error: 0.3854\n",
      "Epoch 12/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3178 - mean_squared_error: 0.3178\n",
      "Epoch 00012: val_loss improved from 0.38540 to 0.35658, saving model to model.h5\n",
      "248/248 [==============================] - 0s 138us/sample - loss: 0.3071 - mean_squared_error: 0.3071 - val_loss: 0.3566 - val_mean_squared_error: 0.3566\n",
      "Epoch 13/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2916 - mean_squared_error: 0.2916\n",
      "Epoch 00013: val_loss improved from 0.35658 to 0.32881, saving model to model.h5\n",
      "248/248 [==============================] - 0s 128us/sample - loss: 0.2802 - mean_squared_error: 0.2802 - val_loss: 0.3288 - val_mean_squared_error: 0.3288\n",
      "Epoch 14/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2694 - mean_squared_error: 0.2694\n",
      "Epoch 00014: val_loss improved from 0.32881 to 0.30245, saving model to model.h5\n",
      "248/248 [==============================] - 0s 120us/sample - loss: 0.2624 - mean_squared_error: 0.2624 - val_loss: 0.3025 - val_mean_squared_error: 0.3025\n",
      "Epoch 15/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1981 - mean_squared_error: 0.1981\n",
      "Epoch 00015: val_loss improved from 0.30245 to 0.27759, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.2249 - mean_squared_error: 0.2249 - val_loss: 0.2776 - val_mean_squared_error: 0.2776\n",
      "Epoch 16/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1937 - mean_squared_error: 0.1937\n",
      "Epoch 00016: val_loss improved from 0.27759 to 0.25416, saving model to model.h5\n",
      "248/248 [==============================] - 0s 126us/sample - loss: 0.1772 - mean_squared_error: 0.1772 - val_loss: 0.2542 - val_mean_squared_error: 0.2542\n",
      "Epoch 17/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1482 - mean_squared_error: 0.1482\n",
      "Epoch 00017: val_loss improved from 0.25416 to 0.23195, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.1557 - mean_squared_error: 0.1557 - val_loss: 0.2320 - val_mean_squared_error: 0.2320\n",
      "Epoch 18/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1784 - mean_squared_error: 0.1784\n",
      "Epoch 00018: val_loss improved from 0.23195 to 0.21130, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.1497 - mean_squared_error: 0.1497 - val_loss: 0.2113 - val_mean_squared_error: 0.2113\n",
      "Epoch 19/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1299 - mean_squared_error: 0.1299\n",
      "Epoch 00019: val_loss improved from 0.21130 to 0.19211, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.1259 - mean_squared_error: 0.1259 - val_loss: 0.1921 - val_mean_squared_error: 0.1921\n",
      "Epoch 20/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1249 - mean_squared_error: 0.1249\n",
      "Epoch 00020: val_loss improved from 0.19211 to 0.17463, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.1169 - mean_squared_error: 0.1169 - val_loss: 0.1746 - val_mean_squared_error: 0.1746\n",
      "Epoch 21/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1045 - mean_squared_error: 0.1045\n",
      "Epoch 00021: val_loss improved from 0.17463 to 0.15865, saving model to model.h5\n",
      "248/248 [==============================] - 0s 132us/sample - loss: 0.1051 - mean_squared_error: 0.1051 - val_loss: 0.1586 - val_mean_squared_error: 0.1586\n",
      "Epoch 22/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1076 - mean_squared_error: 0.1076\n",
      "Epoch 00022: val_loss improved from 0.15865 to 0.14420, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0926 - mean_squared_error: 0.0926 - val_loss: 0.1442 - val_mean_squared_error: 0.1442\n",
      "Epoch 23/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0748 - mean_squared_error: 0.0748\n",
      "Epoch 00023: val_loss improved from 0.14420 to 0.13096, saving model to model.h5\n",
      "248/248 [==============================] - 0s 123us/sample - loss: 0.0851 - mean_squared_error: 0.0851 - val_loss: 0.1310 - val_mean_squared_error: 0.1310\n",
      "Epoch 24/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0740 - mean_squared_error: 0.0740\n",
      "Epoch 00024: val_loss improved from 0.13096 to 0.11917, saving model to model.h5\n",
      "248/248 [==============================] - 0s 128us/sample - loss: 0.0768 - mean_squared_error: 0.0768 - val_loss: 0.1192 - val_mean_squared_error: 0.1192\n",
      "Epoch 25/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0597 - mean_squared_error: 0.0597\n",
      "Epoch 00025: val_loss improved from 0.11917 to 0.10853, saving model to model.h5\n",
      "248/248 [==============================] - 0s 130us/sample - loss: 0.0669 - mean_squared_error: 0.0669 - val_loss: 0.1085 - val_mean_squared_error: 0.1085\n",
      "Epoch 26/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0677 - mean_squared_error: 0.0677\n",
      "Epoch 00026: val_loss improved from 0.10853 to 0.09918, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0600 - mean_squared_error: 0.0600 - val_loss: 0.0992 - val_mean_squared_error: 0.0992\n",
      "Epoch 27/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0651 - mean_squared_error: 0.0651\n",
      "Epoch 00027: val_loss improved from 0.09918 to 0.09102, saving model to model.h5\n",
      "248/248 [==============================] - 0s 122us/sample - loss: 0.0650 - mean_squared_error: 0.0650 - val_loss: 0.0910 - val_mean_squared_error: 0.0910\n",
      "Epoch 28/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0551 - mean_squared_error: 0.0551\n",
      "Epoch 00028: val_loss improved from 0.09102 to 0.08396, saving model to model.h5\n",
      "248/248 [==============================] - 0s 128us/sample - loss: 0.0590 - mean_squared_error: 0.0590 - val_loss: 0.0840 - val_mean_squared_error: 0.0840\n",
      "Epoch 29/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0595 - mean_squared_error: 0.0595\n",
      "Epoch 00029: val_loss improved from 0.08396 to 0.07808, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0520 - mean_squared_error: 0.0520 - val_loss: 0.0781 - val_mean_squared_error: 0.0781\n",
      "Epoch 30/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0365 - mean_squared_error: 0.0365\n",
      "Epoch 00030: val_loss improved from 0.07808 to 0.07302, saving model to model.h5\n",
      "248/248 [==============================] - 0s 130us/sample - loss: 0.0499 - mean_squared_error: 0.0499 - val_loss: 0.0730 - val_mean_squared_error: 0.0730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0597 - mean_squared_error: 0.0597\n",
      "Epoch 00031: val_loss improved from 0.07302 to 0.06900, saving model to model.h5\n",
      "248/248 [==============================] - 0s 126us/sample - loss: 0.0584 - mean_squared_error: 0.0584 - val_loss: 0.0690 - val_mean_squared_error: 0.0690\n",
      "Epoch 32/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0405 - mean_squared_error: 0.0405\n",
      "Epoch 00032: val_loss improved from 0.06900 to 0.06562, saving model to model.h5\n",
      "248/248 [==============================] - 0s 126us/sample - loss: 0.0477 - mean_squared_error: 0.0477 - val_loss: 0.0656 - val_mean_squared_error: 0.0656\n",
      "Epoch 33/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0412 - mean_squared_error: 0.0412\n",
      "Epoch 00033: val_loss improved from 0.06562 to 0.06281, saving model to model.h5\n",
      "248/248 [==============================] - 0s 123us/sample - loss: 0.0446 - mean_squared_error: 0.0446 - val_loss: 0.0628 - val_mean_squared_error: 0.0628\n",
      "Epoch 34/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0348 - mean_squared_error: 0.0348\n",
      "Epoch 00034: val_loss improved from 0.06281 to 0.06060, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0447 - mean_squared_error: 0.0447 - val_loss: 0.0606 - val_mean_squared_error: 0.0606\n",
      "Epoch 35/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Epoch 00035: val_loss improved from 0.06060 to 0.05887, saving model to model.h5\n",
      "248/248 [==============================] - 0s 115us/sample - loss: 0.0448 - mean_squared_error: 0.0448 - val_loss: 0.0589 - val_mean_squared_error: 0.0589\n",
      "Epoch 36/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00036: val_loss improved from 0.05887 to 0.05764, saving model to model.h5\n",
      "248/248 [==============================] - 0s 122us/sample - loss: 0.0430 - mean_squared_error: 0.0430 - val_loss: 0.0576 - val_mean_squared_error: 0.0576\n",
      "Epoch 37/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0446 - mean_squared_error: 0.0446\n",
      "Epoch 00037: val_loss improved from 0.05764 to 0.05687, saving model to model.h5\n",
      "248/248 [==============================] - 0s 128us/sample - loss: 0.0435 - mean_squared_error: 0.0435 - val_loss: 0.0569 - val_mean_squared_error: 0.0569\n",
      "Epoch 38/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0521 - mean_squared_error: 0.0521\n",
      "Epoch 00038: val_loss improved from 0.05687 to 0.05642, saving model to model.h5\n",
      "248/248 [==============================] - 0s 123us/sample - loss: 0.0446 - mean_squared_error: 0.0446 - val_loss: 0.0564 - val_mean_squared_error: 0.0564\n",
      "Epoch 39/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00039: val_loss improved from 0.05642 to 0.05623, saving model to model.h5\n",
      "248/248 [==============================] - 0s 118us/sample - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0562 - val_mean_squared_error: 0.0562\n",
      "Epoch 40/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00040: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0393 - mean_squared_error: 0.0393 - val_loss: 0.0563 - val_mean_squared_error: 0.0563\n",
      "Epoch 41/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0360 - mean_squared_error: 0.0360\n",
      "Epoch 00041: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0564 - val_mean_squared_error: 0.0564\n",
      "Epoch 42/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0401 - mean_squared_error: 0.0401\n",
      "Epoch 00042: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0380 - mean_squared_error: 0.0380 - val_loss: 0.0569 - val_mean_squared_error: 0.0569\n",
      "Epoch 43/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0387 - mean_squared_error: 0.0387\n",
      "Epoch 00043: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0574 - val_mean_squared_error: 0.0574\n",
      "Epoch 44/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Epoch 00044: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0579 - val_mean_squared_error: 0.0579\n",
      "Epoch 45/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0370 - mean_squared_error: 0.0370\n",
      "Epoch 00045: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 34us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0584 - val_mean_squared_error: 0.0584\n",
      "Epoch 46/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 00046: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0367 - mean_squared_error: 0.0367 - val_loss: 0.0589 - val_mean_squared_error: 0.0589\n",
      "Epoch 47/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0501 - mean_squared_error: 0.0501\n",
      "Epoch 00047: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0402 - mean_squared_error: 0.0402 - val_loss: 0.0593 - val_mean_squared_error: 0.0593\n",
      "Epoch 48/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00048: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 34us/sample - loss: 0.0392 - mean_squared_error: 0.0392 - val_loss: 0.0596 - val_mean_squared_error: 0.0596\n",
      "Epoch 49/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0408 - mean_squared_error: 0.0408\n",
      "Epoch 00049: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0374 - mean_squared_error: 0.0374 - val_loss: 0.0598 - val_mean_squared_error: 0.0598\n",
      "Epoch 50/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Epoch 00050: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0394 - mean_squared_error: 0.0394 - val_loss: 0.0600 - val_mean_squared_error: 0.0600\n",
      "Epoch 51/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0386 - mean_squared_error: 0.0386\n",
      "Epoch 00051: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 35us/sample - loss: 0.0403 - mean_squared_error: 0.0403 - val_loss: 0.0600 - val_mean_squared_error: 0.0600\n",
      "Epoch 52/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0344 - mean_squared_error: 0.0344\n",
      "Epoch 00052: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.0600 - val_mean_squared_error: 0.0600\n",
      "Epoch 53/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0425 - mean_squared_error: 0.0425\n",
      "Epoch 00053: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 35us/sample - loss: 0.0392 - mean_squared_error: 0.0392 - val_loss: 0.0600 - val_mean_squared_error: 0.0600\n",
      "Epoch 54/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Epoch 00054: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0387 - mean_squared_error: 0.0387 - val_loss: 0.0600 - val_mean_squared_error: 0.0600\n",
      "Epoch 55/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0422 - mean_squared_error: 0.0422\n",
      "Epoch 00055: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0382 - mean_squared_error: 0.0382 - val_loss: 0.0601 - val_mean_squared_error: 0.0601\n",
      "Epoch 56/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00056: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0599 - val_mean_squared_error: 0.0599\n",
      "Epoch 57/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0352 - mean_squared_error: 0.0352\n",
      "Epoch 00057: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0595 - val_mean_squared_error: 0.0595\n",
      "Epoch 58/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00058: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0593 - val_mean_squared_error: 0.0593\n",
      "Epoch 59/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0422 - mean_squared_error: 0.0422\n",
      "Epoch 00059: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0592 - val_mean_squared_error: 0.0592\n",
      "Epoch 60/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Epoch 00060: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0323 - mean_squared_error: 0.0323 - val_loss: 0.0590 - val_mean_squared_error: 0.0590\n",
      "Epoch 61/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0480 - mean_squared_error: 0.0480\n",
      "Epoch 00061: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 30us/sample - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0590 - val_mean_squared_error: 0.0590\n",
      "Epoch 62/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00062: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0588 - val_mean_squared_error: 0.0588\n",
      "Epoch 63/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0412 - mean_squared_error: 0.0412\n",
      "Epoch 00063: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0587 - val_mean_squared_error: 0.0587\n",
      "Epoch 64/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00064: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0586 - val_mean_squared_error: 0.0586\n",
      "Epoch 65/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00065: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0584 - val_mean_squared_error: 0.0584\n",
      "Epoch 66/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Epoch 00066: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0582 - val_mean_squared_error: 0.0582\n",
      "Epoch 67/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0370 - mean_squared_error: 0.0370\n",
      "Epoch 00067: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0581 - val_mean_squared_error: 0.0581\n",
      "Epoch 68/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00068: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0579 - val_mean_squared_error: 0.0579\n",
      "Epoch 69/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Epoch 00069: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0579 - val_mean_squared_error: 0.0579\n",
      "Epoch 70/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00070: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0577 - val_mean_squared_error: 0.0577\n",
      "Epoch 71/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Epoch 00071: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0574 - val_mean_squared_error: 0.0574\n",
      "Epoch 72/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Epoch 00072: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0571 - val_mean_squared_error: 0.0571\n",
      "Epoch 73/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 00073: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0569 - val_mean_squared_error: 0.0569\n",
      "Epoch 74/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0475 - mean_squared_error: 0.0475\n",
      "Epoch 00074: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0394 - mean_squared_error: 0.0394 - val_loss: 0.0568 - val_mean_squared_error: 0.0568\n",
      "Epoch 75/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00075: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 37us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0567 - val_mean_squared_error: 0.0567\n",
      "Epoch 76/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00076: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0567 - val_mean_squared_error: 0.0567\n",
      "Epoch 77/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00077: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0564 - val_mean_squared_error: 0.0564\n",
      "Epoch 78/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Epoch 00078: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0564 - val_mean_squared_error: 0.0564\n",
      "Epoch 79/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0383 - mean_squared_error: 0.0383\n",
      "Epoch 00079: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0564 - val_mean_squared_error: 0.0564\n",
      "Epoch 80/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0372 - mean_squared_error: 0.0372\n",
      "Epoch 00080: val_loss did not improve from 0.05623\n",
      "248/248 [==============================] - 0s 41us/sample - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0564 - val_mean_squared_error: 0.0564\n",
      "Epoch 81/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00081: val_loss improved from 0.05623 to 0.05619, saving model to model.h5\n",
      "248/248 [==============================] - 0s 135us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0562 - val_mean_squared_error: 0.0562\n",
      "Epoch 82/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0403 - mean_squared_error: 0.0403\n",
      "Epoch 00082: val_loss improved from 0.05619 to 0.05600, saving model to model.h5\n",
      "248/248 [==============================] - 0s 122us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0560 - val_mean_squared_error: 0.0560\n",
      "Epoch 83/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0371 - mean_squared_error: 0.0371\n",
      "Epoch 00083: val_loss improved from 0.05600 to 0.05582, saving model to model.h5\n",
      "248/248 [==============================] - 0s 128us/sample - loss: 0.0398 - mean_squared_error: 0.0398 - val_loss: 0.0558 - val_mean_squared_error: 0.0558\n",
      "Epoch 84/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0382 - mean_squared_error: 0.0382\n",
      "Epoch 00084: val_loss improved from 0.05582 to 0.05564, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0556 - val_mean_squared_error: 0.0556\n",
      "Epoch 85/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00085: val_loss improved from 0.05564 to 0.05541, saving model to model.h5\n",
      "248/248 [==============================] - 0s 134us/sample - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0554 - val_mean_squared_error: 0.0554\n",
      "Epoch 86/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00086: val_loss improved from 0.05541 to 0.05523, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0552 - val_mean_squared_error: 0.0552\n",
      "Epoch 87/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00087: val_loss improved from 0.05523 to 0.05501, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0550 - val_mean_squared_error: 0.0550\n",
      "Epoch 88/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00088: val_loss improved from 0.05501 to 0.05499, saving model to model.h5\n",
      "248/248 [==============================] - 0s 135us/sample - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0550 - val_mean_squared_error: 0.0550\n",
      "Epoch 89/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00089: val_loss improved from 0.05499 to 0.05483, saving model to model.h5\n",
      "248/248 [==============================] - 0s 126us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0548 - val_mean_squared_error: 0.0548\n",
      "Epoch 90/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00090: val_loss did not improve from 0.05483\n",
      "248/248 [==============================] - 0s 39us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0550 - val_mean_squared_error: 0.0550\n",
      "Epoch 91/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Epoch 00091: val_loss did not improve from 0.05483\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0552 - val_mean_squared_error: 0.0552\n",
      "Epoch 92/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Epoch 00092: val_loss did not improve from 0.05483\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0555 - val_mean_squared_error: 0.0555\n",
      "Epoch 93/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00093: val_loss did not improve from 0.05483\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0557 - val_mean_squared_error: 0.0557\n",
      "Epoch 94/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00094: val_loss did not improve from 0.05483\n",
      "248/248 [==============================] - 0s 37us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0556 - val_mean_squared_error: 0.0556\n",
      "Epoch 95/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Epoch 00095: val_loss did not improve from 0.05483\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0552 - val_mean_squared_error: 0.0552\n",
      "Epoch 96/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00096: val_loss improved from 0.05483 to 0.05476, saving model to model.h5\n",
      "248/248 [==============================] - 0s 126us/sample - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0548 - val_mean_squared_error: 0.0548\n",
      "Epoch 97/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00097: val_loss improved from 0.05476 to 0.05437, saving model to model.h5\n",
      "248/248 [==============================] - 0s 122us/sample - loss: 0.0308 - mean_squared_error: 0.0308 - val_loss: 0.0544 - val_mean_squared_error: 0.0544\n",
      "Epoch 98/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 00098: val_loss improved from 0.05437 to 0.05396, saving model to model.h5\n",
      "248/248 [==============================] - 0s 120us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 99/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00099: val_loss improved from 0.05396 to 0.05365, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0536 - val_mean_squared_error: 0.0536\n",
      "Epoch 100/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00100: val_loss improved from 0.05365 to 0.05340, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0534 - val_mean_squared_error: 0.0534\n",
      "Epoch 101/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00101: val_loss improved from 0.05340 to 0.05332, saving model to model.h5\n",
      "248/248 [==============================] - 0s 132us/sample - loss: 0.0323 - mean_squared_error: 0.0323 - val_loss: 0.0533 - val_mean_squared_error: 0.0533\n",
      "Epoch 102/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0334 - mean_squared_error: 0.0334\n",
      "Epoch 00102: val_loss improved from 0.05332 to 0.05330, saving model to model.h5\n",
      "248/248 [==============================] - 0s 132us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0533 - val_mean_squared_error: 0.0533\n",
      "Epoch 103/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00103: val_loss improved from 0.05330 to 0.05318, saving model to model.h5\n",
      "248/248 [==============================] - 0s 131us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0532 - val_mean_squared_error: 0.0532\n",
      "Epoch 104/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00104: val_loss improved from 0.05318 to 0.05312, saving model to model.h5\n",
      "248/248 [==============================] - 0s 122us/sample - loss: 0.0330 - mean_squared_error: 0.0330 - val_loss: 0.0531 - val_mean_squared_error: 0.0531\n",
      "Epoch 105/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0368 - mean_squared_error: 0.0368\n",
      "Epoch 00105: val_loss did not improve from 0.05312\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0532 - val_mean_squared_error: 0.0532\n",
      "Epoch 106/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Epoch 00106: val_loss did not improve from 0.05312\n",
      "248/248 [==============================] - 0s 42us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0532 - val_mean_squared_error: 0.0532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00107: val_loss did not improve from 0.05312\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0532 - val_mean_squared_error: 0.0532\n",
      "Epoch 108/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00108: val_loss did not improve from 0.05312\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0534 - val_mean_squared_error: 0.0534\n",
      "Epoch 109/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0368 - mean_squared_error: 0.0368\n",
      "Epoch 00109: val_loss did not improve from 0.05312\n",
      "248/248 [==============================] - 0s 38us/sample - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n",
      "Epoch 110/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00110: val_loss did not improve from 0.05312\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0538 - val_mean_squared_error: 0.0538\n",
      "Epoch 111/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Epoch 00111: val_loss did not improve from 0.05312\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 112/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Epoch 00112: val_loss did not improve from 0.05312\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0541 - val_mean_squared_error: 0.0541\n",
      "Epoch 113/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0333 - mean_squared_error: 0.0333\n",
      "Epoch 00113: val_loss did not improve from 0.05312\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0541 - val_mean_squared_error: 0.0541\n",
      "Epoch 114/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Epoch 00114: val_loss did not improve from 0.05312\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0541 - val_mean_squared_error: 0.0541\n",
      "Epoch 115/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00115: val_loss did not improve from 0.05312\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0541 - val_mean_squared_error: 0.0541\n",
      "Epoch 116/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00116: val_loss did not improve from 0.05312\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 117/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Epoch 00117: val_loss did not improve from 0.05312\n",
      "248/248 [==============================] - 0s 37us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 118/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00118: val_loss did not improve from 0.05312\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0539 - val_mean_squared_error: 0.0539\n",
      "Epoch 119/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00119: val_loss did not improve from 0.05312\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0539 - val_mean_squared_error: 0.0539\n",
      "Epoch 120/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Epoch 00120: val_loss did not improve from 0.05312\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0539 - val_mean_squared_error: 0.0539\n",
      "Epoch 121/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0399 - mean_squared_error: 0.0399\n",
      "Epoch 00121: val_loss did not improve from 0.05312\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 122/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00122: val_loss did not improve from 0.05312\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n",
      "Epoch 123/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00123: val_loss did not improve from 0.05312\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0536 - val_mean_squared_error: 0.0536\n",
      "Epoch 124/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00124: val_loss did not improve from 0.05312\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0534 - val_mean_squared_error: 0.0534\n",
      "Epoch 125/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0379 - mean_squared_error: 0.0379\n",
      "Epoch 00125: val_loss did not improve from 0.05312\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0533 - val_mean_squared_error: 0.0533\n",
      "Epoch 126/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00126: val_loss did not improve from 0.05312\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0531 - val_mean_squared_error: 0.0531\n",
      "Epoch 127/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00127: val_loss improved from 0.05312 to 0.05312, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0531 - val_mean_squared_error: 0.0531\n",
      "Epoch 128/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00128: val_loss improved from 0.05312 to 0.05302, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0530 - val_mean_squared_error: 0.0530\n",
      "Epoch 129/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00129: val_loss improved from 0.05302 to 0.05286, saving model to model.h5\n",
      "248/248 [==============================] - 0s 169us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 130/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0367 - mean_squared_error: 0.0367\n",
      "Epoch 00130: val_loss improved from 0.05286 to 0.05278, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 131/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00131: val_loss improved from 0.05278 to 0.05252, saving model to model.h5\n",
      "248/248 [==============================] - 0s 169us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0525 - val_mean_squared_error: 0.0525\n",
      "Epoch 132/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00132: val_loss improved from 0.05252 to 0.05232, saving model to model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 124us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n",
      "Epoch 133/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Epoch 00133: val_loss improved from 0.05232 to 0.05229, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.0328 - mean_squared_error: 0.0328 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n",
      "Epoch 134/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0367 - mean_squared_error: 0.0367\n",
      "Epoch 00134: val_loss improved from 0.05229 to 0.05211, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0328 - mean_squared_error: 0.0328 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 135/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Epoch 00135: val_loss improved from 0.05211 to 0.05191, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "Epoch 136/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Epoch 00136: val_loss improved from 0.05191 to 0.05178, saving model to model.h5\n",
      "248/248 [==============================] - 0s 126us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 137/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00137: val_loss improved from 0.05178 to 0.05163, saving model to model.h5\n",
      "248/248 [==============================] - 0s 119us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0516 - val_mean_squared_error: 0.0516\n",
      "Epoch 138/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00138: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0323 - mean_squared_error: 0.0323 - val_loss: 0.0517 - val_mean_squared_error: 0.0517\n",
      "Epoch 139/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00139: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 140/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00140: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 141/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00141: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0525 - val_mean_squared_error: 0.0525\n",
      "Epoch 142/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00142: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0527 - val_mean_squared_error: 0.0527\n",
      "Epoch 143/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00143: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 144/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00144: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0530 - val_mean_squared_error: 0.0530\n",
      "Epoch 145/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0352 - mean_squared_error: 0.0352\n",
      "Epoch 00145: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0533 - val_mean_squared_error: 0.0533\n",
      "Epoch 146/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00146: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0534 - val_mean_squared_error: 0.0534\n",
      "Epoch 147/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00147: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0534 - val_mean_squared_error: 0.0534\n",
      "Epoch 148/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00148: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0533 - val_mean_squared_error: 0.0533\n",
      "Epoch 149/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00149: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0531 - val_mean_squared_error: 0.0531\n",
      "Epoch 150/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Epoch 00150: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0531 - val_mean_squared_error: 0.0531\n",
      "Epoch 151/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0188 - mean_squared_error: 0.0188\n",
      "Epoch 00151: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 152/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00152: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 153/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00153: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0530 - val_mean_squared_error: 0.0530\n",
      "Epoch 154/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00154: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0530 - val_mean_squared_error: 0.0530\n",
      "Epoch 155/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00155: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 156/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00156: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 157/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00157: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 158/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0352 - mean_squared_error: 0.0352\n",
      "Epoch 00158: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0531 - val_mean_squared_error: 0.0531\n",
      "Epoch 159/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00159: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0533 - val_mean_squared_error: 0.0533\n",
      "Epoch 160/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00160: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0533 - val_mean_squared_error: 0.0533\n",
      "Epoch 161/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00161: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0533 - val_mean_squared_error: 0.0533\n",
      "Epoch 162/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00162: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0535 - val_mean_squared_error: 0.0535\n",
      "Epoch 163/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.0185\n",
      "Epoch 00163: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0536 - val_mean_squared_error: 0.0536\n",
      "Epoch 164/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00164: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0538 - val_mean_squared_error: 0.0538\n",
      "Epoch 165/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00165: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0539 - val_mean_squared_error: 0.0539\n",
      "Epoch 166/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00166: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0536 - val_mean_squared_error: 0.0536\n",
      "Epoch 167/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00167: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0330 - mean_squared_error: 0.0330 - val_loss: 0.0531 - val_mean_squared_error: 0.0531\n",
      "Epoch 168/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00168: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 169/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00169: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 170/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00170: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 39us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 171/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00171: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 172/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0430 - mean_squared_error: 0.0430\n",
      "Epoch 00172: val_loss did not improve from 0.05163\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0516 - val_mean_squared_error: 0.0516\n",
      "Epoch 173/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00173: val_loss improved from 0.05163 to 0.05148, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 174/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00174: val_loss did not improve from 0.05148\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 175/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0358 - mean_squared_error: 0.0358\n",
      "Epoch 00175: val_loss did not improve from 0.05148\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0319 - mean_squared_error: 0.0319 - val_loss: 0.0516 - val_mean_squared_error: 0.0516\n",
      "Epoch 176/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00176: val_loss improved from 0.05148 to 0.05146, saving model to model.h5\n",
      "248/248 [==============================] - 0s 107us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 177/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00177: val_loss improved from 0.05146 to 0.05138, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0514 - val_mean_squared_error: 0.0514\n",
      "Epoch 178/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0326 - mean_squared_error: 0.0326\n",
      "Epoch 00178: val_loss improved from 0.05138 to 0.05134, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0513 - val_mean_squared_error: 0.0513\n",
      "Epoch 179/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00179: val_loss improved from 0.05134 to 0.05116, saving model to model.h5\n",
      "248/248 [==============================] - 0s 115us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 180/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00180: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 181/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.0172\n",
      "Epoch 00181: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 182/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00182: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 183/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0404 - mean_squared_error: 0.0404\n",
      "Epoch 00183: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0520 - val_mean_squared_error: 0.0520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00184: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 185/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0393 - mean_squared_error: 0.0393\n",
      "Epoch 00185: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0525 - val_mean_squared_error: 0.0525\n",
      "Epoch 186/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00186: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 187/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Epoch 00187: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 188/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00188: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 189/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0227 - mean_squared_error: 0.0227\n",
      "Epoch 00189: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "Epoch 190/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00190: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 191/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00191: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 192/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00192: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 193/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0370 - mean_squared_error: 0.0370\n",
      "Epoch 00193: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0520 - val_mean_squared_error: 0.0520\n",
      "Epoch 194/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Epoch 00194: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 195/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00195: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0526 - val_mean_squared_error: 0.0526\n",
      "Epoch 196/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00196: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0526 - val_mean_squared_error: 0.0526\n",
      "Epoch 197/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00197: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 198/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00198: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 199/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Epoch 00199: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 200/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0341 - mean_squared_error: 0.0341\n",
      "Epoch 00200: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0527 - val_mean_squared_error: 0.0527\n",
      "Epoch 201/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Epoch 00201: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 202/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00202: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 203/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Epoch 00203: val_loss did not improve from 0.05116\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 204/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00204: val_loss improved from 0.05116 to 0.05107, saving model to model.h5\n",
      "248/248 [==============================] - 0s 135us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0511 - val_mean_squared_error: 0.0511\n",
      "Epoch 205/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00205: val_loss improved from 0.05107 to 0.05071, saving model to model.h5\n",
      "248/248 [==============================] - 0s 128us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 206/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00206: val_loss improved from 0.05071 to 0.05055, saving model to model.h5\n",
      "248/248 [==============================] - 0s 169us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 207/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00207: val_loss improved from 0.05055 to 0.05048, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 208/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00208: val_loss did not improve from 0.05048\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 209/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00209: val_loss did not improve from 0.05048\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 210/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00210: val_loss did not improve from 0.05048\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 211/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00211: val_loss did not improve from 0.05048\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 212/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00212: val_loss did not improve from 0.05048\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 213/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00213: val_loss did not improve from 0.05048\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 214/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00214: val_loss did not improve from 0.05048\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0514 - val_mean_squared_error: 0.0514\n",
      "Epoch 215/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00215: val_loss did not improve from 0.05048\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "Epoch 216/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00216: val_loss did not improve from 0.05048\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 217/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00217: val_loss did not improve from 0.05048\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n",
      "Epoch 218/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00218: val_loss did not improve from 0.05048\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n",
      "Epoch 219/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Epoch 00219: val_loss did not improve from 0.05048\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n",
      "Epoch 220/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00220: val_loss did not improve from 0.05048\n",
      "248/248 [==============================] - 0s 34us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "Epoch 221/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00221: val_loss did not improve from 0.05048\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0516 - val_mean_squared_error: 0.0516\n",
      "Epoch 222/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00222: val_loss did not improve from 0.05048\n",
      "248/248 [==============================] - 0s 37us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 223/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00223: val_loss did not improve from 0.05048\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0517 - val_mean_squared_error: 0.0517\n",
      "Epoch 224/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00224: val_loss did not improve from 0.05048\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0513 - val_mean_squared_error: 0.0513\n",
      "Epoch 225/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Epoch 00225: val_loss did not improve from 0.05048\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 226/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00226: val_loss did not improve from 0.05048\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 227/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00227: val_loss did not improve from 0.05048\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 228/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00228: val_loss did not improve from 0.05048\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 229/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00229: val_loss did not improve from 0.05048\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 230/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00230: val_loss improved from 0.05048 to 0.05039, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 231/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00231: val_loss improved from 0.05039 to 0.05028, saving model to model.h5\n",
      "248/248 [==============================] - 0s 132us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 232/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00232: val_loss did not improve from 0.05028\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 233/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00233: val_loss did not improve from 0.05028\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 234/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00234: val_loss did not improve from 0.05028\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 235/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00235: val_loss did not improve from 0.05028\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00236: val_loss did not improve from 0.05028\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 237/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00237: val_loss did not improve from 0.05028\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 238/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00238: val_loss did not improve from 0.05028\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0511 - val_mean_squared_error: 0.0511\n",
      "Epoch 239/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00239: val_loss did not improve from 0.05028\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0511 - val_mean_squared_error: 0.0511\n",
      "Epoch 240/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00240: val_loss did not improve from 0.05028\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0513 - val_mean_squared_error: 0.0513\n",
      "Epoch 241/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0365 - mean_squared_error: 0.0365\n",
      "Epoch 00241: val_loss did not improve from 0.05028\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 242/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00242: val_loss did not improve from 0.05028\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 243/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0138 - mean_squared_error: 0.0138\n",
      "Epoch 00243: val_loss did not improve from 0.05028\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 244/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0343 - mean_squared_error: 0.0343\n",
      "Epoch 00244: val_loss did not improve from 0.05028\n",
      "248/248 [==============================] - 0s 39us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 245/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00245: val_loss improved from 0.05028 to 0.05018, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 246/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0339 - mean_squared_error: 0.0339\n",
      "Epoch 00246: val_loss improved from 0.05018 to 0.05013, saving model to model.h5\n",
      "248/248 [==============================] - 0s 135us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 247/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00247: val_loss did not improve from 0.05013\n",
      "248/248 [==============================] - 0s 38us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 248/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00248: val_loss did not improve from 0.05013\n",
      "248/248 [==============================] - 0s 34us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 249/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00249: val_loss improved from 0.05013 to 0.05006, saving model to model.h5\n",
      "248/248 [==============================] - 0s 132us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 250/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00250: val_loss did not improve from 0.05006\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 251/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00251: val_loss did not improve from 0.05006\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 252/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00252: val_loss improved from 0.05006 to 0.05001, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 253/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00253: val_loss did not improve from 0.05001\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 254/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00254: val_loss did not improve from 0.05001\n",
      "248/248 [==============================] - 0s 33us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 255/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00255: val_loss did not improve from 0.05001\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 256/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0360 - mean_squared_error: 0.0360\n",
      "Epoch 00256: val_loss did not improve from 0.05001\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 257/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0385 - mean_squared_error: 0.0385\n",
      "Epoch 00257: val_loss did not improve from 0.05001\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0509 - val_mean_squared_error: 0.0509\n",
      "Epoch 258/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0390 - mean_squared_error: 0.0390\n",
      "Epoch 00258: val_loss did not improve from 0.05001\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 259/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0367 - mean_squared_error: 0.0367\n",
      "Epoch 00259: val_loss did not improve from 0.05001\n",
      "248/248 [==============================] - 0s 34us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 260/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00260: val_loss did not improve from 0.05001\n",
      "248/248 [==============================] - 0s 35us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 261/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00261: val_loss improved from 0.05001 to 0.04958, saving model to model.h5\n",
      "248/248 [==============================] - 0s 123us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 262/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00262: val_loss improved from 0.04958 to 0.04938, saving model to model.h5\n",
      "248/248 [==============================] - 0s 122us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 263/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00263: val_loss improved from 0.04938 to 0.04930, saving model to model.h5\n",
      "248/248 [==============================] - 0s 115us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 264/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00264: val_loss improved from 0.04930 to 0.04901, saving model to model.h5\n",
      "248/248 [==============================] - 0s 123us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 265/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00265: val_loss improved from 0.04901 to 0.04895, saving model to model.h5\n",
      "248/248 [==============================] - 0s 123us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 266/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00266: val_loss did not improve from 0.04895\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 267/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00267: val_loss did not improve from 0.04895\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 268/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00268: val_loss did not improve from 0.04895\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 269/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00269: val_loss did not improve from 0.04895\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 270/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00270: val_loss did not improve from 0.04895\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 271/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00271: val_loss did not improve from 0.04895\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 272/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00272: val_loss did not improve from 0.04895\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 273/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00273: val_loss did not improve from 0.04895\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 274/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00274: val_loss did not improve from 0.04895\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 275/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00275: val_loss did not improve from 0.04895\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 276/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00276: val_loss improved from 0.04895 to 0.04882, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 277/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00277: val_loss improved from 0.04882 to 0.04878, saving model to model.h5\n",
      "248/248 [==============================] - 0s 132us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 278/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00278: val_loss improved from 0.04878 to 0.04860, saving model to model.h5\n",
      "248/248 [==============================] - 0s 126us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 279/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00279: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 280/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00280: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 281/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Epoch 00281: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 282/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00282: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 283/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00283: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 284/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00284: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 285/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0173 - mean_squared_error: 0.0173\n",
      "Epoch 00285: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 286/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0187 - mean_squared_error: 0.0187\n",
      "Epoch 00286: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 287/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00287: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 288/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00288: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 289/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00289: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 290/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00290: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 291/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00291: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 292/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00292: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 293/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Epoch 00293: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 294/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00294: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 295/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00295: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 35us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 296/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Epoch 00296: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 297/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00297: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 298/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00298: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 299/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00299: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 300/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00300: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 301/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00301: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 302/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0160 - mean_squared_error: 0.0160\n",
      "Epoch 00302: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 33us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 303/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00303: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 304/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00304: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 305/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00305: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 306/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00306: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 307/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00307: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 308/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00308: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 309/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0326 - mean_squared_error: 0.0326\n",
      "Epoch 00309: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 310/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0174 - mean_squared_error: 0.0174\n",
      "Epoch 00310: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 311/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00311: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 35us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 312/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00312: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 313/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00313: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 314/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00314: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 315/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00315: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 316/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00316: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 317/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00317: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 318/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00318: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 319/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00319: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 320/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0147 - mean_squared_error: 0.0147\n",
      "Epoch 00320: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 321/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00321: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 322/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00322: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 323/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0161 - mean_squared_error: 0.0161\n",
      "Epoch 00323: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 324/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00324: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 325/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00325: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 326/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00326: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 327/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0368 - mean_squared_error: 0.0368\n",
      "Epoch 00327: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 328/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00328: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 329/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00329: val_loss did not improve from 0.04860\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 330/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00330: val_loss improved from 0.04860 to 0.04857, saving model to model.h5\n",
      "248/248 [==============================] - 0s 136us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 331/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00331: val_loss improved from 0.04857 to 0.04816, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0482 - val_mean_squared_error: 0.0482\n",
      "Epoch 332/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 00332: val_loss improved from 0.04816 to 0.04787, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 333/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00333: val_loss improved from 0.04787 to 0.04764, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 334/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00334: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 335/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00335: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 336/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0294 - mean_squared_error: 0.0294\n",
      "Epoch 00336: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 337/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00337: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 338/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00338: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 339/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00339: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 340/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00340: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 341/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00341: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 342/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0366 - mean_squared_error: 0.0366\n",
      "Epoch 00342: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 343/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0171 - mean_squared_error: 0.0171\n",
      "Epoch 00343: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 344/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00344: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 345/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00345: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 346/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00346: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 347/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00347: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 348/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Epoch 00348: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 349/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00349: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 350/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0352 - mean_squared_error: 0.0352\n",
      "Epoch 00350: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 351/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00351: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 352/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00352: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 353/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00353: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 354/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00354: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 355/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00355: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 356/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00356: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 357/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00357: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 358/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00358: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 359/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00359: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 360/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00360: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 361/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00361: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 362/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00362: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 363/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0150 - mean_squared_error: 0.0150\n",
      "Epoch 00363: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 364/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00364: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 365/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0188 - mean_squared_error: 0.0188\n",
      "Epoch 00365: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 366/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00366: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 367/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00367: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0511 - val_mean_squared_error: 0.0511\n",
      "Epoch 368/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 00368: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0509 - val_mean_squared_error: 0.0509\n",
      "Epoch 369/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00369: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 370/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00370: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 371/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00371: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 372/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00372: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 373/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00373: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 374/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00374: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 375/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0147 - mean_squared_error: 0.0147\n",
      "Epoch 00375: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 376/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00376: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 377/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00377: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 378/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00378: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 379/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0176 - mean_squared_error: 0.0176\n",
      "Epoch 00379: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 380/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00380: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 381/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00381: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 382/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00382: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 383/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00383: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 384/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 00384: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 385/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00385: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 386/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00386: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 387/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00387: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 388/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00388: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 389/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00389: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 390/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00390: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 391/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0151 - mean_squared_error: 0.0151\n",
      "Epoch 00391: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 392/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00392: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 393/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00393: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 394/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00394: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 395/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00395: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 396/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00396: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0482 - val_mean_squared_error: 0.0482\n",
      "Epoch 397/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00397: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 398/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00398: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 399/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00399: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 400/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00400: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 401/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00401: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 402/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00402: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0482 - val_mean_squared_error: 0.0482\n",
      "Epoch 403/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00403: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 404/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00404: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 405/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00405: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 406/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00406: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 407/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0122 - mean_squared_error: 0.0122\n",
      "Epoch 00407: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 408/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00408: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 409/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00409: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 410/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00410: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 411/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00411: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 412/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00412: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 413/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00413: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 414/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00414: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 415/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00415: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 416/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00416: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 417/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00417: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 418/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00418: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 419/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0161 - mean_squared_error: 0.0161\n",
      "Epoch 00419: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 420/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00420: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 421/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00421: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 422/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00422: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 423/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00423: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 424/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0121 - mean_squared_error: 0.0121\n",
      "Epoch 00424: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 425/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00425: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 426/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0187 - mean_squared_error: 0.0187\n",
      "Epoch 00426: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 427/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0142 - mean_squared_error: 0.0142\n",
      "Epoch 00427: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 428/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00428: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 429/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0114 - mean_squared_error: 0.0114\n",
      "Epoch 00429: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 430/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00430: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 431/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00431: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 432/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00432: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 433/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 00433: val_loss did not improve from 0.04764\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Running trial 7\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 1.2327 - mean_squared_error: 1.2327\n",
      "Epoch 00001: val_loss improved from inf to 0.67634, saving model to model.h5\n",
      "248/248 [==============================] - 2s 8ms/sample - loss: 1.1545 - mean_squared_error: 1.1545 - val_loss: 0.6763 - val_mean_squared_error: 0.6763\n",
      "Epoch 2/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 1.0453 - mean_squared_error: 1.0453\n",
      "Epoch 00002: val_loss improved from 0.67634 to 0.63807, saving model to model.h5\n",
      "248/248 [==============================] - 0s 113us/sample - loss: 1.0760 - mean_squared_error: 1.0760 - val_loss: 0.6381 - val_mean_squared_error: 0.6381\n",
      "Epoch 3/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.8371 - mean_squared_error: 0.8371\n",
      "Epoch 00003: val_loss improved from 0.63807 to 0.60005, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.9668 - mean_squared_error: 0.9668 - val_loss: 0.6000 - val_mean_squared_error: 0.6000\n",
      "Epoch 4/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.9935 - mean_squared_error: 0.9935\n",
      "Epoch 00004: val_loss improved from 0.60005 to 0.56183, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.8968 - mean_squared_error: 0.8968 - val_loss: 0.5618 - val_mean_squared_error: 0.5618\n",
      "Epoch 5/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 1.0124 - mean_squared_error: 1.0124\n",
      "Epoch 00005: val_loss improved from 0.56183 to 0.52567, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.8086 - mean_squared_error: 0.8086 - val_loss: 0.5257 - val_mean_squared_error: 0.5257\n",
      "Epoch 6/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.7510 - mean_squared_error: 0.7510\n",
      "Epoch 00006: val_loss improved from 0.52567 to 0.48900, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.7380 - mean_squared_error: 0.7380 - val_loss: 0.4890 - val_mean_squared_error: 0.4890\n",
      "Epoch 7/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.6734 - mean_squared_error: 0.6734\n",
      "Epoch 00007: val_loss improved from 0.48900 to 0.45400, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.6573 - mean_squared_error: 0.6573 - val_loss: 0.4540 - val_mean_squared_error: 0.4540\n",
      "Epoch 8/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.4638 - mean_squared_error: 0.4638\n",
      "Epoch 00008: val_loss improved from 0.45400 to 0.41918, saving model to model.h5\n",
      "248/248 [==============================] - 0s 115us/sample - loss: 0.5684 - mean_squared_error: 0.5684 - val_loss: 0.4192 - val_mean_squared_error: 0.4192\n",
      "Epoch 9/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.5206 - mean_squared_error: 0.5206\n",
      "Epoch 00009: val_loss improved from 0.41918 to 0.38534, saving model to model.h5\n",
      "248/248 [==============================] - 0s 120us/sample - loss: 0.5349 - mean_squared_error: 0.5349 - val_loss: 0.3853 - val_mean_squared_error: 0.3853\n",
      "Epoch 10/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.4747 - mean_squared_error: 0.4747\n",
      "Epoch 00010: val_loss improved from 0.38534 to 0.35289, saving model to model.h5\n",
      "248/248 [==============================] - 0s 119us/sample - loss: 0.4812 - mean_squared_error: 0.4812 - val_loss: 0.3529 - val_mean_squared_error: 0.3529\n",
      "Epoch 11/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.4092 - mean_squared_error: 0.4092\n",
      "Epoch 00011: val_loss improved from 0.35289 to 0.32207, saving model to model.h5\n",
      "248/248 [==============================] - 0s 126us/sample - loss: 0.4198 - mean_squared_error: 0.4198 - val_loss: 0.3221 - val_mean_squared_error: 0.3221\n",
      "Epoch 12/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3441 - mean_squared_error: 0.3441\n",
      "Epoch 00012: val_loss improved from 0.32207 to 0.29214, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.3736 - mean_squared_error: 0.3736 - val_loss: 0.2921 - val_mean_squared_error: 0.2921\n",
      "Epoch 13/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3475 - mean_squared_error: 0.3475\n",
      "Epoch 00013: val_loss improved from 0.29214 to 0.26357, saving model to model.h5\n",
      "248/248 [==============================] - 0s 146us/sample - loss: 0.3408 - mean_squared_error: 0.3408 - val_loss: 0.2636 - val_mean_squared_error: 0.2636\n",
      "Epoch 14/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3057 - mean_squared_error: 0.3057\n",
      "Epoch 00014: val_loss improved from 0.26357 to 0.23686, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.2946 - mean_squared_error: 0.2946 - val_loss: 0.2369 - val_mean_squared_error: 0.2369\n",
      "Epoch 15/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3074 - mean_squared_error: 0.3074\n",
      "Epoch 00015: val_loss improved from 0.23686 to 0.21178, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.2738 - mean_squared_error: 0.2738 - val_loss: 0.2118 - val_mean_squared_error: 0.2118\n",
      "Epoch 16/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2350 - mean_squared_error: 0.2350\n",
      "Epoch 00016: val_loss improved from 0.21178 to 0.18827, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.2268 - mean_squared_error: 0.2268 - val_loss: 0.1883 - val_mean_squared_error: 0.1883\n",
      "Epoch 17/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2258 - mean_squared_error: 0.2258\n",
      "Epoch 00017: val_loss improved from 0.18827 to 0.16665, saving model to model.h5\n",
      "248/248 [==============================] - 0s 119us/sample - loss: 0.2064 - mean_squared_error: 0.2064 - val_loss: 0.1666 - val_mean_squared_error: 0.1666\n",
      "Epoch 18/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1571 - mean_squared_error: 0.1571\n",
      "Epoch 00018: val_loss improved from 0.16665 to 0.14682, saving model to model.h5\n",
      "248/248 [==============================] - 0s 113us/sample - loss: 0.1817 - mean_squared_error: 0.1817 - val_loss: 0.1468 - val_mean_squared_error: 0.1468\n",
      "Epoch 19/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1642 - mean_squared_error: 0.1642\n",
      "Epoch 00019: val_loss improved from 0.14682 to 0.12874, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.1687 - mean_squared_error: 0.1687 - val_loss: 0.1287 - val_mean_squared_error: 0.1287\n",
      "Epoch 20/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1551 - mean_squared_error: 0.1551\n",
      "Epoch 00020: val_loss improved from 0.12874 to 0.11275, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.1479 - mean_squared_error: 0.1479 - val_loss: 0.1128 - val_mean_squared_error: 0.1128\n",
      "Epoch 21/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1244 - mean_squared_error: 0.1244\n",
      "Epoch 00021: val_loss improved from 0.11275 to 0.09867, saving model to model.h5\n",
      "248/248 [==============================] - 0s 128us/sample - loss: 0.1350 - mean_squared_error: 0.1350 - val_loss: 0.0987 - val_mean_squared_error: 0.0987\n",
      "Epoch 22/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1329 - mean_squared_error: 0.1329\n",
      "Epoch 00022: val_loss improved from 0.09867 to 0.08629, saving model to model.h5\n",
      "248/248 [==============================] - 0s 132us/sample - loss: 0.1219 - mean_squared_error: 0.1219 - val_loss: 0.0863 - val_mean_squared_error: 0.0863\n",
      "Epoch 23/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1080 - mean_squared_error: 0.1080\n",
      "Epoch 00023: val_loss improved from 0.08629 to 0.07560, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.1088 - mean_squared_error: 0.1088 - val_loss: 0.0756 - val_mean_squared_error: 0.0756\n",
      "Epoch 24/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1026 - mean_squared_error: 0.1026\n",
      "Epoch 00024: val_loss improved from 0.07560 to 0.06660, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0976 - mean_squared_error: 0.0976 - val_loss: 0.0666 - val_mean_squared_error: 0.0666\n",
      "Epoch 25/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1064 - mean_squared_error: 0.1064\n",
      "Epoch 00025: val_loss improved from 0.06660 to 0.05922, saving model to model.h5\n",
      "248/248 [==============================] - 0s 185us/sample - loss: 0.0932 - mean_squared_error: 0.0932 - val_loss: 0.0592 - val_mean_squared_error: 0.0592\n",
      "Epoch 26/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0782 - mean_squared_error: 0.0782\n",
      "Epoch 00026: val_loss improved from 0.05922 to 0.05299, saving model to model.h5\n",
      "248/248 [==============================] - 0s 165us/sample - loss: 0.0848 - mean_squared_error: 0.0848 - val_loss: 0.0530 - val_mean_squared_error: 0.0530\n",
      "Epoch 27/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0797 - mean_squared_error: 0.0797\n",
      "Epoch 00027: val_loss improved from 0.05299 to 0.04799, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0812 - mean_squared_error: 0.0812 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 28/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0770 - mean_squared_error: 0.0770\n",
      "Epoch 00028: val_loss improved from 0.04799 to 0.04398, saving model to model.h5\n",
      "248/248 [==============================] - 0s 126us/sample - loss: 0.0751 - mean_squared_error: 0.0751 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 29/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0557 - mean_squared_error: 0.0557\n",
      "Epoch 00029: val_loss improved from 0.04398 to 0.04070, saving model to model.h5\n",
      "248/248 [==============================] - 0s 149us/sample - loss: 0.0779 - mean_squared_error: 0.0779 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 30/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0677 - mean_squared_error: 0.0677\n",
      "Epoch 00030: val_loss improved from 0.04070 to 0.03818, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.0722 - mean_squared_error: 0.0722 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 31/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0740 - mean_squared_error: 0.0740\n",
      "Epoch 00031: val_loss improved from 0.03818 to 0.03626, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0704 - mean_squared_error: 0.0704 - val_loss: 0.0363 - val_mean_squared_error: 0.0363\n",
      "Epoch 32/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0481 - mean_squared_error: 0.0481\n",
      "Epoch 00032: val_loss improved from 0.03626 to 0.03486, saving model to model.h5\n",
      "248/248 [==============================] - 0s 132us/sample - loss: 0.0629 - mean_squared_error: 0.0629 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0732 - mean_squared_error: 0.0732\n",
      "Epoch 00033: val_loss improved from 0.03486 to 0.03385, saving model to model.h5\n",
      "248/248 [==============================] - 0s 116us/sample - loss: 0.0620 - mean_squared_error: 0.0620 - val_loss: 0.0338 - val_mean_squared_error: 0.0338\n",
      "Epoch 34/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0582 - mean_squared_error: 0.0582\n",
      "Epoch 00034: val_loss improved from 0.03385 to 0.03313, saving model to model.h5\n",
      "248/248 [==============================] - 0s 122us/sample - loss: 0.0624 - mean_squared_error: 0.0624 - val_loss: 0.0331 - val_mean_squared_error: 0.0331\n",
      "Epoch 35/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0600 - mean_squared_error: 0.0600\n",
      "Epoch 00035: val_loss improved from 0.03313 to 0.03261, saving model to model.h5\n",
      "248/248 [==============================] - 0s 130us/sample - loss: 0.0591 - mean_squared_error: 0.0591 - val_loss: 0.0326 - val_mean_squared_error: 0.0326\n",
      "Epoch 36/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0621 - mean_squared_error: 0.0621\n",
      "Epoch 00036: val_loss improved from 0.03261 to 0.03227, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.0541 - mean_squared_error: 0.0541 - val_loss: 0.0323 - val_mean_squared_error: 0.0323\n",
      "Epoch 37/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0443 - mean_squared_error: 0.0443\n",
      "Epoch 00037: val_loss improved from 0.03227 to 0.03206, saving model to model.h5\n",
      "248/248 [==============================] - 0s 116us/sample - loss: 0.0565 - mean_squared_error: 0.0565 - val_loss: 0.0321 - val_mean_squared_error: 0.0321\n",
      "Epoch 38/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0643 - mean_squared_error: 0.0643\n",
      "Epoch 00038: val_loss improved from 0.03206 to 0.03195, saving model to model.h5\n",
      "248/248 [==============================] - 0s 116us/sample - loss: 0.0551 - mean_squared_error: 0.0551 - val_loss: 0.0320 - val_mean_squared_error: 0.0320\n",
      "Epoch 39/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0445 - mean_squared_error: 0.0445\n",
      "Epoch 00039: val_loss improved from 0.03195 to 0.03190, saving model to model.h5\n",
      "248/248 [==============================] - 0s 118us/sample - loss: 0.0560 - mean_squared_error: 0.0560 - val_loss: 0.0319 - val_mean_squared_error: 0.0319\n",
      "Epoch 40/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0484 - mean_squared_error: 0.0484\n",
      "Epoch 00040: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0504 - mean_squared_error: 0.0504 - val_loss: 0.0320 - val_mean_squared_error: 0.0320\n",
      "Epoch 41/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0442 - mean_squared_error: 0.0442\n",
      "Epoch 00041: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0491 - mean_squared_error: 0.0491 - val_loss: 0.0321 - val_mean_squared_error: 0.0321\n",
      "Epoch 42/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0434 - mean_squared_error: 0.0434\n",
      "Epoch 00042: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0545 - mean_squared_error: 0.0545 - val_loss: 0.0323 - val_mean_squared_error: 0.0323\n",
      "Epoch 43/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0591 - mean_squared_error: 0.0591\n",
      "Epoch 00043: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0504 - mean_squared_error: 0.0504 - val_loss: 0.0325 - val_mean_squared_error: 0.0325\n",
      "Epoch 44/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0396 - mean_squared_error: 0.0396\n",
      "Epoch 00044: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0478 - mean_squared_error: 0.0478 - val_loss: 0.0328 - val_mean_squared_error: 0.0328\n",
      "Epoch 45/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0594 - mean_squared_error: 0.0594\n",
      "Epoch 00045: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 33us/sample - loss: 0.0447 - mean_squared_error: 0.0447 - val_loss: 0.0331 - val_mean_squared_error: 0.0331\n",
      "Epoch 46/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0461 - mean_squared_error: 0.0461\n",
      "Epoch 00046: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0512 - mean_squared_error: 0.0512 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 47/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0360 - mean_squared_error: 0.0360\n",
      "Epoch 00047: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0451 - mean_squared_error: 0.0451 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 48/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 00048: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0470 - mean_squared_error: 0.0470 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 49/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0385 - mean_squared_error: 0.0385\n",
      "Epoch 00049: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 50/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0610 - mean_squared_error: 0.0610\n",
      "Epoch 00050: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0489 - mean_squared_error: 0.0489 - val_loss: 0.0344 - val_mean_squared_error: 0.0344\n",
      "Epoch 51/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0557 - mean_squared_error: 0.0557\n",
      "Epoch 00051: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0468 - mean_squared_error: 0.0468 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 52/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0447 - mean_squared_error: 0.0447\n",
      "Epoch 00052: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0491 - mean_squared_error: 0.0491 - val_loss: 0.0348 - val_mean_squared_error: 0.0348\n",
      "Epoch 53/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0552 - mean_squared_error: 0.0552\n",
      "Epoch 00053: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0460 - mean_squared_error: 0.0460 - val_loss: 0.0350 - val_mean_squared_error: 0.0350\n",
      "Epoch 54/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0500 - mean_squared_error: 0.0500\n",
      "Epoch 00054: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0483 - mean_squared_error: 0.0483 - val_loss: 0.0351 - val_mean_squared_error: 0.0351\n",
      "Epoch 55/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0491 - mean_squared_error: 0.0491\n",
      "Epoch 00055: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0490 - mean_squared_error: 0.0490 - val_loss: 0.0351 - val_mean_squared_error: 0.0351\n",
      "Epoch 56/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0489 - mean_squared_error: 0.0489\n",
      "Epoch 00056: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0491 - mean_squared_error: 0.0491 - val_loss: 0.0352 - val_mean_squared_error: 0.0352\n",
      "Epoch 57/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0651 - mean_squared_error: 0.0651\n",
      "Epoch 00057: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0516 - mean_squared_error: 0.0516 - val_loss: 0.0352 - val_mean_squared_error: 0.0352\n",
      "Epoch 58/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0507 - mean_squared_error: 0.0507\n",
      "Epoch 00058: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 34us/sample - loss: 0.0430 - mean_squared_error: 0.0430 - val_loss: 0.0352 - val_mean_squared_error: 0.0352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0421 - mean_squared_error: 0.0421\n",
      "Epoch 00059: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0444 - mean_squared_error: 0.0444 - val_loss: 0.0352 - val_mean_squared_error: 0.0352\n",
      "Epoch 60/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0380 - mean_squared_error: 0.0380\n",
      "Epoch 00060: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0484 - mean_squared_error: 0.0484 - val_loss: 0.0351 - val_mean_squared_error: 0.0351\n",
      "Epoch 61/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0367 - mean_squared_error: 0.0367\n",
      "Epoch 00061: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0350 - val_mean_squared_error: 0.0350\n",
      "Epoch 62/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Epoch 00062: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0391 - mean_squared_error: 0.0391 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 63/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00063: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0426 - mean_squared_error: 0.0426 - val_loss: 0.0348 - val_mean_squared_error: 0.0348\n",
      "Epoch 64/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00064: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0422 - mean_squared_error: 0.0422 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 65/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0424 - mean_squared_error: 0.0424\n",
      "Epoch 00065: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 66/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Epoch 00066: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0375 - mean_squared_error: 0.0375 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 67/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0344 - mean_squared_error: 0.0344\n",
      "Epoch 00067: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0460 - mean_squared_error: 0.0460 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 68/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00068: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0397 - mean_squared_error: 0.0397 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 69/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0386 - mean_squared_error: 0.0386\n",
      "Epoch 00069: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0382 - mean_squared_error: 0.0382 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 70/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0382 - mean_squared_error: 0.0382\n",
      "Epoch 00070: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0478 - mean_squared_error: 0.0478 - val_loss: 0.0348 - val_mean_squared_error: 0.0348\n",
      "Epoch 71/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Epoch 00071: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0401 - mean_squared_error: 0.0401 - val_loss: 0.0348 - val_mean_squared_error: 0.0348\n",
      "Epoch 72/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0405 - mean_squared_error: 0.0405\n",
      "Epoch 00072: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 73/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00073: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 74/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0468 - mean_squared_error: 0.0468\n",
      "Epoch 00074: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0430 - mean_squared_error: 0.0430 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 75/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0549 - mean_squared_error: 0.0549\n",
      "Epoch 00075: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0401 - mean_squared_error: 0.0401 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 76/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Epoch 00076: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0350 - val_mean_squared_error: 0.0350\n",
      "Epoch 77/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0481 - mean_squared_error: 0.0481\n",
      "Epoch 00077: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0380 - mean_squared_error: 0.0380 - val_loss: 0.0351 - val_mean_squared_error: 0.0351\n",
      "Epoch 78/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00078: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0350 - val_mean_squared_error: 0.0350\n",
      "Epoch 79/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0367 - mean_squared_error: 0.0367\n",
      "Epoch 00079: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0381 - mean_squared_error: 0.0381 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 80/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 00080: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0393 - mean_squared_error: 0.0393 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 81/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0388 - mean_squared_error: 0.0388\n",
      "Epoch 00081: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0389 - mean_squared_error: 0.0389 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 82/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0427 - mean_squared_error: 0.0427\n",
      "Epoch 00082: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0372 - mean_squared_error: 0.0372 - val_loss: 0.0348 - val_mean_squared_error: 0.0348\n",
      "Epoch 83/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0445 - mean_squared_error: 0.0445\n",
      "Epoch 00083: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0389 - mean_squared_error: 0.0389 - val_loss: 0.0348 - val_mean_squared_error: 0.0348\n",
      "Epoch 84/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0388 - mean_squared_error: 0.0388\n",
      "Epoch 00084: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 85/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Epoch 00085: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0398 - mean_squared_error: 0.0398 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 86/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 00086: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0374 - mean_squared_error: 0.0374 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 87/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00087: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0348 - val_mean_squared_error: 0.0348\n",
      "Epoch 88/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0445 - mean_squared_error: 0.0445\n",
      "Epoch 00088: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 41us/sample - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 89/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 00089: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0351 - val_mean_squared_error: 0.0351\n",
      "Epoch 90/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Epoch 00090: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0366 - mean_squared_error: 0.0366 - val_loss: 0.0353 - val_mean_squared_error: 0.0353\n",
      "Epoch 91/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00091: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0354 - val_mean_squared_error: 0.0354\n",
      "Epoch 92/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00092: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0355 - val_mean_squared_error: 0.0355\n",
      "Epoch 93/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0419 - mean_squared_error: 0.0419\n",
      "Epoch 00093: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0385 - mean_squared_error: 0.0385 - val_loss: 0.0357 - val_mean_squared_error: 0.0357\n",
      "Epoch 94/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00094: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0375 - mean_squared_error: 0.0375 - val_loss: 0.0359 - val_mean_squared_error: 0.0359\n",
      "Epoch 95/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0423 - mean_squared_error: 0.0423\n",
      "Epoch 00095: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0360 - val_mean_squared_error: 0.0360\n",
      "Epoch 96/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00096: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0399 - mean_squared_error: 0.0399 - val_loss: 0.0360 - val_mean_squared_error: 0.0360\n",
      "Epoch 97/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0396 - mean_squared_error: 0.0396\n",
      "Epoch 00097: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0360 - val_mean_squared_error: 0.0360\n",
      "Epoch 98/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0360 - mean_squared_error: 0.0360\n",
      "Epoch 00098: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 35us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0358 - val_mean_squared_error: 0.0358\n",
      "Epoch 99/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0491 - mean_squared_error: 0.0491\n",
      "Epoch 00099: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0374 - mean_squared_error: 0.0374 - val_loss: 0.0358 - val_mean_squared_error: 0.0358\n",
      "Epoch 100/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00100: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0358 - val_mean_squared_error: 0.0358\n",
      "Epoch 101/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0348 - mean_squared_error: 0.0348\n",
      "Epoch 00101: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0375 - mean_squared_error: 0.0375 - val_loss: 0.0358 - val_mean_squared_error: 0.0358\n",
      "Epoch 102/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00102: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0371 - mean_squared_error: 0.0371 - val_loss: 0.0357 - val_mean_squared_error: 0.0357\n",
      "Epoch 103/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00103: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 34us/sample - loss: 0.0371 - mean_squared_error: 0.0371 - val_loss: 0.0357 - val_mean_squared_error: 0.0357\n",
      "Epoch 104/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0493 - mean_squared_error: 0.0493\n",
      "Epoch 00104: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0381 - mean_squared_error: 0.0381 - val_loss: 0.0356 - val_mean_squared_error: 0.0356\n",
      "Epoch 105/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00105: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0374 - mean_squared_error: 0.0374 - val_loss: 0.0356 - val_mean_squared_error: 0.0356\n",
      "Epoch 106/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0476 - mean_squared_error: 0.0476\n",
      "Epoch 00106: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0356 - val_mean_squared_error: 0.0356\n",
      "Epoch 107/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0346 - mean_squared_error: 0.0346\n",
      "Epoch 00107: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0357 - val_mean_squared_error: 0.0357\n",
      "Epoch 108/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Epoch 00108: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0357 - val_mean_squared_error: 0.0357\n",
      "Epoch 109/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0431 - mean_squared_error: 0.0431\n",
      "Epoch 00109: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0357 - val_mean_squared_error: 0.0357\n",
      "Epoch 110/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00110: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0358 - val_mean_squared_error: 0.0358\n",
      "Epoch 111/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0436 - mean_squared_error: 0.0436\n",
      "Epoch 00111: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0359 - val_mean_squared_error: 0.0359\n",
      "Epoch 112/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00112: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0359 - val_mean_squared_error: 0.0359\n",
      "Epoch 113/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0375 - mean_squared_error: 0.0375\n",
      "Epoch 00113: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0360 - val_mean_squared_error: 0.0360\n",
      "Epoch 114/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00114: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0387 - mean_squared_error: 0.0387 - val_loss: 0.0361 - val_mean_squared_error: 0.0361\n",
      "Epoch 115/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00115: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0362 - val_mean_squared_error: 0.0362\n",
      "Epoch 116/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0326 - mean_squared_error: 0.0326\n",
      "Epoch 00116: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0363 - val_mean_squared_error: 0.0363\n",
      "Epoch 117/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0429 - mean_squared_error: 0.0429\n",
      "Epoch 00117: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n",
      "Epoch 118/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0384 - mean_squared_error: 0.0384\n",
      "Epoch 00118: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0394 - mean_squared_error: 0.0394 - val_loss: 0.0365 - val_mean_squared_error: 0.0365\n",
      "Epoch 119/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Epoch 00119: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0366 - val_mean_squared_error: 0.0366\n",
      "Epoch 120/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00120: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0330 - mean_squared_error: 0.0330 - val_loss: 0.0367 - val_mean_squared_error: 0.0367\n",
      "Epoch 121/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Epoch 00121: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0331 - mean_squared_error: 0.0331 - val_loss: 0.0367 - val_mean_squared_error: 0.0367\n",
      "Epoch 122/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00122: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 123/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0408 - mean_squared_error: 0.0408\n",
      "Epoch 00123: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 124/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00124: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 125/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0434 - mean_squared_error: 0.0434\n",
      "Epoch 00125: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 126/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00126: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 127/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0405 - mean_squared_error: 0.0405\n",
      "Epoch 00127: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 128/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0416 - mean_squared_error: 0.0416\n",
      "Epoch 00128: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0323 - mean_squared_error: 0.0323 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 129/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00129: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 130/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00130: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 131/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00131: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0319 - mean_squared_error: 0.0319 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 132/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0334 - mean_squared_error: 0.0334\n",
      "Epoch 00132: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0308 - mean_squared_error: 0.0308 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 133/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0459 - mean_squared_error: 0.0459\n",
      "Epoch 00133: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 134/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00134: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 135/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00135: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 136/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Epoch 00136: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 137/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Epoch 00137: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 33us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 138/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00138: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 37us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 139/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00139: val_loss did not improve from 0.03190\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Running trial 8\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.5934 - mean_squared_error: 0.5934\n",
      "Epoch 00001: val_loss improved from inf to 0.44772, saving model to model.h5\n",
      "248/248 [==============================] - 2s 8ms/sample - loss: 0.5991 - mean_squared_error: 0.5991 - val_loss: 0.4477 - val_mean_squared_error: 0.4477\n",
      "Epoch 2/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.5734 - mean_squared_error: 0.5734\n",
      "Epoch 00002: val_loss improved from 0.44772 to 0.41893, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.5285 - mean_squared_error: 0.5285 - val_loss: 0.4189 - val_mean_squared_error: 0.4189\n",
      "Epoch 3/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.4974 - mean_squared_error: 0.4974\n",
      "Epoch 00003: val_loss improved from 0.41893 to 0.39123, saving model to model.h5\n",
      "248/248 [==============================] - 0s 116us/sample - loss: 0.4840 - mean_squared_error: 0.4840 - val_loss: 0.3912 - val_mean_squared_error: 0.3912\n",
      "Epoch 4/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.4255 - mean_squared_error: 0.4255\n",
      "Epoch 00004: val_loss improved from 0.39123 to 0.36286, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.4231 - mean_squared_error: 0.4231 - val_loss: 0.3629 - val_mean_squared_error: 0.3629\n",
      "Epoch 5/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.4083 - mean_squared_error: 0.4083\n",
      "Epoch 00005: val_loss improved from 0.36286 to 0.33632, saving model to model.h5\n",
      "248/248 [==============================] - 0s 114us/sample - loss: 0.3751 - mean_squared_error: 0.3751 - val_loss: 0.3363 - val_mean_squared_error: 0.3363\n",
      "Epoch 6/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3307 - mean_squared_error: 0.3307\n",
      "Epoch 00006: val_loss improved from 0.33632 to 0.30967, saving model to model.h5\n",
      "248/248 [==============================] - 0s 128us/sample - loss: 0.3276 - mean_squared_error: 0.3276 - val_loss: 0.3097 - val_mean_squared_error: 0.3097\n",
      "Epoch 7/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3317 - mean_squared_error: 0.3317\n",
      "Epoch 00007: val_loss improved from 0.30967 to 0.28348, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.2954 - mean_squared_error: 0.2954 - val_loss: 0.2835 - val_mean_squared_error: 0.2835\n",
      "Epoch 8/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2493 - mean_squared_error: 0.2493\n",
      "Epoch 00008: val_loss improved from 0.28348 to 0.25816, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.2608 - mean_squared_error: 0.2608 - val_loss: 0.2582 - val_mean_squared_error: 0.2582\n",
      "Epoch 9/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2615 - mean_squared_error: 0.2615\n",
      "Epoch 00009: val_loss improved from 0.25816 to 0.23348, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.2269 - mean_squared_error: 0.2269 - val_loss: 0.2335 - val_mean_squared_error: 0.2335\n",
      "Epoch 10/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1840 - mean_squared_error: 0.1840\n",
      "Epoch 00010: val_loss improved from 0.23348 to 0.21000, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.1902 - mean_squared_error: 0.1902 - val_loss: 0.2100 - val_mean_squared_error: 0.2100\n",
      "Epoch 11/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2107 - mean_squared_error: 0.2107\n",
      "Epoch 00011: val_loss improved from 0.21000 to 0.18786, saving model to model.h5\n",
      "248/248 [==============================] - 0s 140us/sample - loss: 0.1776 - mean_squared_error: 0.1776 - val_loss: 0.1879 - val_mean_squared_error: 0.1879\n",
      "Epoch 12/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1720 - mean_squared_error: 0.1720\n",
      "Epoch 00012: val_loss improved from 0.18786 to 0.16695, saving model to model.h5\n",
      "248/248 [==============================] - 0s 118us/sample - loss: 0.1579 - mean_squared_error: 0.1579 - val_loss: 0.1669 - val_mean_squared_error: 0.1669\n",
      "Epoch 13/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1415 - mean_squared_error: 0.1415\n",
      "Epoch 00013: val_loss improved from 0.16695 to 0.14795, saving model to model.h5\n",
      "248/248 [==============================] - 0s 119us/sample - loss: 0.1358 - mean_squared_error: 0.1358 - val_loss: 0.1480 - val_mean_squared_error: 0.1480\n",
      "Epoch 14/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1355 - mean_squared_error: 0.1355\n",
      "Epoch 00014: val_loss improved from 0.14795 to 0.13088, saving model to model.h5\n",
      "248/248 [==============================] - 0s 123us/sample - loss: 0.1196 - mean_squared_error: 0.1196 - val_loss: 0.1309 - val_mean_squared_error: 0.1309\n",
      "Epoch 15/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0976 - mean_squared_error: 0.0976\n",
      "Epoch 00015: val_loss improved from 0.13088 to 0.11558, saving model to model.h5\n",
      "248/248 [==============================] - 0s 139us/sample - loss: 0.1054 - mean_squared_error: 0.1054 - val_loss: 0.1156 - val_mean_squared_error: 0.1156\n",
      "Epoch 16/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1015 - mean_squared_error: 0.1015\n",
      "Epoch 00016: val_loss improved from 0.11558 to 0.10213, saving model to model.h5\n",
      "248/248 [==============================] - 0s 130us/sample - loss: 0.0928 - mean_squared_error: 0.0928 - val_loss: 0.1021 - val_mean_squared_error: 0.1021\n",
      "Epoch 17/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1056 - mean_squared_error: 0.1056\n",
      "Epoch 00017: val_loss improved from 0.10213 to 0.09025, saving model to model.h5\n",
      "248/248 [==============================] - 0s 120us/sample - loss: 0.0936 - mean_squared_error: 0.0936 - val_loss: 0.0903 - val_mean_squared_error: 0.0903\n",
      "Epoch 18/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0881 - mean_squared_error: 0.0881\n",
      "Epoch 00018: val_loss improved from 0.09025 to 0.08012, saving model to model.h5\n",
      "248/248 [==============================] - 0s 123us/sample - loss: 0.0804 - mean_squared_error: 0.0804 - val_loss: 0.0801 - val_mean_squared_error: 0.0801\n",
      "Epoch 19/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0638 - mean_squared_error: 0.0638\n",
      "Epoch 00019: val_loss improved from 0.08012 to 0.07149, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0718 - mean_squared_error: 0.0718 - val_loss: 0.0715 - val_mean_squared_error: 0.0715\n",
      "Epoch 20/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0758 - mean_squared_error: 0.0758\n",
      "Epoch 00020: val_loss improved from 0.07149 to 0.06428, saving model to model.h5\n",
      "248/248 [==============================] - 0s 134us/sample - loss: 0.0688 - mean_squared_error: 0.0688 - val_loss: 0.0643 - val_mean_squared_error: 0.0643\n",
      "Epoch 21/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0519 - mean_squared_error: 0.0519\n",
      "Epoch 00021: val_loss improved from 0.06428 to 0.05844, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0587 - mean_squared_error: 0.0587 - val_loss: 0.0584 - val_mean_squared_error: 0.0584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0479 - mean_squared_error: 0.0479\n",
      "Epoch 00022: val_loss improved from 0.05844 to 0.05380, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.0578 - mean_squared_error: 0.0578 - val_loss: 0.0538 - val_mean_squared_error: 0.0538\n",
      "Epoch 23/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0593 - mean_squared_error: 0.0593\n",
      "Epoch 00023: val_loss improved from 0.05380 to 0.05036, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0538 - mean_squared_error: 0.0538 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 24/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0439 - mean_squared_error: 0.0439\n",
      "Epoch 00024: val_loss improved from 0.05036 to 0.04777, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0507 - mean_squared_error: 0.0507 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 25/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0443 - mean_squared_error: 0.0443\n",
      "Epoch 00025: val_loss improved from 0.04777 to 0.04586, saving model to model.h5\n",
      "248/248 [==============================] - 0s 118us/sample - loss: 0.0487 - mean_squared_error: 0.0487 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 26/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0399 - mean_squared_error: 0.0399\n",
      "Epoch 00026: val_loss improved from 0.04586 to 0.04462, saving model to model.h5\n",
      "248/248 [==============================] - 0s 120us/sample - loss: 0.0493 - mean_squared_error: 0.0493 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 27/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0529 - mean_squared_error: 0.0529\n",
      "Epoch 00027: val_loss improved from 0.04462 to 0.04398, saving model to model.h5\n",
      "248/248 [==============================] - 0s 111us/sample - loss: 0.0512 - mean_squared_error: 0.0512 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 28/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0550 - mean_squared_error: 0.0550\n",
      "Epoch 00028: val_loss improved from 0.04398 to 0.04385, saving model to model.h5\n",
      "248/248 [==============================] - 0s 118us/sample - loss: 0.0522 - mean_squared_error: 0.0522 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 29/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0448 - mean_squared_error: 0.0448\n",
      "Epoch 00029: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0463 - mean_squared_error: 0.0463 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 30/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0440 - mean_squared_error: 0.0440\n",
      "Epoch 00030: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0448 - mean_squared_error: 0.0448 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 31/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0506 - mean_squared_error: 0.0506\n",
      "Epoch 00031: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0484 - mean_squared_error: 0.0484 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 32/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0443 - mean_squared_error: 0.0443\n",
      "Epoch 00032: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0471 - mean_squared_error: 0.0471 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 33/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0490 - mean_squared_error: 0.0490\n",
      "Epoch 00033: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 34/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0528 - mean_squared_error: 0.0528\n",
      "Epoch 00034: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0496 - mean_squared_error: 0.0496 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 35/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0561 - mean_squared_error: 0.0561\n",
      "Epoch 00035: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0486 - mean_squared_error: 0.0486 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 36/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 00036: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0436 - mean_squared_error: 0.0436 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 37/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0484 - mean_squared_error: 0.0484\n",
      "Epoch 00037: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0484 - mean_squared_error: 0.0484 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 38/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0375 - mean_squared_error: 0.0375\n",
      "Epoch 00038: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0432 - mean_squared_error: 0.0432 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 39/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0475 - mean_squared_error: 0.0475\n",
      "Epoch 00039: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0426 - mean_squared_error: 0.0426 - val_loss: 0.0514 - val_mean_squared_error: 0.0514\n",
      "Epoch 40/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Epoch 00040: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0454 - mean_squared_error: 0.0454 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "Epoch 41/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Epoch 00041: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0429 - mean_squared_error: 0.0429 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 42/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0437 - mean_squared_error: 0.0437\n",
      "Epoch 00042: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0526 - val_mean_squared_error: 0.0526\n",
      "Epoch 43/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0473 - mean_squared_error: 0.0473\n",
      "Epoch 00043: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0401 - mean_squared_error: 0.0401 - val_loss: 0.0527 - val_mean_squared_error: 0.0527\n",
      "Epoch 44/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0442 - mean_squared_error: 0.0442\n",
      "Epoch 00044: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0402 - mean_squared_error: 0.0402 - val_loss: 0.0527 - val_mean_squared_error: 0.0527\n",
      "Epoch 45/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Epoch 00045: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0525 - val_mean_squared_error: 0.0525\n",
      "Epoch 46/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0414 - mean_squared_error: 0.0414\n",
      "Epoch 00046: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0450 - mean_squared_error: 0.0450 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 47/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0516 - mean_squared_error: 0.0516\n",
      "Epoch 00047: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0362 - mean_squared_error: 0.0362\n",
      "Epoch 00048: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0390 - mean_squared_error: 0.0390 - val_loss: 0.0514 - val_mean_squared_error: 0.0514\n",
      "Epoch 49/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0465 - mean_squared_error: 0.0465\n",
      "Epoch 00049: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 50/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0440 - mean_squared_error: 0.0440\n",
      "Epoch 00050: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 51/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0488 - mean_squared_error: 0.0488\n",
      "Epoch 00051: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0446 - mean_squared_error: 0.0446 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 52/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00052: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 53/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0498 - mean_squared_error: 0.0498\n",
      "Epoch 00053: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 54/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0379 - mean_squared_error: 0.0379\n",
      "Epoch 00054: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 55/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0471 - mean_squared_error: 0.0471\n",
      "Epoch 00055: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0398 - mean_squared_error: 0.0398 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 56/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0388 - mean_squared_error: 0.0388\n",
      "Epoch 00056: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 57/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00057: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 58/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Epoch 00058: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 59/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0432 - mean_squared_error: 0.0432\n",
      "Epoch 00059: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 60/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0434 - mean_squared_error: 0.0434\n",
      "Epoch 00060: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0392 - mean_squared_error: 0.0392 - val_loss: 0.0509 - val_mean_squared_error: 0.0509\n",
      "Epoch 61/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0515 - mean_squared_error: 0.0515\n",
      "Epoch 00061: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0385 - mean_squared_error: 0.0385 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 62/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0341 - mean_squared_error: 0.0341\n",
      "Epoch 00062: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0363 - mean_squared_error: 0.0363 - val_loss: 0.0509 - val_mean_squared_error: 0.0509\n",
      "Epoch 63/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00063: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 37us/sample - loss: 0.0370 - mean_squared_error: 0.0370 - val_loss: 0.0509 - val_mean_squared_error: 0.0509\n",
      "Epoch 64/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00064: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0381 - mean_squared_error: 0.0381 - val_loss: 0.0509 - val_mean_squared_error: 0.0509\n",
      "Epoch 65/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0362 - mean_squared_error: 0.0362\n",
      "Epoch 00065: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0365 - mean_squared_error: 0.0365 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 66/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00066: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0511 - val_mean_squared_error: 0.0511\n",
      "Epoch 67/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0368 - mean_squared_error: 0.0368\n",
      "Epoch 00067: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0514 - val_mean_squared_error: 0.0514\n",
      "Epoch 68/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0527 - mean_squared_error: 0.0527\n",
      "Epoch 00068: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 38us/sample - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0516 - val_mean_squared_error: 0.0516\n",
      "Epoch 69/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0424 - mean_squared_error: 0.0424\n",
      "Epoch 00069: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0517 - val_mean_squared_error: 0.0517\n",
      "Epoch 70/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0510 - mean_squared_error: 0.0510\n",
      "Epoch 00070: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0423 - mean_squared_error: 0.0423 - val_loss: 0.0520 - val_mean_squared_error: 0.0520\n",
      "Epoch 71/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0382 - mean_squared_error: 0.0382\n",
      "Epoch 00071: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 72/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00072: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 73/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0519 - mean_squared_error: 0.0519\n",
      "Epoch 00073: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n",
      "Epoch 74/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0384 - mean_squared_error: 0.0384\n",
      "Epoch 00074: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 75/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0390 - mean_squared_error: 0.0390\n",
      "Epoch 00075: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0372 - mean_squared_error: 0.0372 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 76/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00076: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0520 - val_mean_squared_error: 0.0520\n",
      "Epoch 77/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0345 - mean_squared_error: 0.0345\n",
      "Epoch 00077: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0367 - mean_squared_error: 0.0367 - val_loss: 0.0517 - val_mean_squared_error: 0.0517\n",
      "Epoch 78/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Epoch 00078: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0381 - mean_squared_error: 0.0381 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 79/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0407 - mean_squared_error: 0.0407\n",
      "Epoch 00079: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0509 - val_mean_squared_error: 0.0509\n",
      "Epoch 80/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00080: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 81/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0392 - mean_squared_error: 0.0392\n",
      "Epoch 00081: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0365 - mean_squared_error: 0.0365 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 82/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00082: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 83/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00083: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 84/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00084: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 85/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0469 - mean_squared_error: 0.0469\n",
      "Epoch 00085: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 33us/sample - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 86/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00086: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 87/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0390 - mean_squared_error: 0.0390\n",
      "Epoch 00087: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 88/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0480 - mean_squared_error: 0.0480\n",
      "Epoch 00088: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0361 - mean_squared_error: 0.0361 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 89/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0366 - mean_squared_error: 0.0366\n",
      "Epoch 00089: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 90/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00090: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 91/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0406 - mean_squared_error: 0.0406\n",
      "Epoch 00091: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 92/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00092: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 93/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Epoch 00093: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0363 - mean_squared_error: 0.0363 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 94/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00094: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 95/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00095: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 96/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00096: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 97/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0380 - mean_squared_error: 0.0380\n",
      "Epoch 00097: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 98/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00098: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 99/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00099: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 100/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00100: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 101/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0409 - mean_squared_error: 0.0409\n",
      "Epoch 00101: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 102/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Epoch 00102: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 103/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00103: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 104/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00104: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 105/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00105: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 106/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00106: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 107/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0360 - mean_squared_error: 0.0360\n",
      "Epoch 00107: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0320 - mean_squared_error: 0.0320 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 108/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00108: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0333 - mean_squared_error: 0.0333 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 109/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00109: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0382 - mean_squared_error: 0.0382 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 110/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Epoch 00110: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 111/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0348 - mean_squared_error: 0.0348\n",
      "Epoch 00111: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0365 - mean_squared_error: 0.0365 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 112/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00112: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 113/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00113: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 114/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0346 - mean_squared_error: 0.0346\n",
      "Epoch 00114: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 115/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00115: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 116/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0376 - mean_squared_error: 0.0376\n",
      "Epoch 00116: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 117/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00117: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 118/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00118: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 119/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00119: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0331 - mean_squared_error: 0.0331 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 120/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00120: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 121/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0405 - mean_squared_error: 0.0405\n",
      "Epoch 00121: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 122/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0307 - mean_squared_error: 0.0307\n",
      "Epoch 00122: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 123/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0414 - mean_squared_error: 0.0414\n",
      "Epoch 00123: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 124/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00124: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 125/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00125: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 126/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00126: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 127/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00127: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 128/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0376 - mean_squared_error: 0.0376\n",
      "Epoch 00128: val_loss did not improve from 0.04385\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Running trial 9\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.9438 - mean_squared_error: 0.9438\n",
      "Epoch 00001: val_loss improved from inf to 0.74353, saving model to model.h5\n",
      "248/248 [==============================] - 2s 8ms/sample - loss: 1.0453 - mean_squared_error: 1.0453 - val_loss: 0.7435 - val_mean_squared_error: 0.7435\n",
      "Epoch 2/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.9310 - mean_squared_error: 0.9310\n",
      "Epoch 00002: val_loss improved from 0.74353 to 0.70825, saving model to model.h5\n",
      "248/248 [==============================] - 0s 118us/sample - loss: 0.9548 - mean_squared_error: 0.9548 - val_loss: 0.7082 - val_mean_squared_error: 0.7082\n",
      "Epoch 3/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.8921 - mean_squared_error: 0.8921\n",
      "Epoch 00003: val_loss improved from 0.70825 to 0.67395, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.8698 - mean_squared_error: 0.8698 - val_loss: 0.6740 - val_mean_squared_error: 0.6740\n",
      "Epoch 4/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.7528 - mean_squared_error: 0.7528\n",
      "Epoch 00004: val_loss improved from 0.67395 to 0.64008, saving model to model.h5\n",
      "248/248 [==============================] - 0s 123us/sample - loss: 0.7862 - mean_squared_error: 0.7862 - val_loss: 0.6401 - val_mean_squared_error: 0.6401\n",
      "Epoch 5/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.7386 - mean_squared_error: 0.7386\n",
      "Epoch 00005: val_loss improved from 0.64008 to 0.60664, saving model to model.h5\n",
      "248/248 [==============================] - 0s 123us/sample - loss: 0.7336 - mean_squared_error: 0.7336 - val_loss: 0.6066 - val_mean_squared_error: 0.6066\n",
      "Epoch 6/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.7242 - mean_squared_error: 0.7242\n",
      "Epoch 00006: val_loss improved from 0.60664 to 0.57429, saving model to model.h5\n",
      "248/248 [==============================] - 0s 136us/sample - loss: 0.6491 - mean_squared_error: 0.6491 - val_loss: 0.5743 - val_mean_squared_error: 0.5743\n",
      "Epoch 7/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.5728 - mean_squared_error: 0.5728\n",
      "Epoch 00007: val_loss improved from 0.57429 to 0.54206, saving model to model.h5\n",
      "248/248 [==============================] - 0s 127us/sample - loss: 0.5908 - mean_squared_error: 0.5908 - val_loss: 0.5421 - val_mean_squared_error: 0.5421\n",
      "Epoch 8/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.5159 - mean_squared_error: 0.5159\n",
      "Epoch 00008: val_loss improved from 0.54206 to 0.51046, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.5269 - mean_squared_error: 0.5269 - val_loss: 0.5105 - val_mean_squared_error: 0.5105\n",
      "Epoch 9/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.4950 - mean_squared_error: 0.4950\n",
      "Epoch 00009: val_loss improved from 0.51046 to 0.47932, saving model to model.h5\n",
      "248/248 [==============================] - 0s 123us/sample - loss: 0.4620 - mean_squared_error: 0.4620 - val_loss: 0.4793 - val_mean_squared_error: 0.4793\n",
      "Epoch 10/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3996 - mean_squared_error: 0.3996\n",
      "Epoch 00010: val_loss improved from 0.47932 to 0.44939, saving model to model.h5\n",
      "248/248 [==============================] - 0s 131us/sample - loss: 0.4097 - mean_squared_error: 0.4097 - val_loss: 0.4494 - val_mean_squared_error: 0.4494\n",
      "Epoch 11/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3553 - mean_squared_error: 0.3553\n",
      "Epoch 00011: val_loss improved from 0.44939 to 0.41988, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.3663 - mean_squared_error: 0.3663 - val_loss: 0.4199 - val_mean_squared_error: 0.4199\n",
      "Epoch 12/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3412 - mean_squared_error: 0.3412\n",
      "Epoch 00012: val_loss improved from 0.41988 to 0.39107, saving model to model.h5\n",
      "248/248 [==============================] - 0s 151us/sample - loss: 0.3193 - mean_squared_error: 0.3193 - val_loss: 0.3911 - val_mean_squared_error: 0.3911\n",
      "Epoch 13/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2804 - mean_squared_error: 0.2804\n",
      "Epoch 00013: val_loss improved from 0.39107 to 0.36290, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.2829 - mean_squared_error: 0.2829 - val_loss: 0.3629 - val_mean_squared_error: 0.3629\n",
      "Epoch 14/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2896 - mean_squared_error: 0.2896\n",
      "Epoch 00014: val_loss improved from 0.36290 to 0.33568, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.2505 - mean_squared_error: 0.2505 - val_loss: 0.3357 - val_mean_squared_error: 0.3357\n",
      "Epoch 15/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1938 - mean_squared_error: 0.1938\n",
      "Epoch 00015: val_loss improved from 0.33568 to 0.30932, saving model to model.h5\n",
      "248/248 [==============================] - 0s 136us/sample - loss: 0.2158 - mean_squared_error: 0.2158 - val_loss: 0.3093 - val_mean_squared_error: 0.3093\n",
      "Epoch 16/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1967 - mean_squared_error: 0.1967\n",
      "Epoch 00016: val_loss improved from 0.30932 to 0.28393, saving model to model.h5\n",
      "248/248 [==============================] - 0s 127us/sample - loss: 0.1844 - mean_squared_error: 0.1844 - val_loss: 0.2839 - val_mean_squared_error: 0.2839\n",
      "Epoch 17/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1562 - mean_squared_error: 0.1562\n",
      "Epoch 00017: val_loss improved from 0.28393 to 0.25942, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.1572 - mean_squared_error: 0.1572 - val_loss: 0.2594 - val_mean_squared_error: 0.2594\n",
      "Epoch 18/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1313 - mean_squared_error: 0.1313\n",
      "Epoch 00018: val_loss improved from 0.25942 to 0.23626, saving model to model.h5\n",
      "248/248 [==============================] - 0s 127us/sample - loss: 0.1372 - mean_squared_error: 0.1372 - val_loss: 0.2363 - val_mean_squared_error: 0.2363\n",
      "Epoch 19/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1270 - mean_squared_error: 0.1270\n",
      "Epoch 00019: val_loss improved from 0.23626 to 0.21451, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.1314 - mean_squared_error: 0.1314 - val_loss: 0.2145 - val_mean_squared_error: 0.2145\n",
      "Epoch 20/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0982 - mean_squared_error: 0.0982\n",
      "Epoch 00020: val_loss improved from 0.21451 to 0.19429, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.1093 - mean_squared_error: 0.1093 - val_loss: 0.1943 - val_mean_squared_error: 0.1943\n",
      "Epoch 21/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1083 - mean_squared_error: 0.1083\n",
      "Epoch 00021: val_loss improved from 0.19429 to 0.17542, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0999 - mean_squared_error: 0.0999 - val_loss: 0.1754 - val_mean_squared_error: 0.1754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0797 - mean_squared_error: 0.0797\n",
      "Epoch 00022: val_loss improved from 0.17542 to 0.15813, saving model to model.h5\n",
      "248/248 [==============================] - 0s 131us/sample - loss: 0.0835 - mean_squared_error: 0.0835 - val_loss: 0.1581 - val_mean_squared_error: 0.1581\n",
      "Epoch 23/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0766 - mean_squared_error: 0.0766\n",
      "Epoch 00023: val_loss improved from 0.15813 to 0.14266, saving model to model.h5\n",
      "248/248 [==============================] - 0s 126us/sample - loss: 0.0740 - mean_squared_error: 0.0740 - val_loss: 0.1427 - val_mean_squared_error: 0.1427\n",
      "Epoch 24/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0680 - mean_squared_error: 0.0680\n",
      "Epoch 00024: val_loss improved from 0.14266 to 0.12864, saving model to model.h5\n",
      "248/248 [==============================] - 0s 142us/sample - loss: 0.0685 - mean_squared_error: 0.0685 - val_loss: 0.1286 - val_mean_squared_error: 0.1286\n",
      "Epoch 25/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0624 - mean_squared_error: 0.0624\n",
      "Epoch 00025: val_loss improved from 0.12864 to 0.11613, saving model to model.h5\n",
      "248/248 [==============================] - 0s 131us/sample - loss: 0.0545 - mean_squared_error: 0.0545 - val_loss: 0.1161 - val_mean_squared_error: 0.1161\n",
      "Epoch 26/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0701 - mean_squared_error: 0.0701\n",
      "Epoch 00026: val_loss improved from 0.11613 to 0.10526, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.0576 - mean_squared_error: 0.0576 - val_loss: 0.1053 - val_mean_squared_error: 0.1053\n",
      "Epoch 27/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0569 - mean_squared_error: 0.0569\n",
      "Epoch 00027: val_loss improved from 0.10526 to 0.09577, saving model to model.h5\n",
      "248/248 [==============================] - 0s 122us/sample - loss: 0.0497 - mean_squared_error: 0.0497 - val_loss: 0.0958 - val_mean_squared_error: 0.0958\n",
      "Epoch 28/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0455 - mean_squared_error: 0.0455\n",
      "Epoch 00028: val_loss improved from 0.09577 to 0.08757, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.0572 - mean_squared_error: 0.0572 - val_loss: 0.0876 - val_mean_squared_error: 0.0876\n",
      "Epoch 29/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0425 - mean_squared_error: 0.0425\n",
      "Epoch 00029: val_loss improved from 0.08757 to 0.08055, saving model to model.h5\n",
      "248/248 [==============================] - 0s 131us/sample - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0805 - val_mean_squared_error: 0.0805\n",
      "Epoch 30/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0395 - mean_squared_error: 0.0395\n",
      "Epoch 00030: val_loss improved from 0.08055 to 0.07467, saving model to model.h5\n",
      "248/248 [==============================] - 0s 131us/sample - loss: 0.0449 - mean_squared_error: 0.0449 - val_loss: 0.0747 - val_mean_squared_error: 0.0747\n",
      "Epoch 31/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0367 - mean_squared_error: 0.0367\n",
      "Epoch 00031: val_loss improved from 0.07467 to 0.06970, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0402 - mean_squared_error: 0.0402 - val_loss: 0.0697 - val_mean_squared_error: 0.0697\n",
      "Epoch 32/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0475 - mean_squared_error: 0.0475\n",
      "Epoch 00032: val_loss improved from 0.06970 to 0.06554, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.0418 - mean_squared_error: 0.0418 - val_loss: 0.0655 - val_mean_squared_error: 0.0655\n",
      "Epoch 33/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0417 - mean_squared_error: 0.0417\n",
      "Epoch 00033: val_loss improved from 0.06554 to 0.06203, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.0422 - mean_squared_error: 0.0422 - val_loss: 0.0620 - val_mean_squared_error: 0.0620\n",
      "Epoch 34/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00034: val_loss improved from 0.06203 to 0.05915, saving model to model.h5\n",
      "248/248 [==============================] - 0s 116us/sample - loss: 0.0395 - mean_squared_error: 0.0395 - val_loss: 0.0591 - val_mean_squared_error: 0.0591\n",
      "Epoch 35/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0483 - mean_squared_error: 0.0483\n",
      "Epoch 00035: val_loss improved from 0.05915 to 0.05677, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0417 - mean_squared_error: 0.0417 - val_loss: 0.0568 - val_mean_squared_error: 0.0568\n",
      "Epoch 36/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0334 - mean_squared_error: 0.0334\n",
      "Epoch 00036: val_loss improved from 0.05677 to 0.05482, saving model to model.h5\n",
      "248/248 [==============================] - 0s 134us/sample - loss: 0.0386 - mean_squared_error: 0.0386 - val_loss: 0.0548 - val_mean_squared_error: 0.0548\n",
      "Epoch 37/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0435 - mean_squared_error: 0.0435\n",
      "Epoch 00037: val_loss improved from 0.05482 to 0.05333, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0533 - val_mean_squared_error: 0.0533\n",
      "Epoch 38/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Epoch 00038: val_loss improved from 0.05333 to 0.05212, saving model to model.h5\n",
      "248/248 [==============================] - 0s 127us/sample - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 39/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0447 - mean_squared_error: 0.0447\n",
      "Epoch 00039: val_loss improved from 0.05212 to 0.05115, saving model to model.h5\n",
      "248/248 [==============================] - 0s 128us/sample - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0511 - val_mean_squared_error: 0.0511\n",
      "Epoch 40/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0343 - mean_squared_error: 0.0343\n",
      "Epoch 00040: val_loss improved from 0.05115 to 0.05041, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 41/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00041: val_loss improved from 0.05041 to 0.05005, saving model to model.h5\n",
      "248/248 [==============================] - 0s 132us/sample - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 42/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00042: val_loss improved from 0.05005 to 0.04991, saving model to model.h5\n",
      "248/248 [==============================] - 0s 119us/sample - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 43/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0402 - mean_squared_error: 0.0402\n",
      "Epoch 00043: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 34us/sample - loss: 0.0426 - mean_squared_error: 0.0426 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 44/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0435 - mean_squared_error: 0.0435\n",
      "Epoch 00044: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 45/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0343 - mean_squared_error: 0.0343\n",
      "Epoch 00045: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 37us/sample - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 46/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 00046: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 47/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00047: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0511 - val_mean_squared_error: 0.0511\n",
      "Epoch 48/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00048: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0514 - val_mean_squared_error: 0.0514\n",
      "Epoch 49/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0461 - mean_squared_error: 0.0461\n",
      "Epoch 00049: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0381 - mean_squared_error: 0.0381 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 50/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00050: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0323 - mean_squared_error: 0.0323 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 51/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0374 - mean_squared_error: 0.0374\n",
      "Epoch 00051: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0363 - mean_squared_error: 0.0363 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 52/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Epoch 00052: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0526 - val_mean_squared_error: 0.0526\n",
      "Epoch 53/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00053: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0527 - val_mean_squared_error: 0.0527\n",
      "Epoch 54/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0394 - mean_squared_error: 0.0394\n",
      "Epoch 00054: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 55/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0398 - mean_squared_error: 0.0398\n",
      "Epoch 00055: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0384 - mean_squared_error: 0.0384 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 56/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0393 - mean_squared_error: 0.0393\n",
      "Epoch 00056: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0370 - mean_squared_error: 0.0370 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 57/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0396 - mean_squared_error: 0.0396\n",
      "Epoch 00057: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0526 - val_mean_squared_error: 0.0526\n",
      "Epoch 58/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0391 - mean_squared_error: 0.0391\n",
      "Epoch 00058: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0372 - mean_squared_error: 0.0372 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 59/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00059: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 60/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00060: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 61/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 00061: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 37us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 62/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00062: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 63/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00063: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 64/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00064: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 65/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 00065: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 66/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0400 - mean_squared_error: 0.0400\n",
      "Epoch 00066: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0328 - mean_squared_error: 0.0328 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 67/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00067: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 31us/sample - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 68/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00068: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 69/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0420 - mean_squared_error: 0.0420\n",
      "Epoch 00069: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 70/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00070: val_loss did not improve from 0.04991\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0328 - mean_squared_error: 0.0328 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 71/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0426 - mean_squared_error: 0.0426\n",
      "Epoch 00071: val_loss improved from 0.04991 to 0.04988, saving model to model.h5\n",
      "248/248 [==============================] - 0s 123us/sample - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 72/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Epoch 00072: val_loss improved from 0.04988 to 0.04964, saving model to model.h5\n",
      "248/248 [==============================] - 0s 127us/sample - loss: 0.0382 - mean_squared_error: 0.0382 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 73/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0345 - mean_squared_error: 0.0345\n",
      "Epoch 00073: val_loss improved from 0.04964 to 0.04941, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 74/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00074: val_loss improved from 0.04941 to 0.04917, saving model to model.h5\n",
      "248/248 [==============================] - 0s 120us/sample - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 75/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0422 - mean_squared_error: 0.0422\n",
      "Epoch 00075: val_loss improved from 0.04917 to 0.04904, saving model to model.h5\n",
      "248/248 [==============================] - 0s 127us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 76/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00076: val_loss improved from 0.04904 to 0.04894, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 77/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Epoch 00077: val_loss improved from 0.04894 to 0.04886, saving model to model.h5\n",
      "248/248 [==============================] - 0s 122us/sample - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 78/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0392 - mean_squared_error: 0.0392\n",
      "Epoch 00078: val_loss improved from 0.04886 to 0.04884, saving model to model.h5\n",
      "248/248 [==============================] - 0s 128us/sample - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 79/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00079: val_loss improved from 0.04884 to 0.04867, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 80/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0427 - mean_squared_error: 0.0427\n",
      "Epoch 00080: val_loss improved from 0.04867 to 0.04852, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 81/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0402 - mean_squared_error: 0.0402\n",
      "Epoch 00081: val_loss improved from 0.04852 to 0.04835, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0331 - mean_squared_error: 0.0331 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 82/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00082: val_loss improved from 0.04835 to 0.04822, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0482 - val_mean_squared_error: 0.0482\n",
      "Epoch 83/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0294 - mean_squared_error: 0.0294\n",
      "Epoch 00083: val_loss improved from 0.04822 to 0.04815, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 84/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0411 - mean_squared_error: 0.0411\n",
      "Epoch 00084: val_loss improved from 0.04815 to 0.04810, saving model to model.h5\n",
      "248/248 [==============================] - 0s 127us/sample - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 85/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00085: val_loss improved from 0.04810 to 0.04794, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 86/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0344 - mean_squared_error: 0.0344\n",
      "Epoch 00086: val_loss improved from 0.04794 to 0.04779, saving model to model.h5\n",
      "248/248 [==============================] - 0s 126us/sample - loss: 0.0363 - mean_squared_error: 0.0363 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 87/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0392 - mean_squared_error: 0.0392\n",
      "Epoch 00087: val_loss improved from 0.04779 to 0.04765, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 88/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00088: val_loss improved from 0.04765 to 0.04740, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 89/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00089: val_loss improved from 0.04740 to 0.04717, saving model to model.h5\n",
      "248/248 [==============================] - 0s 127us/sample - loss: 0.0320 - mean_squared_error: 0.0320 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 90/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00090: val_loss improved from 0.04717 to 0.04698, saving model to model.h5\n",
      "248/248 [==============================] - 0s 140us/sample - loss: 0.0320 - mean_squared_error: 0.0320 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 91/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0366 - mean_squared_error: 0.0366\n",
      "Epoch 00091: val_loss improved from 0.04698 to 0.04678, saving model to model.h5\n",
      "248/248 [==============================] - 0s 130us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 92/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0352 - mean_squared_error: 0.0352\n",
      "Epoch 00092: val_loss improved from 0.04678 to 0.04658, saving model to model.h5\n",
      "248/248 [==============================] - 0s 132us/sample - loss: 0.0323 - mean_squared_error: 0.0323 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 93/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0372 - mean_squared_error: 0.0372\n",
      "Epoch 00093: val_loss improved from 0.04658 to 0.04637, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 94/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0442 - mean_squared_error: 0.0442\n",
      "Epoch 00094: val_loss improved from 0.04637 to 0.04620, saving model to model.h5\n",
      "248/248 [==============================] - 0s 130us/sample - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 95/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00095: val_loss improved from 0.04620 to 0.04613, saving model to model.h5\n",
      "248/248 [==============================] - 0s 134us/sample - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00096: val_loss improved from 0.04613 to 0.04612, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 97/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Epoch 00097: val_loss did not improve from 0.04612\n",
      "248/248 [==============================] - 0s 35us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 98/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0382 - mean_squared_error: 0.0382\n",
      "Epoch 00098: val_loss did not improve from 0.04612\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 99/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0410 - mean_squared_error: 0.0410\n",
      "Epoch 00099: val_loss did not improve from 0.04612\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0370 - mean_squared_error: 0.0370 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 100/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0343 - mean_squared_error: 0.0343\n",
      "Epoch 00100: val_loss did not improve from 0.04612\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 101/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0326 - mean_squared_error: 0.0326\n",
      "Epoch 00101: val_loss did not improve from 0.04612\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 102/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00102: val_loss did not improve from 0.04612\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 103/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00103: val_loss did not improve from 0.04612\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 104/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 00104: val_loss did not improve from 0.04612\n",
      "248/248 [==============================] - 0s 37us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 105/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Epoch 00105: val_loss did not improve from 0.04612\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 106/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0412 - mean_squared_error: 0.0412\n",
      "Epoch 00106: val_loss did not improve from 0.04612\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 107/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Epoch 00107: val_loss did not improve from 0.04612\n",
      "248/248 [==============================] - 0s 33us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 108/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0420 - mean_squared_error: 0.0420\n",
      "Epoch 00108: val_loss did not improve from 0.04612\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 109/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00109: val_loss did not improve from 0.04612\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 110/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0407 - mean_squared_error: 0.0407\n",
      "Epoch 00110: val_loss did not improve from 0.04612\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 111/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00111: val_loss did not improve from 0.04612\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 112/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00112: val_loss did not improve from 0.04612\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 113/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00113: val_loss improved from 0.04612 to 0.04587, saving model to model.h5\n",
      "248/248 [==============================] - 0s 132us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 114/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00114: val_loss improved from 0.04587 to 0.04556, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 115/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00115: val_loss improved from 0.04556 to 0.04522, saving model to model.h5\n",
      "248/248 [==============================] - 0s 139us/sample - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 116/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00116: val_loss improved from 0.04522 to 0.04490, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.0319 - mean_squared_error: 0.0319 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 117/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Epoch 00117: val_loss improved from 0.04490 to 0.04482, saving model to model.h5\n",
      "248/248 [==============================] - 0s 131us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 118/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00118: val_loss improved from 0.04482 to 0.04475, saving model to model.h5\n",
      "248/248 [==============================] - 0s 120us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 119/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00119: val_loss improved from 0.04475 to 0.04467, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 120/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0451 - mean_squared_error: 0.0451\n",
      "Epoch 00120: val_loss did not improve from 0.04467\n",
      "248/248 [==============================] - 0s 41us/sample - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 121/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00121: val_loss improved from 0.04467 to 0.04466, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 122/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0362 - mean_squared_error: 0.0362\n",
      "Epoch 00122: val_loss improved from 0.04466 to 0.04464, saving model to model.h5\n",
      "248/248 [==============================] - 0s 131us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 123/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00123: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 124/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00124: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 125/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00125: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 126/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Epoch 00126: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 127/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00127: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 128/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0391 - mean_squared_error: 0.0391\n",
      "Epoch 00128: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0330 - mean_squared_error: 0.0330 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 129/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0395 - mean_squared_error: 0.0395\n",
      "Epoch 00129: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 130/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0341 - mean_squared_error: 0.0341\n",
      "Epoch 00130: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 31us/sample - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 131/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00131: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 132/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0326 - mean_squared_error: 0.0326\n",
      "Epoch 00132: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 133/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Epoch 00133: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 41us/sample - loss: 0.0331 - mean_squared_error: 0.0331 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 134/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00134: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 135/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00135: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 136/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0372 - mean_squared_error: 0.0372\n",
      "Epoch 00136: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0331 - mean_squared_error: 0.0331 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 137/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00137: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 138/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00138: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 139/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00139: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 140/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Epoch 00140: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 141/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00141: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 142/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00142: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 143/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00143: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 144/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0348 - mean_squared_error: 0.0348\n",
      "Epoch 00144: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 145/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0374 - mean_squared_error: 0.0374\n",
      "Epoch 00145: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 37us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 146/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00146: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 34us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00147: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 35us/sample - loss: 0.0320 - mean_squared_error: 0.0320 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 148/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00148: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 149/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00149: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 150/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00150: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 151/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00151: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 152/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0259 - mean_squared_error: 0.0259\n",
      "Epoch 00152: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 153/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00153: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0319 - mean_squared_error: 0.0319 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 154/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00154: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 155/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00155: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 156/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00156: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 157/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0428 - mean_squared_error: 0.0428\n",
      "Epoch 00157: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 158/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00158: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 35us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 159/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00159: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 34us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 160/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0295 - mean_squared_error: 0.0295\n",
      "Epoch 00160: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 161/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Epoch 00161: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 162/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Epoch 00162: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 163/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00163: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 164/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00164: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 165/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0346 - mean_squared_error: 0.0346\n",
      "Epoch 00165: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 166/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0398 - mean_squared_error: 0.0398\n",
      "Epoch 00166: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 167/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00167: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 168/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00168: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 169/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00169: val_loss did not improve from 0.04464\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 170/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 00170: val_loss improved from 0.04464 to 0.04451, saving model to model.h5\n",
      "248/248 [==============================] - 0s 147us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 171/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00171: val_loss improved from 0.04451 to 0.04445, saving model to model.h5\n",
      "248/248 [==============================] - 0s 138us/sample - loss: 0.0328 - mean_squared_error: 0.0328 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 172/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0376 - mean_squared_error: 0.0376\n",
      "Epoch 00172: val_loss did not improve from 0.04445\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00173: val_loss did not improve from 0.04445\n",
      "248/248 [==============================] - 0s 39us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 174/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00174: val_loss improved from 0.04445 to 0.04433, saving model to model.h5\n",
      "248/248 [==============================] - 0s 132us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 175/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00175: val_loss improved from 0.04433 to 0.04417, saving model to model.h5\n",
      "248/248 [==============================] - 0s 134us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 176/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00176: val_loss improved from 0.04417 to 0.04401, saving model to model.h5\n",
      "248/248 [==============================] - 0s 126us/sample - loss: 0.0320 - mean_squared_error: 0.0320 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 177/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0365 - mean_squared_error: 0.0365\n",
      "Epoch 00177: val_loss improved from 0.04401 to 0.04401, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 178/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0385 - mean_squared_error: 0.0385\n",
      "Epoch 00178: val_loss did not improve from 0.04401\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 179/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00179: val_loss did not improve from 0.04401\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0308 - mean_squared_error: 0.0308 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 180/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Epoch 00180: val_loss did not improve from 0.04401\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 181/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00181: val_loss did not improve from 0.04401\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 182/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00182: val_loss did not improve from 0.04401\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 183/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Epoch 00183: val_loss did not improve from 0.04401\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 184/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00184: val_loss did not improve from 0.04401\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 185/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00185: val_loss did not improve from 0.04401\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 186/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00186: val_loss did not improve from 0.04401\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 187/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00187: val_loss did not improve from 0.04401\n",
      "248/248 [==============================] - 0s 28us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 188/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00188: val_loss did not improve from 0.04401\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 189/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0425 - mean_squared_error: 0.0425\n",
      "Epoch 00189: val_loss improved from 0.04401 to 0.04396, saving model to model.h5\n",
      "248/248 [==============================] - 0s 126us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 190/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00190: val_loss improved from 0.04396 to 0.04381, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 191/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00191: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 192/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00192: val_loss improved from 0.04381 to 0.04381, saving model to model.h5\n",
      "248/248 [==============================] - 0s 119us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 193/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0410 - mean_squared_error: 0.0410\n",
      "Epoch 00193: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 37us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 194/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00194: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 195/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00195: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 196/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00196: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 197/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00197: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 34us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 198/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00198: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 199/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Epoch 00199: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 200/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00200: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 201/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00201: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 38us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 202/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00202: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 203/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00203: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 204/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0380 - mean_squared_error: 0.0380\n",
      "Epoch 00204: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 205/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00205: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 206/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00206: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 207/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00207: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 208/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0160 - mean_squared_error: 0.0160\n",
      "Epoch 00208: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 209/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Epoch 00209: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 210/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00210: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 211/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00211: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 212/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0379 - mean_squared_error: 0.0379\n",
      "Epoch 00212: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 213/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Epoch 00213: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 214/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0379 - mean_squared_error: 0.0379\n",
      "Epoch 00214: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 215/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0294 - mean_squared_error: 0.0294\n",
      "Epoch 00215: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 216/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.0172\n",
      "Epoch 00216: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 217/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00217: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 218/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0259 - mean_squared_error: 0.0259\n",
      "Epoch 00218: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 219/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00219: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 220/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00220: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 221/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00221: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 222/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00222: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 223/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00223: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 224/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00224: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 225/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00225: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 226/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00226: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 227/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00227: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 228/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00228: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 229/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00229: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 230/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00230: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 231/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Epoch 00231: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 232/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00232: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 233/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00233: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 234/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0307 - mean_squared_error: 0.0307\n",
      "Epoch 00234: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 235/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00235: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 236/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00236: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 237/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00237: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 238/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00238: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 239/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00239: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 240/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00240: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 241/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Epoch 00241: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 35us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 242/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Epoch 00242: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 243/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0344 - mean_squared_error: 0.0344\n",
      "Epoch 00243: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 244/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00244: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 245/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00245: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 246/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00246: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 247/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00247: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 248/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0410 - mean_squared_error: 0.0410\n",
      "Epoch 00248: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 249/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00249: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 250/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00250: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 251/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0231 - mean_squared_error: 0.0231\n",
      "Epoch 00251: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 252/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00252: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 253/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00253: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 254/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00254: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 255/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0184 - mean_squared_error: 0.0184\n",
      "Epoch 00255: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 256/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0112 - mean_squared_error: 0.0112\n",
      "Epoch 00256: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 257/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00257: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 258/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00258: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 259/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00259: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 260/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0345 - mean_squared_error: 0.0345\n",
      "Epoch 00260: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 261/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0379 - mean_squared_error: 0.0379\n",
      "Epoch 00261: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 262/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00262: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 263/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00263: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 264/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00264: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 265/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00265: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 266/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00266: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 267/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Epoch 00267: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 268/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00268: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 269/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00269: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 270/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00270: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 271/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00271: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 272/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00272: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 273/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00273: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 274/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00274: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 275/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00275: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 276/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00276: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 277/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00277: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 37us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 278/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00278: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 279/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00279: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 280/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00280: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 281/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00281: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 282/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00282: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 283/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Epoch 00283: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 284/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0402 - mean_squared_error: 0.0402\n",
      "Epoch 00284: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 285/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Epoch 00285: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 286/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00286: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 287/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0152 - mean_squared_error: 0.0152\n",
      "Epoch 00287: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 288/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00288: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 289/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00289: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 290/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Epoch 00290: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 291/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00291: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 292/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00292: val_loss did not improve from 0.04381\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Running trial 10\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 1.2813 - mean_squared_error: 1.2813\n",
      "Epoch 00001: val_loss improved from inf to 0.87398, saving model to model.h5\n",
      "248/248 [==============================] - 2s 8ms/sample - loss: 1.2730 - mean_squared_error: 1.2730 - val_loss: 0.8740 - val_mean_squared_error: 0.8740\n",
      "Epoch 2/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 1.3215 - mean_squared_error: 1.3215\n",
      "Epoch 00002: val_loss improved from 0.87398 to 0.83855, saving model to model.h5\n",
      "248/248 [==============================] - 0s 119us/sample - loss: 1.1700 - mean_squared_error: 1.1701 - val_loss: 0.8385 - val_mean_squared_error: 0.8385\n",
      "Epoch 3/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.8646 - mean_squared_error: 0.8646\n",
      "Epoch 00003: val_loss improved from 0.83855 to 0.80123, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 1.0739 - mean_squared_error: 1.0739 - val_loss: 0.8012 - val_mean_squared_error: 0.8012\n",
      "Epoch 4/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.9903 - mean_squared_error: 0.9903\n",
      "Epoch 00004: val_loss improved from 0.80123 to 0.76573, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.9876 - mean_squared_error: 0.9876 - val_loss: 0.7657 - val_mean_squared_error: 0.7657\n",
      "Epoch 5/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.9307 - mean_squared_error: 0.9307\n",
      "Epoch 00005: val_loss improved from 0.76573 to 0.73155, saving model to model.h5\n",
      "248/248 [==============================] - 0s 126us/sample - loss: 0.9080 - mean_squared_error: 0.9080 - val_loss: 0.7316 - val_mean_squared_error: 0.7316\n",
      "Epoch 6/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.8971 - mean_squared_error: 0.8971\n",
      "Epoch 00006: val_loss improved from 0.73155 to 0.69748, saving model to model.h5\n",
      "248/248 [==============================] - 0s 120us/sample - loss: 0.8558 - mean_squared_error: 0.8558 - val_loss: 0.6975 - val_mean_squared_error: 0.6975\n",
      "Epoch 7/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.7937 - mean_squared_error: 0.7937\n",
      "Epoch 00007: val_loss improved from 0.69748 to 0.66375, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.7671 - mean_squared_error: 0.7671 - val_loss: 0.6637 - val_mean_squared_error: 0.6637\n",
      "Epoch 8/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.8114 - mean_squared_error: 0.8114\n",
      "Epoch 00008: val_loss improved from 0.66375 to 0.63040, saving model to model.h5\n",
      "248/248 [==============================] - 0s 112us/sample - loss: 0.7232 - mean_squared_error: 0.7232 - val_loss: 0.6304 - val_mean_squared_error: 0.6304\n",
      "Epoch 9/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.6697 - mean_squared_error: 0.6697\n",
      "Epoch 00009: val_loss improved from 0.63040 to 0.59773, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.6243 - mean_squared_error: 0.6243 - val_loss: 0.5977 - val_mean_squared_error: 0.5977\n",
      "Epoch 10/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.5191 - mean_squared_error: 0.5191\n",
      "Epoch 00010: val_loss improved from 0.59773 to 0.56488, saving model to model.h5\n",
      "248/248 [==============================] - 0s 123us/sample - loss: 0.5808 - mean_squared_error: 0.5808 - val_loss: 0.5649 - val_mean_squared_error: 0.5649\n",
      "Epoch 11/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.5690 - mean_squared_error: 0.5690\n",
      "Epoch 00011: val_loss improved from 0.56488 to 0.53335, saving model to model.h5\n",
      "248/248 [==============================] - 0s 119us/sample - loss: 0.5208 - mean_squared_error: 0.5208 - val_loss: 0.5334 - val_mean_squared_error: 0.5334\n",
      "Epoch 12/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.5912 - mean_squared_error: 0.5912\n",
      "Epoch 00012: val_loss improved from 0.53335 to 0.50261, saving model to model.h5\n",
      "248/248 [==============================] - 0s 126us/sample - loss: 0.4851 - mean_squared_error: 0.4851 - val_loss: 0.5026 - val_mean_squared_error: 0.5026\n",
      "Epoch 13/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3684 - mean_squared_error: 0.3684\n",
      "Epoch 00013: val_loss improved from 0.50261 to 0.47228, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.4296 - mean_squared_error: 0.4296 - val_loss: 0.4723 - val_mean_squared_error: 0.4723\n",
      "Epoch 14/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.4537 - mean_squared_error: 0.4537\n",
      "Epoch 00014: val_loss improved from 0.47228 to 0.44283, saving model to model.h5\n",
      "248/248 [==============================] - 0s 115us/sample - loss: 0.3825 - mean_squared_error: 0.3825 - val_loss: 0.4428 - val_mean_squared_error: 0.4428\n",
      "Epoch 15/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3878 - mean_squared_error: 0.3878\n",
      "Epoch 00015: val_loss improved from 0.44283 to 0.41374, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.3365 - mean_squared_error: 0.3365 - val_loss: 0.4137 - val_mean_squared_error: 0.4137\n",
      "Epoch 16/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.3246 - mean_squared_error: 0.3246\n",
      "Epoch 00016: val_loss improved from 0.41374 to 0.38558, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.3193 - mean_squared_error: 0.3193 - val_loss: 0.3856 - val_mean_squared_error: 0.3856\n",
      "Epoch 17/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2991 - mean_squared_error: 0.2991\n",
      "Epoch 00017: val_loss improved from 0.38558 to 0.35879, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.2970 - mean_squared_error: 0.2970 - val_loss: 0.3588 - val_mean_squared_error: 0.3588\n",
      "Epoch 18/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.2940 - mean_squared_error: 0.2940\n",
      "Epoch 00018: val_loss improved from 0.35879 to 0.33302, saving model to model.h5\n",
      "248/248 [==============================] - 0s 130us/sample - loss: 0.2668 - mean_squared_error: 0.2668 - val_loss: 0.3330 - val_mean_squared_error: 0.3330\n",
      "Epoch 19/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1957 - mean_squared_error: 0.1957\n",
      "Epoch 00019: val_loss improved from 0.33302 to 0.30835, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.2256 - mean_squared_error: 0.2256 - val_loss: 0.3083 - val_mean_squared_error: 0.3083\n",
      "Epoch 20/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1894 - mean_squared_error: 0.1894\n",
      "Epoch 00020: val_loss improved from 0.30835 to 0.28448, saving model to model.h5\n",
      "248/248 [==============================] - 0s 119us/sample - loss: 0.2007 - mean_squared_error: 0.2007 - val_loss: 0.2845 - val_mean_squared_error: 0.2845\n",
      "Epoch 21/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1890 - mean_squared_error: 0.1890\n",
      "Epoch 00021: val_loss improved from 0.28448 to 0.26190, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.1854 - mean_squared_error: 0.1854 - val_loss: 0.2619 - val_mean_squared_error: 0.2619\n",
      "Epoch 22/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1686 - mean_squared_error: 0.1686\n",
      "Epoch 00022: val_loss improved from 0.26190 to 0.24084, saving model to model.h5\n",
      "248/248 [==============================] - 0s 115us/sample - loss: 0.1705 - mean_squared_error: 0.1705 - val_loss: 0.2408 - val_mean_squared_error: 0.2408\n",
      "Epoch 23/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1519 - mean_squared_error: 0.1519\n",
      "Epoch 00023: val_loss improved from 0.24084 to 0.22103, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.1436 - mean_squared_error: 0.1436 - val_loss: 0.2210 - val_mean_squared_error: 0.2210\n",
      "Epoch 24/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1263 - mean_squared_error: 0.1263\n",
      "Epoch 00024: val_loss improved from 0.22103 to 0.20261, saving model to model.h5\n",
      "248/248 [==============================] - 0s 113us/sample - loss: 0.1344 - mean_squared_error: 0.1344 - val_loss: 0.2026 - val_mean_squared_error: 0.2026\n",
      "Epoch 25/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1264 - mean_squared_error: 0.1264\n",
      "Epoch 00025: val_loss improved from 0.20261 to 0.18545, saving model to model.h5\n",
      "248/248 [==============================] - 0s 118us/sample - loss: 0.1268 - mean_squared_error: 0.1268 - val_loss: 0.1855 - val_mean_squared_error: 0.1855\n",
      "Epoch 26/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1325 - mean_squared_error: 0.1325\n",
      "Epoch 00026: val_loss improved from 0.18545 to 0.16981, saving model to model.h5\n",
      "248/248 [==============================] - 0s 120us/sample - loss: 0.1128 - mean_squared_error: 0.1128 - val_loss: 0.1698 - val_mean_squared_error: 0.1698\n",
      "Epoch 27/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.1073 - mean_squared_error: 0.1073\n",
      "Epoch 00027: val_loss improved from 0.16981 to 0.15551, saving model to model.h5\n",
      "248/248 [==============================] - 0s 123us/sample - loss: 0.1001 - mean_squared_error: 0.1001 - val_loss: 0.1555 - val_mean_squared_error: 0.1555\n",
      "Epoch 28/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0856 - mean_squared_error: 0.0856\n",
      "Epoch 00028: val_loss improved from 0.15551 to 0.14246, saving model to model.h5\n",
      "248/248 [==============================] - 0s 128us/sample - loss: 0.0916 - mean_squared_error: 0.0916 - val_loss: 0.1425 - val_mean_squared_error: 0.1425\n",
      "Epoch 29/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0740 - mean_squared_error: 0.0740\n",
      "Epoch 00029: val_loss improved from 0.14246 to 0.13052, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0803 - mean_squared_error: 0.0803 - val_loss: 0.1305 - val_mean_squared_error: 0.1305\n",
      "Epoch 30/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0752 - mean_squared_error: 0.0752\n",
      "Epoch 00030: val_loss improved from 0.13052 to 0.11975, saving model to model.h5\n",
      "248/248 [==============================] - 0s 115us/sample - loss: 0.0731 - mean_squared_error: 0.0731 - val_loss: 0.1197 - val_mean_squared_error: 0.1197\n",
      "Epoch 31/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0681 - mean_squared_error: 0.0681\n",
      "Epoch 00031: val_loss improved from 0.11975 to 0.11013, saving model to model.h5\n",
      "248/248 [==============================] - 0s 116us/sample - loss: 0.0719 - mean_squared_error: 0.0719 - val_loss: 0.1101 - val_mean_squared_error: 0.1101\n",
      "Epoch 32/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0696 - mean_squared_error: 0.0696\n",
      "Epoch 00032: val_loss improved from 0.11013 to 0.10154, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0681 - mean_squared_error: 0.0681 - val_loss: 0.1015 - val_mean_squared_error: 0.1015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0552 - mean_squared_error: 0.0552\n",
      "Epoch 00033: val_loss improved from 0.10154 to 0.09388, saving model to model.h5\n",
      "248/248 [==============================] - 0s 114us/sample - loss: 0.0648 - mean_squared_error: 0.0648 - val_loss: 0.0939 - val_mean_squared_error: 0.0939\n",
      "Epoch 34/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0591 - mean_squared_error: 0.0591\n",
      "Epoch 00034: val_loss improved from 0.09388 to 0.08719, saving model to model.h5\n",
      "248/248 [==============================] - 0s 119us/sample - loss: 0.0606 - mean_squared_error: 0.0606 - val_loss: 0.0872 - val_mean_squared_error: 0.0872\n",
      "Epoch 35/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0588 - mean_squared_error: 0.0588\n",
      "Epoch 00035: val_loss improved from 0.08719 to 0.08129, saving model to model.h5\n",
      "248/248 [==============================] - 0s 115us/sample - loss: 0.0651 - mean_squared_error: 0.0651 - val_loss: 0.0813 - val_mean_squared_error: 0.0813\n",
      "Epoch 36/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0590 - mean_squared_error: 0.0590\n",
      "Epoch 00036: val_loss improved from 0.08129 to 0.07610, saving model to model.h5\n",
      "248/248 [==============================] - 0s 118us/sample - loss: 0.0556 - mean_squared_error: 0.0556 - val_loss: 0.0761 - val_mean_squared_error: 0.0761\n",
      "Epoch 37/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0506 - mean_squared_error: 0.0506\n",
      "Epoch 00037: val_loss improved from 0.07610 to 0.07167, saving model to model.h5\n",
      "248/248 [==============================] - 0s 116us/sample - loss: 0.0496 - mean_squared_error: 0.0496 - val_loss: 0.0717 - val_mean_squared_error: 0.0717\n",
      "Epoch 38/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0447 - mean_squared_error: 0.0447\n",
      "Epoch 00038: val_loss improved from 0.07167 to 0.06788, saving model to model.h5\n",
      "248/248 [==============================] - 0s 120us/sample - loss: 0.0500 - mean_squared_error: 0.0500 - val_loss: 0.0679 - val_mean_squared_error: 0.0679\n",
      "Epoch 39/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0419 - mean_squared_error: 0.0419\n",
      "Epoch 00039: val_loss improved from 0.06788 to 0.06478, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0461 - mean_squared_error: 0.0461 - val_loss: 0.0648 - val_mean_squared_error: 0.0648\n",
      "Epoch 40/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0421 - mean_squared_error: 0.0421\n",
      "Epoch 00040: val_loss improved from 0.06478 to 0.06212, saving model to model.h5\n",
      "248/248 [==============================] - 0s 113us/sample - loss: 0.0458 - mean_squared_error: 0.0458 - val_loss: 0.0621 - val_mean_squared_error: 0.0621\n",
      "Epoch 41/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0407 - mean_squared_error: 0.0407\n",
      "Epoch 00041: val_loss improved from 0.06212 to 0.05990, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0599 - val_mean_squared_error: 0.0599\n",
      "Epoch 42/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0490 - mean_squared_error: 0.0490\n",
      "Epoch 00042: val_loss improved from 0.05990 to 0.05809, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.0426 - mean_squared_error: 0.0426 - val_loss: 0.0581 - val_mean_squared_error: 0.0581\n",
      "Epoch 43/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0346 - mean_squared_error: 0.0346\n",
      "Epoch 00043: val_loss improved from 0.05809 to 0.05659, saving model to model.h5\n",
      "248/248 [==============================] - 0s 127us/sample - loss: 0.0423 - mean_squared_error: 0.0423 - val_loss: 0.0566 - val_mean_squared_error: 0.0566\n",
      "Epoch 44/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0455 - mean_squared_error: 0.0455\n",
      "Epoch 00044: val_loss improved from 0.05659 to 0.05534, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0435 - mean_squared_error: 0.0435 - val_loss: 0.0553 - val_mean_squared_error: 0.0553\n",
      "Epoch 45/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0450 - mean_squared_error: 0.0450\n",
      "Epoch 00045: val_loss improved from 0.05534 to 0.05439, saving model to model.h5\n",
      "248/248 [==============================] - 0s 151us/sample - loss: 0.0417 - mean_squared_error: 0.0417 - val_loss: 0.0544 - val_mean_squared_error: 0.0544\n",
      "Epoch 46/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Epoch 00046: val_loss improved from 0.05439 to 0.05370, saving model to model.h5\n",
      "248/248 [==============================] - 0s 119us/sample - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n",
      "Epoch 47/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0441 - mean_squared_error: 0.0441\n",
      "Epoch 00047: val_loss improved from 0.05370 to 0.05309, saving model to model.h5\n",
      "248/248 [==============================] - 0s 123us/sample - loss: 0.0427 - mean_squared_error: 0.0427 - val_loss: 0.0531 - val_mean_squared_error: 0.0531\n",
      "Epoch 48/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0426 - mean_squared_error: 0.0426\n",
      "Epoch 00048: val_loss improved from 0.05309 to 0.05262, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0526 - val_mean_squared_error: 0.0526\n",
      "Epoch 49/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0406 - mean_squared_error: 0.0406\n",
      "Epoch 00049: val_loss improved from 0.05262 to 0.05227, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0397 - mean_squared_error: 0.0397 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n",
      "Epoch 50/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0475 - mean_squared_error: 0.0475\n",
      "Epoch 00050: val_loss improved from 0.05227 to 0.05198, saving model to model.h5\n",
      "248/248 [==============================] - 0s 126us/sample - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0520 - val_mean_squared_error: 0.0520\n",
      "Epoch 51/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0446 - mean_squared_error: 0.0446\n",
      "Epoch 00051: val_loss improved from 0.05198 to 0.05176, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0390 - mean_squared_error: 0.0390 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 52/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0415 - mean_squared_error: 0.0415\n",
      "Epoch 00052: val_loss improved from 0.05176 to 0.05170, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0517 - val_mean_squared_error: 0.0517\n",
      "Epoch 53/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00053: val_loss improved from 0.05170 to 0.05164, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0516 - val_mean_squared_error: 0.0516\n",
      "Epoch 54/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0429 - mean_squared_error: 0.0429\n",
      "Epoch 00054: val_loss did not improve from 0.05164\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0398 - mean_squared_error: 0.0398 - val_loss: 0.0517 - val_mean_squared_error: 0.0517\n",
      "Epoch 55/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0398 - mean_squared_error: 0.0398\n",
      "Epoch 00055: val_loss did not improve from 0.05164\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0363 - mean_squared_error: 0.0363 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 56/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0483 - mean_squared_error: 0.0483\n",
      "Epoch 00056: val_loss did not improve from 0.05164\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0400 - mean_squared_error: 0.0400 - val_loss: 0.0517 - val_mean_squared_error: 0.0517\n",
      "Epoch 57/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 00057: val_loss did not improve from 0.05164\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0331 - mean_squared_error: 0.0331 - val_loss: 0.0517 - val_mean_squared_error: 0.0517\n",
      "Epoch 58/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0399 - mean_squared_error: 0.0399\n",
      "Epoch 00058: val_loss did not improve from 0.05164\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0516 - val_mean_squared_error: 0.0516\n",
      "Epoch 59/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0377 - mean_squared_error: 0.0377\n",
      "Epoch 00059: val_loss improved from 0.05164 to 0.05156, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0394 - mean_squared_error: 0.0394 - val_loss: 0.0516 - val_mean_squared_error: 0.0516\n",
      "Epoch 60/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0453 - mean_squared_error: 0.0453\n",
      "Epoch 00060: val_loss improved from 0.05156 to 0.05136, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0397 - mean_squared_error: 0.0397 - val_loss: 0.0514 - val_mean_squared_error: 0.0514\n",
      "Epoch 61/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0471 - mean_squared_error: 0.0471\n",
      "Epoch 00061: val_loss improved from 0.05136 to 0.05105, saving model to model.h5\n",
      "248/248 [==============================] - 0s 120us/sample - loss: 0.0388 - mean_squared_error: 0.0388 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 62/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0454 - mean_squared_error: 0.0454\n",
      "Epoch 00062: val_loss improved from 0.05105 to 0.05075, saving model to model.h5\n",
      "248/248 [==============================] - 0s 122us/sample - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 63/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0358 - mean_squared_error: 0.0358\n",
      "Epoch 00063: val_loss improved from 0.05075 to 0.05053, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.0424 - mean_squared_error: 0.0424 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 64/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0430 - mean_squared_error: 0.0430\n",
      "Epoch 00064: val_loss improved from 0.05053 to 0.05026, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 65/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0372 - mean_squared_error: 0.0372\n",
      "Epoch 00065: val_loss improved from 0.05026 to 0.04999, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0374 - mean_squared_error: 0.0374 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 66/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0401 - mean_squared_error: 0.0401\n",
      "Epoch 00066: val_loss improved from 0.04999 to 0.04977, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 67/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00067: val_loss improved from 0.04977 to 0.04963, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 68/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00068: val_loss improved from 0.04963 to 0.04950, saving model to model.h5\n",
      "248/248 [==============================] - 0s 138us/sample - loss: 0.0397 - mean_squared_error: 0.0397 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 69/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00069: val_loss improved from 0.04950 to 0.04945, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 70/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Epoch 00070: val_loss improved from 0.04945 to 0.04936, saving model to model.h5\n",
      "248/248 [==============================] - 0s 116us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 71/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0426 - mean_squared_error: 0.0426\n",
      "Epoch 00071: val_loss improved from 0.04936 to 0.04928, saving model to model.h5\n",
      "248/248 [==============================] - 0s 114us/sample - loss: 0.0374 - mean_squared_error: 0.0374 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 72/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0345 - mean_squared_error: 0.0345\n",
      "Epoch 00072: val_loss improved from 0.04928 to 0.04915, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0331 - mean_squared_error: 0.0331 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 73/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Epoch 00073: val_loss improved from 0.04915 to 0.04890, saving model to model.h5\n",
      "248/248 [==============================] - 0s 116us/sample - loss: 0.0396 - mean_squared_error: 0.0396 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 74/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00074: val_loss improved from 0.04890 to 0.04866, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 75/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00075: val_loss improved from 0.04866 to 0.04837, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 76/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00076: val_loss improved from 0.04837 to 0.04810, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 77/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Epoch 00077: val_loss improved from 0.04810 to 0.04793, saving model to model.h5\n",
      "248/248 [==============================] - 0s 133us/sample - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 78/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00078: val_loss improved from 0.04793 to 0.04779, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 79/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0362 - mean_squared_error: 0.0362\n",
      "Epoch 00079: val_loss improved from 0.04779 to 0.04762, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 80/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0388 - mean_squared_error: 0.0388\n",
      "Epoch 00080: val_loss improved from 0.04762 to 0.04746, saving model to model.h5\n",
      "248/248 [==============================] - 0s 118us/sample - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0403 - mean_squared_error: 0.0403\n",
      "Epoch 00081: val_loss improved from 0.04746 to 0.04735, saving model to model.h5\n",
      "248/248 [==============================] - 0s 127us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 82/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 00082: val_loss improved from 0.04735 to 0.04724, saving model to model.h5\n",
      "248/248 [==============================] - 0s 122us/sample - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 83/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00083: val_loss improved from 0.04724 to 0.04723, saving model to model.h5\n",
      "248/248 [==============================] - 0s 115us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 84/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Epoch 00084: val_loss did not improve from 0.04723\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 85/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0391 - mean_squared_error: 0.0391\n",
      "Epoch 00085: val_loss did not improve from 0.04723\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 86/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00086: val_loss improved from 0.04723 to 0.04713, saving model to model.h5\n",
      "248/248 [==============================] - 0s 122us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 87/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0397 - mean_squared_error: 0.0397\n",
      "Epoch 00087: val_loss improved from 0.04713 to 0.04706, saving model to model.h5\n",
      "248/248 [==============================] - 0s 116us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 88/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0399 - mean_squared_error: 0.0399\n",
      "Epoch 00088: val_loss improved from 0.04706 to 0.04692, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 89/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00089: val_loss improved from 0.04692 to 0.04673, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 90/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00090: val_loss improved from 0.04673 to 0.04656, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 91/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0341 - mean_squared_error: 0.0341\n",
      "Epoch 00091: val_loss improved from 0.04656 to 0.04643, saving model to model.h5\n",
      "248/248 [==============================] - 0s 139us/sample - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 92/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00092: val_loss improved from 0.04643 to 0.04633, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 93/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00093: val_loss improved from 0.04633 to 0.04631, saving model to model.h5\n",
      "248/248 [==============================] - 0s 120us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 94/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00094: val_loss improved from 0.04631 to 0.04619, saving model to model.h5\n",
      "248/248 [==============================] - 0s 130us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 95/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Epoch 00095: val_loss improved from 0.04619 to 0.04619, saving model to model.h5\n",
      "248/248 [==============================] - 0s 130us/sample - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 96/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0397 - mean_squared_error: 0.0397\n",
      "Epoch 00096: val_loss improved from 0.04619 to 0.04614, saving model to model.h5\n",
      "248/248 [==============================] - 0s 128us/sample - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 97/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0391 - mean_squared_error: 0.0391\n",
      "Epoch 00097: val_loss improved from 0.04614 to 0.04604, saving model to model.h5\n",
      "248/248 [==============================] - 0s 126us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 98/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00098: val_loss improved from 0.04604 to 0.04590, saving model to model.h5\n",
      "248/248 [==============================] - 0s 119us/sample - loss: 0.0320 - mean_squared_error: 0.0320 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 99/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00099: val_loss improved from 0.04590 to 0.04587, saving model to model.h5\n",
      "248/248 [==============================] - 0s 132us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 100/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00100: val_loss improved from 0.04587 to 0.04578, saving model to model.h5\n",
      "248/248 [==============================] - 0s 124us/sample - loss: 0.0323 - mean_squared_error: 0.0323 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 101/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00101: val_loss improved from 0.04578 to 0.04565, saving model to model.h5\n",
      "248/248 [==============================] - 0s 123us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 102/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0410 - mean_squared_error: 0.0410\n",
      "Epoch 00102: val_loss improved from 0.04565 to 0.04554, saving model to model.h5\n",
      "248/248 [==============================] - 0s 118us/sample - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 103/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00103: val_loss improved from 0.04554 to 0.04533, saving model to model.h5\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0308 - mean_squared_error: 0.0308 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 104/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00104: val_loss improved from 0.04533 to 0.04515, saving model to model.h5\n",
      "248/248 [==============================] - 0s 117us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00105: val_loss improved from 0.04515 to 0.04494, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 106/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Epoch 00106: val_loss improved from 0.04494 to 0.04480, saving model to model.h5\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 107/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00107: val_loss improved from 0.04480 to 0.04471, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 108/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0307 - mean_squared_error: 0.0307\n",
      "Epoch 00108: val_loss improved from 0.04471 to 0.04468, saving model to model.h5\n",
      "248/248 [==============================] - 0s 122us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 109/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00109: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 37us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 110/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00110: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 111/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0439 - mean_squared_error: 0.0439\n",
      "Epoch 00111: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 112/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00112: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 113/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00113: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 114/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00114: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 115/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0334 - mean_squared_error: 0.0334\n",
      "Epoch 00115: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 116/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00116: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 117/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00117: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 118/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0294 - mean_squared_error: 0.0294\n",
      "Epoch 00118: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 119/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00119: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0308 - mean_squared_error: 0.0308 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 120/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00120: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 35us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 121/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00121: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 122/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0339 - mean_squared_error: 0.0339\n",
      "Epoch 00122: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 123/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00123: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 124/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00124: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 125/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00125: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 126/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Epoch 00126: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 127/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00127: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 128/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00128: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 129/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0135 - mean_squared_error: 0.0135\n",
      "Epoch 00129: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 130/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00130: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00131: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 132/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00132: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 133/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0227 - mean_squared_error: 0.0227\n",
      "Epoch 00133: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 134/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00134: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 135/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00135: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 42us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 136/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00136: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 137/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00137: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 138/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00138: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 139/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0184 - mean_squared_error: 0.0184\n",
      "Epoch 00139: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 140/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0424 - mean_squared_error: 0.0424\n",
      "Epoch 00140: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 141/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0377 - mean_squared_error: 0.0377\n",
      "Epoch 00141: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 142/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00142: val_loss did not improve from 0.04468\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 143/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00143: val_loss improved from 0.04468 to 0.04457, saving model to model.h5\n",
      "248/248 [==============================] - 0s 134us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 144/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00144: val_loss improved from 0.04457 to 0.04440, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 145/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0404 - mean_squared_error: 0.0404\n",
      "Epoch 00145: val_loss improved from 0.04440 to 0.04437, saving model to model.h5\n",
      "248/248 [==============================] - 0s 125us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 146/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00146: val_loss improved from 0.04437 to 0.04434, saving model to model.h5\n",
      "248/248 [==============================] - 0s 153us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 147/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00147: val_loss improved from 0.04434 to 0.04430, saving model to model.h5\n",
      "248/248 [==============================] - 0s 141us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 148/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00148: val_loss did not improve from 0.04430\n",
      "248/248 [==============================] - 0s 34us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 149/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0181 - mean_squared_error: 0.0181\n",
      "Epoch 00149: val_loss improved from 0.04430 to 0.04429, saving model to model.h5\n",
      "248/248 [==============================] - 0s 121us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 150/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00150: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 151/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00151: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 152/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00152: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 153/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00153: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 154/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00154: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 155/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00155: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 156/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00156: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00157: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 37us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 158/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00158: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 38us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 159/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00159: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 37us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 160/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0344 - mean_squared_error: 0.0344\n",
      "Epoch 00160: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 31us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 161/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00161: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 162/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00162: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 163/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00163: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 164/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00164: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 165/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00165: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 35us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 166/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Epoch 00166: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 167/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00167: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 168/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00168: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 169/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00169: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 170/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00170: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 171/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0227 - mean_squared_error: 0.0227\n",
      "Epoch 00171: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 172/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00172: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 173/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00173: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 174/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0294 - mean_squared_error: 0.0294\n",
      "Epoch 00174: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 175/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Epoch 00175: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 176/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0404 - mean_squared_error: 0.0404\n",
      "Epoch 00176: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 177/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00177: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 178/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0156 - mean_squared_error: 0.0156\n",
      "Epoch 00178: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 179/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00179: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 180/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0152 - mean_squared_error: 0.0152\n",
      "Epoch 00180: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 181/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00181: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 182/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00182: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 183/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00183: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 184/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00184: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 35us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 185/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0341 - mean_squared_error: 0.0341\n",
      "Epoch 00185: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 186/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00186: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 187/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00187: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 188/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0153 - mean_squared_error: 0.0153\n",
      "Epoch 00188: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 34us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 189/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00189: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 190/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00190: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 191/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00191: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 192/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00192: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 193/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0171 - mean_squared_error: 0.0171\n",
      "Epoch 00193: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 194/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00194: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 195/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00195: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 196/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00196: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 197/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00197: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 198/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00198: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 199/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00199: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 200/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00200: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 201/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00201: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 202/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00202: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 203/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00203: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 204/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00204: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 205/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00205: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 206/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00206: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 207/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00207: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 37us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 208/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Epoch 00208: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 209/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00209: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 210/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00210: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 211/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00211: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 212/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00212: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 213/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00213: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 214/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0146 - mean_squared_error: 0.0146\n",
      "Epoch 00214: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 215/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00215: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 216/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0339 - mean_squared_error: 0.0339\n",
      "Epoch 00216: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 217/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00217: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 218/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00218: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 219/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00219: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 220/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00220: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 221/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00221: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 222/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00222: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 223/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00223: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 224/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00224: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 35us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 225/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00225: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 226/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00226: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 227/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00227: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 228/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00228: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 229/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0184 - mean_squared_error: 0.0184\n",
      "Epoch 00229: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 230/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00230: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 231/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00231: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 38us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 232/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0157 - mean_squared_error: 0.0157\n",
      "Epoch 00232: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 38us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 233/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00233: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 234/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00234: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 235/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00235: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 236/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00236: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 237/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00237: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 238/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00238: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 239/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00239: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 240/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00240: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 241/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00241: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 242/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00242: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 243/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00243: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 244/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0144 - mean_squared_error: 0.0144\n",
      "Epoch 00244: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 31us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 245/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Epoch 00245: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 246/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00246: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 247/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00247: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 32us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 248/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00248: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 249/1000\n",
      "128/248 [==============>...............] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00249: val_loss did not improve from 0.04429\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n"
     ]
    }
   ],
   "source": [
    "test_pred_gru  = []\n",
    "\n",
    "for fold in range(trials):\n",
    "    print(f'Running trial {fold+1}')\n",
    "    print(X_train.shape)\n",
    "    \n",
    "    X_t_reshaped   = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_val_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "   \n",
    "\n",
    "    check = ModelCheckpoint('model.h5', verbose=1, save_best_only=True)\n",
    "\n",
    "    early = EarlyStopping(patience=100)\n",
    "\n",
    "    optimizer = Adam(lr=0.00001)\n",
    "    model = Sequential()\n",
    "    model.add(GRU(50, input_shape=(1, 6)))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(loss='mse', optimizer='Adam', metrics=['mse'])\n",
    "\n",
    "    \n",
    "\n",
    "    history = model.fit(X_t_reshaped, \n",
    "                    y_train, \n",
    "                    validation_split = 0.2,\n",
    "                    epochs=1000, \n",
    "                    batch_size=128, \n",
    "                    verbose=1, callbacks=[check, early])\n",
    "    X_val_reshaped = X_val_reshaped.reshape(X_test.shape[0], 6)\n",
    "\n",
    "    #running function\n",
    "    test_forecast = X_test\n",
    "    y_gru = forecast(model, test_forecast, gwl, steps_ahead)\n",
    "    y_gru = np.array(y_gru)\n",
    "    \n",
    "    #metrics for test\n",
    "    test_pred_gru.append(y_gru)\n",
    "    mse_gru = mean_squared_error(y_test, y_gru)\n",
    "    r2_gru = r2_score(y_test, y_gru)\n",
    "    gru_ave_r2.append(r2_gru)\n",
    "    gru_ave_mse.append(mse_gru)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c9f2b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_r2_new = ave(gru_ave_r2)\n",
    "gru_mse_new = ave(gru_ave_mse)\n",
    "gru_ave_r2\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca84c7d9",
   "metadata": {},
   "source": [
    "## FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "618260f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 0\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9340 - mean_squared_error: 0.9340\n",
      "Epoch 00001: val_loss improved from inf to 0.38777, saving model to model.h5\n",
      "248/248 [==============================] - 2s 9ms/sample - loss: 0.7980 - mean_squared_error: 0.7980 - val_loss: 0.3878 - val_mean_squared_error: 0.3878\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5969 - mean_squared_error: 0.5969\n",
      "Epoch 00002: val_loss improved from 0.38777 to 0.27855, saving model to model.h5\n",
      "248/248 [==============================] - 0s 145us/sample - loss: 0.5246 - mean_squared_error: 0.5246 - val_loss: 0.2786 - val_mean_squared_error: 0.2786\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3433 - mean_squared_error: 0.3433\n",
      "Epoch 00003: val_loss improved from 0.27855 to 0.20960, saving model to model.h5\n",
      "248/248 [==============================] - 0s 158us/sample - loss: 0.3417 - mean_squared_error: 0.3417 - val_loss: 0.2096 - val_mean_squared_error: 0.2096\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2558 - mean_squared_error: 0.2558\n",
      "Epoch 00004: val_loss improved from 0.20960 to 0.15113, saving model to model.h5\n",
      "248/248 [==============================] - 0s 146us/sample - loss: 0.2113 - mean_squared_error: 0.2113 - val_loss: 0.1511 - val_mean_squared_error: 0.1511\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1750 - mean_squared_error: 0.1750\n",
      "Epoch 00005: val_loss improved from 0.15113 to 0.10603, saving model to model.h5\n",
      "248/248 [==============================] - 0s 147us/sample - loss: 0.1355 - mean_squared_error: 0.1355 - val_loss: 0.1060 - val_mean_squared_error: 0.1060\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1057 - mean_squared_error: 0.1057\n",
      "Epoch 00006: val_loss improved from 0.10603 to 0.07666, saving model to model.h5\n",
      "248/248 [==============================] - 0s 157us/sample - loss: 0.0834 - mean_squared_error: 0.0834 - val_loss: 0.0767 - val_mean_squared_error: 0.0767\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0580 - mean_squared_error: 0.0580\n",
      "Epoch 00007: val_loss improved from 0.07666 to 0.05814, saving model to model.h5\n",
      "248/248 [==============================] - 0s 148us/sample - loss: 0.0588 - mean_squared_error: 0.0588 - val_loss: 0.0581 - val_mean_squared_error: 0.0581\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0432 - mean_squared_error: 0.0432\n",
      "Epoch 00008: val_loss improved from 0.05814 to 0.05306, saving model to model.h5\n",
      "248/248 [==============================] - 0s 161us/sample - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0531 - val_mean_squared_error: 0.0531\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0472 - mean_squared_error: 0.0472\n",
      "Epoch 00009: val_loss improved from 0.05306 to 0.05048, saving model to model.h5\n",
      "248/248 [==============================] - 0s 148us/sample - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Epoch 00010: val_loss improved from 0.05048 to 0.05016, saving model to model.h5\n",
      "248/248 [==============================] - 0s 144us/sample - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00011: val_loss improved from 0.05016 to 0.04478, saving model to model.h5\n",
      "248/248 [==============================] - 0s 151us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00012: val_loss improved from 0.04478 to 0.04184, saving model to model.h5\n",
      "248/248 [==============================] - 0s 158us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00013: val_loss did not improve from 0.04184\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00014: val_loss did not improve from 0.04184\n",
      "248/248 [==============================] - 0s 47us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00015: val_loss did not improve from 0.04184\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00016: val_loss did not improve from 0.04184\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00017: val_loss improved from 0.04184 to 0.03997, saving model to model.h5\n",
      "248/248 [==============================] - 0s 152us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00018: val_loss did not improve from 0.03997\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00019: val_loss did not improve from 0.03997\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0541 - val_mean_squared_error: 0.0541\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 00020: val_loss did not improve from 0.03997\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00021: val_loss did not improve from 0.03997\n",
      "248/248 [==============================] - 0s 42us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00022: val_loss did not improve from 0.03997\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00023: val_loss did not improve from 0.03997\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0555 - val_mean_squared_error: 0.0555\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0153 - mean_squared_error: 0.0153\n",
      "Epoch 00024: val_loss did not improve from 0.03997\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0587 - val_mean_squared_error: 0.0587\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00025: val_loss did not improve from 0.03997\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00026: val_loss did not improve from 0.03997\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00027: val_loss did not improve from 0.03997\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00028: val_loss did not improve from 0.03997\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Epoch 00029: val_loss did not improve from 0.03997\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00030: val_loss did not improve from 0.03997\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0146 - mean_squared_error: 0.0146\n",
      "Epoch 00031: val_loss did not improve from 0.03997\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00032: val_loss did not improve from 0.03997\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00033: val_loss did not improve from 0.03997\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00034: val_loss improved from 0.03997 to 0.03989, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00035: val_loss did not improve from 0.03989\n",
      "248/248 [==============================] - 0s 46us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0686 - val_mean_squared_error: 0.0686\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00036: val_loss did not improve from 0.03989\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0174 - mean_squared_error: 0.0174\n",
      "Epoch 00037: val_loss did not improve from 0.03989\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0530 - val_mean_squared_error: 0.0530\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00038: val_loss did not improve from 0.03989\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0587 - val_mean_squared_error: 0.0587\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00039: val_loss did not improve from 0.03989\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00040: val_loss did not improve from 0.03989\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00041: val_loss did not improve from 0.03989\n",
      "248/248 [==============================] - 0s 49us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0577 - val_mean_squared_error: 0.0577\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00042: val_loss did not improve from 0.03989\n",
      "248/248 [==============================] - 0s 42us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00043: val_loss did not improve from 0.03989\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00044: val_loss did not improve from 0.03989\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00045: val_loss did not improve from 0.03989\n",
      "248/248 [==============================] - 0s 43us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00046: val_loss did not improve from 0.03989\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00047: val_loss did not improve from 0.03989\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0136 - mean_squared_error: 0.0136\n",
      "Epoch 00048: val_loss did not improve from 0.03989\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0538 - val_mean_squared_error: 0.0538\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0176 - mean_squared_error: 0.0176\n",
      "Epoch 00049: val_loss did not improve from 0.03989\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0548 - val_mean_squared_error: 0.0548\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00050: val_loss did not improve from 0.03989\n",
      "248/248 [==============================] - 0s 42us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00051: val_loss did not improve from 0.03989\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0129 - mean_squared_error: 0.0129\n",
      "Epoch 00052: val_loss did not improve from 0.03989\n",
      "248/248 [==============================] - 0s 41us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 00053: val_loss did not improve from 0.03989\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00054: val_loss did not improve from 0.03989\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0575 - val_mean_squared_error: 0.0575\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00055: val_loss did not improve from 0.03989\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00056: val_loss did not improve from 0.03989\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00057: val_loss improved from 0.03989 to 0.03957, saving model to model.h5\n",
      "248/248 [==============================] - 0s 194us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00058: val_loss did not improve from 0.03957\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00059: val_loss did not improve from 0.03957\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00060: val_loss did not improve from 0.03957\n",
      "248/248 [==============================] - 0s 47us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00061: val_loss did not improve from 0.03957\n",
      "248/248 [==============================] - 0s 45us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00062: val_loss did not improve from 0.03957\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0526 - val_mean_squared_error: 0.0526\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00063: val_loss did not improve from 0.03957\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0520 - val_mean_squared_error: 0.0520\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00064: val_loss did not improve from 0.03957\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00065: val_loss did not improve from 0.03957\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0618 - val_mean_squared_error: 0.0618\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00066: val_loss did not improve from 0.03957\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00067: val_loss did not improve from 0.03957\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0170 - mean_squared_error: 0.0170\n",
      "Epoch 00068: val_loss did not improve from 0.03957\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00069: val_loss did not improve from 0.03957\n",
      "248/248 [==============================] - 0s 43us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0372 - mean_squared_error: 0.0372\n",
      "Epoch 00070: val_loss did not improve from 0.03957\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00071: val_loss did not improve from 0.03957\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00072: val_loss did not improve from 0.03957\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00073: val_loss did not improve from 0.03957\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00074: val_loss did not improve from 0.03957\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0679 - val_mean_squared_error: 0.0679\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00075: val_loss did not improve from 0.03957\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00076: val_loss did not improve from 0.03957\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00077: val_loss did not improve from 0.03957\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0155 - mean_squared_error: 0.0155\n",
      "Epoch 00078: val_loss did not improve from 0.03957\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0606 - val_mean_squared_error: 0.0606\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00079: val_loss did not improve from 0.03957\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0539 - val_mean_squared_error: 0.0539\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00080: val_loss did not improve from 0.03957\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0623 - val_mean_squared_error: 0.0623\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00081: val_loss improved from 0.03957 to 0.03947, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0333 - mean_squared_error: 0.0333\n",
      "Epoch 00082: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00083: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0520 - val_mean_squared_error: 0.0520\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00084: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00085: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0578 - val_mean_squared_error: 0.0578\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00086: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 41us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0556 - val_mean_squared_error: 0.0556\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00087: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0146 - mean_squared_error: 0.0146\n",
      "Epoch 00088: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00089: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0514 - val_mean_squared_error: 0.0514\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0108 - mean_squared_error: 0.0108\n",
      "Epoch 00090: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0307 - mean_squared_error: 0.0307\n",
      "Epoch 00091: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00092: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0578 - val_mean_squared_error: 0.0578\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00093: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00094: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00095: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00096: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 36us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0527 - val_mean_squared_error: 0.0527\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0153 - mean_squared_error: 0.0153\n",
      "Epoch 00097: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 45us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0130 - mean_squared_error: 0.0130\n",
      "Epoch 00098: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0641 - val_mean_squared_error: 0.0641\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00099: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0150 - mean_squared_error: 0.0150\n",
      "Epoch 00100: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 46us/sample - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0602 - val_mean_squared_error: 0.0602\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00101: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0173 - mean_squared_error: 0.0173\n",
      "Epoch 00102: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00103: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 41us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0602 - val_mean_squared_error: 0.0602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00104: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0084 - mean_squared_error: 0.0084\n",
      "Epoch 00105: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00106: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 43us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0545 - val_mean_squared_error: 0.0545\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0167 - mean_squared_error: 0.0167\n",
      "Epoch 00107: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0602 - val_mean_squared_error: 0.0602\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00108: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 39us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00109: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00110: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0534 - val_mean_squared_error: 0.0534\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00111: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 47us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00112: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0555 - val_mean_squared_error: 0.0555\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00113: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 38us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00114: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 39us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00115: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00116: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0572 - val_mean_squared_error: 0.0572\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Epoch 00117: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0399 - mean_squared_error: 0.0399\n",
      "Epoch 00118: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0548 - val_mean_squared_error: 0.0548\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00119: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0564 - val_mean_squared_error: 0.0564\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00120: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0641 - val_mean_squared_error: 0.0641\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00121: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0562 - val_mean_squared_error: 0.0562\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0161 - mean_squared_error: 0.0161\n",
      "Epoch 00122: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0603 - val_mean_squared_error: 0.0603\n",
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00123: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0103 - mean_squared_error: 0.0103\n",
      "Epoch 00124: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0619 - val_mean_squared_error: 0.0619\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0151 - mean_squared_error: 0.0151\n",
      "Epoch 00125: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00126: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0113 - mean_squared_error: 0.0113\n",
      "Epoch 00127: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0554 - val_mean_squared_error: 0.0554\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0155 - mean_squared_error: 0.0155\n",
      "Epoch 00128: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00129: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0573 - val_mean_squared_error: 0.0573\n",
      "Epoch 130/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0137 - mean_squared_error: 0.0137\n",
      "Epoch 00130: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0583 - val_mean_squared_error: 0.0583\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00131: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 45us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0579 - val_mean_squared_error: 0.0579\n",
      "Epoch 132/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00132: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0509 - val_mean_squared_error: 0.0509\n",
      "Epoch 133/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00133: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 134/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00134: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 42us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0615 - val_mean_squared_error: 0.0615\n",
      "Epoch 135/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0133 - mean_squared_error: 0.0133\n",
      "Epoch 00135: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 45us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 136/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00136: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 137/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00137: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 138/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00138: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 139/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0088 - mean_squared_error: 0.0088\n",
      "Epoch 00139: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0576 - val_mean_squared_error: 0.0576\n",
      "Epoch 140/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Epoch 00140: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n",
      "Epoch 141/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0129 - mean_squared_error: 0.0129\n",
      "Epoch 00141: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 142/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00142: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0615 - val_mean_squared_error: 0.0615\n",
      "Epoch 143/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00143: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0612 - val_mean_squared_error: 0.0612\n",
      "Epoch 144/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0102 - mean_squared_error: 0.0102\n",
      "Epoch 00144: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0592 - val_mean_squared_error: 0.0592\n",
      "Epoch 145/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00145: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 146/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00146: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0578 - val_mean_squared_error: 0.0578\n",
      "Epoch 147/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00147: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 42us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 148/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0156 - mean_squared_error: 0.0156\n",
      "Epoch 00148: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 149/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00149: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 43us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 150/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00150: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 151/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00151: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 152/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00152: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0622 - val_mean_squared_error: 0.0622\n",
      "Epoch 153/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00153: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0564 - val_mean_squared_error: 0.0564\n",
      "Epoch 154/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0161 - mean_squared_error: 0.0161\n",
      "Epoch 00154: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 155/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00155: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 156/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0171 - mean_squared_error: 0.0171\n",
      "Epoch 00156: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0536 - val_mean_squared_error: 0.0536\n",
      "Epoch 157/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0139 - mean_squared_error: 0.0139\n",
      "Epoch 00157: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 158/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00158: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 159/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00159: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0633 - val_mean_squared_error: 0.0633\n",
      "Epoch 160/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00160: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 161/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00161: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0514 - val_mean_squared_error: 0.0514\n",
      "Epoch 162/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0151 - mean_squared_error: 0.0151\n",
      "Epoch 00162: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 45us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 163/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00163: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 164/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00164: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 165/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00165: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0543 - val_mean_squared_error: 0.0543\n",
      "Epoch 166/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00166: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0513 - val_mean_squared_error: 0.0513\n",
      "Epoch 167/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00167: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0585 - val_mean_squared_error: 0.0585\n",
      "Epoch 168/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00168: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 169/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00169: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 170/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00170: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0527 - val_mean_squared_error: 0.0527\n",
      "Epoch 171/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00171: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0565 - val_mean_squared_error: 0.0565\n",
      "Epoch 172/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00172: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 173/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00173: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 174/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00174: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 175/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0155 - mean_squared_error: 0.0155\n",
      "Epoch 00175: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 43us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0572 - val_mean_squared_error: 0.0572\n",
      "Epoch 176/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0109 - mean_squared_error: 0.0109\n",
      "Epoch 00176: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 177/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0132 - mean_squared_error: 0.0132\n",
      "Epoch 00177: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 178/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00178: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 179/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0135 - mean_squared_error: 0.0135\n",
      "Epoch 00179: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0575 - val_mean_squared_error: 0.0575\n",
      "Epoch 180/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0109 - mean_squared_error: 0.0109\n",
      "Epoch 00180: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0653 - val_mean_squared_error: 0.0653\n",
      "Epoch 181/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00181: val_loss did not improve from 0.03947\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0160 - mean_squared_error: 0.0160 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Running fold 1\n",
      "(310, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.3620 - mean_squared_error: 1.3620\n",
      "Epoch 00001: val_loss improved from inf to 0.71913, saving model to model.h5\n",
      "248/248 [==============================] - 2s 10ms/sample - loss: 1.4335 - mean_squared_error: 1.4335 - val_loss: 0.7191 - val_mean_squared_error: 0.7191\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9567 - mean_squared_error: 0.9567\n",
      "Epoch 00002: val_loss improved from 0.71913 to 0.64469, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 1.1272 - mean_squared_error: 1.1272 - val_loss: 0.6447 - val_mean_squared_error: 0.6447\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.0417 - mean_squared_error: 1.0417\n",
      "Epoch 00003: val_loss improved from 0.64469 to 0.56050, saving model to model.h5\n",
      "248/248 [==============================] - 0s 191us/sample - loss: 1.0030 - mean_squared_error: 1.0030 - val_loss: 0.5605 - val_mean_squared_error: 0.5605\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9984 - mean_squared_error: 0.9984\n",
      "Epoch 00004: val_loss improved from 0.56050 to 0.43967, saving model to model.h5\n",
      "248/248 [==============================] - 0s 185us/sample - loss: 0.9027 - mean_squared_error: 0.9027 - val_loss: 0.4397 - val_mean_squared_error: 0.4397\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8364 - mean_squared_error: 0.8364\n",
      "Epoch 00005: val_loss improved from 0.43967 to 0.32145, saving model to model.h5\n",
      "248/248 [==============================] - 0s 176us/sample - loss: 0.7966 - mean_squared_error: 0.7966 - val_loss: 0.3215 - val_mean_squared_error: 0.3215\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6834 - mean_squared_error: 0.6834\n",
      "Epoch 00006: val_loss improved from 0.32145 to 0.20662, saving model to model.h5\n",
      "248/248 [==============================] - 0s 184us/sample - loss: 0.6754 - mean_squared_error: 0.6754 - val_loss: 0.2066 - val_mean_squared_error: 0.2066\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6256 - mean_squared_error: 0.6256\n",
      "Epoch 00007: val_loss improved from 0.20662 to 0.12826, saving model to model.h5\n",
      "248/248 [==============================] - 0s 176us/sample - loss: 0.5283 - mean_squared_error: 0.5283 - val_loss: 0.1283 - val_mean_squared_error: 0.1283\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3835 - mean_squared_error: 0.3835\n",
      "Epoch 00008: val_loss improved from 0.12826 to 0.08204, saving model to model.h5\n",
      "248/248 [==============================] - 0s 174us/sample - loss: 0.3563 - mean_squared_error: 0.3563 - val_loss: 0.0820 - val_mean_squared_error: 0.0820\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2898 - mean_squared_error: 0.2898\n",
      "Epoch 00009: val_loss improved from 0.08204 to 0.06757, saving model to model.h5\n",
      "248/248 [==============================] - 0s 181us/sample - loss: 0.2050 - mean_squared_error: 0.2050 - val_loss: 0.0676 - val_mean_squared_error: 0.0676\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1003 - mean_squared_error: 0.1003\n",
      "Epoch 00010: val_loss improved from 0.06757 to 0.05460, saving model to model.h5\n",
      "248/248 [==============================] - 0s 180us/sample - loss: 0.1021 - mean_squared_error: 0.1021 - val_loss: 0.0546 - val_mean_squared_error: 0.0546\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0479 - mean_squared_error: 0.0479\n",
      "Epoch 00011: val_loss improved from 0.05460 to 0.04476, saving model to model.h5\n",
      "248/248 [==============================] - 0s 174us/sample - loss: 0.0522 - mean_squared_error: 0.0522 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00012: val_loss improved from 0.04476 to 0.04428, saving model to model.h5\n",
      "248/248 [==============================] - 0s 171us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0362 - mean_squared_error: 0.0362\n",
      "Epoch 00013: val_loss improved from 0.04428 to 0.03554, saving model to model.h5\n",
      "248/248 [==============================] - 0s 184us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0355 - val_mean_squared_error: 0.0355\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00014: val_loss did not improve from 0.03554\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00015: val_loss did not improve from 0.03554\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00016: val_loss did not improve from 0.03554\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00017: val_loss did not improve from 0.03554\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0632 - val_mean_squared_error: 0.0632\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00018: val_loss did not improve from 0.03554\n",
      "248/248 [==============================] - 0s 47us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00019: val_loss did not improve from 0.03554\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00020: val_loss did not improve from 0.03554\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0361 - val_mean_squared_error: 0.0361\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00021: val_loss did not improve from 0.03554\n",
      "248/248 [==============================] - 0s 45us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Epoch 00022: val_loss did not improve from 0.03554\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0151 - mean_squared_error: 0.0151\n",
      "Epoch 00023: val_loss did not improve from 0.03554\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00024: val_loss did not improve from 0.03554\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Epoch 00025: val_loss did not improve from 0.03554\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00026: val_loss did not improve from 0.03554\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00027: val_loss did not improve from 0.03554\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Epoch 00028: val_loss improved from 0.03554 to 0.03452, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0345 - val_mean_squared_error: 0.0345\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00029: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00030: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00031: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0110 - mean_squared_error: 0.0110\n",
      "Epoch 00032: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00033: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0365 - val_mean_squared_error: 0.0365\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0082 - mean_squared_error: 0.0082\n",
      "Epoch 00034: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0759 - val_mean_squared_error: 0.0759\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0173 - mean_squared_error: 0.0173\n",
      "Epoch 00035: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0509 - val_mean_squared_error: 0.0509\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00036: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0650 - val_mean_squared_error: 0.0650\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00037: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00038: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0543 - val_mean_squared_error: 0.0543\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0143 - mean_squared_error: 0.0143\n",
      "Epoch 00039: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00040: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0556 - val_mean_squared_error: 0.0556\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00041: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0141 - mean_squared_error: 0.0141\n",
      "Epoch 00042: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0603 - val_mean_squared_error: 0.0603\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00043: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00044: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 46us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0567 - val_mean_squared_error: 0.0567\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00045: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00046: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0660 - val_mean_squared_error: 0.0660\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00047: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00048: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0155 - mean_squared_error: 0.0155\n",
      "Epoch 00049: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0152 - mean_squared_error: 0.0152\n",
      "Epoch 00050: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0100 - mean_squared_error: 0.0100\n",
      "Epoch 00051: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0573 - val_mean_squared_error: 0.0573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0098 - mean_squared_error: 0.0098\n",
      "Epoch 00052: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00053: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0572 - val_mean_squared_error: 0.0572\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0259 - mean_squared_error: 0.0259\n",
      "Epoch 00054: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 47us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00055: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0513 - val_mean_squared_error: 0.0513\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00056: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00057: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00058: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00059: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00060: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 46us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0176 - mean_squared_error: 0.0176\n",
      "Epoch 00061: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0683 - val_mean_squared_error: 0.0683\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00062: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00063: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0100 - mean_squared_error: 0.0100\n",
      "Epoch 00064: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0653 - val_mean_squared_error: 0.0653\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00065: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0568 - val_mean_squared_error: 0.0568\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0227 - mean_squared_error: 0.0227\n",
      "Epoch 00066: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00067: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0620 - val_mean_squared_error: 0.0620\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00068: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0121 - mean_squared_error: 0.0121\n",
      "Epoch 00069: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0548 - val_mean_squared_error: 0.0548\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00070: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0658 - val_mean_squared_error: 0.0658\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00071: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0534 - val_mean_squared_error: 0.0534\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00072: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0156 - mean_squared_error: 0.0156\n",
      "Epoch 00073: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0563 - val_mean_squared_error: 0.0563\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00074: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0141 - mean_squared_error: 0.0141\n",
      "Epoch 00075: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00076: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Epoch 00077: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 78/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00078: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00079: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 46us/sample - loss: 0.0173 - mean_squared_error: 0.0173 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00080: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 47us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0624 - val_mean_squared_error: 0.0624\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00081: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0579 - val_mean_squared_error: 0.0579\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00082: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00083: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 47us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0579 - val_mean_squared_error: 0.0579\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00084: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00085: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00086: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 42us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0568 - val_mean_squared_error: 0.0568\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00087: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0616 - val_mean_squared_error: 0.0616\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00088: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00089: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0509 - val_mean_squared_error: 0.0509\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0115 - mean_squared_error: 0.0115\n",
      "Epoch 00090: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0653 - val_mean_squared_error: 0.0653\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00091: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00092: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0681 - val_mean_squared_error: 0.0681\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00093: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0534 - val_mean_squared_error: 0.0534\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00094: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0083 - mean_squared_error: 0.0083\n",
      "Epoch 00095: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0565 - val_mean_squared_error: 0.0565\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00096: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0516 - val_mean_squared_error: 0.0516\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00097: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00098: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00099: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 46us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00100: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 45us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0526 - val_mean_squared_error: 0.0526\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00101: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 47us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0574 - val_mean_squared_error: 0.0574\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00102: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0596 - val_mean_squared_error: 0.0596\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00103: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 50us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0730 - val_mean_squared_error: 0.0730\n",
      "Epoch 104/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0108 - mean_squared_error: 0.0108\n",
      "Epoch 00104: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 46us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0557 - val_mean_squared_error: 0.0557\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00105: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00106: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0602 - val_mean_squared_error: 0.0602\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0135 - mean_squared_error: 0.0135\n",
      "Epoch 00107: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0122 - mean_squared_error: 0.0122\n",
      "Epoch 00108: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0614 - val_mean_squared_error: 0.0614\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00109: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0627 - val_mean_squared_error: 0.0627\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0093 - mean_squared_error: 0.0093\n",
      "Epoch 00110: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0873 - val_mean_squared_error: 0.0873\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0295 - mean_squared_error: 0.0295\n",
      "Epoch 00111: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0530 - val_mean_squared_error: 0.0530\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00112: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 40us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0628 - val_mean_squared_error: 0.0628\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00113: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00114: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00115: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0525 - val_mean_squared_error: 0.0525\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0086 - mean_squared_error: 0.0086\n",
      "Epoch 00116: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0178 - mean_squared_error: 0.0178\n",
      "Epoch 00117: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0564 - val_mean_squared_error: 0.0564\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00118: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0159 - mean_squared_error: 0.0159 - val_loss: 0.0662 - val_mean_squared_error: 0.0662\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00119: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Epoch 00120: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0693 - val_mean_squared_error: 0.0693\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0140 - mean_squared_error: 0.0140\n",
      "Epoch 00121: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 45us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0569 - val_mean_squared_error: 0.0569\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00122: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0554 - val_mean_squared_error: 0.0554\n",
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00123: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0526 - val_mean_squared_error: 0.0526\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00124: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00125: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0639 - val_mean_squared_error: 0.0639\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00126: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0082 - mean_squared_error: 0.0082\n",
      "Epoch 00127: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0088 - mean_squared_error: 0.0088\n",
      "Epoch 00128: val_loss did not improve from 0.03452\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Running fold 2\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8384 - mean_squared_error: 0.8384\n",
      "Epoch 00001: val_loss improved from inf to 0.75582, saving model to model.h5\n",
      "248/248 [==============================] - 3s 11ms/sample - loss: 0.8348 - mean_squared_error: 0.8348 - val_loss: 0.7558 - val_mean_squared_error: 0.7558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5613 - mean_squared_error: 0.5613\n",
      "Epoch 00002: val_loss improved from 0.75582 to 0.69341, saving model to model.h5\n",
      "248/248 [==============================] - 0s 207us/sample - loss: 0.4651 - mean_squared_error: 0.4651 - val_loss: 0.6934 - val_mean_squared_error: 0.6934\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3172 - mean_squared_error: 0.3172\n",
      "Epoch 00003: val_loss improved from 0.69341 to 0.62769, saving model to model.h5\n",
      "248/248 [==============================] - 0s 211us/sample - loss: 0.2773 - mean_squared_error: 0.2773 - val_loss: 0.6277 - val_mean_squared_error: 0.6277\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2292 - mean_squared_error: 0.2292\n",
      "Epoch 00004: val_loss improved from 0.62769 to 0.53488, saving model to model.h5\n",
      "248/248 [==============================] - 0s 208us/sample - loss: 0.2231 - mean_squared_error: 0.2231 - val_loss: 0.5349 - val_mean_squared_error: 0.5349\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1882 - mean_squared_error: 0.1882\n",
      "Epoch 00005: val_loss improved from 0.53488 to 0.41091, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 0.1807 - mean_squared_error: 0.1807 - val_loss: 0.4109 - val_mean_squared_error: 0.4109\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1067 - mean_squared_error: 0.1067\n",
      "Epoch 00006: val_loss improved from 0.41091 to 0.26777, saving model to model.h5\n",
      "248/248 [==============================] - 0s 209us/sample - loss: 0.1266 - mean_squared_error: 0.1266 - val_loss: 0.2678 - val_mean_squared_error: 0.2678\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0994 - mean_squared_error: 0.0994\n",
      "Epoch 00007: val_loss improved from 0.26777 to 0.14657, saving model to model.h5\n",
      "248/248 [==============================] - 0s 209us/sample - loss: 0.0791 - mean_squared_error: 0.0791 - val_loss: 0.1466 - val_mean_squared_error: 0.1466\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0576 - mean_squared_error: 0.0576\n",
      "Epoch 00008: val_loss improved from 0.14657 to 0.07343, saving model to model.h5\n",
      "248/248 [==============================] - 0s 212us/sample - loss: 0.0422 - mean_squared_error: 0.0422 - val_loss: 0.0734 - val_mean_squared_error: 0.0734\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00009: val_loss improved from 0.07343 to 0.04950, saving model to model.h5\n",
      "248/248 [==============================] - 0s 215us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00010: val_loss improved from 0.04950 to 0.04247, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00011: val_loss did not improve from 0.04247\n",
      "248/248 [==============================] - 0s 50us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00012: val_loss did not improve from 0.04247\n",
      "248/248 [==============================] - 0s 53us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00013: val_loss improved from 0.04247 to 0.03839, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00014: val_loss improved from 0.03839 to 0.03503, saving model to model.h5\n",
      "248/248 [==============================] - 0s 208us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0350 - val_mean_squared_error: 0.0350\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00015: val_loss did not improve from 0.03503\n",
      "248/248 [==============================] - 0s 55us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00016: val_loss did not improve from 0.03503\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0654 - val_mean_squared_error: 0.0654\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00017: val_loss did not improve from 0.03503\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0101 - mean_squared_error: 0.0101\n",
      "Epoch 00018: val_loss improved from 0.03503 to 0.02884, saving model to model.h5\n",
      "248/248 [==============================] - 0s 217us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0288 - val_mean_squared_error: 0.0288\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00019: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00020: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0108 - mean_squared_error: 0.0108\n",
      "Epoch 00021: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0150 - mean_squared_error: 0.0150\n",
      "Epoch 00022: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00023: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0102 - mean_squared_error: 0.0102\n",
      "Epoch 00024: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 61us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00025: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0538 - val_mean_squared_error: 0.0538\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00026: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00027: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00028: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0333 - val_mean_squared_error: 0.0333\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00029: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0575 - val_mean_squared_error: 0.0575\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00030: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00031: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0679 - val_mean_squared_error: 0.0679\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00032: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0145 - mean_squared_error: 0.0145\n",
      "Epoch 00033: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0872 - val_mean_squared_error: 0.0872\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00034: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00035: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0116 - mean_squared_error: 0.0116\n",
      "Epoch 00036: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0358 - val_mean_squared_error: 0.0358\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00037: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0578 - val_mean_squared_error: 0.0578\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0159 - mean_squared_error: 0.0159\n",
      "Epoch 00038: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00039: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00040: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00041: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0160 - mean_squared_error: 0.0160\n",
      "Epoch 00042: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0156 - mean_squared_error: 0.0156\n",
      "Epoch 00043: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0374 - mean_squared_error: 0.0374\n",
      "Epoch 00044: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 53us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00045: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0548 - val_mean_squared_error: 0.0548\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0146 - mean_squared_error: 0.0146\n",
      "Epoch 00046: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0477 - mean_squared_error: 0.0477\n",
      "Epoch 00047: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0671 - val_mean_squared_error: 0.0671\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00048: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0150 - mean_squared_error: 0.0150\n",
      "Epoch 00049: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0604 - val_mean_squared_error: 0.0604\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00050: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0757 - val_mean_squared_error: 0.0757\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00051: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00052: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0557 - val_mean_squared_error: 0.0557\n",
      "Epoch 53/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00053: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0676 - val_mean_squared_error: 0.0676\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00054: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0570 - val_mean_squared_error: 0.0570\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Epoch 00055: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 51us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0610 - val_mean_squared_error: 0.0610\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.0185\n",
      "Epoch 00056: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 51us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0122 - mean_squared_error: 0.0122\n",
      "Epoch 00057: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0162 - mean_squared_error: 0.0162 - val_loss: 0.0669 - val_mean_squared_error: 0.0669\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00058: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 51us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0604 - val_mean_squared_error: 0.0604\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00059: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00060: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0556 - val_mean_squared_error: 0.0556\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00061: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00062: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 61us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0509 - val_mean_squared_error: 0.0509\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0141 - mean_squared_error: 0.0141\n",
      "Epoch 00063: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0765 - val_mean_squared_error: 0.0765\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00064: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00065: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0138 - mean_squared_error: 0.0138\n",
      "Epoch 00066: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0601 - val_mean_squared_error: 0.0601\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00067: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00068: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00069: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0740 - val_mean_squared_error: 0.0740\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00070: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00071: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0623 - val_mean_squared_error: 0.0623\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00072: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0673 - val_mean_squared_error: 0.0673\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 00073: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0590 - val_mean_squared_error: 0.0590\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00074: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.0185\n",
      "Epoch 00075: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00076: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0705 - val_mean_squared_error: 0.0705\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00077: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00078: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 50us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0601 - val_mean_squared_error: 0.0601\n",
      "Epoch 79/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00079: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0121 - mean_squared_error: 0.0121\n",
      "Epoch 00080: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0166 - mean_squared_error: 0.0166 - val_loss: 0.0845 - val_mean_squared_error: 0.0845\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00081: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 50us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00082: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0575 - val_mean_squared_error: 0.0575\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00083: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0113 - mean_squared_error: 0.0113\n",
      "Epoch 00084: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0169 - mean_squared_error: 0.0169 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00085: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0184 - mean_squared_error: 0.0184\n",
      "Epoch 00086: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0175 - mean_squared_error: 0.0175 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0181 - mean_squared_error: 0.0181\n",
      "Epoch 00087: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0581 - val_mean_squared_error: 0.0581\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00088: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0091 - mean_squared_error: 0.0091\n",
      "Epoch 00089: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0513 - val_mean_squared_error: 0.0513\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00090: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 49us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00091: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0721 - val_mean_squared_error: 0.0721\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00092: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00093: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 50us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00094: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0716 - val_mean_squared_error: 0.0716\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00095: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0534 - val_mean_squared_error: 0.0534\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00096: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00097: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0559 - val_mean_squared_error: 0.0559\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00098: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0104 - mean_squared_error: 0.0104\n",
      "Epoch 00099: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00100: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0643 - val_mean_squared_error: 0.0643\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00101: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0162 - mean_squared_error: 0.0162 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00102: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 46us/sample - loss: 0.0170 - mean_squared_error: 0.0170 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00103: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0643 - val_mean_squared_error: 0.0643\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0158 - mean_squared_error: 0.0158\n",
      "Epoch 00104: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 50us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0804 - val_mean_squared_error: 0.0804\n",
      "Epoch 105/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00105: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0619 - val_mean_squared_error: 0.0619\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00106: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0656 - val_mean_squared_error: 0.0656\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00107: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 50us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0615 - val_mean_squared_error: 0.0615\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00108: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0559 - val_mean_squared_error: 0.0559\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00109: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0083 - mean_squared_error: 0.0083\n",
      "Epoch 00110: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0520 - val_mean_squared_error: 0.0520\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0181 - mean_squared_error: 0.0181\n",
      "Epoch 00111: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0639 - val_mean_squared_error: 0.0639\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0087 - mean_squared_error: 0.0087\n",
      "Epoch 00112: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0482 - val_mean_squared_error: 0.0482\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00113: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0550 - val_mean_squared_error: 0.0550\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0160 - mean_squared_error: 0.0160\n",
      "Epoch 00114: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0588 - val_mean_squared_error: 0.0588\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00115: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0701 - val_mean_squared_error: 0.0701\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00116: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0588 - val_mean_squared_error: 0.0588\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00117: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 44us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00118: val_loss did not improve from 0.02884\n",
      "248/248 [==============================] - 0s 50us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Running fold 3\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.7042 - mean_squared_error: 1.7042\n",
      "Epoch 00001: val_loss improved from inf to 0.73402, saving model to model.h5\n",
      "248/248 [==============================] - 3s 11ms/sample - loss: 1.3754 - mean_squared_error: 1.3754 - val_loss: 0.7340 - val_mean_squared_error: 0.7340\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.3864 - mean_squared_error: 1.3864\n",
      "Epoch 00002: val_loss improved from 0.73402 to 0.67253, saving model to model.h5\n",
      "248/248 [==============================] - 0s 253us/sample - loss: 1.1199 - mean_squared_error: 1.1199 - val_loss: 0.6725 - val_mean_squared_error: 0.6725\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.2063 - mean_squared_error: 1.2063\n",
      "Epoch 00003: val_loss improved from 0.67253 to 0.55801, saving model to model.h5\n",
      "248/248 [==============================] - 0s 243us/sample - loss: 1.0113 - mean_squared_error: 1.0113 - val_loss: 0.5580 - val_mean_squared_error: 0.5580\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9942 - mean_squared_error: 0.9942\n",
      "Epoch 00004: val_loss improved from 0.55801 to 0.41143, saving model to model.h5\n",
      "248/248 [==============================] - 0s 252us/sample - loss: 0.9190 - mean_squared_error: 0.9190 - val_loss: 0.4114 - val_mean_squared_error: 0.4114\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9130 - mean_squared_error: 0.9130\n",
      "Epoch 00005: val_loss improved from 0.41143 to 0.24280, saving model to model.h5\n",
      "248/248 [==============================] - 0s 273us/sample - loss: 0.8141 - mean_squared_error: 0.8141 - val_loss: 0.2428 - val_mean_squared_error: 0.2428\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6353 - mean_squared_error: 0.6353\n",
      "Epoch 00006: val_loss improved from 0.24280 to 0.11442, saving model to model.h5\n",
      "248/248 [==============================] - 0s 292us/sample - loss: 0.6877 - mean_squared_error: 0.6877 - val_loss: 0.1144 - val_mean_squared_error: 0.1144\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4845 - mean_squared_error: 0.4845\n",
      "Epoch 00007: val_loss improved from 0.11442 to 0.05740, saving model to model.h5\n",
      "248/248 [==============================] - 0s 258us/sample - loss: 0.5163 - mean_squared_error: 0.5163 - val_loss: 0.0574 - val_mean_squared_error: 0.0574\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3888 - mean_squared_error: 0.3888\n",
      "Epoch 00008: val_loss improved from 0.05740 to 0.05078, saving model to model.h5\n",
      "248/248 [==============================] - 0s 267us/sample - loss: 0.2686 - mean_squared_error: 0.2686 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0840 - mean_squared_error: 0.0840\n",
      "Epoch 00009: val_loss improved from 0.05078 to 0.04802, saving model to model.h5\n",
      "248/248 [==============================] - 0s 251us/sample - loss: 0.0672 - mean_squared_error: 0.0672 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0295 - mean_squared_error: 0.0295\n",
      "Epoch 00010: val_loss did not improve from 0.04802\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00011: val_loss did not improve from 0.04802\n",
      "248/248 [==============================] - 0s 55us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0556 - val_mean_squared_error: 0.0556\n",
      "Epoch 12/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00012: val_loss did not improve from 0.04802\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0701 - val_mean_squared_error: 0.0701\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00013: val_loss improved from 0.04802 to 0.03549, saving model to model.h5\n",
      "248/248 [==============================] - 0s 295us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0355 - val_mean_squared_error: 0.0355\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00014: val_loss did not improve from 0.03549\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0653 - val_mean_squared_error: 0.0653\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00015: val_loss did not improve from 0.03549\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0139 - mean_squared_error: 0.0139\n",
      "Epoch 00016: val_loss did not improve from 0.03549\n",
      "248/248 [==============================] - 0s 58us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0181 - mean_squared_error: 0.0181\n",
      "Epoch 00017: val_loss did not improve from 0.03549\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0630 - val_mean_squared_error: 0.0630\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00018: val_loss did not improve from 0.03549\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00019: val_loss did not improve from 0.03549\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0583 - val_mean_squared_error: 0.0583\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00020: val_loss did not improve from 0.03549\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0356 - val_mean_squared_error: 0.0356\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00021: val_loss did not improve from 0.03549\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00022: val_loss did not improve from 0.03549\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0566 - val_mean_squared_error: 0.0566\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Epoch 00023: val_loss did not improve from 0.03549\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0635 - val_mean_squared_error: 0.0635\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00024: val_loss did not improve from 0.03549\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0175 - mean_squared_error: 0.0175 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0101 - mean_squared_error: 0.0101\n",
      "Epoch 00025: val_loss improved from 0.03549 to 0.03491, saving model to model.h5\n",
      "248/248 [==============================] - 0s 318us/sample - loss: 0.0173 - mean_squared_error: 0.0173 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00026: val_loss did not improve from 0.03491\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00027: val_loss did not improve from 0.03491\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0609 - val_mean_squared_error: 0.0609\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00028: val_loss did not improve from 0.03491\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0652 - val_mean_squared_error: 0.0652\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00029: val_loss did not improve from 0.03491\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0650 - val_mean_squared_error: 0.0650\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00030: val_loss did not improve from 0.03491\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00031: val_loss did not improve from 0.03491\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0617 - val_mean_squared_error: 0.0617\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00032: val_loss improved from 0.03491 to 0.03490, saving model to model.h5\n",
      "248/248 [==============================] - 0s 298us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00033: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00034: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 58us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0517 - val_mean_squared_error: 0.0517\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00035: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0843 - val_mean_squared_error: 0.0843\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00036: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0573 - val_mean_squared_error: 0.0573\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0130 - mean_squared_error: 0.0130\n",
      "Epoch 00037: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00038: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0110 - mean_squared_error: 0.0110\n",
      "Epoch 00039: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Epoch 00040: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0156 - mean_squared_error: 0.0156\n",
      "Epoch 00041: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00042: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0560 - val_mean_squared_error: 0.0560\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00043: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0535 - val_mean_squared_error: 0.0535\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00044: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0177 - mean_squared_error: 0.0177\n",
      "Epoch 00045: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0729 - val_mean_squared_error: 0.0729\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0446 - mean_squared_error: 0.0446\n",
      "Epoch 00046: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0170 - mean_squared_error: 0.0170\n",
      "Epoch 00047: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0883 - val_mean_squared_error: 0.0883\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00048: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0575 - val_mean_squared_error: 0.0575\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0089 - mean_squared_error: 0.0089\n",
      "Epoch 00049: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0169 - mean_squared_error: 0.0169 - val_loss: 0.0831 - val_mean_squared_error: 0.0831\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0294 - mean_squared_error: 0.0294\n",
      "Epoch 00050: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0628 - val_mean_squared_error: 0.0628\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00051: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0142 - mean_squared_error: 0.0142\n",
      "Epoch 00052: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0608 - val_mean_squared_error: 0.0608\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0144 - mean_squared_error: 0.0144\n",
      "Epoch 00053: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0930 - val_mean_squared_error: 0.0930\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00054: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0582 - val_mean_squared_error: 0.0582\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00055: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0718 - val_mean_squared_error: 0.0718\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 00056: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0862 - val_mean_squared_error: 0.0862\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00057: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00058: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 63us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0665 - val_mean_squared_error: 0.0665\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00059: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0531 - val_mean_squared_error: 0.0531\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0160 - mean_squared_error: 0.0160\n",
      "Epoch 00060: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0586 - val_mean_squared_error: 0.0586\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00061: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 54us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00062: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0551 - val_mean_squared_error: 0.0551\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00063: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 64/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00064: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0637 - val_mean_squared_error: 0.0637\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00065: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00066: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0801 - val_mean_squared_error: 0.0801\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00067: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0808 - val_mean_squared_error: 0.0808\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0384 - mean_squared_error: 0.0384\n",
      "Epoch 00068: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0607 - val_mean_squared_error: 0.0607\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00069: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0692 - val_mean_squared_error: 0.0692\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00070: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0580 - val_mean_squared_error: 0.0580\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0181 - mean_squared_error: 0.0181\n",
      "Epoch 00071: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0187 - mean_squared_error: 0.0187\n",
      "Epoch 00072: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Epoch 00073: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0526 - val_mean_squared_error: 0.0526\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00074: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0764 - val_mean_squared_error: 0.0764\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0153 - mean_squared_error: 0.0153\n",
      "Epoch 00075: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0165 - mean_squared_error: 0.0165 - val_loss: 0.0723 - val_mean_squared_error: 0.0723\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 00076: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00077: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0160 - mean_squared_error: 0.0160\n",
      "Epoch 00078: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0570 - val_mean_squared_error: 0.0570\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0153 - mean_squared_error: 0.0153\n",
      "Epoch 00079: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00080: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0716 - val_mean_squared_error: 0.0716\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00081: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0688 - val_mean_squared_error: 0.0688\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00082: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0543 - val_mean_squared_error: 0.0543\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0143 - mean_squared_error: 0.0143\n",
      "Epoch 00083: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0717 - val_mean_squared_error: 0.0717\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0142 - mean_squared_error: 0.0142\n",
      "Epoch 00084: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 49us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0557 - val_mean_squared_error: 0.0557\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00085: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00086: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0734 - val_mean_squared_error: 0.0734\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00087: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 50us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0534 - val_mean_squared_error: 0.0534\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00088: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 54us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0545 - val_mean_squared_error: 0.0545\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0070 - mean_squared_error: 0.0070\n",
      "Epoch 00089: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0620 - val_mean_squared_error: 0.0620\n",
      "Epoch 90/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0392 - mean_squared_error: 0.0392\n",
      "Epoch 00090: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 55us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00091: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0151 - mean_squared_error: 0.0151\n",
      "Epoch 00092: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0920 - val_mean_squared_error: 0.0920\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00093: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Epoch 00094: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0536 - val_mean_squared_error: 0.0536\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00095: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0791 - val_mean_squared_error: 0.0791\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00096: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 51us/sample - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0605 - val_mean_squared_error: 0.0605\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0139 - mean_squared_error: 0.0139\n",
      "Epoch 00097: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0166 - mean_squared_error: 0.0166 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0140 - mean_squared_error: 0.0140\n",
      "Epoch 00098: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0144 - mean_squared_error: 0.0144\n",
      "Epoch 00099: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 55us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0136 - mean_squared_error: 0.0136\n",
      "Epoch 00100: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0633 - val_mean_squared_error: 0.0633\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0107 - mean_squared_error: 0.0107\n",
      "Epoch 00101: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0802 - val_mean_squared_error: 0.0802\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00102: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0860 - val_mean_squared_error: 0.0860\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00103: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00104: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0817 - val_mean_squared_error: 0.0817\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0333 - mean_squared_error: 0.0333\n",
      "Epoch 00105: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0592 - val_mean_squared_error: 0.0592\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0178 - mean_squared_error: 0.0178\n",
      "Epoch 00106: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 00107: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0604 - val_mean_squared_error: 0.0604\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0080 - mean_squared_error: 0.0080\n",
      "Epoch 00108: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0160 - mean_squared_error: 0.0160 - val_loss: 0.0637 - val_mean_squared_error: 0.0637\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00109: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0840 - val_mean_squared_error: 0.0840\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0138 - mean_squared_error: 0.0138\n",
      "Epoch 00110: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00111: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00112: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0569 - val_mean_squared_error: 0.0569\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00113: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0133 - mean_squared_error: 0.0133\n",
      "Epoch 00114: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0159 - mean_squared_error: 0.0159 - val_loss: 0.0482 - val_mean_squared_error: 0.0482\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00115: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 55us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 116/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00116: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 51us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0852 - val_mean_squared_error: 0.0852\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00117: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0722 - val_mean_squared_error: 0.0722\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00118: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0169 - mean_squared_error: 0.0169 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00119: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0562 - val_mean_squared_error: 0.0562\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00120: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0694 - val_mean_squared_error: 0.0694\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0109 - mean_squared_error: 0.0109\n",
      "Epoch 00121: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0231 - mean_squared_error: 0.0231\n",
      "Epoch 00122: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 61us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0574 - val_mean_squared_error: 0.0574\n",
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00123: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0629 - val_mean_squared_error: 0.0629\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0129 - mean_squared_error: 0.0129\n",
      "Epoch 00124: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0174 - mean_squared_error: 0.0174 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00125: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0140 - mean_squared_error: 0.0140\n",
      "Epoch 00126: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0174 - mean_squared_error: 0.0174\n",
      "Epoch 00127: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00128: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0536 - val_mean_squared_error: 0.0536\n",
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00129: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0749 - val_mean_squared_error: 0.0749\n",
      "Epoch 130/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00130: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 48us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00131: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0567 - val_mean_squared_error: 0.0567\n",
      "Epoch 132/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0107 - mean_squared_error: 0.0107\n",
      "Epoch 00132: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0163 - mean_squared_error: 0.0163 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Running fold 4\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.0108 - mean_squared_error: 1.0108\n",
      "Epoch 00001: val_loss improved from inf to 0.30054, saving model to model.h5\n",
      "248/248 [==============================] - 3s 12ms/sample - loss: 1.1072 - mean_squared_error: 1.1072 - val_loss: 0.3005 - val_mean_squared_error: 0.3005\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.1302 - mean_squared_error: 1.1302\n",
      "Epoch 00002: val_loss improved from 0.30054 to 0.07170, saving model to model.h5\n",
      "248/248 [==============================] - 0s 293us/sample - loss: 0.8969 - mean_squared_error: 0.8969 - val_loss: 0.0717 - val_mean_squared_error: 0.0717\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8091 - mean_squared_error: 0.8091\n",
      "Epoch 00003: val_loss improved from 0.07170 to 0.05162, saving model to model.h5\n",
      "248/248 [==============================] - 0s 310us/sample - loss: 0.8248 - mean_squared_error: 0.8248 - val_loss: 0.0516 - val_mean_squared_error: 0.0516\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7043 - mean_squared_error: 0.7043\n",
      "Epoch 00004: val_loss did not improve from 0.05162\n",
      "248/248 [==============================] - 0s 61us/sample - loss: 0.7952 - mean_squared_error: 0.7952 - val_loss: 0.0709 - val_mean_squared_error: 0.0709\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7276 - mean_squared_error: 0.7276\n",
      "Epoch 00005: val_loss improved from 0.05162 to 0.03375, saving model to model.h5\n",
      "248/248 [==============================] - 0s 326us/sample - loss: 0.7673 - mean_squared_error: 0.7673 - val_loss: 0.0338 - val_mean_squared_error: 0.0338\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6009 - mean_squared_error: 0.6009\n",
      "Epoch 00006: val_loss did not improve from 0.03375\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.7423 - mean_squared_error: 0.7423 - val_loss: 0.0824 - val_mean_squared_error: 0.0824\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8366 - mean_squared_error: 0.8366\n",
      "Epoch 00007: val_loss did not improve from 0.03375\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.7060 - mean_squared_error: 0.7060 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4961 - mean_squared_error: 0.4961\n",
      "Epoch 00008: val_loss did not improve from 0.03375\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.6353 - mean_squared_error: 0.6353 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5237 - mean_squared_error: 0.5237\n",
      "Epoch 00009: val_loss did not improve from 0.03375\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.4860 - mean_squared_error: 0.4860 - val_loss: 0.0358 - val_mean_squared_error: 0.0358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2521 - mean_squared_error: 0.2521\n",
      "Epoch 00010: val_loss did not improve from 0.03375\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.2346 - mean_squared_error: 0.2346 - val_loss: 0.0742 - val_mean_squared_error: 0.0742\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0494 - mean_squared_error: 0.0494\n",
      "Epoch 00011: val_loss did not improve from 0.03375\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0495 - mean_squared_error: 0.0495 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00012: val_loss did not improve from 0.03375\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00013: val_loss did not improve from 0.03375\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0833 - val_mean_squared_error: 0.0833\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00014: val_loss did not improve from 0.03375\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00015: val_loss did not improve from 0.03375\n",
      "248/248 [==============================] - 0s 58us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 00016: val_loss did not improve from 0.03375\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0517 - val_mean_squared_error: 0.0517\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00017: val_loss did not improve from 0.03375\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00018: val_loss did not improve from 0.03375\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0343 - val_mean_squared_error: 0.0343\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0171 - mean_squared_error: 0.0171\n",
      "Epoch 00019: val_loss did not improve from 0.03375\n",
      "248/248 [==============================] - 0s 57us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00020: val_loss improved from 0.03375 to 0.03365, saving model to model.h5\n",
      "248/248 [==============================] - 0s 303us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0400 - mean_squared_error: 0.0400\n",
      "Epoch 00021: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 67us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0580 - val_mean_squared_error: 0.0580\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00022: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00023: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0663 - val_mean_squared_error: 0.0663\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00024: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0798 - val_mean_squared_error: 0.0798\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0396 - mean_squared_error: 0.0396\n",
      "Epoch 00025: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00026: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 58us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0748 - val_mean_squared_error: 0.0748\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00027: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00028: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.1115 - val_mean_squared_error: 0.1115\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00029: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00030: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0181 - mean_squared_error: 0.0181\n",
      "Epoch 00031: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0700 - val_mean_squared_error: 0.0700\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00032: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0585 - val_mean_squared_error: 0.0585\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00033: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 62us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00034: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0920 - val_mean_squared_error: 0.0920\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0307 - mean_squared_error: 0.0307\n",
      "Epoch 00035: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0643 - val_mean_squared_error: 0.0643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 00036: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0690 - val_mean_squared_error: 0.0690\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0121 - mean_squared_error: 0.0121\n",
      "Epoch 00037: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0599 - val_mean_squared_error: 0.0599\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00038: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00039: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0620 - val_mean_squared_error: 0.0620\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0129 - mean_squared_error: 0.0129\n",
      "Epoch 00040: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00041: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00042: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00043: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 53us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0580 - val_mean_squared_error: 0.0580\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0158 - mean_squared_error: 0.0158\n",
      "Epoch 00044: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00045: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 59us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0405 - mean_squared_error: 0.0405\n",
      "Epoch 00046: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0592 - val_mean_squared_error: 0.0592\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0231 - mean_squared_error: 0.0231\n",
      "Epoch 00047: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00048: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0173 - mean_squared_error: 0.0173 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00049: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0597 - val_mean_squared_error: 0.0597\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00050: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0984 - val_mean_squared_error: 0.0984\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0460 - mean_squared_error: 0.0460\n",
      "Epoch 00051: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0320 - mean_squared_error: 0.0320 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0448 - mean_squared_error: 0.0448\n",
      "Epoch 00052: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0600 - val_mean_squared_error: 0.0600\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0161 - mean_squared_error: 0.0161\n",
      "Epoch 00053: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0543 - val_mean_squared_error: 0.0543\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0113 - mean_squared_error: 0.0113\n",
      "Epoch 00054: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0372 - mean_squared_error: 0.0372\n",
      "Epoch 00055: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 58us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0628 - val_mean_squared_error: 0.0628\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0171 - mean_squared_error: 0.0171\n",
      "Epoch 00056: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0615 - val_mean_squared_error: 0.0615\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00057: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00058: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0617 - val_mean_squared_error: 0.0617\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0540 - mean_squared_error: 0.0540\n",
      "Epoch 00059: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 59us/sample - loss: 0.0367 - mean_squared_error: 0.0367 - val_loss: 0.0638 - val_mean_squared_error: 0.0638\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.0172\n",
      "Epoch 00060: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0151 - mean_squared_error: 0.0151\n",
      "Epoch 00061: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0170 - mean_squared_error: 0.0170 - val_loss: 0.0854 - val_mean_squared_error: 0.0854\n",
      "Epoch 62/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00062: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0146 - mean_squared_error: 0.0146\n",
      "Epoch 00063: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.1057 - val_mean_squared_error: 0.1057\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00064: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00065: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0683 - val_mean_squared_error: 0.0683\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00066: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0643 - val_mean_squared_error: 0.0643\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00067: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00068: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0978 - val_mean_squared_error: 0.0978\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0227 - mean_squared_error: 0.0227\n",
      "Epoch 00069: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0643 - val_mean_squared_error: 0.0643\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00070: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0781 - val_mean_squared_error: 0.0781\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00071: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0126 - mean_squared_error: 0.0126\n",
      "Epoch 00072: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.1025 - val_mean_squared_error: 0.1025\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00073: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00074: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00075: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0751 - val_mean_squared_error: 0.0751\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00076: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0630 - val_mean_squared_error: 0.0630\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00077: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0168 - mean_squared_error: 0.0168 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00078: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.1084 - val_mean_squared_error: 0.1084\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00079: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0536 - val_mean_squared_error: 0.0536\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0088 - mean_squared_error: 0.0088\n",
      "Epoch 00080: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0163 - mean_squared_error: 0.0163 - val_loss: 0.0979 - val_mean_squared_error: 0.0979\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00081: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00082: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0686 - val_mean_squared_error: 0.0686\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0134 - mean_squared_error: 0.0134\n",
      "Epoch 00083: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00084: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0721 - val_mean_squared_error: 0.0721\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0446 - mean_squared_error: 0.0446\n",
      "Epoch 00085: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0737 - val_mean_squared_error: 0.0737\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00086: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00087: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 88/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00088: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0901 - val_mean_squared_error: 0.0901\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00089: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0670 - val_mean_squared_error: 0.0670\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0152 - mean_squared_error: 0.0152\n",
      "Epoch 00090: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0170 - mean_squared_error: 0.0170\n",
      "Epoch 00091: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0620 - val_mean_squared_error: 0.0620\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00092: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00093: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0227 - mean_squared_error: 0.0227\n",
      "Epoch 00094: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0848 - val_mean_squared_error: 0.0848\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00095: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0900 - val_mean_squared_error: 0.0900\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00096: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0722 - val_mean_squared_error: 0.0722\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0111 - mean_squared_error: 0.0111\n",
      "Epoch 00097: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0121 - mean_squared_error: 0.0121\n",
      "Epoch 00098: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0131 - mean_squared_error: 0.0131\n",
      "Epoch 00099: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.1082 - val_mean_squared_error: 0.1082\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00100: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0100 - mean_squared_error: 0.0100\n",
      "Epoch 00101: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0165 - mean_squared_error: 0.0165 - val_loss: 0.0844 - val_mean_squared_error: 0.0844\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00102: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0095 - mean_squared_error: 0.0095\n",
      "Epoch 00103: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0176 - mean_squared_error: 0.0176 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00104: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0601 - val_mean_squared_error: 0.0601\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00105: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0632 - val_mean_squared_error: 0.0632\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00106: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0552 - val_mean_squared_error: 0.0552\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0086 - mean_squared_error: 0.0086\n",
      "Epoch 00107: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00108: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0760 - val_mean_squared_error: 0.0760\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0103 - mean_squared_error: 0.0103\n",
      "Epoch 00109: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0738 - val_mean_squared_error: 0.0738\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00110: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.0185\n",
      "Epoch 00111: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.1049 - val_mean_squared_error: 0.1049\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00112: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0538 - val_mean_squared_error: 0.0538\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0161 - mean_squared_error: 0.0161\n",
      "Epoch 00113: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0170 - mean_squared_error: 0.0170 - val_loss: 0.0535 - val_mean_squared_error: 0.0535\n",
      "Epoch 114/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00114: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0896 - val_mean_squared_error: 0.0896\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00115: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0367 - mean_squared_error: 0.0367\n",
      "Epoch 00116: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0579 - val_mean_squared_error: 0.0579\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00117: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0655 - val_mean_squared_error: 0.0655\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00118: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 93us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0155 - mean_squared_error: 0.0155\n",
      "Epoch 00119: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0888 - val_mean_squared_error: 0.0888\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00120: val_loss did not improve from 0.03365\n",
      "248/248 [==============================] - 0s 61us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0625 - val_mean_squared_error: 0.0625\n",
      "Running fold 5\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 1s - loss: 1.1056 - mean_squared_error: 1.1056\n",
      "Epoch 00001: val_loss improved from inf to 0.77291, saving model to model.h5\n",
      "248/248 [==============================] - 3s 13ms/sample - loss: 1.1446 - mean_squared_error: 1.1446 - val_loss: 0.7729 - val_mean_squared_error: 0.7729\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.1405 - mean_squared_error: 1.1405\n",
      "Epoch 00002: val_loss improved from 0.77291 to 0.76399, saving model to model.h5\n",
      "248/248 [==============================] - 0s 310us/sample - loss: 0.8219 - mean_squared_error: 0.8219 - val_loss: 0.7640 - val_mean_squared_error: 0.7640\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4202 - mean_squared_error: 0.4202\n",
      "Epoch 00003: val_loss improved from 0.76399 to 0.71373, saving model to model.h5\n",
      "248/248 [==============================] - 0s 303us/sample - loss: 0.3557 - mean_squared_error: 0.3557 - val_loss: 0.7137 - val_mean_squared_error: 0.7137\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3109 - mean_squared_error: 0.3109\n",
      "Epoch 00004: val_loss improved from 0.71373 to 0.64388, saving model to model.h5\n",
      "248/248 [==============================] - 0s 339us/sample - loss: 0.2553 - mean_squared_error: 0.2553 - val_loss: 0.6439 - val_mean_squared_error: 0.6439\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2412 - mean_squared_error: 0.2412\n",
      "Epoch 00005: val_loss improved from 0.64388 to 0.52298, saving model to model.h5\n",
      "248/248 [==============================] - 0s 311us/sample - loss: 0.2176 - mean_squared_error: 0.2176 - val_loss: 0.5230 - val_mean_squared_error: 0.5230\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2129 - mean_squared_error: 0.2129\n",
      "Epoch 00006: val_loss improved from 0.52298 to 0.27248, saving model to model.h5\n",
      "248/248 [==============================] - 0s 315us/sample - loss: 0.1635 - mean_squared_error: 0.1635 - val_loss: 0.2725 - val_mean_squared_error: 0.2725\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1001 - mean_squared_error: 0.1001\n",
      "Epoch 00007: val_loss improved from 0.27248 to 0.05707, saving model to model.h5\n",
      "248/248 [==============================] - 0s 315us/sample - loss: 0.0678 - mean_squared_error: 0.0678 - val_loss: 0.0571 - val_mean_squared_error: 0.0571\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00008: val_loss improved from 0.05707 to 0.04754, saving model to model.h5\n",
      "248/248 [==============================] - 0s 312us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 00009: val_loss did not improve from 0.04754\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0659 - val_mean_squared_error: 0.0659\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00010: val_loss improved from 0.04754 to 0.04370, saving model to model.h5\n",
      "248/248 [==============================] - 0s 312us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00011: val_loss did not improve from 0.04370\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00012: val_loss improved from 0.04370 to 0.03269, saving model to model.h5\n",
      "248/248 [==============================] - 0s 318us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0327 - val_mean_squared_error: 0.0327\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00013: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 63us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00014: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0653 - val_mean_squared_error: 0.0653\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00015: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0356 - val_mean_squared_error: 0.0356\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00016: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 63us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00017: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0516 - val_mean_squared_error: 0.0516\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0474 - mean_squared_error: 0.0474\n",
      "Epoch 00018: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0695 - val_mean_squared_error: 0.0695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0155 - mean_squared_error: 0.0155\n",
      "Epoch 00019: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00020: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0566 - val_mean_squared_error: 0.0566\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00021: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00022: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0959 - val_mean_squared_error: 0.0959\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00023: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0602 - val_mean_squared_error: 0.0602\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0138 - mean_squared_error: 0.0138\n",
      "Epoch 00024: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0616 - mean_squared_error: 0.0616\n",
      "Epoch 00025: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0145 - mean_squared_error: 0.0145\n",
      "Epoch 00026: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0778 - val_mean_squared_error: 0.0778\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00027: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0563 - val_mean_squared_error: 0.0563\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0148 - mean_squared_error: 0.0148\n",
      "Epoch 00028: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 67us/sample - loss: 0.0168 - mean_squared_error: 0.0168 - val_loss: 0.0589 - val_mean_squared_error: 0.0589\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00029: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0155 - mean_squared_error: 0.0155\n",
      "Epoch 00030: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 62us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0740 - val_mean_squared_error: 0.0740\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00031: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0372 - mean_squared_error: 0.0372 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0604 - mean_squared_error: 0.0604\n",
      "Epoch 00032: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0558 - val_mean_squared_error: 0.0558\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00033: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0142 - mean_squared_error: 0.0142\n",
      "Epoch 00034: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 59us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.1026 - val_mean_squared_error: 0.1026\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Epoch 00035: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0638 - val_mean_squared_error: 0.0638\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00036: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0358 - mean_squared_error: 0.0358\n",
      "Epoch 00037: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 62us/sample - loss: 0.0330 - mean_squared_error: 0.0330 - val_loss: 0.0603 - val_mean_squared_error: 0.0603\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0139 - mean_squared_error: 0.0139\n",
      "Epoch 00038: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0403 - mean_squared_error: 0.0403\n",
      "Epoch 00039: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0167 - mean_squared_error: 0.0167\n",
      "Epoch 00040: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 00041: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0620 - val_mean_squared_error: 0.0620\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00042: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0374 - mean_squared_error: 0.0374 - val_loss: 0.0562 - val_mean_squared_error: 0.0562\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0542 - mean_squared_error: 0.0542\n",
      "Epoch 00043: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 63us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0850 - val_mean_squared_error: 0.0850\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0147 - mean_squared_error: 0.0147\n",
      "Epoch 00044: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0541 - val_mean_squared_error: 0.0541\n",
      "Epoch 45/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0151 - mean_squared_error: 0.0151\n",
      "Epoch 00045: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.1086 - val_mean_squared_error: 0.1086\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0360 - mean_squared_error: 0.0360\n",
      "Epoch 00046: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00047: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0755 - val_mean_squared_error: 0.0755\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0413 - mean_squared_error: 0.0413\n",
      "Epoch 00048: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0398 - mean_squared_error: 0.0398\n",
      "Epoch 00049: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0844 - val_mean_squared_error: 0.0844\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00050: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 58us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0555 - val_mean_squared_error: 0.0555\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00051: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0587 - val_mean_squared_error: 0.0587\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0187 - mean_squared_error: 0.0187\n",
      "Epoch 00052: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0466 - mean_squared_error: 0.0466\n",
      "Epoch 00053: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0422 - mean_squared_error: 0.0422 - val_loss: 0.0829 - val_mean_squared_error: 0.0829\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0148 - mean_squared_error: 0.0148\n",
      "Epoch 00054: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0575 - val_mean_squared_error: 0.0575\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0111 - mean_squared_error: 0.0111\n",
      "Epoch 00055: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0170 - mean_squared_error: 0.0170 - val_loss: 0.0609 - val_mean_squared_error: 0.0609\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00056: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 61us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00057: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0378 - mean_squared_error: 0.0378 - val_loss: 0.0986 - val_mean_squared_error: 0.0986\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0647 - mean_squared_error: 0.0647\n",
      "Epoch 00058: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0665 - val_mean_squared_error: 0.0665\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00059: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0170 - mean_squared_error: 0.0170 - val_loss: 0.0850 - val_mean_squared_error: 0.0850\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0184 - mean_squared_error: 0.0184\n",
      "Epoch 00060: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0120 - mean_squared_error: 0.0120\n",
      "Epoch 00061: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 61us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0464 - mean_squared_error: 0.0464\n",
      "Epoch 00062: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0735 - val_mean_squared_error: 0.0735\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00063: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0538 - val_mean_squared_error: 0.0538\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00064: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0513 - val_mean_squared_error: 0.0513\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0119 - mean_squared_error: 0.0119\n",
      "Epoch 00065: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0741 - val_mean_squared_error: 0.0741\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0259 - mean_squared_error: 0.0259\n",
      "Epoch 00066: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0755 - val_mean_squared_error: 0.0755\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0135 - mean_squared_error: 0.0135\n",
      "Epoch 00067: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00068: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0597 - val_mean_squared_error: 0.0597\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00069: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0383 - mean_squared_error: 0.0383 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0365 - mean_squared_error: 0.0365\n",
      "Epoch 00070: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 71/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0153 - mean_squared_error: 0.0153\n",
      "Epoch 00071: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0814 - val_mean_squared_error: 0.0814\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0167 - mean_squared_error: 0.0167\n",
      "Epoch 00072: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0542 - val_mean_squared_error: 0.0542\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00073: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0584 - val_mean_squared_error: 0.0584\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.0185\n",
      "Epoch 00074: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00075: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0544 - val_mean_squared_error: 0.0544\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00076: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00077: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00078: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00079: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00080: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.1185 - val_mean_squared_error: 0.1185\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00081: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0832 - val_mean_squared_error: 0.0832\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00082: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0559 - val_mean_squared_error: 0.0559\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0118 - mean_squared_error: 0.0118\n",
      "Epoch 00083: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 62us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0333 - mean_squared_error: 0.0333\n",
      "Epoch 00084: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0676 - val_mean_squared_error: 0.0676\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00085: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 62us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0712 - val_mean_squared_error: 0.0712\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0360 - mean_squared_error: 0.0360\n",
      "Epoch 00086: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0542 - val_mean_squared_error: 0.0542\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00087: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 59us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0704 - val_mean_squared_error: 0.0704\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 00088: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0150 - mean_squared_error: 0.0150 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00089: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0615 - mean_squared_error: 0.0615\n",
      "Epoch 00090: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 63us/sample - loss: 0.0399 - mean_squared_error: 0.0399 - val_loss: 0.0748 - val_mean_squared_error: 0.0748\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00091: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0178 - mean_squared_error: 0.0178\n",
      "Epoch 00092: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0881 - val_mean_squared_error: 0.0881\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0155 - mean_squared_error: 0.0155\n",
      "Epoch 00093: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Epoch 00094: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 57us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0835 - val_mean_squared_error: 0.0835\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00095: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0363 - mean_squared_error: 0.0363 - val_loss: 0.0917 - val_mean_squared_error: 0.0917\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00096: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0797 - val_mean_squared_error: 0.0797\n",
      "Epoch 97/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00097: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0172 - mean_squared_error: 0.0172 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0144 - mean_squared_error: 0.0144\n",
      "Epoch 00098: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0173 - mean_squared_error: 0.0173 - val_loss: 0.0898 - val_mean_squared_error: 0.0898\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00099: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0538 - val_mean_squared_error: 0.0538\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00100: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0540 - val_mean_squared_error: 0.0540\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Epoch 00101: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0764 - val_mean_squared_error: 0.0764\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00102: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0671 - val_mean_squared_error: 0.0671\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0429 - mean_squared_error: 0.0429\n",
      "Epoch 00103: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 52us/sample - loss: 0.0400 - mean_squared_error: 0.0400 - val_loss: 0.0720 - val_mean_squared_error: 0.0720\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0088 - mean_squared_error: 0.0088\n",
      "Epoch 00104: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 59us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00105: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00106: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0636 - val_mean_squared_error: 0.0636\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00107: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0127 - mean_squared_error: 0.0127\n",
      "Epoch 00108: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0169 - mean_squared_error: 0.0169 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0149 - mean_squared_error: 0.0149\n",
      "Epoch 00109: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0790 - val_mean_squared_error: 0.0790\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 00110: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0581 - val_mean_squared_error: 0.0581\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Epoch 00111: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 56us/sample - loss: 0.0402 - mean_squared_error: 0.0402 - val_loss: 0.0554 - val_mean_squared_error: 0.0554\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Epoch 00112: val_loss did not improve from 0.03269\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Running fold 6\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 1s - loss: 0.9730 - mean_squared_error: 0.9730\n",
      "Epoch 00001: val_loss improved from inf to 0.78949, saving model to model.h5\n",
      "248/248 [==============================] - 4s 14ms/sample - loss: 0.8212 - mean_squared_error: 0.8212 - val_loss: 0.7895 - val_mean_squared_error: 0.7895\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2757 - mean_squared_error: 0.2757\n",
      "Epoch 00002: val_loss improved from 0.78949 to 0.71041, saving model to model.h5\n",
      "248/248 [==============================] - 0s 436us/sample - loss: 0.2914 - mean_squared_error: 0.2914 - val_loss: 0.7104 - val_mean_squared_error: 0.7104\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2547 - mean_squared_error: 0.2547\n",
      "Epoch 00003: val_loss improved from 0.71041 to 0.66586, saving model to model.h5\n",
      "248/248 [==============================] - 0s 378us/sample - loss: 0.2559 - mean_squared_error: 0.2559 - val_loss: 0.6659 - val_mean_squared_error: 0.6659\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2173 - mean_squared_error: 0.2173\n",
      "Epoch 00004: val_loss improved from 0.66586 to 0.61896, saving model to model.h5\n",
      "248/248 [==============================] - 0s 343us/sample - loss: 0.2318 - mean_squared_error: 0.2318 - val_loss: 0.6190 - val_mean_squared_error: 0.6190\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2284 - mean_squared_error: 0.2284\n",
      "Epoch 00005: val_loss improved from 0.61896 to 0.54965, saving model to model.h5\n",
      "248/248 [==============================] - 0s 360us/sample - loss: 0.2139 - mean_squared_error: 0.2139 - val_loss: 0.5497 - val_mean_squared_error: 0.5497\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1678 - mean_squared_error: 0.1678\n",
      "Epoch 00006: val_loss improved from 0.54965 to 0.38531, saving model to model.h5\n",
      "248/248 [==============================] - 0s 388us/sample - loss: 0.1858 - mean_squared_error: 0.1858 - val_loss: 0.3853 - val_mean_squared_error: 0.3853\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1572 - mean_squared_error: 0.1572\n",
      "Epoch 00007: val_loss improved from 0.38531 to 0.10321, saving model to model.h5\n",
      "248/248 [==============================] - 0s 360us/sample - loss: 0.1189 - mean_squared_error: 0.1189 - val_loss: 0.1032 - val_mean_squared_error: 0.1032\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0477 - mean_squared_error: 0.0477\n",
      "Epoch 00008: val_loss improved from 0.10321 to 0.04222, saving model to model.h5\n",
      "248/248 [==============================] - 0s 361us/sample - loss: 0.0454 - mean_squared_error: 0.0454 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Epoch 00009: val_loss did not improve from 0.04222\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 10/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Epoch 00010: val_loss improved from 0.04222 to 0.03917, saving model to model.h5\n",
      "248/248 [==============================] - 0s 407us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0127 - mean_squared_error: 0.0127\n",
      "Epoch 00011: val_loss did not improve from 0.03917\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0581 - val_mean_squared_error: 0.0581\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00012: val_loss improved from 0.03917 to 0.03743, saving model to model.h5\n",
      "248/248 [==============================] - 0s 356us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00013: val_loss did not improve from 0.03743\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00014: val_loss improved from 0.03743 to 0.03389, saving model to model.h5\n",
      "248/248 [==============================] - 0s 410us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0522 - mean_squared_error: 0.0522\n",
      "Epoch 00015: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00016: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0722 - val_mean_squared_error: 0.0722\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00017: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00018: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0167 - mean_squared_error: 0.0167\n",
      "Epoch 00019: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0763 - val_mean_squared_error: 0.0763\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00020: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00021: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0552 - val_mean_squared_error: 0.0552\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00022: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0626 - val_mean_squared_error: 0.0626\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0259 - mean_squared_error: 0.0259\n",
      "Epoch 00023: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0600 - mean_squared_error: 0.0600\n",
      "Epoch 00024: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0450 - mean_squared_error: 0.0450 - val_loss: 0.0615 - val_mean_squared_error: 0.0615\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Epoch 00025: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00026: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0151 - mean_squared_error: 0.0151\n",
      "Epoch 00027: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.1323 - val_mean_squared_error: 0.1323\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0472 - mean_squared_error: 0.0472\n",
      "Epoch 00028: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0527 - val_mean_squared_error: 0.0527\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0142 - mean_squared_error: 0.0142\n",
      "Epoch 00029: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0902 - val_mean_squared_error: 0.0902\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00030: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0611 - mean_squared_error: 0.0611\n",
      "Epoch 00031: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0507 - mean_squared_error: 0.0507 - val_loss: 0.0575 - val_mean_squared_error: 0.0575\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0161 - mean_squared_error: 0.0161\n",
      "Epoch 00032: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0162 - mean_squared_error: 0.0162 - val_loss: 0.0922 - val_mean_squared_error: 0.0922\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00033: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0125 - mean_squared_error: 0.0125\n",
      "Epoch 00034: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0651 - val_mean_squared_error: 0.0651\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0130 - mean_squared_error: 0.0130\n",
      "Epoch 00035: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0172 - mean_squared_error: 0.0172 - val_loss: 0.0656 - val_mean_squared_error: 0.0656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0450 - mean_squared_error: 0.0450\n",
      "Epoch 00036: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00037: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0553 - val_mean_squared_error: 0.0553\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0086 - mean_squared_error: 0.0086\n",
      "Epoch 00038: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0679 - mean_squared_error: 0.0679\n",
      "Epoch 00039: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0522 - mean_squared_error: 0.0522 - val_loss: 0.0640 - val_mean_squared_error: 0.0640\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0294 - mean_squared_error: 0.0294\n",
      "Epoch 00040: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00041: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0635 - val_mean_squared_error: 0.0635\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0383 - mean_squared_error: 0.0383\n",
      "Epoch 00042: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0606 - val_mean_squared_error: 0.0606\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00043: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00044: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0690 - val_mean_squared_error: 0.0690\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00045: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0919 - val_mean_squared_error: 0.0919\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0139 - mean_squared_error: 0.0139\n",
      "Epoch 00046: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00047: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0681 - val_mean_squared_error: 0.0681\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0452 - mean_squared_error: 0.0452\n",
      "Epoch 00048: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0400 - mean_squared_error: 0.0400 - val_loss: 0.0564 - val_mean_squared_error: 0.0564\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.0172\n",
      "Epoch 00049: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0580 - val_mean_squared_error: 0.0580\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00050: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0571 - val_mean_squared_error: 0.0571\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0449 - mean_squared_error: 0.0449\n",
      "Epoch 00051: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.1085 - val_mean_squared_error: 0.1085\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00052: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00053: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0576 - val_mean_squared_error: 0.0576\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00054: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0611 - val_mean_squared_error: 0.0611\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00055: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.1047 - val_mean_squared_error: 0.1047\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Epoch 00056: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 67us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0808 - val_mean_squared_error: 0.0808\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00057: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0436 - mean_squared_error: 0.0436\n",
      "Epoch 00058: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0703 - val_mean_squared_error: 0.0703\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00059: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0187 - mean_squared_error: 0.0187\n",
      "Epoch 00060: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.0185\n",
      "Epoch 00061: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0757 - val_mean_squared_error: 0.0757\n",
      "Epoch 62/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0109 - mean_squared_error: 0.0109\n",
      "Epoch 00062: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0535 - mean_squared_error: 0.0535\n",
      "Epoch 00063: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0553 - mean_squared_error: 0.0553 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00064: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0594 - val_mean_squared_error: 0.0594\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00065: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0650 - val_mean_squared_error: 0.0650\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0156 - mean_squared_error: 0.0156\n",
      "Epoch 00066: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00067: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0489 - mean_squared_error: 0.0489\n",
      "Epoch 00068: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0361 - mean_squared_error: 0.0361 - val_loss: 0.1269 - val_mean_squared_error: 0.1269\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00069: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0546 - val_mean_squared_error: 0.0546\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00070: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00071: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0720 - val_mean_squared_error: 0.0720\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00072: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00073: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0857 - val_mean_squared_error: 0.0857\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0157 - mean_squared_error: 0.0157\n",
      "Epoch 00074: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0676 - mean_squared_error: 0.0676\n",
      "Epoch 00075: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 63us/sample - loss: 0.0386 - mean_squared_error: 0.0386 - val_loss: 0.0696 - val_mean_squared_error: 0.0696\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0188 - mean_squared_error: 0.0188\n",
      "Epoch 00076: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0164 - mean_squared_error: 0.0164 - val_loss: 0.0568 - val_mean_squared_error: 0.0568\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00077: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0328 - mean_squared_error: 0.0328 - val_loss: 0.0630 - val_mean_squared_error: 0.0630\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0387 - mean_squared_error: 0.0387\n",
      "Epoch 00078: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.1018 - val_mean_squared_error: 0.1018\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0307 - mean_squared_error: 0.0307\n",
      "Epoch 00079: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0684 - val_mean_squared_error: 0.0684\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00080: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0517 - val_mean_squared_error: 0.0517\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0591 - mean_squared_error: 0.0591\n",
      "Epoch 00081: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 63us/sample - loss: 0.0366 - mean_squared_error: 0.0366 - val_loss: 0.0951 - val_mean_squared_error: 0.0951\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00082: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0545 - val_mean_squared_error: 0.0545\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0454 - mean_squared_error: 0.0454\n",
      "Epoch 00083: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0330 - mean_squared_error: 0.0330 - val_loss: 0.0625 - val_mean_squared_error: 0.0625\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00084: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 61us/sample - loss: 0.0155 - mean_squared_error: 0.0155 - val_loss: 0.0808 - val_mean_squared_error: 0.0808\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0181 - mean_squared_error: 0.0181\n",
      "Epoch 00085: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0585 - val_mean_squared_error: 0.0585\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 00086: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0545 - val_mean_squared_error: 0.0545\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0174 - mean_squared_error: 0.0174\n",
      "Epoch 00087: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 88/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00088: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0718 - val_mean_squared_error: 0.0718\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0181 - mean_squared_error: 0.0181\n",
      "Epoch 00089: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0556 - val_mean_squared_error: 0.0556\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0470 - mean_squared_error: 0.0470\n",
      "Epoch 00090: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0399 - mean_squared_error: 0.0399 - val_loss: 0.1119 - val_mean_squared_error: 0.1119\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00091: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00092: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 58us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0143 - mean_squared_error: 0.0143\n",
      "Epoch 00093: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00094: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0891 - val_mean_squared_error: 0.0891\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00095: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 63us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0587 - val_mean_squared_error: 0.0587\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00096: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0627 - val_mean_squared_error: 0.0627\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0171 - mean_squared_error: 0.0171\n",
      "Epoch 00097: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0634 - val_mean_squared_error: 0.0634\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0544 - mean_squared_error: 0.0544\n",
      "Epoch 00098: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0457 - mean_squared_error: 0.0457 - val_loss: 0.0716 - val_mean_squared_error: 0.0716\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0158 - mean_squared_error: 0.0158\n",
      "Epoch 00099: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0173 - mean_squared_error: 0.0173 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00100: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0718 - val_mean_squared_error: 0.0718\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00101: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0706 - val_mean_squared_error: 0.0706\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00102: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0691 - val_mean_squared_error: 0.0691\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00103: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0569 - val_mean_squared_error: 0.0569\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00104: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0802 - val_mean_squared_error: 0.0802\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 00105: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00106: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0547 - val_mean_squared_error: 0.0547\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 00107: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0717 - val_mean_squared_error: 0.0717\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00108: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0573 - val_mean_squared_error: 0.0573\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0344 - mean_squared_error: 0.0344\n",
      "Epoch 00109: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0136 - mean_squared_error: 0.0136\n",
      "Epoch 00110: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0983 - val_mean_squared_error: 0.0983\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00111: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0579 - val_mean_squared_error: 0.0579\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0607 - mean_squared_error: 0.0607\n",
      "Epoch 00112: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0444 - mean_squared_error: 0.0444 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00113: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 114/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00114: val_loss did not improve from 0.03389\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0674 - val_mean_squared_error: 0.0674\n",
      "Running fold 7\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 1s - loss: 0.8166 - mean_squared_error: 0.8166\n",
      "Epoch 00001: val_loss improved from inf to 0.72467, saving model to model.h5\n",
      "248/248 [==============================] - 4s 15ms/sample - loss: 0.5824 - mean_squared_error: 0.5824 - val_loss: 0.7247 - val_mean_squared_error: 0.7247\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2608 - mean_squared_error: 0.2608\n",
      "Epoch 00002: val_loss improved from 0.72467 to 0.65496, saving model to model.h5\n",
      "248/248 [==============================] - 0s 519us/sample - loss: 0.2610 - mean_squared_error: 0.2610 - val_loss: 0.6550 - val_mean_squared_error: 0.6550\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2519 - mean_squared_error: 0.2519\n",
      "Epoch 00003: val_loss improved from 0.65496 to 0.60271, saving model to model.h5\n",
      "248/248 [==============================] - 0s 487us/sample - loss: 0.2361 - mean_squared_error: 0.2361 - val_loss: 0.6027 - val_mean_squared_error: 0.6027\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2144 - mean_squared_error: 0.2144\n",
      "Epoch 00004: val_loss improved from 0.60271 to 0.52685, saving model to model.h5\n",
      "248/248 [==============================] - 0s 452us/sample - loss: 0.2106 - mean_squared_error: 0.2106 - val_loss: 0.5269 - val_mean_squared_error: 0.5269\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1942 - mean_squared_error: 0.1942\n",
      "Epoch 00005: val_loss improved from 0.52685 to 0.33208, saving model to model.h5\n",
      "248/248 [==============================] - 0s 433us/sample - loss: 0.1881 - mean_squared_error: 0.1881 - val_loss: 0.3321 - val_mean_squared_error: 0.3321\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1396 - mean_squared_error: 0.1396\n",
      "Epoch 00006: val_loss improved from 0.33208 to 0.06109, saving model to model.h5\n",
      "248/248 [==============================] - 0s 430us/sample - loss: 0.0893 - mean_squared_error: 0.0893 - val_loss: 0.0611 - val_mean_squared_error: 0.0611\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00007: val_loss improved from 0.06109 to 0.05056, saving model to model.h5\n",
      "248/248 [==============================] - 0s 488us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0369 - mean_squared_error: 0.0369\n",
      "Epoch 00008: val_loss improved from 0.05056 to 0.04376, saving model to model.h5\n",
      "248/248 [==============================] - 0s 405us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00009: val_loss did not improve from 0.04376\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0624 - val_mean_squared_error: 0.0624\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0132 - mean_squared_error: 0.0132\n",
      "Epoch 00010: val_loss improved from 0.04376 to 0.03763, saving model to model.h5\n",
      "248/248 [==============================] - 0s 418us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00011: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0615 - val_mean_squared_error: 0.0615\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0148 - mean_squared_error: 0.0148\n",
      "Epoch 00012: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 63us/sample - loss: 0.0485 - mean_squared_error: 0.0485 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0140 - mean_squared_error: 0.0140\n",
      "Epoch 00013: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00014: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0634 - mean_squared_error: 0.0634\n",
      "Epoch 00015: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0455 - mean_squared_error: 0.0455 - val_loss: 0.0872 - val_mean_squared_error: 0.0872\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00016: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0572 - val_mean_squared_error: 0.0572\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0110 - mean_squared_error: 0.0110\n",
      "Epoch 00017: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0496 - mean_squared_error: 0.0496\n",
      "Epoch 00018: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 89us/sample - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0647 - val_mean_squared_error: 0.0647\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00019: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0328 - mean_squared_error: 0.0328 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00020: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Epoch 00021: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0805 - val_mean_squared_error: 0.0805\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0482 - mean_squared_error: 0.0482\n",
      "Epoch 00022: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0444 - mean_squared_error: 0.0444 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00023: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 67us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0648 - val_mean_squared_error: 0.0648\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00024: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 25/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0352 - mean_squared_error: 0.0352\n",
      "Epoch 00025: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0782 - val_mean_squared_error: 0.0782\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0142 - mean_squared_error: 0.0142\n",
      "Epoch 00026: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 67us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00027: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0184 - mean_squared_error: 0.0184\n",
      "Epoch 00028: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0928 - val_mean_squared_error: 0.0928\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0877 - mean_squared_error: 0.0877\n",
      "Epoch 00029: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0485 - mean_squared_error: 0.0485 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0132 - mean_squared_error: 0.0132\n",
      "Epoch 00030: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0836 - val_mean_squared_error: 0.0836\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Epoch 00031: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00032: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.1181 - val_mean_squared_error: 0.1181\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00033: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0832 - val_mean_squared_error: 0.0832\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00034: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0173 - mean_squared_error: 0.0173 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00035: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0735 - val_mean_squared_error: 0.0735\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0458 - mean_squared_error: 0.0458\n",
      "Epoch 00036: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0426 - mean_squared_error: 0.0426 - val_loss: 0.0592 - val_mean_squared_error: 0.0592\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0176 - mean_squared_error: 0.0176\n",
      "Epoch 00037: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.1047 - val_mean_squared_error: 0.1047\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00038: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00039: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0631 - val_mean_squared_error: 0.0631\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0125 - mean_squared_error: 0.0125\n",
      "Epoch 00040: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0156 - mean_squared_error: 0.0156 - val_loss: 0.0772 - val_mean_squared_error: 0.0772\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0146 - mean_squared_error: 0.0146\n",
      "Epoch 00041: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0579 - val_mean_squared_error: 0.0579\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0369 - mean_squared_error: 0.0369\n",
      "Epoch 00042: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0702 - mean_squared_error: 0.0702 - val_loss: 0.0526 - val_mean_squared_error: 0.0526\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0440 - mean_squared_error: 0.0440\n",
      "Epoch 00043: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0156 - mean_squared_error: 0.0156\n",
      "Epoch 00044: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0667 - val_mean_squared_error: 0.0667\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00045: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00046: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0650 - val_mean_squared_error: 0.0650\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0610 - mean_squared_error: 0.0610\n",
      "Epoch 00047: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0480 - mean_squared_error: 0.0480 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0150 - mean_squared_error: 0.0150\n",
      "Epoch 00048: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0718 - val_mean_squared_error: 0.0718\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0127 - mean_squared_error: 0.0127\n",
      "Epoch 00049: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00050: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0660 - val_mean_squared_error: 0.0660\n",
      "Epoch 51/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0153 - mean_squared_error: 0.0153\n",
      "Epoch 00051: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0129 - mean_squared_error: 0.0129\n",
      "Epoch 00052: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0166 - mean_squared_error: 0.0166 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0348 - mean_squared_error: 0.0348\n",
      "Epoch 00053: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 62us/sample - loss: 0.0591 - mean_squared_error: 0.0591 - val_loss: 0.0976 - val_mean_squared_error: 0.0976\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00054: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0623 - val_mean_squared_error: 0.0623\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0151 - mean_squared_error: 0.0151\n",
      "Epoch 00055: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00056: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00057: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0593 - val_mean_squared_error: 0.0593\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00058: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00059: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 00060: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0839 - val_mean_squared_error: 0.0839\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0136 - mean_squared_error: 0.0136\n",
      "Epoch 00061: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 67us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00062: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0580 - val_mean_squared_error: 0.0580\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00063: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0646 - val_mean_squared_error: 0.0646\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0607 - mean_squared_error: 0.0607\n",
      "Epoch 00064: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0390 - mean_squared_error: 0.0390 - val_loss: 0.0525 - val_mean_squared_error: 0.0525\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00065: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 62us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0807 - val_mean_squared_error: 0.0807\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00066: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0526 - val_mean_squared_error: 0.0526\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00067: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0110 - mean_squared_error: 0.0110\n",
      "Epoch 00068: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00069: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00070: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0779 - val_mean_squared_error: 0.0779\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 00071: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0845 - val_mean_squared_error: 0.0845\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00072: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0544 - val_mean_squared_error: 0.0544\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0176 - mean_squared_error: 0.0176\n",
      "Epoch 00073: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00074: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0642 - val_mean_squared_error: 0.0642\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00075: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0686 - val_mean_squared_error: 0.0686\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Epoch 00076: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0382 - mean_squared_error: 0.0382 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 77/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 00077: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 61us/sample - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0621 - val_mean_squared_error: 0.0621\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0145 - mean_squared_error: 0.0145\n",
      "Epoch 00078: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.1043 - val_mean_squared_error: 0.1043\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0684 - mean_squared_error: 0.0684\n",
      "Epoch 00079: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0455 - mean_squared_error: 0.0455 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00080: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00081: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0670 - val_mean_squared_error: 0.0670\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0158 - mean_squared_error: 0.0158\n",
      "Epoch 00082: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0701 - val_mean_squared_error: 0.0701\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0177 - mean_squared_error: 0.0177\n",
      "Epoch 00083: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00084: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0750 - val_mean_squared_error: 0.0750\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00085: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0654 - val_mean_squared_error: 0.0654\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00086: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0925 - val_mean_squared_error: 0.0925\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00087: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0162 - mean_squared_error: 0.0162 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00088: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0672 - val_mean_squared_error: 0.0672\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Epoch 00089: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0170 - mean_squared_error: 0.0170 - val_loss: 0.0583 - val_mean_squared_error: 0.0583\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00090: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0431 - mean_squared_error: 0.0431 - val_loss: 0.0646 - val_mean_squared_error: 0.0646\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00091: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00092: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0561 - val_mean_squared_error: 0.0561\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00093: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0698 - val_mean_squared_error: 0.0698\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00094: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Epoch 00095: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 63us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0693 - val_mean_squared_error: 0.0693\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00096: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0666 - val_mean_squared_error: 0.0666\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Epoch 00097: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0328 - mean_squared_error: 0.0328 - val_loss: 0.0808 - val_mean_squared_error: 0.0808\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00098: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 61us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00099: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0616 - val_mean_squared_error: 0.0616\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00100: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00101: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0786 - val_mean_squared_error: 0.0786\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Epoch 00102: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0602 - val_mean_squared_error: 0.0602\n",
      "Epoch 103/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0131 - mean_squared_error: 0.0131\n",
      "Epoch 00103: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0622 - val_mean_squared_error: 0.0622\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0395 - mean_squared_error: 0.0395\n",
      "Epoch 00104: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0591 - val_mean_squared_error: 0.0591\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Epoch 00105: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 63us/sample - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.1026 - val_mean_squared_error: 0.1026\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00106: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 67us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00107: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0604 - val_mean_squared_error: 0.0604\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0534 - mean_squared_error: 0.0534\n",
      "Epoch 00108: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0330 - mean_squared_error: 0.0330 - val_loss: 0.0696 - val_mean_squared_error: 0.0696\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00109: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0165 - mean_squared_error: 0.0165 - val_loss: 0.0762 - val_mean_squared_error: 0.0762\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00110: val_loss did not improve from 0.03763\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0160 - mean_squared_error: 0.0160 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Running fold 8\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 1s - loss: 1.2433 - mean_squared_error: 1.2433\n",
      "Epoch 00001: val_loss improved from inf to 0.77885, saving model to model.h5\n",
      "248/248 [==============================] - 4s 16ms/sample - loss: 0.9927 - mean_squared_error: 0.9927 - val_loss: 0.7788 - val_mean_squared_error: 0.7788\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2959 - mean_squared_error: 0.2959\n",
      "Epoch 00002: val_loss improved from 0.77885 to 0.72920, saving model to model.h5\n",
      "248/248 [==============================] - 0s 415us/sample - loss: 0.3497 - mean_squared_error: 0.3497 - val_loss: 0.7292 - val_mean_squared_error: 0.7292\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2569 - mean_squared_error: 0.2569\n",
      "Epoch 00003: val_loss improved from 0.72920 to 0.68520, saving model to model.h5\n",
      "248/248 [==============================] - 0s 414us/sample - loss: 0.2594 - mean_squared_error: 0.2594 - val_loss: 0.6852 - val_mean_squared_error: 0.6852\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2611 - mean_squared_error: 0.2611\n",
      "Epoch 00004: val_loss improved from 0.68520 to 0.64575, saving model to model.h5\n",
      "248/248 [==============================] - 0s 438us/sample - loss: 0.2402 - mean_squared_error: 0.2402 - val_loss: 0.6458 - val_mean_squared_error: 0.6458\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2437 - mean_squared_error: 0.2437\n",
      "Epoch 00005: val_loss improved from 0.64575 to 0.58885, saving model to model.h5\n",
      "248/248 [==============================] - 0s 416us/sample - loss: 0.2268 - mean_squared_error: 0.2268 - val_loss: 0.5888 - val_mean_squared_error: 0.5888\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2472 - mean_squared_error: 0.2472\n",
      "Epoch 00006: val_loss improved from 0.58885 to 0.46947, saving model to model.h5\n",
      "248/248 [==============================] - 0s 438us/sample - loss: 0.2394 - mean_squared_error: 0.2394 - val_loss: 0.4695 - val_mean_squared_error: 0.4695\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1664 - mean_squared_error: 0.1664\n",
      "Epoch 00007: val_loss improved from 0.46947 to 0.11019, saving model to model.h5\n",
      "248/248 [==============================] - 0s 434us/sample - loss: 0.1435 - mean_squared_error: 0.1435 - val_loss: 0.1102 - val_mean_squared_error: 0.1102\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00008: val_loss improved from 0.11019 to 0.05786, saving model to model.h5\n",
      "248/248 [==============================] - 0s 455us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0579 - val_mean_squared_error: 0.0579\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0151 - mean_squared_error: 0.0151\n",
      "Epoch 00009: val_loss improved from 0.05786 to 0.04871, saving model to model.h5\n",
      "248/248 [==============================] - 0s 443us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0137 - mean_squared_error: 0.0137\n",
      "Epoch 00010: val_loss did not improve from 0.04871\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0176 - mean_squared_error: 0.0176 - val_loss: 0.1026 - val_mean_squared_error: 0.1026\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00011: val_loss did not improve from 0.04871\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1628 - mean_squared_error: 0.1628\n",
      "Epoch 00012: val_loss did not improve from 0.04871\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.1163 - mean_squared_error: 0.1163 - val_loss: 0.1165 - val_mean_squared_error: 0.1165\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00013: val_loss improved from 0.04871 to 0.04585, saving model to model.h5\n",
      "248/248 [==============================] - 0s 505us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0341 - mean_squared_error: 0.0341\n",
      "Epoch 00014: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0546 - val_mean_squared_error: 0.0546\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0139 - mean_squared_error: 0.0139\n",
      "Epoch 00015: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0925 - val_mean_squared_error: 0.0925\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00016: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Epoch 00017: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0566 - val_mean_squared_error: 0.0566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Epoch 00018: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0729 - val_mean_squared_error: 0.0729\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0618 - mean_squared_error: 0.0618\n",
      "Epoch 00019: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0932 - val_mean_squared_error: 0.0932\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0451 - mean_squared_error: 0.0451\n",
      "Epoch 00020: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0458 - mean_squared_error: 0.0458 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00021: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0885 - val_mean_squared_error: 0.0885\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00022: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00023: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0455 - mean_squared_error: 0.0455 - val_loss: 0.0591 - val_mean_squared_error: 0.0591\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0387 - mean_squared_error: 0.0387\n",
      "Epoch 00024: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0554 - val_mean_squared_error: 0.0554\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00025: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0709 - val_mean_squared_error: 0.0709\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00026: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0737 - val_mean_squared_error: 0.0737\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00027: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0526 - mean_squared_error: 0.0526 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0371 - mean_squared_error: 0.0371\n",
      "Epoch 00028: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.0849 - val_mean_squared_error: 0.0849\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00029: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0173 - mean_squared_error: 0.0173 - val_loss: 0.0629 - val_mean_squared_error: 0.0629\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00030: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0585 - val_mean_squared_error: 0.0585\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0121 - mean_squared_error: 0.0121\n",
      "Epoch 00031: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.1387 - val_mean_squared_error: 0.1387\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0385 - mean_squared_error: 0.0385\n",
      "Epoch 00032: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0637 - val_mean_squared_error: 0.0637\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0902 - mean_squared_error: 0.0902\n",
      "Epoch 00033: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0759 - mean_squared_error: 0.0759 - val_loss: 0.0637 - val_mean_squared_error: 0.0637\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00034: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0833 - val_mean_squared_error: 0.0833\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0171 - mean_squared_error: 0.0171\n",
      "Epoch 00035: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00036: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0763 - val_mean_squared_error: 0.0763\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00037: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00038: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0624 - mean_squared_error: 0.0624 - val_loss: 0.0888 - val_mean_squared_error: 0.0888\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00039: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0687 - val_mean_squared_error: 0.0687\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00040: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0767 - val_mean_squared_error: 0.0767\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00041: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0746 - val_mean_squared_error: 0.0746\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0109 - mean_squared_error: 0.0109\n",
      "Epoch 00042: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0552 - val_mean_squared_error: 0.0552\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Epoch 00043: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0444 - mean_squared_error: 0.0444 - val_loss: 0.0712 - val_mean_squared_error: 0.0712\n",
      "Epoch 44/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0510 - mean_squared_error: 0.0510\n",
      "Epoch 00044: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0426 - mean_squared_error: 0.0426 - val_loss: 0.0825 - val_mean_squared_error: 0.0825\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0393 - mean_squared_error: 0.0393\n",
      "Epoch 00045: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00046: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0546 - val_mean_squared_error: 0.0546\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0120 - mean_squared_error: 0.0120\n",
      "Epoch 00047: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.1191 - val_mean_squared_error: 0.1191\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00048: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0527 - val_mean_squared_error: 0.0527\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00049: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0839 - val_mean_squared_error: 0.0839\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0764 - mean_squared_error: 0.0764\n",
      "Epoch 00050: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0669 - mean_squared_error: 0.0669 - val_loss: 0.0557 - val_mean_squared_error: 0.0557\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00051: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0558 - val_mean_squared_error: 0.0558\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00052: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0750 - val_mean_squared_error: 0.0750\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0090 - mean_squared_error: 0.0090\n",
      "Epoch 00053: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0164 - mean_squared_error: 0.0164 - val_loss: 0.0544 - val_mean_squared_error: 0.0544\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0135 - mean_squared_error: 0.0135\n",
      "Epoch 00054: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0160 - mean_squared_error: 0.0160 - val_loss: 0.0858 - val_mean_squared_error: 0.0858\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00055: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0478 - mean_squared_error: 0.0478 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00056: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00057: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0829 - val_mean_squared_error: 0.0829\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00058: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0568 - val_mean_squared_error: 0.0568\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0436 - mean_squared_error: 0.0436\n",
      "Epoch 00059: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0522 - mean_squared_error: 0.0522 - val_loss: 0.0539 - val_mean_squared_error: 0.0539\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00060: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0788 - val_mean_squared_error: 0.0788\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 00061: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0166 - mean_squared_error: 0.0166 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0176 - mean_squared_error: 0.0176\n",
      "Epoch 00062: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.1055 - val_mean_squared_error: 0.1055\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0395 - mean_squared_error: 0.0395\n",
      "Epoch 00063: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0518 - mean_squared_error: 0.0518 - val_loss: 0.1118 - val_mean_squared_error: 0.1118\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0581 - mean_squared_error: 0.0581\n",
      "Epoch 00064: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0442 - mean_squared_error: 0.0442\n",
      "Epoch 00065: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0957 - val_mean_squared_error: 0.0957\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00066: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0600 - val_mean_squared_error: 0.0600\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00067: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0972 - val_mean_squared_error: 0.0972\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00068: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0542 - val_mean_squared_error: 0.0542\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0468 - mean_squared_error: 0.0468\n",
      "Epoch 00069: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0593 - val_mean_squared_error: 0.0593\n",
      "Epoch 70/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00070: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.1113 - val_mean_squared_error: 0.1113\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00071: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0367 - mean_squared_error: 0.0367\n",
      "Epoch 00072: val_loss did not improve from 0.04585\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0385 - mean_squared_error: 0.0385 - val_loss: 0.0669 - val_mean_squared_error: 0.0669\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0085 - mean_squared_error: 0.0085\n",
      "Epoch 00073: val_loss improved from 0.04585 to 0.04558, saving model to model.h5\n",
      "248/248 [==============================] - 0s 452us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00074: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0543 - val_mean_squared_error: 0.0543\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0341 - mean_squared_error: 0.0341\n",
      "Epoch 00075: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0487 - mean_squared_error: 0.0487 - val_loss: 0.0718 - val_mean_squared_error: 0.0718\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00076: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0152 - mean_squared_error: 0.0152\n",
      "Epoch 00077: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0805 - val_mean_squared_error: 0.0805\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Epoch 00078: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0339 - mean_squared_error: 0.0339\n",
      "Epoch 00079: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0934 - val_mean_squared_error: 0.0934\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0561 - mean_squared_error: 0.0561\n",
      "Epoch 00080: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 86us/sample - loss: 0.0527 - mean_squared_error: 0.0527 - val_loss: 0.0511 - val_mean_squared_error: 0.0511\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0147 - mean_squared_error: 0.0147\n",
      "Epoch 00081: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0772 - val_mean_squared_error: 0.0772\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00082: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0602 - val_mean_squared_error: 0.0602\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0132 - mean_squared_error: 0.0132\n",
      "Epoch 00083: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0780 - val_mean_squared_error: 0.0780\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00084: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0396 - mean_squared_error: 0.0396\n",
      "Epoch 00085: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0494 - mean_squared_error: 0.0494 - val_loss: 0.0800 - val_mean_squared_error: 0.0800\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0503 - mean_squared_error: 0.0503\n",
      "Epoch 00086: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0448 - mean_squared_error: 0.0448 - val_loss: 0.0676 - val_mean_squared_error: 0.0676\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00087: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0641 - val_mean_squared_error: 0.0641\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0149 - mean_squared_error: 0.0149\n",
      "Epoch 00088: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0650 - val_mean_squared_error: 0.0650\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00089: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.1015 - val_mean_squared_error: 0.1015\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0418 - mean_squared_error: 0.0418\n",
      "Epoch 00090: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0447 - mean_squared_error: 0.0447 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0856 - mean_squared_error: 0.0856\n",
      "Epoch 00091: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0722 - mean_squared_error: 0.0722 - val_loss: 0.0741 - val_mean_squared_error: 0.0741\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00092: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0163 - mean_squared_error: 0.0163 - val_loss: 0.0890 - val_mean_squared_error: 0.0890\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00093: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0644 - val_mean_squared_error: 0.0644\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0157 - mean_squared_error: 0.0157\n",
      "Epoch 00094: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0802 - val_mean_squared_error: 0.0802\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0441 - mean_squared_error: 0.0441\n",
      "Epoch 00095: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 96/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00096: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0860 - val_mean_squared_error: 0.0860\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0344 - mean_squared_error: 0.0344\n",
      "Epoch 00097: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0103 - mean_squared_error: 0.0103\n",
      "Epoch 00098: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00099: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0433 - mean_squared_error: 0.0433 - val_loss: 0.1079 - val_mean_squared_error: 0.1079\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00100: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0132 - mean_squared_error: 0.0132\n",
      "Epoch 00101: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.1345 - val_mean_squared_error: 0.1345\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0560 - mean_squared_error: 0.0560\n",
      "Epoch 00102: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0435 - mean_squared_error: 0.0435 - val_loss: 0.0681 - val_mean_squared_error: 0.0681\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0107 - mean_squared_error: 0.0107\n",
      "Epoch 00103: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0902 - val_mean_squared_error: 0.0902\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0383 - mean_squared_error: 0.0383\n",
      "Epoch 00104: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0463 - mean_squared_error: 0.0463 - val_loss: 0.0688 - val_mean_squared_error: 0.0688\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00105: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0827 - val_mean_squared_error: 0.0827\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00106: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0652 - val_mean_squared_error: 0.0652\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00107: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0571 - val_mean_squared_error: 0.0571\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00108: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0848 - val_mean_squared_error: 0.0848\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00109: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0569 - val_mean_squared_error: 0.0569\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00110: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0465 - mean_squared_error: 0.0465 - val_loss: 0.0605 - val_mean_squared_error: 0.0605\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.0172\n",
      "Epoch 00111: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.1011 - val_mean_squared_error: 0.1011\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00112: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.1038 - val_mean_squared_error: 0.1038\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00113: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0387 - mean_squared_error: 0.0387\n",
      "Epoch 00114: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0974 - val_mean_squared_error: 0.0974\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00115: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0707 - val_mean_squared_error: 0.0707\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0713 - mean_squared_error: 0.0713\n",
      "Epoch 00116: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0624 - val_mean_squared_error: 0.0624\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00117: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0752 - val_mean_squared_error: 0.0752\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00118: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0401 - mean_squared_error: 0.0401 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0453 - mean_squared_error: 0.0453\n",
      "Epoch 00119: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0639 - val_mean_squared_error: 0.0639\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00120: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0172 - mean_squared_error: 0.0172 - val_loss: 0.1073 - val_mean_squared_error: 0.1073\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0398 - mean_squared_error: 0.0398\n",
      "Epoch 00121: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0395 - mean_squared_error: 0.0395 - val_loss: 0.0762 - val_mean_squared_error: 0.0762\n",
      "Epoch 122/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0114 - mean_squared_error: 0.0114\n",
      "Epoch 00122: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0348 - mean_squared_error: 0.0348\n",
      "Epoch 00123: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0736 - val_mean_squared_error: 0.0736\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0466 - mean_squared_error: 0.0466\n",
      "Epoch 00124: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0397 - mean_squared_error: 0.0397 - val_loss: 0.0632 - val_mean_squared_error: 0.0632\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00125: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0550 - val_mean_squared_error: 0.0550\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0147 - mean_squared_error: 0.0147\n",
      "Epoch 00126: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0626 - val_mean_squared_error: 0.0626\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00127: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0720 - val_mean_squared_error: 0.0720\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0587 - mean_squared_error: 0.0587\n",
      "Epoch 00128: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0370 - mean_squared_error: 0.0370 - val_loss: 0.1444 - val_mean_squared_error: 0.1444\n",
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00129: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0715 - val_mean_squared_error: 0.0715\n",
      "Epoch 130/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0167 - mean_squared_error: 0.0167\n",
      "Epoch 00130: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0660 - val_mean_squared_error: 0.0660\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0113 - mean_squared_error: 0.0113\n",
      "Epoch 00131: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0639 - val_mean_squared_error: 0.0639\n",
      "Epoch 132/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00132: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 61us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0856 - val_mean_squared_error: 0.0856\n",
      "Epoch 133/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00133: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0393 - mean_squared_error: 0.0393 - val_loss: 0.0632 - val_mean_squared_error: 0.0632\n",
      "Epoch 134/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0635 - mean_squared_error: 0.0635\n",
      "Epoch 00134: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0547 - mean_squared_error: 0.0547 - val_loss: 0.1100 - val_mean_squared_error: 0.1100\n",
      "Epoch 135/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00135: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0482 - val_mean_squared_error: 0.0482\n",
      "Epoch 136/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00136: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0814 - val_mean_squared_error: 0.0814\n",
      "Epoch 137/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00137: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0617 - val_mean_squared_error: 0.0617\n",
      "Epoch 138/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Epoch 00138: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.1146 - val_mean_squared_error: 0.1146\n",
      "Epoch 139/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Epoch 00139: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0641 - val_mean_squared_error: 0.0641\n",
      "Epoch 140/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00140: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.1231 - val_mean_squared_error: 0.1231\n",
      "Epoch 141/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0442 - mean_squared_error: 0.0442\n",
      "Epoch 00141: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 101us/sample - loss: 0.0474 - mean_squared_error: 0.0474 - val_loss: 0.0591 - val_mean_squared_error: 0.0591\n",
      "Epoch 142/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00142: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0535 - val_mean_squared_error: 0.0535\n",
      "Epoch 143/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00143: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0577 - val_mean_squared_error: 0.0577\n",
      "Epoch 144/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00144: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0934 - val_mean_squared_error: 0.0934\n",
      "Epoch 145/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0139 - mean_squared_error: 0.0139\n",
      "Epoch 00145: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0579 - val_mean_squared_error: 0.0579\n",
      "Epoch 146/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00146: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.1188 - val_mean_squared_error: 0.1188\n",
      "Epoch 147/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0603 - mean_squared_error: 0.0603\n",
      "Epoch 00147: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0323 - mean_squared_error: 0.0323 - val_loss: 0.0694 - val_mean_squared_error: 0.0694\n",
      "Epoch 148/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00148: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 149/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0159 - mean_squared_error: 0.0159\n",
      "Epoch 00149: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0738 - val_mean_squared_error: 0.0738\n",
      "Epoch 150/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0151 - mean_squared_error: 0.0151\n",
      "Epoch 00150: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 67us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 151/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00151: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 82us/sample - loss: 0.0476 - mean_squared_error: 0.0476 - val_loss: 0.0603 - val_mean_squared_error: 0.0603\n",
      "Epoch 152/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00152: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0777 - val_mean_squared_error: 0.0777\n",
      "Epoch 153/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00153: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 154/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00154: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0383 - mean_squared_error: 0.0383 - val_loss: 0.1388 - val_mean_squared_error: 0.1388\n",
      "Epoch 155/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0345 - mean_squared_error: 0.0345\n",
      "Epoch 00155: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0598 - val_mean_squared_error: 0.0598\n",
      "Epoch 156/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0140 - mean_squared_error: 0.0140\n",
      "Epoch 00156: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0169 - mean_squared_error: 0.0169 - val_loss: 0.0850 - val_mean_squared_error: 0.0850\n",
      "Epoch 157/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0170 - mean_squared_error: 0.0170\n",
      "Epoch 00157: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 158/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0123 - mean_squared_error: 0.0123\n",
      "Epoch 00158: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 159/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0577 - mean_squared_error: 0.0577\n",
      "Epoch 00159: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0380 - mean_squared_error: 0.0380 - val_loss: 0.0708 - val_mean_squared_error: 0.0708\n",
      "Epoch 160/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00160: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.1026 - val_mean_squared_error: 0.1026\n",
      "Epoch 161/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00161: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0596 - val_mean_squared_error: 0.0596\n",
      "Epoch 162/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00162: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0628 - val_mean_squared_error: 0.0628\n",
      "Epoch 163/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0375 - mean_squared_error: 0.0375\n",
      "Epoch 00163: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0941 - val_mean_squared_error: 0.0941\n",
      "Epoch 164/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00164: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0568 - val_mean_squared_error: 0.0568\n",
      "Epoch 165/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00165: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0584 - val_mean_squared_error: 0.0584\n",
      "Epoch 166/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00166: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 167/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0554 - mean_squared_error: 0.0554\n",
      "Epoch 00167: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0632 - val_mean_squared_error: 0.0632\n",
      "Epoch 168/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00168: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0598 - val_mean_squared_error: 0.0598\n",
      "Epoch 169/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00169: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.1157 - val_mean_squared_error: 0.1157\n",
      "Epoch 170/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00170: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0725 - val_mean_squared_error: 0.0725\n",
      "Epoch 171/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00171: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0447 - mean_squared_error: 0.0447 - val_loss: 0.0654 - val_mean_squared_error: 0.0654\n",
      "Epoch 172/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00172: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0794 - val_mean_squared_error: 0.0794\n",
      "Epoch 173/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0170 - mean_squared_error: 0.0170\n",
      "Epoch 00173: val_loss did not improve from 0.04558\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0743 - val_mean_squared_error: 0.0743\n",
      "Running fold 9\n",
      "(310, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 1s - loss: 1.4378 - mean_squared_error: 1.4378\n",
      "Epoch 00001: val_loss improved from inf to 0.74119, saving model to model.h5\n",
      "248/248 [==============================] - 4s 17ms/sample - loss: 1.0370 - mean_squared_error: 1.0370 - val_loss: 0.7412 - val_mean_squared_error: 0.7412\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3674 - mean_squared_error: 0.3674\n",
      "Epoch 00002: val_loss improved from 0.74119 to 0.68478, saving model to model.h5\n",
      "248/248 [==============================] - 0s 470us/sample - loss: 0.3082 - mean_squared_error: 0.3082 - val_loss: 0.6848 - val_mean_squared_error: 0.6848\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3303 - mean_squared_error: 0.3303\n",
      "Epoch 00003: val_loss improved from 0.68478 to 0.64503, saving model to model.h5\n",
      "248/248 [==============================] - 0s 486us/sample - loss: 0.2869 - mean_squared_error: 0.2869 - val_loss: 0.6450 - val_mean_squared_error: 0.6450\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2058 - mean_squared_error: 0.2058\n",
      "Epoch 00004: val_loss improved from 0.64503 to 0.60178, saving model to model.h5\n",
      "248/248 [==============================] - 0s 459us/sample - loss: 0.2262 - mean_squared_error: 0.2262 - val_loss: 0.6018 - val_mean_squared_error: 0.6018\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2515 - mean_squared_error: 0.2515\n",
      "Epoch 00005: val_loss improved from 0.60178 to 0.55143, saving model to model.h5\n",
      "248/248 [==============================] - 0s 481us/sample - loss: 0.2174 - mean_squared_error: 0.2174 - val_loss: 0.5514 - val_mean_squared_error: 0.5514\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2276 - mean_squared_error: 0.2276\n",
      "Epoch 00006: val_loss improved from 0.55143 to 0.44990, saving model to model.h5\n",
      "248/248 [==============================] - 0s 450us/sample - loss: 0.1943 - mean_squared_error: 0.1943 - val_loss: 0.4499 - val_mean_squared_error: 0.4499\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1453 - mean_squared_error: 0.1453\n",
      "Epoch 00007: val_loss improved from 0.44990 to 0.15289, saving model to model.h5\n",
      "248/248 [==============================] - 0s 448us/sample - loss: 0.1342 - mean_squared_error: 0.1342 - val_loss: 0.1529 - val_mean_squared_error: 0.1529\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0487 - mean_squared_error: 0.0487\n",
      "Epoch 00008: val_loss improved from 0.15289 to 0.05227, saving model to model.h5\n",
      "248/248 [==============================] - 0s 459us/sample - loss: 0.0424 - mean_squared_error: 0.0424 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0894 - mean_squared_error: 0.0894\n",
      "Epoch 00009: val_loss did not improve from 0.05227\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0547 - mean_squared_error: 0.0547 - val_loss: 0.0637 - val_mean_squared_error: 0.0637\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00010: val_loss did not improve from 0.05227\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0544 - val_mean_squared_error: 0.0544\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0486 - mean_squared_error: 0.0486\n",
      "Epoch 00011: val_loss did not improve from 0.05227\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0607 - val_mean_squared_error: 0.0607\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Epoch 00012: val_loss did not improve from 0.05227\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0473 - mean_squared_error: 0.0473 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00013: val_loss did not improve from 0.05227\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0585 - val_mean_squared_error: 0.0585\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0153 - mean_squared_error: 0.0153\n",
      "Epoch 00014: val_loss improved from 0.05227 to 0.05217, saving model to model.h5\n",
      "248/248 [==============================] - 0s 496us/sample - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00015: val_loss did not improve from 0.05217\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0527 - mean_squared_error: 0.0527 - val_loss: 0.0886 - val_mean_squared_error: 0.0886\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Epoch 00016: val_loss did not improve from 0.05217\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0550 - val_mean_squared_error: 0.0550\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0405 - mean_squared_error: 0.0405\n",
      "Epoch 00017: val_loss did not improve from 0.05217\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0981 - val_mean_squared_error: 0.0981\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0402 - mean_squared_error: 0.0402\n",
      "Epoch 00018: val_loss improved from 0.05217 to 0.04589, saving model to model.h5\n",
      "248/248 [==============================] - 0s 529us/sample - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00019: val_loss did not improve from 0.04589\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0897 - val_mean_squared_error: 0.0897\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00020: val_loss did not improve from 0.04589\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0527 - val_mean_squared_error: 0.0527\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00021: val_loss did not improve from 0.04589\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.1097 - val_mean_squared_error: 0.1097\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00022: val_loss did not improve from 0.04589\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0643 - val_mean_squared_error: 0.0643\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0572 - mean_squared_error: 0.0572\n",
      "Epoch 00023: val_loss did not improve from 0.04589\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0614 - mean_squared_error: 0.0614 - val_loss: 0.0922 - val_mean_squared_error: 0.0922\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00024: val_loss did not improve from 0.04589\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00025: val_loss did not improve from 0.04589\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0644 - val_mean_squared_error: 0.0644\n",
      "Epoch 26/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0159 - mean_squared_error: 0.0159\n",
      "Epoch 00026: val_loss did not improve from 0.04589\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0551 - val_mean_squared_error: 0.0551\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00027: val_loss did not improve from 0.04589\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0445 - mean_squared_error: 0.0445 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00028: val_loss did not improve from 0.04589\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00029: val_loss did not improve from 0.04589\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0664 - val_mean_squared_error: 0.0664\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0142 - mean_squared_error: 0.0142\n",
      "Epoch 00030: val_loss did not improve from 0.04589\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0973 - mean_squared_error: 0.0973\n",
      "Epoch 00031: val_loss did not improve from 0.04589\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0622 - mean_squared_error: 0.0622 - val_loss: 0.1186 - val_mean_squared_error: 0.1186\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0445 - mean_squared_error: 0.0445\n",
      "Epoch 00032: val_loss did not improve from 0.04589\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0433 - mean_squared_error: 0.0433 - val_loss: 0.0571 - val_mean_squared_error: 0.0571\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0431 - mean_squared_error: 0.0431\n",
      "Epoch 00033: val_loss did not improve from 0.04589\n",
      "248/248 [==============================] - 0s 83us/sample - loss: 0.0428 - mean_squared_error: 0.0428 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00034: val_loss did not improve from 0.04589\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0813 - val_mean_squared_error: 0.0813\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0147 - mean_squared_error: 0.0147\n",
      "Epoch 00035: val_loss improved from 0.04589 to 0.04539, saving model to model.h5\n",
      "248/248 [==============================] - 0s 497us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0487 - mean_squared_error: 0.0487\n",
      "Epoch 00036: val_loss did not improve from 0.04539\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0390 - mean_squared_error: 0.0390 - val_loss: 0.0648 - val_mean_squared_error: 0.0648\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Epoch 00037: val_loss did not improve from 0.04539\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0497 - mean_squared_error: 0.0497\n",
      "Epoch 00038: val_loss did not improve from 0.04539\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0491 - mean_squared_error: 0.0491 - val_loss: 0.0631 - val_mean_squared_error: 0.0631\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.0172\n",
      "Epoch 00039: val_loss did not improve from 0.04539\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.1001 - val_mean_squared_error: 0.1001\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00040: val_loss did not improve from 0.04539\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0319 - mean_squared_error: 0.0319 - val_loss: 0.0600 - val_mean_squared_error: 0.0600\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0641 - mean_squared_error: 0.0641\n",
      "Epoch 00041: val_loss did not improve from 0.04539\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0455 - mean_squared_error: 0.0455 - val_loss: 0.0755 - val_mean_squared_error: 0.0755\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00042: val_loss improved from 0.04539 to 0.04461, saving model to model.h5\n",
      "248/248 [==============================] - 0s 498us/sample - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0475 - mean_squared_error: 0.0475\n",
      "Epoch 00043: val_loss did not improve from 0.04461\n",
      "248/248 [==============================] - 0s 83us/sample - loss: 0.0319 - mean_squared_error: 0.0319 - val_loss: 0.0642 - val_mean_squared_error: 0.0642\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0137 - mean_squared_error: 0.0137\n",
      "Epoch 00044: val_loss did not improve from 0.04461\n",
      "248/248 [==============================] - 0s 86us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0857 - val_mean_squared_error: 0.0857\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0407 - mean_squared_error: 0.0407\n",
      "Epoch 00045: val_loss did not improve from 0.04461\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0402 - mean_squared_error: 0.0402 - val_loss: 0.0536 - val_mean_squared_error: 0.0536\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Epoch 00046: val_loss did not improve from 0.04461\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0975 - val_mean_squared_error: 0.0975\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0600 - mean_squared_error: 0.0600\n",
      "Epoch 00047: val_loss did not improve from 0.04461\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0422 - mean_squared_error: 0.0422 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Epoch 00048: val_loss did not improve from 0.04461\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0614 - val_mean_squared_error: 0.0614\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00049: val_loss did not improve from 0.04461\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0163 - mean_squared_error: 0.0163 - val_loss: 0.0893 - val_mean_squared_error: 0.0893\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Epoch 00050: val_loss did not improve from 0.04461\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0596 - mean_squared_error: 0.0596 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0409 - mean_squared_error: 0.0409\n",
      "Epoch 00051: val_loss did not improve from 0.04461\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0901 - val_mean_squared_error: 0.0901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00052: val_loss did not improve from 0.04461\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0573 - val_mean_squared_error: 0.0573\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00053: val_loss did not improve from 0.04461\n",
      "248/248 [==============================] - 0s 89us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0426 - mean_squared_error: 0.0426\n",
      "Epoch 00054: val_loss did not improve from 0.04461\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0690 - val_mean_squared_error: 0.0690\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00055: val_loss did not improve from 0.04461\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0892 - val_mean_squared_error: 0.0892\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0589 - mean_squared_error: 0.0589\n",
      "Epoch 00056: val_loss improved from 0.04461 to 0.04459, saving model to model.h5\n",
      "248/248 [==============================] - 0s 483us/sample - loss: 0.0552 - mean_squared_error: 0.0552 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0682 - mean_squared_error: 0.0682\n",
      "Epoch 00057: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0778 - val_mean_squared_error: 0.0778\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00058: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0173 - mean_squared_error: 0.0173 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00059: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0554 - val_mean_squared_error: 0.0554\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0473 - mean_squared_error: 0.0473\n",
      "Epoch 00060: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0553 - mean_squared_error: 0.0553 - val_loss: 0.0961 - val_mean_squared_error: 0.0961\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00061: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0151 - mean_squared_error: 0.0151 - val_loss: 0.0533 - val_mean_squared_error: 0.0533\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00062: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.1190 - val_mean_squared_error: 0.1190\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00063: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0550 - val_mean_squared_error: 0.0550\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00064: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0829 - val_mean_squared_error: 0.0829\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0376 - mean_squared_error: 0.0376\n",
      "Epoch 00065: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0454 - mean_squared_error: 0.0454\n",
      "Epoch 00066: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0434 - mean_squared_error: 0.0434 - val_loss: 0.0615 - val_mean_squared_error: 0.0615\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00067: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0548 - val_mean_squared_error: 0.0548\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0464 - mean_squared_error: 0.0464\n",
      "Epoch 00068: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0413 - mean_squared_error: 0.0413\n",
      "Epoch 00069: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.1224 - val_mean_squared_error: 0.1224\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00070: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0841 - val_mean_squared_error: 0.0841\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00071: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0676 - val_mean_squared_error: 0.0676\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00072: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0466 - mean_squared_error: 0.0466 - val_loss: 0.0557 - val_mean_squared_error: 0.0557\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0125 - mean_squared_error: 0.0125\n",
      "Epoch 00073: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.1079 - val_mean_squared_error: 0.1079\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00074: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 89us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0707 - val_mean_squared_error: 0.0707\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00075: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0432 - mean_squared_error: 0.0432 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00076: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0866 - val_mean_squared_error: 0.0866\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0437 - mean_squared_error: 0.0437\n",
      "Epoch 00077: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0627 - val_mean_squared_error: 0.0627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00078: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0796 - val_mean_squared_error: 0.0796\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00079: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0517 - val_mean_squared_error: 0.0517\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00080: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.1068 - val_mean_squared_error: 0.1068\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00081: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0862 - val_mean_squared_error: 0.0862\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00082: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0388 - mean_squared_error: 0.0388 - val_loss: 0.0714 - val_mean_squared_error: 0.0714\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0435 - mean_squared_error: 0.0435\n",
      "Epoch 00083: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0554 - val_mean_squared_error: 0.0554\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0089 - mean_squared_error: 0.0089\n",
      "Epoch 00084: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.1376 - val_mean_squared_error: 0.1376\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00085: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0712 - val_mean_squared_error: 0.0712\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0157 - mean_squared_error: 0.0157\n",
      "Epoch 00086: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0169 - mean_squared_error: 0.0169 - val_loss: 0.0699 - val_mean_squared_error: 0.0699\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00087: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 82us/sample - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0384 - mean_squared_error: 0.0384\n",
      "Epoch 00088: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0575 - val_mean_squared_error: 0.0575\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0075 - mean_squared_error: 0.0075\n",
      "Epoch 00089: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0631 - mean_squared_error: 0.0631\n",
      "Epoch 00090: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0400 - mean_squared_error: 0.0400 - val_loss: 0.0671 - val_mean_squared_error: 0.0671\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00091: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0175 - mean_squared_error: 0.0175 - val_loss: 0.0633 - val_mean_squared_error: 0.0633\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0431 - mean_squared_error: 0.0431\n",
      "Epoch 00092: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0495 - mean_squared_error: 0.0495 - val_loss: 0.0748 - val_mean_squared_error: 0.0748\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00093: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 83us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0608 - val_mean_squared_error: 0.0608\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 00094: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0688 - val_mean_squared_error: 0.0688\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0135 - mean_squared_error: 0.0135\n",
      "Epoch 00095: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0396 - mean_squared_error: 0.0396\n",
      "Epoch 00096: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.1011 - val_mean_squared_error: 0.1011\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00097: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0598 - val_mean_squared_error: 0.0598\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00098: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 86us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0569 - mean_squared_error: 0.0569\n",
      "Epoch 00099: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.1019 - val_mean_squared_error: 0.1019\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00100: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0593 - val_mean_squared_error: 0.0593\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00101: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0668 - val_mean_squared_error: 0.0668\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00102: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0556 - val_mean_squared_error: 0.0556\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0125 - mean_squared_error: 0.0125\n",
      "Epoch 00103: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0767 - val_mean_squared_error: 0.0767\n",
      "Epoch 104/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00104: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00105: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0643 - mean_squared_error: 0.0643\n",
      "Epoch 00106: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0480 - mean_squared_error: 0.0480 - val_loss: 0.0703 - val_mean_squared_error: 0.0703\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0348 - mean_squared_error: 0.0348\n",
      "Epoch 00107: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0530 - val_mean_squared_error: 0.0530\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0097 - mean_squared_error: 0.0097\n",
      "Epoch 00108: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0965 - val_mean_squared_error: 0.0965\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0365 - mean_squared_error: 0.0365\n",
      "Epoch 00109: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0403 - mean_squared_error: 0.0403 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00110: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0610 - val_mean_squared_error: 0.0610\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00111: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0682 - val_mean_squared_error: 0.0682\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00112: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0523 - mean_squared_error: 0.0523\n",
      "Epoch 00113: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0478 - mean_squared_error: 0.0478 - val_loss: 0.0934 - val_mean_squared_error: 0.0934\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00114: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0542 - val_mean_squared_error: 0.0542\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0171 - mean_squared_error: 0.0171\n",
      "Epoch 00115: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00116: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0834 - val_mean_squared_error: 0.0834\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00117: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00118: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0625 - val_mean_squared_error: 0.0625\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0151 - mean_squared_error: 0.0151\n",
      "Epoch 00119: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0977 - val_mean_squared_error: 0.0977\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00120: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0712 - val_mean_squared_error: 0.0712\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0167 - mean_squared_error: 0.0167\n",
      "Epoch 00121: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0764 - mean_squared_error: 0.0764\n",
      "Epoch 00122: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0588 - val_mean_squared_error: 0.0588\n",
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00123: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0721 - val_mean_squared_error: 0.0721\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00124: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.1170 - val_mean_squared_error: 0.1170\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00125: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0526 - val_mean_squared_error: 0.0526\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0167 - mean_squared_error: 0.0167\n",
      "Epoch 00126: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0533 - val_mean_squared_error: 0.0533\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0765 - mean_squared_error: 0.0765\n",
      "Epoch 00127: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0665 - mean_squared_error: 0.0665 - val_loss: 0.0548 - val_mean_squared_error: 0.0548\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0123 - mean_squared_error: 0.0123\n",
      "Epoch 00128: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0176 - mean_squared_error: 0.0176 - val_loss: 0.0873 - val_mean_squared_error: 0.0873\n",
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00129: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 130/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00130: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0520 - val_mean_squared_error: 0.0520\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0376 - mean_squared_error: 0.0376\n",
      "Epoch 00131: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0390 - mean_squared_error: 0.0390 - val_loss: 0.1189 - val_mean_squared_error: 0.1189\n",
      "Epoch 132/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00132: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0606 - val_mean_squared_error: 0.0606\n",
      "Epoch 133/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00133: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0824 - val_mean_squared_error: 0.0824\n",
      "Epoch 134/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0362 - mean_squared_error: 0.0362\n",
      "Epoch 00134: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.0561 - val_mean_squared_error: 0.0561\n",
      "Epoch 135/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0422 - mean_squared_error: 0.0422\n",
      "Epoch 00135: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0402 - mean_squared_error: 0.0402 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 136/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00136: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0671 - val_mean_squared_error: 0.0671\n",
      "Epoch 137/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00137: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0765 - val_mean_squared_error: 0.0765\n",
      "Epoch 138/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0128 - mean_squared_error: 0.0128\n",
      "Epoch 00138: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.1001 - val_mean_squared_error: 0.1001\n",
      "Epoch 139/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0341 - mean_squared_error: 0.0341\n",
      "Epoch 00139: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 140/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0468 - mean_squared_error: 0.0468\n",
      "Epoch 00140: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0693 - val_mean_squared_error: 0.0693\n",
      "Epoch 141/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00141: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0721 - val_mean_squared_error: 0.0721\n",
      "Epoch 142/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00142: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0544 - val_mean_squared_error: 0.0544\n",
      "Epoch 143/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00143: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.1205 - val_mean_squared_error: 0.1205\n",
      "Epoch 144/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0400 - mean_squared_error: 0.0400\n",
      "Epoch 00144: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0492 - mean_squared_error: 0.0492 - val_loss: 0.0666 - val_mean_squared_error: 0.0666\n",
      "Epoch 145/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00145: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0582 - val_mean_squared_error: 0.0582\n",
      "Epoch 146/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0259 - mean_squared_error: 0.0259\n",
      "Epoch 00146: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.1212 - val_mean_squared_error: 0.1212\n",
      "Epoch 147/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0343 - mean_squared_error: 0.0343\n",
      "Epoch 00147: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n",
      "Epoch 148/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00148: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0566 - val_mean_squared_error: 0.0566\n",
      "Epoch 149/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0105 - mean_squared_error: 0.0105\n",
      "Epoch 00149: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 97us/sample - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 150/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00150: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0817 - val_mean_squared_error: 0.0817\n",
      "Epoch 151/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0764 - mean_squared_error: 0.0764\n",
      "Epoch 00151: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0443 - mean_squared_error: 0.0443 - val_loss: 0.0906 - val_mean_squared_error: 0.0906\n",
      "Epoch 152/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00152: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0598 - val_mean_squared_error: 0.0598\n",
      "Epoch 153/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0145 - mean_squared_error: 0.0145\n",
      "Epoch 00153: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 154/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0406 - mean_squared_error: 0.0406\n",
      "Epoch 00154: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.1087 - val_mean_squared_error: 0.1087\n",
      "Epoch 155/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00155: val_loss did not improve from 0.04459\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0534 - val_mean_squared_error: 0.0534\n",
      "Epoch 156/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00156: val_loss did not improve from 0.04459\n",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0653 - val_mean_squared_error: 0.0653\n"
     ]
    }
   ],
   "source": [
    "# X_test.shape\n",
    "test_pred_ffnn  = []\n",
    "\n",
    "for fold in range(trials):\n",
    "    print(f'Running fold {fold}')\n",
    "    print(X_train.shape)\n",
    "\n",
    "\n",
    "    X_t_reshaped   = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_val_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "    \n",
    "    check = ModelCheckpoint('model.h5', verbose=1, save_best_only=True)\n",
    "\n",
    "    early = EarlyStopping(patience=100)\n",
    "\n",
    "    # Add fully connected layer with a ReLU activation function\n",
    "    model.add(Dense(units=32, activation='relu', input_shape=(X_t_reshaped.shape[1], X_t_reshaped.shape[2])))\n",
    "\n",
    "    # Add fully connected layer with a ReLU activation function\n",
    "    model.add(Dense(units=32, activation='relu'))\n",
    "\n",
    "    # Add fully connected layer with no activation function\n",
    "    model.add(Dense(units=1))\n",
    "    #model.add(Flatten())\n",
    "\n",
    "    # Compile neural network\n",
    "    model.compile(loss='mse', optimizer='rmsprop', metrics=['mse'])\n",
    "    history = model.fit(X_t_reshaped, \n",
    "                    y_train, \n",
    "                    validation_split = 0.2,\n",
    "                    epochs=1000, \n",
    "                    batch_size=96, \n",
    "                    verbose=1, callbacks=[check, early])\n",
    "    \n",
    "\n",
    "    #running function\n",
    "    test_forecast = X_test\n",
    "    y_ff = forecast(model, test_forecast, gwl, steps_ahead)\n",
    "    y_ff = np.array(y_ff)\n",
    "    \n",
    "     #metrics for test\n",
    "    test_pred_ffnn.append(y_ff)\n",
    "    mse_ff = mean_squared_error(y_test, y_ff)\n",
    "    r2_ff = r2_score(y_test, y_ff)\n",
    "    ffnn_ave_r2.append(r2_ff)\n",
    "    ffnn_ave_mse.append(mse_ff)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9a259b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8186606909784431,\n",
       " 0.8159537053341347,\n",
       " 0.8130948816291078,\n",
       " 0.813600461481261,\n",
       " 0.8043667388723509,\n",
       " 0.8162681800462156,\n",
       " 0.8084919187080053,\n",
       " 0.8330129855161243,\n",
       " 0.8024068422016792,\n",
       " 0.7237205350248963]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffnn_r2_new = ave(ffnn_ave_r2)\n",
    "ffnn_mse_new = ave(ffnn_ave_mse)\n",
    "ffnn_ave_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841cb8c0",
   "metadata": {},
   "source": [
    "## LSTM-GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73c3a862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running trial 1\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.1021 - mean_squared_error: 1.1021\n",
      "Epoch 00001: val_loss improved from inf to 0.71179, saving model to model.h5\n",
      "248/248 [==============================] - 4s 16ms/sample - loss: 1.0580 - mean_squared_error: 1.0580 - val_loss: 0.7118 - val_mean_squared_error: 0.7118\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9814 - mean_squared_error: 0.9814\n",
      "Epoch 00002: val_loss improved from 0.71179 to 0.69111, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.9924 - mean_squared_error: 0.9924 - val_loss: 0.6911 - val_mean_squared_error: 0.6911\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8793 - mean_squared_error: 0.8793\n",
      "Epoch 00003: val_loss improved from 0.69111 to 0.67042, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.9222 - mean_squared_error: 0.9222 - val_loss: 0.6704 - val_mean_squared_error: 0.6704\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.0425 - mean_squared_error: 1.0425\n",
      "Epoch 00004: val_loss improved from 0.67042 to 0.64775, saving model to model.h5\n",
      "248/248 [==============================] - 0s 210us/sample - loss: 0.8612 - mean_squared_error: 0.8612 - val_loss: 0.6477 - val_mean_squared_error: 0.6477\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8157 - mean_squared_error: 0.8157\n",
      "Epoch 00005: val_loss improved from 0.64775 to 0.62249, saving model to model.h5\n",
      "248/248 [==============================] - 0s 211us/sample - loss: 0.7804 - mean_squared_error: 0.7804 - val_loss: 0.6225 - val_mean_squared_error: 0.6225\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6756 - mean_squared_error: 0.6756\n",
      "Epoch 00006: val_loss improved from 0.62249 to 0.59593, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.7082 - mean_squared_error: 0.7082 - val_loss: 0.5959 - val_mean_squared_error: 0.5959\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7197 - mean_squared_error: 0.7197\n",
      "Epoch 00007: val_loss improved from 0.59593 to 0.56772, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.6266 - mean_squared_error: 0.6266 - val_loss: 0.5677 - val_mean_squared_error: 0.5677\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6232 - mean_squared_error: 0.6232\n",
      "Epoch 00008: val_loss improved from 0.56772 to 0.53648, saving model to model.h5\n",
      "248/248 [==============================] - 0s 196us/sample - loss: 0.5502 - mean_squared_error: 0.5502 - val_loss: 0.5365 - val_mean_squared_error: 0.5365\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4523 - mean_squared_error: 0.4523\n",
      "Epoch 00009: val_loss improved from 0.53648 to 0.50218, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.4625 - mean_squared_error: 0.4625 - val_loss: 0.5022 - val_mean_squared_error: 0.5022\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4165 - mean_squared_error: 0.4165\n",
      "Epoch 00010: val_loss improved from 0.50218 to 0.46626, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.3789 - mean_squared_error: 0.3789 - val_loss: 0.4663 - val_mean_squared_error: 0.4663\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3148 - mean_squared_error: 0.3148\n",
      "Epoch 00011: val_loss improved from 0.46626 to 0.42825, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.2960 - mean_squared_error: 0.2960 - val_loss: 0.4282 - val_mean_squared_error: 0.4282\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2402 - mean_squared_error: 0.2402\n",
      "Epoch 00012: val_loss improved from 0.42825 to 0.38895, saving model to model.h5\n",
      "248/248 [==============================] - 0s 203us/sample - loss: 0.2417 - mean_squared_error: 0.2417 - val_loss: 0.3889 - val_mean_squared_error: 0.3889\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1911 - mean_squared_error: 0.1911\n",
      "Epoch 00013: val_loss improved from 0.38895 to 0.34769, saving model to model.h5\n",
      "248/248 [==============================] - 0s 208us/sample - loss: 0.1857 - mean_squared_error: 0.1857 - val_loss: 0.3477 - val_mean_squared_error: 0.3477\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1512 - mean_squared_error: 0.1512\n",
      "Epoch 00014: val_loss improved from 0.34769 to 0.30692, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.1595 - mean_squared_error: 0.1595 - val_loss: 0.3069 - val_mean_squared_error: 0.3069\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1117 - mean_squared_error: 0.1117\n",
      "Epoch 00015: val_loss improved from 0.30692 to 0.26777, saving model to model.h5\n",
      "248/248 [==============================] - 0s 215us/sample - loss: 0.1335 - mean_squared_error: 0.1335 - val_loss: 0.2678 - val_mean_squared_error: 0.2678\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1425 - mean_squared_error: 0.1425\n",
      "Epoch 00016: val_loss improved from 0.26777 to 0.23202, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.1217 - mean_squared_error: 0.1217 - val_loss: 0.2320 - val_mean_squared_error: 0.2320\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1067 - mean_squared_error: 0.1067\n",
      "Epoch 00017: val_loss improved from 0.23202 to 0.20070, saving model to model.h5\n",
      "248/248 [==============================] - 0s 194us/sample - loss: 0.1023 - mean_squared_error: 0.1023 - val_loss: 0.2007 - val_mean_squared_error: 0.2007\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0854 - mean_squared_error: 0.0854\n",
      "Epoch 00018: val_loss improved from 0.20070 to 0.17401, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.0922 - mean_squared_error: 0.0922 - val_loss: 0.1740 - val_mean_squared_error: 0.1740\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1038 - mean_squared_error: 0.1038\n",
      "Epoch 00019: val_loss improved from 0.17401 to 0.15115, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0833 - mean_squared_error: 0.0833 - val_loss: 0.1511 - val_mean_squared_error: 0.1511\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0817 - mean_squared_error: 0.0817\n",
      "Epoch 00020: val_loss improved from 0.15115 to 0.13193, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.0651 - mean_squared_error: 0.0651 - val_loss: 0.1319 - val_mean_squared_error: 0.1319\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0711 - mean_squared_error: 0.0711\n",
      "Epoch 00021: val_loss improved from 0.13193 to 0.11599, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.0610 - mean_squared_error: 0.0610 - val_loss: 0.1160 - val_mean_squared_error: 0.1160\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0676 - mean_squared_error: 0.0676\n",
      "Epoch 00022: val_loss improved from 0.11599 to 0.10234, saving model to model.h5\n",
      "248/248 [==============================] - 0s 204us/sample - loss: 0.0593 - mean_squared_error: 0.0593 - val_loss: 0.1023 - val_mean_squared_error: 0.1023\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0633 - mean_squared_error: 0.0633\n",
      "Epoch 00023: val_loss improved from 0.10234 to 0.09121, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0547 - mean_squared_error: 0.0547 - val_loss: 0.0912 - val_mean_squared_error: 0.0912\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0431 - mean_squared_error: 0.0431\n",
      "Epoch 00024: val_loss improved from 0.09121 to 0.08252, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0525 - mean_squared_error: 0.0525 - val_loss: 0.0825 - val_mean_squared_error: 0.0825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0511 - mean_squared_error: 0.0511\n",
      "Epoch 00025: val_loss improved from 0.08252 to 0.07591, saving model to model.h5\n",
      "248/248 [==============================] - 0s 206us/sample - loss: 0.0487 - mean_squared_error: 0.0487 - val_loss: 0.0759 - val_mean_squared_error: 0.0759\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0501 - mean_squared_error: 0.0501\n",
      "Epoch 00026: val_loss improved from 0.07591 to 0.07050, saving model to model.h5\n",
      "248/248 [==============================] - 0s 180us/sample - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0705 - val_mean_squared_error: 0.0705\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0370 - mean_squared_error: 0.0370\n",
      "Epoch 00027: val_loss improved from 0.07050 to 0.06652, saving model to model.h5\n",
      "248/248 [==============================] - 0s 184us/sample - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0665 - val_mean_squared_error: 0.0665\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0396 - mean_squared_error: 0.0396\n",
      "Epoch 00028: val_loss improved from 0.06652 to 0.06355, saving model to model.h5\n",
      "248/248 [==============================] - 0s 181us/sample - loss: 0.0432 - mean_squared_error: 0.0432 - val_loss: 0.0635 - val_mean_squared_error: 0.0635\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0409 - mean_squared_error: 0.0409\n",
      "Epoch 00029: val_loss improved from 0.06355 to 0.06133, saving model to model.h5\n",
      "248/248 [==============================] - 0s 184us/sample - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0613 - val_mean_squared_error: 0.0613\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0427 - mean_squared_error: 0.0427\n",
      "Epoch 00030: val_loss improved from 0.06133 to 0.05993, saving model to model.h5\n",
      "248/248 [==============================] - 0s 191us/sample - loss: 0.0429 - mean_squared_error: 0.0429 - val_loss: 0.0599 - val_mean_squared_error: 0.0599\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0368 - mean_squared_error: 0.0368\n",
      "Epoch 00031: val_loss improved from 0.05993 to 0.05930, saving model to model.h5\n",
      "248/248 [==============================] - 0s 218us/sample - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0593 - val_mean_squared_error: 0.0593\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0429 - mean_squared_error: 0.0429\n",
      "Epoch 00032: val_loss improved from 0.05930 to 0.05916, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.0394 - mean_squared_error: 0.0394 - val_loss: 0.0592 - val_mean_squared_error: 0.0592\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0406 - mean_squared_error: 0.0406\n",
      "Epoch 00033: val_loss did not improve from 0.05916\n",
      "248/248 [==============================] - 0s 93us/sample - loss: 0.0443 - mean_squared_error: 0.0443 - val_loss: 0.0593 - val_mean_squared_error: 0.0593\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0444 - mean_squared_error: 0.0444\n",
      "Epoch 00034: val_loss did not improve from 0.05916\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0596 - val_mean_squared_error: 0.0596\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0371 - mean_squared_error: 0.0371\n",
      "Epoch 00035: val_loss did not improve from 0.05916\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0395 - mean_squared_error: 0.0395 - val_loss: 0.0595 - val_mean_squared_error: 0.0595\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Epoch 00036: val_loss improved from 0.05916 to 0.05906, saving model to model.h5\n",
      "248/248 [==============================] - 0s 215us/sample - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0591 - val_mean_squared_error: 0.0591\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0383 - mean_squared_error: 0.0383\n",
      "Epoch 00037: val_loss improved from 0.05906 to 0.05873, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0587 - val_mean_squared_error: 0.0587\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0380 - mean_squared_error: 0.0380\n",
      "Epoch 00038: val_loss improved from 0.05873 to 0.05827, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0400 - mean_squared_error: 0.0400 - val_loss: 0.0583 - val_mean_squared_error: 0.0583\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00039: val_loss improved from 0.05827 to 0.05799, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0580 - val_mean_squared_error: 0.0580\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0503 - mean_squared_error: 0.0503\n",
      "Epoch 00040: val_loss improved from 0.05799 to 0.05774, saving model to model.h5\n",
      "248/248 [==============================] - 0s 187us/sample - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0577 - val_mean_squared_error: 0.0577\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00041: val_loss improved from 0.05774 to 0.05744, saving model to model.h5\n",
      "248/248 [==============================] - 0s 179us/sample - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0574 - val_mean_squared_error: 0.0574\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0479 - mean_squared_error: 0.0479\n",
      "Epoch 00042: val_loss improved from 0.05744 to 0.05724, saving model to model.h5\n",
      "248/248 [==============================] - 0s 181us/sample - loss: 0.0398 - mean_squared_error: 0.0398 - val_loss: 0.0572 - val_mean_squared_error: 0.0572\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0379 - mean_squared_error: 0.0379\n",
      "Epoch 00043: val_loss improved from 0.05724 to 0.05682, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0568 - val_mean_squared_error: 0.0568\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0385 - mean_squared_error: 0.0385\n",
      "Epoch 00044: val_loss improved from 0.05682 to 0.05641, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0564 - val_mean_squared_error: 0.0564\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00045: val_loss improved from 0.05641 to 0.05592, saving model to model.h5\n",
      "248/248 [==============================] - 0s 191us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0559 - val_mean_squared_error: 0.0559\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0407 - mean_squared_error: 0.0407\n",
      "Epoch 00046: val_loss improved from 0.05592 to 0.05550, saving model to model.h5\n",
      "248/248 [==============================] - 0s 199us/sample - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0555 - val_mean_squared_error: 0.0555\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0458 - mean_squared_error: 0.0458\n",
      "Epoch 00047: val_loss improved from 0.05550 to 0.05529, saving model to model.h5\n",
      "248/248 [==============================] - 0s 207us/sample - loss: 0.0365 - mean_squared_error: 0.0365 - val_loss: 0.0553 - val_mean_squared_error: 0.0553\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0470 - mean_squared_error: 0.0470\n",
      "Epoch 00048: val_loss did not improve from 0.05529\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0388 - mean_squared_error: 0.0388 - val_loss: 0.0553 - val_mean_squared_error: 0.0553\n",
      "Epoch 49/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00049: val_loss improved from 0.05529 to 0.05517, saving model to model.h5\n",
      "248/248 [==============================] - 0s 233us/sample - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0552 - val_mean_squared_error: 0.0552\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00050: val_loss improved from 0.05517 to 0.05500, saving model to model.h5\n",
      "248/248 [==============================] - 0s 183us/sample - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0550 - val_mean_squared_error: 0.0550\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0390 - mean_squared_error: 0.0390\n",
      "Epoch 00051: val_loss improved from 0.05500 to 0.05488, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00052: val_loss improved from 0.05488 to 0.05451, saving model to model.h5\n",
      "248/248 [==============================] - 0s 196us/sample - loss: 0.0378 - mean_squared_error: 0.0378 - val_loss: 0.0545 - val_mean_squared_error: 0.0545\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0377 - mean_squared_error: 0.0377\n",
      "Epoch 00053: val_loss improved from 0.05451 to 0.05442, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0544 - val_mean_squared_error: 0.0544\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00054: val_loss improved from 0.05442 to 0.05424, saving model to model.h5\n",
      "248/248 [==============================] - 0s 183us/sample - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0542 - val_mean_squared_error: 0.0542\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00055: val_loss did not improve from 0.05424\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0544 - val_mean_squared_error: 0.0544\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0358 - mean_squared_error: 0.0358\n",
      "Epoch 00056: val_loss did not improve from 0.05424\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0547 - val_mean_squared_error: 0.0547\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00057: val_loss did not improve from 0.05424\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00058: val_loss did not improve from 0.05424\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0379 - mean_squared_error: 0.0379 - val_loss: 0.0547 - val_mean_squared_error: 0.0547\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0396 - mean_squared_error: 0.0396\n",
      "Epoch 00059: val_loss did not improve from 0.05424\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0548 - val_mean_squared_error: 0.0548\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00060: val_loss did not improve from 0.05424\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0374 - mean_squared_error: 0.0374 - val_loss: 0.0551 - val_mean_squared_error: 0.0551\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00061: val_loss did not improve from 0.05424\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0331 - mean_squared_error: 0.0331 - val_loss: 0.0552 - val_mean_squared_error: 0.0552\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0460 - mean_squared_error: 0.0460\n",
      "Epoch 00062: val_loss did not improve from 0.05424\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00063: val_loss did not improve from 0.05424\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0333 - mean_squared_error: 0.0333 - val_loss: 0.0543 - val_mean_squared_error: 0.0543\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00064: val_loss improved from 0.05424 to 0.05347, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0535 - val_mean_squared_error: 0.0535\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0397 - mean_squared_error: 0.0397\n",
      "Epoch 00065: val_loss improved from 0.05347 to 0.05283, saving model to model.h5\n",
      "248/248 [==============================] - 0s 194us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0528 - val_mean_squared_error: 0.0528\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00066: val_loss improved from 0.05283 to 0.05189, saving model to model.h5\n",
      "248/248 [==============================] - 0s 188us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0341 - mean_squared_error: 0.0341\n",
      "Epoch 00067: val_loss improved from 0.05189 to 0.05179, saving model to model.h5\n",
      "248/248 [==============================] - 0s 186us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00068: val_loss improved from 0.05179 to 0.05143, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0514 - val_mean_squared_error: 0.0514\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0378 - mean_squared_error: 0.0378\n",
      "Epoch 00069: val_loss improved from 0.05143 to 0.05084, saving model to model.h5\n",
      "248/248 [==============================] - 0s 190us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00070: val_loss improved from 0.05084 to 0.05065, saving model to model.h5\n",
      "248/248 [==============================] - 0s 196us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00071: val_loss did not improve from 0.05065\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0509 - val_mean_squared_error: 0.0509\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00072: val_loss did not improve from 0.05065\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0374 - mean_squared_error: 0.0374\n",
      "Epoch 00073: val_loss did not improve from 0.05065\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Epoch 00074: val_loss did not improve from 0.05065\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00075: val_loss improved from 0.05065 to 0.05043, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 00076: val_loss improved from 0.05043 to 0.04998, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00077: val_loss improved from 0.04998 to 0.04929, saving model to model.h5\n",
      "248/248 [==============================] - 0s 188us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 00078: val_loss improved from 0.04929 to 0.04894, saving model to model.h5\n",
      "248/248 [==============================] - 0s 184us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00079: val_loss improved from 0.04894 to 0.04893, saving model to model.h5\n",
      "248/248 [==============================] - 0s 182us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0380 - mean_squared_error: 0.0380\n",
      "Epoch 00080: val_loss did not improve from 0.04893\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0173 - mean_squared_error: 0.0173\n",
      "Epoch 00081: val_loss did not improve from 0.04893\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00082: val_loss did not improve from 0.04893\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Epoch 00083: val_loss did not improve from 0.04893\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0379 - mean_squared_error: 0.0379\n",
      "Epoch 00084: val_loss improved from 0.04893 to 0.04875, saving model to model.h5\n",
      "248/248 [==============================] - 0s 250us/sample - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0434 - mean_squared_error: 0.0434\n",
      "Epoch 00085: val_loss improved from 0.04875 to 0.04841, saving model to model.h5\n",
      "248/248 [==============================] - 0s 227us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Epoch 00086: val_loss did not improve from 0.04841\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00087: val_loss did not improve from 0.04841\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0525 - mean_squared_error: 0.0525\n",
      "Epoch 00088: val_loss did not improve from 0.04841\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00089: val_loss did not improve from 0.04841\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00090: val_loss improved from 0.04841 to 0.04833, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00091: val_loss improved from 0.04833 to 0.04791, saving model to model.h5\n",
      "248/248 [==============================] - 0s 192us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0144 - mean_squared_error: 0.0144\n",
      "Epoch 00092: val_loss improved from 0.04791 to 0.04716, saving model to model.h5\n",
      "248/248 [==============================] - 0s 190us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0173 - mean_squared_error: 0.0173\n",
      "Epoch 00093: val_loss improved from 0.04716 to 0.04634, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00094: val_loss improved from 0.04634 to 0.04541, saving model to model.h5\n",
      "248/248 [==============================] - 0s 187us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00095: val_loss improved from 0.04541 to 0.04487, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00096: val_loss improved from 0.04487 to 0.04453, saving model to model.h5\n",
      "248/248 [==============================] - 0s 191us/sample - loss: 0.0331 - mean_squared_error: 0.0331 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00097: val_loss did not improve from 0.04453\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00098: val_loss did not improve from 0.04453\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0333 - mean_squared_error: 0.0333\n",
      "Epoch 00099: val_loss did not improve from 0.04453\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00100: val_loss did not improve from 0.04453\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00101: val_loss did not improve from 0.04453\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0348 - mean_squared_error: 0.0348\n",
      "Epoch 00102: val_loss did not improve from 0.04453\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00103: val_loss did not improve from 0.04453\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00104: val_loss did not improve from 0.04453\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00105: val_loss did not improve from 0.04453\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0160 - mean_squared_error: 0.0160\n",
      "Epoch 00106: val_loss did not improve from 0.04453\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00107: val_loss did not improve from 0.04453\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0178 - mean_squared_error: 0.0178\n",
      "Epoch 00108: val_loss did not improve from 0.04453\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0131 - mean_squared_error: 0.0131\n",
      "Epoch 00109: val_loss did not improve from 0.04453\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00110: val_loss did not improve from 0.04453\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00111: val_loss did not improve from 0.04453\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00112: val_loss did not improve from 0.04453\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00113: val_loss did not improve from 0.04453\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0439 - mean_squared_error: 0.0439\n",
      "Epoch 00114: val_loss did not improve from 0.04453\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0341 - mean_squared_error: 0.0341\n",
      "Epoch 00115: val_loss did not improve from 0.04453\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00116: val_loss improved from 0.04453 to 0.04399, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0386 - mean_squared_error: 0.0386\n",
      "Epoch 00117: val_loss improved from 0.04399 to 0.04370, saving model to model.h5\n",
      "248/248 [==============================] - 0s 190us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00118: val_loss improved from 0.04370 to 0.04351, saving model to model.h5\n",
      "248/248 [==============================] - 0s 204us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00119: val_loss improved from 0.04351 to 0.04322, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00120: val_loss did not improve from 0.04322\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00121: val_loss did not improve from 0.04322\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0413 - mean_squared_error: 0.0413\n",
      "Epoch 00122: val_loss did not improve from 0.04322\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00123: val_loss did not improve from 0.04322\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0344 - mean_squared_error: 0.0344\n",
      "Epoch 00124: val_loss did not improve from 0.04322\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0365 - mean_squared_error: 0.0365\n",
      "Epoch 00125: val_loss did not improve from 0.04322\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00126: val_loss did not improve from 0.04322\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00127: val_loss did not improve from 0.04322\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0184 - mean_squared_error: 0.0184\n",
      "Epoch 00128: val_loss improved from 0.04322 to 0.04319, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00129: val_loss improved from 0.04319 to 0.04283, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 130/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00130: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00131: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 132/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0151 - mean_squared_error: 0.0151\n",
      "Epoch 00132: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 133/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00133: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 134/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00134: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 135/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00135: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 136/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Epoch 00136: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 137/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00137: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 138/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00138: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 139/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00139: val_loss did not improve from 0.04283\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 140/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00140: val_loss improved from 0.04283 to 0.04216, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 141/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00141: val_loss improved from 0.04216 to 0.04151, saving model to model.h5\n",
      "248/248 [==============================] - 0s 186us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 142/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00142: val_loss improved from 0.04151 to 0.04129, saving model to model.h5\n",
      "248/248 [==============================] - 0s 192us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 143/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00143: val_loss improved from 0.04129 to 0.04122, saving model to model.h5\n",
      "248/248 [==============================] - 0s 181us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 144/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00144: val_loss did not improve from 0.04122\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 145/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0411 - mean_squared_error: 0.0411\n",
      "Epoch 00145: val_loss did not improve from 0.04122\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 146/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00146: val_loss did not improve from 0.04122\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 147/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00147: val_loss did not improve from 0.04122\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 148/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00148: val_loss did not improve from 0.04122\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 149/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00149: val_loss did not improve from 0.04122\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 150/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00150: val_loss did not improve from 0.04122\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00151: val_loss did not improve from 0.04122\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 152/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00152: val_loss did not improve from 0.04122\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 153/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0367 - mean_squared_error: 0.0367\n",
      "Epoch 00153: val_loss did not improve from 0.04122\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 154/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00154: val_loss did not improve from 0.04122\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 155/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00155: val_loss did not improve from 0.04122\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 156/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00156: val_loss improved from 0.04122 to 0.04055, saving model to model.h5\n",
      "248/248 [==============================] - 0s 211us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 157/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0156 - mean_squared_error: 0.0156\n",
      "Epoch 00157: val_loss improved from 0.04055 to 0.04036, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 158/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00158: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 159/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 00159: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 160/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00160: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 161/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00161: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 162/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0372 - mean_squared_error: 0.0372\n",
      "Epoch 00162: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 163/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00163: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 164/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0352 - mean_squared_error: 0.0352\n",
      "Epoch 00164: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 59us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 165/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0259 - mean_squared_error: 0.0259\n",
      "Epoch 00165: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 166/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00166: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 167/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0135 - mean_squared_error: 0.0135\n",
      "Epoch 00167: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 168/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00168: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 169/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00169: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 170/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0184 - mean_squared_error: 0.0184\n",
      "Epoch 00170: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 171/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00171: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 172/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00172: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 173/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00173: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 174/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0171 - mean_squared_error: 0.0171\n",
      "Epoch 00174: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 175/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00175: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 176/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Epoch 00176: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00177: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 178/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00178: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 179/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00179: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 180/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00180: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 181/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00181: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 182/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00182: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 183/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00183: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 184/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00184: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 185/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00185: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 186/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00186: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 187/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00187: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 63us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 188/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00188: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 189/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00189: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 190/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0122 - mean_squared_error: 0.0122\n",
      "Epoch 00190: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 63us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 191/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00191: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 192/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00192: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 67us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 193/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00193: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 194/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00194: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 195/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Epoch 00195: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 196/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00196: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 197/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0184 - mean_squared_error: 0.0184\n",
      "Epoch 00197: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 198/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00198: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 199/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00199: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 200/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00200: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 201/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0141 - mean_squared_error: 0.0141\n",
      "Epoch 00201: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 202/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00202: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 203/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00203: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 204/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00204: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 205/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00205: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 63us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 206/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00206: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 207/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00207: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 208/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00208: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 209/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00209: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 210/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00210: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 211/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00211: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 212/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00212: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 213/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00213: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 214/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00214: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 215/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00215: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 216/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0135 - mean_squared_error: 0.0135\n",
      "Epoch 00216: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 217/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00217: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 218/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00218: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 219/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00219: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 220/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 00220: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 221/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00221: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 222/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0188 - mean_squared_error: 0.0188\n",
      "Epoch 00222: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 63us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 223/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0227 - mean_squared_error: 0.0227\n",
      "Epoch 00223: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 224/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0150 - mean_squared_error: 0.0150\n",
      "Epoch 00224: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 225/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00225: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 226/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00226: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 227/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0170 - mean_squared_error: 0.0170\n",
      "Epoch 00227: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 228/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0103 - mean_squared_error: 0.0103\n",
      "Epoch 00228: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 229/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00229: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 230/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00230: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 231/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00231: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 232/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0154 - mean_squared_error: 0.0154\n",
      "Epoch 00232: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 233/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00233: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 234/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00234: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 235/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00235: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 236/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00236: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 237/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00237: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 238/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00238: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 239/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0176 - mean_squared_error: 0.0176\n",
      "Epoch 00239: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 240/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00240: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 241/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Epoch 00241: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 242/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0150 - mean_squared_error: 0.0150\n",
      "Epoch 00242: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 243/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0174 - mean_squared_error: 0.0174\n",
      "Epoch 00243: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 244/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00244: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 245/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00245: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 246/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00246: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 247/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00247: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 248/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0231 - mean_squared_error: 0.0231\n",
      "Epoch 00248: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 249/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0174 - mean_squared_error: 0.0174\n",
      "Epoch 00249: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 250/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00250: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 251/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00251: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 252/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00252: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 253/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00253: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 254/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0094 - mean_squared_error: 0.0094\n",
      "Epoch 00254: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 255/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00255: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 256/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00256: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 257/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00257: val_loss did not improve from 0.04036\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Running trial 2\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.0098 - mean_squared_error: 1.0098\n",
      "Epoch 00001: val_loss improved from inf to 0.71648, saving model to model.h5\n",
      "248/248 [==============================] - 4s 17ms/sample - loss: 1.0837 - mean_squared_error: 1.0837 - val_loss: 0.7165 - val_mean_squared_error: 0.7165\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9652 - mean_squared_error: 0.9652\n",
      "Epoch 00002: val_loss improved from 0.71648 to 0.69338, saving model to model.h5\n",
      "248/248 [==============================] - 0s 212us/sample - loss: 1.0176 - mean_squared_error: 1.0176 - val_loss: 0.6934 - val_mean_squared_error: 0.6934\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8727 - mean_squared_error: 0.8727\n",
      "Epoch 00003: val_loss improved from 0.69338 to 0.66965, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.9437 - mean_squared_error: 0.9437 - val_loss: 0.6696 - val_mean_squared_error: 0.6696\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8343 - mean_squared_error: 0.8343\n",
      "Epoch 00004: val_loss improved from 0.66965 to 0.64448, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.8735 - mean_squared_error: 0.8735 - val_loss: 0.6445 - val_mean_squared_error: 0.6445\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6259 - mean_squared_error: 0.6259\n",
      "Epoch 00005: val_loss improved from 0.64448 to 0.61683, saving model to model.h5\n",
      "248/248 [==============================] - 0s 210us/sample - loss: 0.8021 - mean_squared_error: 0.8021 - val_loss: 0.6168 - val_mean_squared_error: 0.6168\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7726 - mean_squared_error: 0.7726\n",
      "Epoch 00006: val_loss improved from 0.61683 to 0.58720, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.7268 - mean_squared_error: 0.7268 - val_loss: 0.5872 - val_mean_squared_error: 0.5872\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5343 - mean_squared_error: 0.5343\n",
      "Epoch 00007: val_loss improved from 0.58720 to 0.55489, saving model to model.h5\n",
      "248/248 [==============================] - 0s 212us/sample - loss: 0.6375 - mean_squared_error: 0.6375 - val_loss: 0.5549 - val_mean_squared_error: 0.5549\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5339 - mean_squared_error: 0.5339\n",
      "Epoch 00008: val_loss improved from 0.55489 to 0.52068, saving model to model.h5\n",
      "248/248 [==============================] - 0s 207us/sample - loss: 0.5611 - mean_squared_error: 0.5611 - val_loss: 0.5207 - val_mean_squared_error: 0.5207\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4870 - mean_squared_error: 0.4870\n",
      "Epoch 00009: val_loss improved from 0.52068 to 0.48448, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.4740 - mean_squared_error: 0.4740 - val_loss: 0.4845 - val_mean_squared_error: 0.4845\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4150 - mean_squared_error: 0.4150\n",
      "Epoch 00010: val_loss improved from 0.48448 to 0.44656, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.3911 - mean_squared_error: 0.3911 - val_loss: 0.4466 - val_mean_squared_error: 0.4466\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3556 - mean_squared_error: 0.3556\n",
      "Epoch 00011: val_loss improved from 0.44656 to 0.40652, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.3139 - mean_squared_error: 0.3139 - val_loss: 0.4065 - val_mean_squared_error: 0.4065\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2733 - mean_squared_error: 0.2733\n",
      "Epoch 00012: val_loss improved from 0.40652 to 0.36546, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.2470 - mean_squared_error: 0.2470 - val_loss: 0.3655 - val_mean_squared_error: 0.3655\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2372 - mean_squared_error: 0.2372\n",
      "Epoch 00013: val_loss improved from 0.36546 to 0.32426, saving model to model.h5\n",
      "248/248 [==============================] - 0s 203us/sample - loss: 0.1911 - mean_squared_error: 0.1911 - val_loss: 0.3243 - val_mean_squared_error: 0.3243\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2130 - mean_squared_error: 0.2130\n",
      "Epoch 00014: val_loss improved from 0.32426 to 0.28453, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.1680 - mean_squared_error: 0.1680 - val_loss: 0.2845 - val_mean_squared_error: 0.2845\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1539 - mean_squared_error: 0.1539\n",
      "Epoch 00015: val_loss improved from 0.28453 to 0.24822, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.1491 - mean_squared_error: 0.1491 - val_loss: 0.2482 - val_mean_squared_error: 0.2482\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1303 - mean_squared_error: 0.1303\n",
      "Epoch 00016: val_loss improved from 0.24822 to 0.21588, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.1326 - mean_squared_error: 0.1326 - val_loss: 0.2159 - val_mean_squared_error: 0.2159\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1109 - mean_squared_error: 0.1109\n",
      "Epoch 00017: val_loss improved from 0.21588 to 0.18790, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.1235 - mean_squared_error: 0.1235 - val_loss: 0.1879 - val_mean_squared_error: 0.1879\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1032 - mean_squared_error: 0.1032\n",
      "Epoch 00018: val_loss improved from 0.18790 to 0.16361, saving model to model.h5\n",
      "248/248 [==============================] - 0s 191us/sample - loss: 0.1052 - mean_squared_error: 0.1052 - val_loss: 0.1636 - val_mean_squared_error: 0.1636\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0915 - mean_squared_error: 0.0915\n",
      "Epoch 00019: val_loss improved from 0.16361 to 0.14241, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.0889 - mean_squared_error: 0.0889 - val_loss: 0.1424 - val_mean_squared_error: 0.1424\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0783 - mean_squared_error: 0.0783\n",
      "Epoch 00020: val_loss improved from 0.14241 to 0.12443, saving model to model.h5\n",
      "248/248 [==============================] - 0s 194us/sample - loss: 0.0783 - mean_squared_error: 0.0783 - val_loss: 0.1244 - val_mean_squared_error: 0.1244\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0732 - mean_squared_error: 0.0732\n",
      "Epoch 00021: val_loss improved from 0.12443 to 0.10856, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.0662 - mean_squared_error: 0.0662 - val_loss: 0.1086 - val_mean_squared_error: 0.1086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0744 - mean_squared_error: 0.0744\n",
      "Epoch 00022: val_loss improved from 0.10856 to 0.09515, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0591 - mean_squared_error: 0.0591 - val_loss: 0.0952 - val_mean_squared_error: 0.0952\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0727 - mean_squared_error: 0.0727\n",
      "Epoch 00023: val_loss improved from 0.09515 to 0.08337, saving model to model.h5\n",
      "248/248 [==============================] - 0s 187us/sample - loss: 0.0568 - mean_squared_error: 0.0568 - val_loss: 0.0834 - val_mean_squared_error: 0.0834\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0596 - mean_squared_error: 0.0596\n",
      "Epoch 00024: val_loss improved from 0.08337 to 0.07361, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.0566 - mean_squared_error: 0.0566 - val_loss: 0.0736 - val_mean_squared_error: 0.0736\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0475 - mean_squared_error: 0.0475\n",
      "Epoch 00025: val_loss improved from 0.07361 to 0.06572, saving model to model.h5\n",
      "248/248 [==============================] - 0s 214us/sample - loss: 0.0518 - mean_squared_error: 0.0518 - val_loss: 0.0657 - val_mean_squared_error: 0.0657\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0456 - mean_squared_error: 0.0456\n",
      "Epoch 00026: val_loss improved from 0.06572 to 0.05970, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.0521 - mean_squared_error: 0.0521 - val_loss: 0.0597 - val_mean_squared_error: 0.0597\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 00027: val_loss improved from 0.05970 to 0.05533, saving model to model.h5\n",
      "248/248 [==============================] - 0s 226us/sample - loss: 0.0434 - mean_squared_error: 0.0434 - val_loss: 0.0553 - val_mean_squared_error: 0.0553\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Epoch 00028: val_loss improved from 0.05533 to 0.05208, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0450 - mean_squared_error: 0.0450 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0511 - mean_squared_error: 0.0511\n",
      "Epoch 00029: val_loss improved from 0.05208 to 0.04975, saving model to model.h5\n",
      "248/248 [==============================] - 0s 203us/sample - loss: 0.0426 - mean_squared_error: 0.0426 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0531 - mean_squared_error: 0.0531\n",
      "Epoch 00030: val_loss improved from 0.04975 to 0.04801, saving model to model.h5\n",
      "248/248 [==============================] - 0s 217us/sample - loss: 0.0425 - mean_squared_error: 0.0425 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0562 - mean_squared_error: 0.0562\n",
      "Epoch 00031: val_loss improved from 0.04801 to 0.04704, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.0425 - mean_squared_error: 0.0425 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00032: val_loss improved from 0.04704 to 0.04642, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0429 - mean_squared_error: 0.0429\n",
      "Epoch 00033: val_loss improved from 0.04642 to 0.04615, saving model to model.h5\n",
      "248/248 [==============================] - 0s 199us/sample - loss: 0.0393 - mean_squared_error: 0.0393 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0488 - mean_squared_error: 0.0488\n",
      "Epoch 00034: val_loss did not improve from 0.04615\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0427 - mean_squared_error: 0.0427 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0605 - mean_squared_error: 0.0605\n",
      "Epoch 00035: val_loss did not improve from 0.04615\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0418 - mean_squared_error: 0.0418 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0487 - mean_squared_error: 0.0487\n",
      "Epoch 00036: val_loss did not improve from 0.04615\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0392 - mean_squared_error: 0.0392 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0398 - mean_squared_error: 0.0398\n",
      "Epoch 00037: val_loss did not improve from 0.04615\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0404 - mean_squared_error: 0.0404\n",
      "Epoch 00038: val_loss did not improve from 0.04615\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0453 - mean_squared_error: 0.0453\n",
      "Epoch 00039: val_loss did not improve from 0.04615\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0361 - mean_squared_error: 0.0361 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0442 - mean_squared_error: 0.0442\n",
      "Epoch 00040: val_loss did not improve from 0.04615\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.0474 - val_mean_squared_error: 0.0474\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00041: val_loss did not improve from 0.04615\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Epoch 00042: val_loss did not improve from 0.04615\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00043: val_loss improved from 0.04615 to 0.04609, saving model to model.h5\n",
      "248/248 [==============================] - 0s 212us/sample - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00044: val_loss improved from 0.04609 to 0.04586, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0384 - mean_squared_error: 0.0384 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0449 - mean_squared_error: 0.0449\n",
      "Epoch 00045: val_loss improved from 0.04586 to 0.04572, saving model to model.h5\n",
      "248/248 [==============================] - 0s 187us/sample - loss: 0.0370 - mean_squared_error: 0.0370 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00046: val_loss improved from 0.04572 to 0.04525, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Epoch 00047: val_loss improved from 0.04525 to 0.04502, saving model to model.h5\n",
      "248/248 [==============================] - 0s 206us/sample - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0358 - mean_squared_error: 0.0358\n",
      "Epoch 00048: val_loss did not improve from 0.04502\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00049: val_loss did not improve from 0.04502\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00050: val_loss did not improve from 0.04502\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Epoch 00051: val_loss did not improve from 0.04502\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0333 - mean_squared_error: 0.0333 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00052: val_loss did not improve from 0.04502\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0383 - mean_squared_error: 0.0383\n",
      "Epoch 00053: val_loss did not improve from 0.04502\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Epoch 00054: val_loss did not improve from 0.04502\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0319 - mean_squared_error: 0.0319 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00055: val_loss did not improve from 0.04502\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0374 - mean_squared_error: 0.0374\n",
      "Epoch 00056: val_loss did not improve from 0.04502\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0390 - mean_squared_error: 0.0390\n",
      "Epoch 00057: val_loss did not improve from 0.04502\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00058: val_loss did not improve from 0.04502\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0376 - mean_squared_error: 0.0376\n",
      "Epoch 00059: val_loss did not improve from 0.04502\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0372 - mean_squared_error: 0.0372 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00060: val_loss did not improve from 0.04502\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00061: val_loss did not improve from 0.04502\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0393 - mean_squared_error: 0.0393\n",
      "Epoch 00062: val_loss did not improve from 0.04502\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Epoch 00063: val_loss did not improve from 0.04502\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0339 - mean_squared_error: 0.0339 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00064: val_loss did not improve from 0.04502\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0365 - mean_squared_error: 0.0365 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0339 - mean_squared_error: 0.0339\n",
      "Epoch 00065: val_loss improved from 0.04502 to 0.04499, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00066: val_loss improved from 0.04499 to 0.04461, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00067: val_loss improved from 0.04461 to 0.04460, saving model to model.h5\n",
      "248/248 [==============================] - 0s 191us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00068: val_loss improved from 0.04460 to 0.04453, saving model to model.h5\n",
      "248/248 [==============================] - 0s 185us/sample - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00069: val_loss improved from 0.04453 to 0.04416, saving model to model.h5\n",
      "248/248 [==============================] - 0s 188us/sample - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Epoch 00070: val_loss improved from 0.04416 to 0.04383, saving model to model.h5\n",
      "248/248 [==============================] - 0s 186us/sample - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00071: val_loss improved from 0.04383 to 0.04347, saving model to model.h5\n",
      "248/248 [==============================] - 0s 194us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 72/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00072: val_loss did not improve from 0.04347\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0377 - mean_squared_error: 0.0377\n",
      "Epoch 00073: val_loss did not improve from 0.04347\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00074: val_loss did not improve from 0.04347\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0435 - mean_squared_error: 0.0435\n",
      "Epoch 00075: val_loss did not improve from 0.04347\n",
      "248/248 [==============================] - 0s 61us/sample - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00076: val_loss improved from 0.04347 to 0.04344, saving model to model.h5\n",
      "248/248 [==============================] - 0s 192us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00077: val_loss improved from 0.04344 to 0.04282, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0380 - mean_squared_error: 0.0380\n",
      "Epoch 00078: val_loss improved from 0.04282 to 0.04256, saving model to model.h5\n",
      "248/248 [==============================] - 0s 186us/sample - loss: 0.0328 - mean_squared_error: 0.0328 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Epoch 00079: val_loss improved from 0.04256 to 0.04244, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0374 - mean_squared_error: 0.0374\n",
      "Epoch 00080: val_loss improved from 0.04244 to 0.04234, saving model to model.h5\n",
      "248/248 [==============================] - 0s 194us/sample - loss: 0.0328 - mean_squared_error: 0.0328 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00081: val_loss improved from 0.04234 to 0.04215, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00082: val_loss did not improve from 0.04215\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00083: val_loss did not improve from 0.04215\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0447 - mean_squared_error: 0.0447\n",
      "Epoch 00084: val_loss did not improve from 0.04215\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00085: val_loss did not improve from 0.04215\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00086: val_loss did not improve from 0.04215\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0371 - mean_squared_error: 0.0371\n",
      "Epoch 00087: val_loss did not improve from 0.04215\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00088: val_loss improved from 0.04215 to 0.04210, saving model to model.h5\n",
      "248/248 [==============================] - 0s 220us/sample - loss: 0.0319 - mean_squared_error: 0.0319 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00089: val_loss improved from 0.04210 to 0.04167, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Epoch 00090: val_loss improved from 0.04167 to 0.04103, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0400 - mean_squared_error: 0.0400\n",
      "Epoch 00091: val_loss improved from 0.04103 to 0.04066, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00092: val_loss improved from 0.04066 to 0.03986, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00093: val_loss improved from 0.03986 to 0.03947, saving model to model.h5\n",
      "248/248 [==============================] - 0s 199us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0341 - mean_squared_error: 0.0341\n",
      "Epoch 00094: val_loss improved from 0.03947 to 0.03945, saving model to model.h5\n",
      "248/248 [==============================] - 0s 192us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00095: val_loss did not improve from 0.03945\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0308 - mean_squared_error: 0.0308 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00096: val_loss did not improve from 0.03945\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0429 - mean_squared_error: 0.0429\n",
      "Epoch 00097: val_loss did not improve from 0.03945\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0424 - mean_squared_error: 0.0424\n",
      "Epoch 00098: val_loss did not improve from 0.03945\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00099: val_loss did not improve from 0.03945\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Epoch 00100: val_loss did not improve from 0.03945\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00101: val_loss did not improve from 0.03945\n",
      "248/248 [==============================] - 0s 62us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00102: val_loss did not improve from 0.03945\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00103: val_loss did not improve from 0.03945\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0382 - mean_squared_error: 0.0382\n",
      "Epoch 00104: val_loss did not improve from 0.03945\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0259 - mean_squared_error: 0.0259\n",
      "Epoch 00105: val_loss did not improve from 0.03945\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00106: val_loss did not improve from 0.03945\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00107: val_loss did not improve from 0.03945\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00108: val_loss did not improve from 0.03945\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00109: val_loss did not improve from 0.03945\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00110: val_loss did not improve from 0.03945\n",
      "248/248 [==============================] - 0s 63us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00111: val_loss improved from 0.03945 to 0.03929, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00112: val_loss improved from 0.03929 to 0.03908, saving model to model.h5\n",
      "248/248 [==============================] - 0s 253us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00113: val_loss did not improve from 0.03908\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Epoch 00114: val_loss did not improve from 0.03908\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Epoch 00115: val_loss did not improve from 0.03908\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00116: val_loss did not improve from 0.03908\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00117: val_loss did not improve from 0.03908\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00118: val_loss did not improve from 0.03908\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0295 - mean_squared_error: 0.0295\n",
      "Epoch 00119: val_loss improved from 0.03908 to 0.03857, saving model to model.h5\n",
      "248/248 [==============================] - 0s 192us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0294 - mean_squared_error: 0.0294\n",
      "Epoch 00120: val_loss improved from 0.03857 to 0.03796, saving model to model.h5\n",
      "248/248 [==============================] - 0s 185us/sample - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00121: val_loss improved from 0.03796 to 0.03783, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00122: val_loss improved from 0.03783 to 0.03775, saving model to model.h5\n",
      "248/248 [==============================] - 0s 185us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00123: val_loss did not improve from 0.03775\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00124: val_loss did not improve from 0.03775\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0360 - mean_squared_error: 0.0360\n",
      "Epoch 00125: val_loss did not improve from 0.03775\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0398 - mean_squared_error: 0.0398\n",
      "Epoch 00126: val_loss did not improve from 0.03775\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0348 - mean_squared_error: 0.0348\n",
      "Epoch 00127: val_loss did not improve from 0.03775\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0339 - mean_squared_error: 0.0339\n",
      "Epoch 00128: val_loss improved from 0.03775 to 0.03774, saving model to model.h5\n",
      "248/248 [==============================] - 0s 237us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00129: val_loss improved from 0.03774 to 0.03744, saving model to model.h5\n",
      "248/248 [==============================] - 0s 186us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 130/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00130: val_loss improved from 0.03744 to 0.03715, saving model to model.h5\n",
      "248/248 [==============================] - 0s 184us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00131: val_loss did not improve from 0.03715\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 132/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00132: val_loss did not improve from 0.03715\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 133/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00133: val_loss did not improve from 0.03715\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 134/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00134: val_loss did not improve from 0.03715\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 135/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0139 - mean_squared_error: 0.0139\n",
      "Epoch 00135: val_loss did not improve from 0.03715\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 136/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00136: val_loss did not improve from 0.03715\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0383 - val_mean_squared_error: 0.0383\n",
      "Epoch 137/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00137: val_loss did not improve from 0.03715\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 138/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00138: val_loss improved from 0.03715 to 0.03688, saving model to model.h5\n",
      "248/248 [==============================] - 0s 183us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 139/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0259 - mean_squared_error: 0.0259\n",
      "Epoch 00139: val_loss improved from 0.03688 to 0.03611, saving model to model.h5\n",
      "248/248 [==============================] - 0s 191us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0361 - val_mean_squared_error: 0.0361\n",
      "Epoch 140/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00140: val_loss improved from 0.03611 to 0.03597, saving model to model.h5\n",
      "248/248 [==============================] - 0s 185us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0360 - val_mean_squared_error: 0.0360\n",
      "Epoch 141/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00141: val_loss did not improve from 0.03597\n",
      "248/248 [==============================] - 0s 67us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0361 - val_mean_squared_error: 0.0361\n",
      "Epoch 142/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00142: val_loss did not improve from 0.03597\n",
      "248/248 [==============================] - 0s 63us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0361 - val_mean_squared_error: 0.0361\n",
      "Epoch 143/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00143: val_loss improved from 0.03597 to 0.03581, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0358 - val_mean_squared_error: 0.0358\n",
      "Epoch 144/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Epoch 00144: val_loss did not improve from 0.03581\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n",
      "Epoch 145/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00145: val_loss did not improve from 0.03581\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 146/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00146: val_loss did not improve from 0.03581\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 147/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 00147: val_loss did not improve from 0.03581\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 148/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00148: val_loss did not improve from 0.03581\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 149/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00149: val_loss did not improve from 0.03581\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 150/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 00150: val_loss did not improve from 0.03581\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0365 - val_mean_squared_error: 0.0365\n",
      "Epoch 151/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00151: val_loss did not improve from 0.03581\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0362 - val_mean_squared_error: 0.0362\n",
      "Epoch 152/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00152: val_loss did not improve from 0.03581\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0360 - val_mean_squared_error: 0.0360\n",
      "Epoch 153/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Epoch 00153: val_loss did not improve from 0.03581\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0365 - val_mean_squared_error: 0.0365\n",
      "Epoch 154/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00154: val_loss did not improve from 0.03581\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 155/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00155: val_loss did not improve from 0.03581\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 156/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00156: val_loss did not improve from 0.03581\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 157/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00157: val_loss did not improve from 0.03581\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 158/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00158: val_loss did not improve from 0.03581\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 159/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0161 - mean_squared_error: 0.0161\n",
      "Epoch 00159: val_loss did not improve from 0.03581\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0366 - val_mean_squared_error: 0.0366\n",
      "Epoch 160/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Epoch 00160: val_loss did not improve from 0.03581\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0366 - val_mean_squared_error: 0.0366\n",
      "Epoch 161/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0378 - mean_squared_error: 0.0378\n",
      "Epoch 00161: val_loss did not improve from 0.03581\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0363 - val_mean_squared_error: 0.0363\n",
      "Epoch 162/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00162: val_loss did not improve from 0.03581\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0358 - val_mean_squared_error: 0.0358\n",
      "Epoch 163/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00163: val_loss improved from 0.03581 to 0.03534, saving model to model.h5\n",
      "248/248 [==============================] - 0s 191us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0353 - val_mean_squared_error: 0.0353\n",
      "Epoch 164/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00164: val_loss improved from 0.03534 to 0.03533, saving model to model.h5\n",
      "248/248 [==============================] - 0s 184us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0353 - val_mean_squared_error: 0.0353\n",
      "Epoch 165/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00165: val_loss did not improve from 0.03533\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0354 - val_mean_squared_error: 0.0354\n",
      "Epoch 166/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00166: val_loss did not improve from 0.03533\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0355 - val_mean_squared_error: 0.0355\n",
      "Epoch 167/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00167: val_loss did not improve from 0.03533\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0354 - val_mean_squared_error: 0.0354\n",
      "Epoch 168/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0393 - mean_squared_error: 0.0393\n",
      "Epoch 00168: val_loss did not improve from 0.03533\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0361 - val_mean_squared_error: 0.0361\n",
      "Epoch 169/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0401 - mean_squared_error: 0.0401\n",
      "Epoch 00169: val_loss did not improve from 0.03533\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0361 - val_mean_squared_error: 0.0361\n",
      "Epoch 170/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00170: val_loss did not improve from 0.03533\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0359 - val_mean_squared_error: 0.0359\n",
      "Epoch 171/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.0185\n",
      "Epoch 00171: val_loss did not improve from 0.03533\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0355 - val_mean_squared_error: 0.0355\n",
      "Epoch 172/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00172: val_loss did not improve from 0.03533\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0355 - val_mean_squared_error: 0.0355\n",
      "Epoch 173/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00173: val_loss did not improve from 0.03533\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0355 - val_mean_squared_error: 0.0355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0385 - mean_squared_error: 0.0385\n",
      "Epoch 00174: val_loss did not improve from 0.03533\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0356 - val_mean_squared_error: 0.0356\n",
      "Epoch 175/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00175: val_loss did not improve from 0.03533\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0354 - val_mean_squared_error: 0.0354\n",
      "Epoch 176/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00176: val_loss improved from 0.03533 to 0.03496, saving model to model.h5\n",
      "248/248 [==============================] - 0s 239us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0350 - val_mean_squared_error: 0.0350\n",
      "Epoch 177/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00177: val_loss improved from 0.03496 to 0.03490, saving model to model.h5\n",
      "248/248 [==============================] - 0s 186us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 178/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0140 - mean_squared_error: 0.0140\n",
      "Epoch 00178: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0351 - val_mean_squared_error: 0.0351\n",
      "Epoch 179/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00179: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0362 - val_mean_squared_error: 0.0362\n",
      "Epoch 180/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00180: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 181/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00181: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 182/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00182: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 183/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00183: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 184/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00184: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 185/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00185: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0366 - val_mean_squared_error: 0.0366\n",
      "Epoch 186/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00186: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0359 - val_mean_squared_error: 0.0359\n",
      "Epoch 187/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00187: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0355 - val_mean_squared_error: 0.0355\n",
      "Epoch 188/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0390 - mean_squared_error: 0.0390\n",
      "Epoch 00188: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0356 - val_mean_squared_error: 0.0356\n",
      "Epoch 189/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00189: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0356 - val_mean_squared_error: 0.0356\n",
      "Epoch 190/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00190: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0357 - val_mean_squared_error: 0.0357\n",
      "Epoch 191/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00191: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0361 - val_mean_squared_error: 0.0361\n",
      "Epoch 192/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Epoch 00192: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0362 - val_mean_squared_error: 0.0362\n",
      "Epoch 193/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00193: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0360 - val_mean_squared_error: 0.0360\n",
      "Epoch 194/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00194: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 92us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0362 - val_mean_squared_error: 0.0362\n",
      "Epoch 195/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00195: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0363 - val_mean_squared_error: 0.0363\n",
      "Epoch 196/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00196: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n",
      "Epoch 197/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00197: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0367 - val_mean_squared_error: 0.0367\n",
      "Epoch 198/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00198: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 101us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0362 - val_mean_squared_error: 0.0362\n",
      "Epoch 199/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00199: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00200: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 93us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0363 - val_mean_squared_error: 0.0363\n",
      "Epoch 201/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00201: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0366 - val_mean_squared_error: 0.0366\n",
      "Epoch 202/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00202: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n",
      "Epoch 203/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00203: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0361 - val_mean_squared_error: 0.0361\n",
      "Epoch 204/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00204: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0353 - val_mean_squared_error: 0.0353\n",
      "Epoch 205/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0385 - mean_squared_error: 0.0385\n",
      "Epoch 00205: val_loss did not improve from 0.03490\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 206/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0121 - mean_squared_error: 0.0121\n",
      "Epoch 00206: val_loss improved from 0.03490 to 0.03464, saving model to model.h5\n",
      "248/248 [==============================] - 0s 245us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0346 - val_mean_squared_error: 0.0346\n",
      "Epoch 207/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00207: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0350 - val_mean_squared_error: 0.0350\n",
      "Epoch 208/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00208: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0353 - val_mean_squared_error: 0.0353\n",
      "Epoch 209/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0171 - mean_squared_error: 0.0171\n",
      "Epoch 00209: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 62us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0357 - val_mean_squared_error: 0.0357\n",
      "Epoch 210/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00210: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0359 - val_mean_squared_error: 0.0359\n",
      "Epoch 211/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Epoch 00211: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0360 - val_mean_squared_error: 0.0360\n",
      "Epoch 212/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00212: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0356 - val_mean_squared_error: 0.0356\n",
      "Epoch 213/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00213: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0353 - val_mean_squared_error: 0.0353\n",
      "Epoch 214/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00214: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 215/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00215: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0348 - val_mean_squared_error: 0.0348\n",
      "Epoch 216/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00216: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0347 - val_mean_squared_error: 0.0347\n",
      "Epoch 217/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00217: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0360 - val_mean_squared_error: 0.0360\n",
      "Epoch 218/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00218: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 219/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00219: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 220/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00220: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 221/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 00221: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 222/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00222: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 223/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00223: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 67us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 224/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00224: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 225/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0134 - mean_squared_error: 0.0134\n",
      "Epoch 00225: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00226: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0363 - val_mean_squared_error: 0.0363\n",
      "Epoch 227/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00227: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0361 - val_mean_squared_error: 0.0361\n",
      "Epoch 228/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0438 - mean_squared_error: 0.0438\n",
      "Epoch 00228: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0362 - val_mean_squared_error: 0.0362\n",
      "Epoch 229/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0307 - mean_squared_error: 0.0307\n",
      "Epoch 00229: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0365 - val_mean_squared_error: 0.0365\n",
      "Epoch 230/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00230: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 231/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00231: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 232/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0174 - mean_squared_error: 0.0174\n",
      "Epoch 00232: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 233/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00233: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 234/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00234: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 235/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00235: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 236/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00236: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 237/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0124 - mean_squared_error: 0.0124\n",
      "Epoch 00237: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 238/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0141 - mean_squared_error: 0.0141\n",
      "Epoch 00238: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 239/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00239: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 240/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00240: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 241/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00241: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 60us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 242/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00242: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 243/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00243: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 244/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00244: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 245/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00245: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 246/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00246: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 247/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00247: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 248/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00248: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 67us/sample - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 249/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0090 - mean_squared_error: 0.0090\n",
      "Epoch 00249: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 250/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00250: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 251/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00251: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 252/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00252: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 253/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00253: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 254/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00254: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 255/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00255: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 256/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00256: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 257/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00257: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 258/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00258: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 259/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00259: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 260/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00260: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 261/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0115 - mean_squared_error: 0.0115\n",
      "Epoch 00261: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 262/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00262: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 263/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00263: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 264/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00264: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 265/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0130 - mean_squared_error: 0.0130\n",
      "Epoch 00265: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 266/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00266: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 267/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0227 - mean_squared_error: 0.0227\n",
      "Epoch 00267: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 268/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00268: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 269/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0173 - mean_squared_error: 0.0173\n",
      "Epoch 00269: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 270/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00270: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 271/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00271: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 272/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00272: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 273/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00273: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 274/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00274: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 275/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00275: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 276/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0173 - mean_squared_error: 0.0173\n",
      "Epoch 00276: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 277/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00277: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 278/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00278: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 279/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00279: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 280/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0102 - mean_squared_error: 0.0102\n",
      "Epoch 00280: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 281/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00281: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 282/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.0185\n",
      "Epoch 00282: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 283/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0176 - mean_squared_error: 0.0176\n",
      "Epoch 00283: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 284/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00284: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 285/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00285: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 286/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Epoch 00286: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 287/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00287: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 288/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00288: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 289/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0170 - mean_squared_error: 0.0170\n",
      "Epoch 00289: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 290/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00290: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 291/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00291: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 292/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00292: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 293/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0156 - mean_squared_error: 0.0156\n",
      "Epoch 00293: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 294/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00294: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 295/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0119 - mean_squared_error: 0.0119\n",
      "Epoch 00295: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 296/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0160 - mean_squared_error: 0.0160\n",
      "Epoch 00296: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 297/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0227 - mean_squared_error: 0.0227\n",
      "Epoch 00297: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 298/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00298: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 299/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Epoch 00299: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 300/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0125 - mean_squared_error: 0.0125\n",
      "Epoch 00300: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 301/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00301: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 302/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0114 - mean_squared_error: 0.0114\n",
      "Epoch 00302: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 303/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Epoch 00303: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 304/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0188 - mean_squared_error: 0.0188\n",
      "Epoch 00304: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 305/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0153 - mean_squared_error: 0.0153\n",
      "Epoch 00305: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 306/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00306: val_loss did not improve from 0.03464\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Running trial 3\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 1s - loss: 0.9448 - mean_squared_error: 0.9448\n",
      "Epoch 00001: val_loss improved from inf to 0.67424, saving model to model.h5\n",
      "248/248 [==============================] - 5s 19ms/sample - loss: 1.0255 - mean_squared_error: 1.0255 - val_loss: 0.6742 - val_mean_squared_error: 0.6742\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9652 - mean_squared_error: 0.9652\n",
      "Epoch 00002: val_loss improved from 0.67424 to 0.64304, saving model to model.h5\n",
      "248/248 [==============================] - 0s 215us/sample - loss: 0.9398 - mean_squared_error: 0.9398 - val_loss: 0.6430 - val_mean_squared_error: 0.6430\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7794 - mean_squared_error: 0.7794\n",
      "Epoch 00003: val_loss improved from 0.64304 to 0.61279, saving model to model.h5\n",
      "248/248 [==============================] - 0s 206us/sample - loss: 0.8537 - mean_squared_error: 0.8537 - val_loss: 0.6128 - val_mean_squared_error: 0.6128\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6504 - mean_squared_error: 0.6504\n",
      "Epoch 00004: val_loss improved from 0.61279 to 0.58115, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.7665 - mean_squared_error: 0.7665 - val_loss: 0.5812 - val_mean_squared_error: 0.5812\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7033 - mean_squared_error: 0.7033\n",
      "Epoch 00005: val_loss improved from 0.58115 to 0.54874, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.6876 - mean_squared_error: 0.6876 - val_loss: 0.5487 - val_mean_squared_error: 0.5487\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6365 - mean_squared_error: 0.6365\n",
      "Epoch 00006: val_loss improved from 0.54874 to 0.51439, saving model to model.h5\n",
      "248/248 [==============================] - 0s 223us/sample - loss: 0.6001 - mean_squared_error: 0.6001 - val_loss: 0.5144 - val_mean_squared_error: 0.5144\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5373 - mean_squared_error: 0.5373\n",
      "Epoch 00007: val_loss improved from 0.51439 to 0.47721, saving model to model.h5\n",
      "248/248 [==============================] - 0s 214us/sample - loss: 0.5139 - mean_squared_error: 0.5139 - val_loss: 0.4772 - val_mean_squared_error: 0.4772\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3711 - mean_squared_error: 0.3711\n",
      "Epoch 00008: val_loss improved from 0.47721 to 0.43895, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.4173 - mean_squared_error: 0.4173 - val_loss: 0.4389 - val_mean_squared_error: 0.4389\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3491 - mean_squared_error: 0.3491\n",
      "Epoch 00009: val_loss improved from 0.43895 to 0.39865, saving model to model.h5\n",
      "248/248 [==============================] - 0s 190us/sample - loss: 0.3363 - mean_squared_error: 0.3363 - val_loss: 0.3987 - val_mean_squared_error: 0.3987\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3117 - mean_squared_error: 0.3117\n",
      "Epoch 00010: val_loss improved from 0.39865 to 0.35688, saving model to model.h5\n",
      "248/248 [==============================] - 0s 199us/sample - loss: 0.2618 - mean_squared_error: 0.2618 - val_loss: 0.3569 - val_mean_squared_error: 0.3569\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2310 - mean_squared_error: 0.2310\n",
      "Epoch 00011: val_loss improved from 0.35688 to 0.31317, saving model to model.h5\n",
      "248/248 [==============================] - 0s 217us/sample - loss: 0.2159 - mean_squared_error: 0.2159 - val_loss: 0.3132 - val_mean_squared_error: 0.3132\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1777 - mean_squared_error: 0.1777\n",
      "Epoch 00012: val_loss improved from 0.31317 to 0.27053, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.1840 - mean_squared_error: 0.1840 - val_loss: 0.2705 - val_mean_squared_error: 0.2705\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1399 - mean_squared_error: 0.1399\n",
      "Epoch 00013: val_loss improved from 0.27053 to 0.23093, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.1496 - mean_squared_error: 0.1496 - val_loss: 0.2309 - val_mean_squared_error: 0.2309\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1190 - mean_squared_error: 0.1190\n",
      "Epoch 00014: val_loss improved from 0.23093 to 0.19619, saving model to model.h5\n",
      "248/248 [==============================] - 0s 199us/sample - loss: 0.1373 - mean_squared_error: 0.1373 - val_loss: 0.1962 - val_mean_squared_error: 0.1962\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1445 - mean_squared_error: 0.1445\n",
      "Epoch 00015: val_loss improved from 0.19619 to 0.16625, saving model to model.h5\n",
      "248/248 [==============================] - 0s 192us/sample - loss: 0.1200 - mean_squared_error: 0.1200 - val_loss: 0.1662 - val_mean_squared_error: 0.1662\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0910 - mean_squared_error: 0.0910\n",
      "Epoch 00016: val_loss improved from 0.16625 to 0.14132, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.0945 - mean_squared_error: 0.0945 - val_loss: 0.1413 - val_mean_squared_error: 0.1413\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0858 - mean_squared_error: 0.0858\n",
      "Epoch 00017: val_loss improved from 0.14132 to 0.12060, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.0837 - mean_squared_error: 0.0837 - val_loss: 0.1206 - val_mean_squared_error: 0.1206\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0861 - mean_squared_error: 0.0861\n",
      "Epoch 00018: val_loss improved from 0.12060 to 0.10342, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.0741 - mean_squared_error: 0.0741 - val_loss: 0.1034 - val_mean_squared_error: 0.1034\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0612 - mean_squared_error: 0.0612\n",
      "Epoch 00019: val_loss improved from 0.10342 to 0.08907, saving model to model.h5\n",
      "248/248 [==============================] - 0s 192us/sample - loss: 0.0641 - mean_squared_error: 0.0641 - val_loss: 0.0891 - val_mean_squared_error: 0.0891\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0716 - mean_squared_error: 0.0716\n",
      "Epoch 00020: val_loss improved from 0.08907 to 0.07738, saving model to model.h5\n",
      "248/248 [==============================] - 0s 188us/sample - loss: 0.0547 - mean_squared_error: 0.0547 - val_loss: 0.0774 - val_mean_squared_error: 0.0774\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0484 - mean_squared_error: 0.0484\n",
      "Epoch 00021: val_loss improved from 0.07738 to 0.06834, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.0498 - mean_squared_error: 0.0498 - val_loss: 0.0683 - val_mean_squared_error: 0.0683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0437 - mean_squared_error: 0.0437\n",
      "Epoch 00022: val_loss improved from 0.06834 to 0.06135, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0481 - mean_squared_error: 0.0481 - val_loss: 0.0613 - val_mean_squared_error: 0.0613\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0442 - mean_squared_error: 0.0442\n",
      "Epoch 00023: val_loss improved from 0.06135 to 0.05626, saving model to model.h5\n",
      "248/248 [==============================] - 0s 241us/sample - loss: 0.0465 - mean_squared_error: 0.0465 - val_loss: 0.0563 - val_mean_squared_error: 0.0563\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0583 - mean_squared_error: 0.0583\n",
      "Epoch 00024: val_loss improved from 0.05626 to 0.05302, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.0450 - mean_squared_error: 0.0450 - val_loss: 0.0530 - val_mean_squared_error: 0.0530\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0396 - mean_squared_error: 0.0396\n",
      "Epoch 00025: val_loss improved from 0.05302 to 0.05087, saving model to model.h5\n",
      "248/248 [==============================] - 0s 206us/sample - loss: 0.0425 - mean_squared_error: 0.0425 - val_loss: 0.0509 - val_mean_squared_error: 0.0509\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0487 - mean_squared_error: 0.0487\n",
      "Epoch 00026: val_loss improved from 0.05087 to 0.04962, saving model to model.h5\n",
      "248/248 [==============================] - 0s 190us/sample - loss: 0.0458 - mean_squared_error: 0.0458 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0339 - mean_squared_error: 0.0339\n",
      "Epoch 00027: val_loss improved from 0.04962 to 0.04899, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Epoch 00028: val_loss did not improve from 0.04899\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0383 - mean_squared_error: 0.0383 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0394 - mean_squared_error: 0.0394\n",
      "Epoch 00029: val_loss did not improve from 0.04899\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0451 - mean_squared_error: 0.0451\n",
      "Epoch 00030: val_loss did not improve from 0.04899\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0425 - mean_squared_error: 0.0425\n",
      "Epoch 00031: val_loss did not improve from 0.04899\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0386 - mean_squared_error: 0.0386 - val_loss: 0.0500 - val_mean_squared_error: 0.0500\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0487 - mean_squared_error: 0.0487\n",
      "Epoch 00032: val_loss did not improve from 0.04899\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0385 - mean_squared_error: 0.0385 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0416 - mean_squared_error: 0.0416\n",
      "Epoch 00033: val_loss did not improve from 0.04899\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0381 - mean_squared_error: 0.0381 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0365 - mean_squared_error: 0.0365\n",
      "Epoch 00034: val_loss did not improve from 0.04899\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0371 - mean_squared_error: 0.0371 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0452 - mean_squared_error: 0.0452\n",
      "Epoch 00035: val_loss did not improve from 0.04899\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00036: val_loss did not improve from 0.04899\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0395 - mean_squared_error: 0.0395\n",
      "Epoch 00037: val_loss did not improve from 0.04899\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0343 - mean_squared_error: 0.0343\n",
      "Epoch 00038: val_loss did not improve from 0.04899\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Epoch 00039: val_loss did not improve from 0.04899\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0361 - mean_squared_error: 0.0361 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0400 - mean_squared_error: 0.0400\n",
      "Epoch 00040: val_loss did not improve from 0.04899\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Epoch 00041: val_loss improved from 0.04899 to 0.04887, saving model to model.h5\n",
      "248/248 [==============================] - 0s 209us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0390 - mean_squared_error: 0.0390\n",
      "Epoch 00042: val_loss improved from 0.04887 to 0.04855, saving model to model.h5\n",
      "248/248 [==============================] - 0s 220us/sample - loss: 0.0366 - mean_squared_error: 0.0366 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0454 - mean_squared_error: 0.0454\n",
      "Epoch 00043: val_loss improved from 0.04855 to 0.04848, saving model to model.h5\n",
      "248/248 [==============================] - 0s 188us/sample - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Epoch 00044: val_loss did not improve from 0.04848\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0365 - mean_squared_error: 0.0365 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0362 - mean_squared_error: 0.0362\n",
      "Epoch 00045: val_loss did not improve from 0.04848\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00046: val_loss did not improve from 0.04848\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 47/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0386 - mean_squared_error: 0.0386\n",
      "Epoch 00047: val_loss did not improve from 0.04848\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00048: val_loss did not improve from 0.04848\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00049: val_loss did not improve from 0.04848\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0366 - mean_squared_error: 0.0366 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00050: val_loss did not improve from 0.04848\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Epoch 00051: val_loss did not improve from 0.04848\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0365 - mean_squared_error: 0.0365 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0415 - mean_squared_error: 0.0415\n",
      "Epoch 00052: val_loss did not improve from 0.04848\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0363 - mean_squared_error: 0.0363 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00053: val_loss improved from 0.04848 to 0.04829, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00054: val_loss improved from 0.04829 to 0.04811, saving model to model.h5\n",
      "248/248 [==============================] - 0s 218us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00055: val_loss did not improve from 0.04811\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00056: val_loss did not improve from 0.04811\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0308 - mean_squared_error: 0.0308 - val_loss: 0.0482 - val_mean_squared_error: 0.0482\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00057: val_loss did not improve from 0.04811\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0323 - mean_squared_error: 0.0323 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00058: val_loss did not improve from 0.04811\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0482 - val_mean_squared_error: 0.0482\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0443 - mean_squared_error: 0.0443\n",
      "Epoch 00059: val_loss did not improve from 0.04811\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Epoch 00060: val_loss did not improve from 0.04811\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00061: val_loss did not improve from 0.04811\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Epoch 00062: val_loss did not improve from 0.04811\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0376 - mean_squared_error: 0.0376\n",
      "Epoch 00063: val_loss did not improve from 0.04811\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0319 - mean_squared_error: 0.0319 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00064: val_loss did not improve from 0.04811\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Epoch 00065: val_loss did not improve from 0.04811\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0365 - mean_squared_error: 0.0365\n",
      "Epoch 00066: val_loss did not improve from 0.04811\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Epoch 00067: val_loss improved from 0.04811 to 0.04805, saving model to model.h5\n",
      "248/248 [==============================] - 0s 256us/sample - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00068: val_loss improved from 0.04805 to 0.04770, saving model to model.h5\n",
      "248/248 [==============================] - 0s 190us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00069: val_loss improved from 0.04770 to 0.04749, saving model to model.h5\n",
      "248/248 [==============================] - 0s 187us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00070: val_loss improved from 0.04749 to 0.04728, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0387 - mean_squared_error: 0.0387\n",
      "Epoch 00071: val_loss improved from 0.04728 to 0.04705, saving model to model.h5\n",
      "248/248 [==============================] - 0s 187us/sample - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Epoch 00072: val_loss improved from 0.04705 to 0.04690, saving model to model.h5\n",
      "248/248 [==============================] - 0s 192us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00073: val_loss improved from 0.04690 to 0.04658, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0387 - mean_squared_error: 0.0387\n",
      "Epoch 00074: val_loss improved from 0.04658 to 0.04616, saving model to model.h5\n",
      "248/248 [==============================] - 0s 182us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0418 - mean_squared_error: 0.0418\n",
      "Epoch 00075: val_loss improved from 0.04616 to 0.04564, saving model to model.h5\n",
      "248/248 [==============================] - 0s 188us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0435 - mean_squared_error: 0.0435\n",
      "Epoch 00076: val_loss improved from 0.04564 to 0.04539, saving model to model.h5\n",
      "248/248 [==============================] - 0s 206us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00077: val_loss improved from 0.04539 to 0.04537, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0424 - mean_squared_error: 0.0424\n",
      "Epoch 00078: val_loss did not improve from 0.04537\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00079: val_loss did not improve from 0.04537\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0401 - mean_squared_error: 0.0401\n",
      "Epoch 00080: val_loss did not improve from 0.04537\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0328 - mean_squared_error: 0.0328 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00081: val_loss did not improve from 0.04537\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00082: val_loss did not improve from 0.04537\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00083: val_loss improved from 0.04537 to 0.04535, saving model to model.h5\n",
      "248/248 [==============================] - 0s 199us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00084: val_loss improved from 0.04535 to 0.04507, saving model to model.h5\n",
      "248/248 [==============================] - 0s 206us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00085: val_loss did not improve from 0.04507\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Epoch 00086: val_loss did not improve from 0.04507\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00087: val_loss did not improve from 0.04507\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00088: val_loss did not improve from 0.04507\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00089: val_loss did not improve from 0.04507\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00090: val_loss did not improve from 0.04507\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0320 - mean_squared_error: 0.0320 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Epoch 00091: val_loss did not improve from 0.04507\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00092: val_loss did not improve from 0.04507\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00093: val_loss did not improve from 0.04507\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00094: val_loss did not improve from 0.04507\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00095: val_loss improved from 0.04507 to 0.04467, saving model to model.h5\n",
      "248/248 [==============================] - 0s 229us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0384 - mean_squared_error: 0.0384\n",
      "Epoch 00096: val_loss improved from 0.04467 to 0.04432, saving model to model.h5\n",
      "248/248 [==============================] - 0s 261us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0334 - mean_squared_error: 0.0334\n",
      "Epoch 00097: val_loss improved from 0.04432 to 0.04428, saving model to model.h5\n",
      "248/248 [==============================] - 0s 257us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 98/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0370 - mean_squared_error: 0.0370\n",
      "Epoch 00098: val_loss did not improve from 0.04428\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0307 - mean_squared_error: 0.0307\n",
      "Epoch 00099: val_loss did not improve from 0.04428\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00100: val_loss did not improve from 0.04428\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00101: val_loss did not improve from 0.04428\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 00102: val_loss did not improve from 0.04428\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00103: val_loss did not improve from 0.04428\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0140 - mean_squared_error: 0.0140\n",
      "Epoch 00104: val_loss did not improve from 0.04428\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00105: val_loss did not improve from 0.04428\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00106: val_loss did not improve from 0.04428\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0308 - mean_squared_error: 0.0308 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Epoch 00107: val_loss did not improve from 0.04428\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0176 - mean_squared_error: 0.0176\n",
      "Epoch 00108: val_loss improved from 0.04428 to 0.04407, saving model to model.h5\n",
      "248/248 [==============================] - 0s 217us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00109: val_loss did not improve from 0.04407\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00110: val_loss improved from 0.04407 to 0.04406, saving model to model.h5\n",
      "248/248 [==============================] - 0s 217us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00111: val_loss did not improve from 0.04406\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00112: val_loss did not improve from 0.04406\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00113: val_loss improved from 0.04406 to 0.04368, saving model to model.h5\n",
      "248/248 [==============================] - 0s 247us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00114: val_loss improved from 0.04368 to 0.04276, saving model to model.h5\n",
      "248/248 [==============================] - 0s 221us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0385 - mean_squared_error: 0.0385\n",
      "Epoch 00115: val_loss improved from 0.04276 to 0.04272, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00116: val_loss improved from 0.04272 to 0.04199, saving model to model.h5\n",
      "248/248 [==============================] - 0s 192us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00117: val_loss improved from 0.04199 to 0.04168, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0227 - mean_squared_error: 0.0227\n",
      "Epoch 00118: val_loss improved from 0.04168 to 0.04133, saving model to model.h5\n",
      "248/248 [==============================] - 0s 177us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00119: val_loss did not improve from 0.04133\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0188 - mean_squared_error: 0.0188\n",
      "Epoch 00120: val_loss did not improve from 0.04133\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00121: val_loss did not improve from 0.04133\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00122: val_loss did not improve from 0.04133\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 123/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00123: val_loss did not improve from 0.04133\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00124: val_loss did not improve from 0.04133\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00125: val_loss did not improve from 0.04133\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0358 - mean_squared_error: 0.0358\n",
      "Epoch 00126: val_loss did not improve from 0.04133\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00127: val_loss did not improve from 0.04133\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00128: val_loss did not improve from 0.04133\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0451 - mean_squared_error: 0.0451\n",
      "Epoch 00129: val_loss did not improve from 0.04133\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 130/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00130: val_loss did not improve from 0.04133\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00131: val_loss did not improve from 0.04133\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 132/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00132: val_loss did not improve from 0.04133\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 133/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00133: val_loss did not improve from 0.04133\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 134/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00134: val_loss did not improve from 0.04133\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 135/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00135: val_loss did not improve from 0.04133\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 136/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00136: val_loss did not improve from 0.04133\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 137/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00137: val_loss did not improve from 0.04133\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 138/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00138: val_loss did not improve from 0.04133\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 139/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00139: val_loss did not improve from 0.04133\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 140/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0341 - mean_squared_error: 0.0341\n",
      "Epoch 00140: val_loss did not improve from 0.04133\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 141/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 00141: val_loss improved from 0.04133 to 0.04104, saving model to model.h5\n",
      "248/248 [==============================] - 0s 215us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 142/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Epoch 00142: val_loss improved from 0.04104 to 0.04101, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 143/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00143: val_loss did not improve from 0.04101\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 144/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0341 - mean_squared_error: 0.0341\n",
      "Epoch 00144: val_loss did not improve from 0.04101\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 145/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00145: val_loss did not improve from 0.04101\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 146/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0167 - mean_squared_error: 0.0167\n",
      "Epoch 00146: val_loss did not improve from 0.04101\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 147/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00147: val_loss did not improve from 0.04101\n",
      "248/248 [==============================] - 0s 67us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 148/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00148: val_loss did not improve from 0.04101\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00149: val_loss did not improve from 0.04101\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 150/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00150: val_loss did not improve from 0.04101\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 151/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00151: val_loss did not improve from 0.04101\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 152/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00152: val_loss did not improve from 0.04101\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 153/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00153: val_loss improved from 0.04101 to 0.04085, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 154/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0148 - mean_squared_error: 0.0148\n",
      "Epoch 00154: val_loss improved from 0.04085 to 0.04074, saving model to model.h5\n",
      "248/248 [==============================] - 0s 209us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 155/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0326 - mean_squared_error: 0.0326\n",
      "Epoch 00155: val_loss did not improve from 0.04074\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 156/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00156: val_loss did not improve from 0.04074\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 157/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00157: val_loss did not improve from 0.04074\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 158/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Epoch 00158: val_loss improved from 0.04074 to 0.03998, saving model to model.h5\n",
      "248/248 [==============================] - 0s 207us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 159/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00159: val_loss improved from 0.03998 to 0.03890, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 160/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00160: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 161/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00161: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 162/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00162: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 96us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 163/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00163: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 164/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00164: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 165/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00165: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 166/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00166: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 167/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00167: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 168/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00168: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 169/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00169: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 170/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00170: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 171/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00171: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 172/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00172: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 173/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0114 - mean_squared_error: 0.0114\n",
      "Epoch 00173: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 174/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00174: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0371 - mean_squared_error: 0.0371\n",
      "Epoch 00175: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 176/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00176: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 177/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00177: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 178/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00178: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 67us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 179/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00179: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 180/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00180: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 181/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Epoch 00181: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 182/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00182: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 183/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00183: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 184/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0227 - mean_squared_error: 0.0227\n",
      "Epoch 00184: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 185/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00185: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 186/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00186: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 187/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0146 - mean_squared_error: 0.0146\n",
      "Epoch 00187: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 188/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00188: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 189/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00189: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 190/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00190: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 191/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0188 - mean_squared_error: 0.0188\n",
      "Epoch 00191: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 192/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0146 - mean_squared_error: 0.0146\n",
      "Epoch 00192: val_loss did not improve from 0.03890\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 193/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00193: val_loss improved from 0.03890 to 0.03879, saving model to model.h5\n",
      "248/248 [==============================] - 0s 204us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 194/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0181 - mean_squared_error: 0.0181\n",
      "Epoch 00194: val_loss improved from 0.03879 to 0.03769, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 195/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0399 - mean_squared_error: 0.0399\n",
      "Epoch 00195: val_loss improved from 0.03769 to 0.03751, saving model to model.h5\n",
      "248/248 [==============================] - 0s 221us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 196/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00196: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 197/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00197: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 198/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00198: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 199/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00199: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 200/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0110 - mean_squared_error: 0.0110\n",
      "Epoch 00200: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0084 - mean_squared_error: 0.0084\n",
      "Epoch 00201: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 202/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00202: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 203/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Epoch 00203: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 204/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0118 - mean_squared_error: 0.0118\n",
      "Epoch 00204: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 205/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00205: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 206/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00206: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 207/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00207: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 208/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00208: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 209/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0294 - mean_squared_error: 0.0294\n",
      "Epoch 00209: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 210/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0174 - mean_squared_error: 0.0174\n",
      "Epoch 00210: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 211/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00211: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 212/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00212: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 213/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0112 - mean_squared_error: 0.0112\n",
      "Epoch 00213: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 214/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00214: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 215/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0171 - mean_squared_error: 0.0171\n",
      "Epoch 00215: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 216/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00216: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 217/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00217: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 218/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00218: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 219/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0117 - mean_squared_error: 0.0117\n",
      "Epoch 00219: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 220/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00220: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 221/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00221: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 222/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00222: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 223/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0113 - mean_squared_error: 0.0113\n",
      "Epoch 00223: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 224/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0160 - mean_squared_error: 0.0160\n",
      "Epoch 00224: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 225/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0188 - mean_squared_error: 0.0188\n",
      "Epoch 00225: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 226/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0120 - mean_squared_error: 0.0120\n",
      "Epoch 00226: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 227/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00227: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 228/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0155 - mean_squared_error: 0.0155\n",
      "Epoch 00228: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 229/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00229: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 230/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00230: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 231/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00231: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 232/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Epoch 00232: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 233/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00233: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 234/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00234: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 235/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Epoch 00235: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 236/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00236: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 237/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0119 - mean_squared_error: 0.0119\n",
      "Epoch 00237: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 238/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00238: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 239/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0174 - mean_squared_error: 0.0174\n",
      "Epoch 00239: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 240/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00240: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 241/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00241: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 242/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0177 - mean_squared_error: 0.0177\n",
      "Epoch 00242: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 243/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00243: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 244/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0188 - mean_squared_error: 0.0188\n",
      "Epoch 00244: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 245/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 00245: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 246/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0132 - mean_squared_error: 0.0132\n",
      "Epoch 00246: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 247/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00247: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 248/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0171 - mean_squared_error: 0.0171\n",
      "Epoch 00248: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 249/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00249: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 250/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00250: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 251/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00251: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 252/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00252: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 253/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00253: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 254/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0372 - mean_squared_error: 0.0372\n",
      "Epoch 00254: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 255/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0144 - mean_squared_error: 0.0144\n",
      "Epoch 00255: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 256/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00256: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 257/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00257: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 258/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00258: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 259/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00259: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 260/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0178 - mean_squared_error: 0.0178\n",
      "Epoch 00260: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 261/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00261: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 262/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.0185\n",
      "Epoch 00262: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 263/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00263: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 264/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0307 - mean_squared_error: 0.0307\n",
      "Epoch 00264: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 265/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00265: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 266/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Epoch 00266: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 267/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00267: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 268/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00268: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 269/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0295 - mean_squared_error: 0.0295\n",
      "Epoch 00269: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 270/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00270: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 271/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00271: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 272/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00272: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 273/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0181 - mean_squared_error: 0.0181\n",
      "Epoch 00273: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 274/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00274: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 275/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00275: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 276/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0153 - mean_squared_error: 0.0153\n",
      "Epoch 00276: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 277/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00277: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 67us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 278/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0132 - mean_squared_error: 0.0132\n",
      "Epoch 00278: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 279/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00279: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 280/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00280: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 281/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0118 - mean_squared_error: 0.0118\n",
      "Epoch 00281: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 282/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0131 - mean_squared_error: 0.0131\n",
      "Epoch 00282: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 283/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00283: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 284/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00284: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 285/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00285: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 286/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00286: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 287/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00287: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 288/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0158 - mean_squared_error: 0.0158\n",
      "Epoch 00288: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 289/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00289: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 290/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0343 - mean_squared_error: 0.0343\n",
      "Epoch 00290: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 291/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0153 - mean_squared_error: 0.0153\n",
      "Epoch 00291: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 292/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00292: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 293/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0157 - mean_squared_error: 0.0157\n",
      "Epoch 00293: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 294/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00294: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 295/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00295: val_loss did not improve from 0.03751\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Running trial 4\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 1s - loss: 0.9525 - mean_squared_error: 0.9525\n",
      "Epoch 00001: val_loss improved from inf to 0.66175, saving model to model.h5\n",
      "248/248 [==============================] - 5s 18ms/sample - loss: 1.0150 - mean_squared_error: 1.0150 - val_loss: 0.6617 - val_mean_squared_error: 0.6617\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9829 - mean_squared_error: 0.9829\n",
      "Epoch 00002: val_loss improved from 0.66175 to 0.63530, saving model to model.h5\n",
      "248/248 [==============================] - 0s 191us/sample - loss: 0.9381 - mean_squared_error: 0.9381 - val_loss: 0.6353 - val_mean_squared_error: 0.6353\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9520 - mean_squared_error: 0.9520\n",
      "Epoch 00003: val_loss improved from 0.63530 to 0.60809, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.8610 - mean_squared_error: 0.8610 - val_loss: 0.6081 - val_mean_squared_error: 0.6081\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8358 - mean_squared_error: 0.8358\n",
      "Epoch 00004: val_loss improved from 0.60809 to 0.58021, saving model to model.h5\n",
      "248/248 [==============================] - 0s 196us/sample - loss: 0.7776 - mean_squared_error: 0.7776 - val_loss: 0.5802 - val_mean_squared_error: 0.5802\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6162 - mean_squared_error: 0.6162\n",
      "Epoch 00005: val_loss improved from 0.58021 to 0.54998, saving model to model.h5\n",
      "248/248 [==============================] - 0s 196us/sample - loss: 0.6922 - mean_squared_error: 0.6922 - val_loss: 0.5500 - val_mean_squared_error: 0.5500\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6713 - mean_squared_error: 0.6713\n",
      "Epoch 00006: val_loss improved from 0.54998 to 0.51894, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.6220 - mean_squared_error: 0.6220 - val_loss: 0.5189 - val_mean_squared_error: 0.5189\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5519 - mean_squared_error: 0.5519\n",
      "Epoch 00007: val_loss improved from 0.51894 to 0.48540, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.5340 - mean_squared_error: 0.5340 - val_loss: 0.4854 - val_mean_squared_error: 0.4854\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4742 - mean_squared_error: 0.4742\n",
      "Epoch 00008: val_loss improved from 0.48540 to 0.45030, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.4546 - mean_squared_error: 0.4546 - val_loss: 0.4503 - val_mean_squared_error: 0.4503\n",
      "Epoch 9/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3271 - mean_squared_error: 0.3271\n",
      "Epoch 00009: val_loss improved from 0.45030 to 0.41325, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.3772 - mean_squared_error: 0.3772 - val_loss: 0.4133 - val_mean_squared_error: 0.4133\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2807 - mean_squared_error: 0.2807\n",
      "Epoch 00010: val_loss improved from 0.41325 to 0.37523, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.3069 - mean_squared_error: 0.3069 - val_loss: 0.3752 - val_mean_squared_error: 0.3752\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2497 - mean_squared_error: 0.2497\n",
      "Epoch 00011: val_loss improved from 0.37523 to 0.33625, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.2523 - mean_squared_error: 0.2523 - val_loss: 0.3362 - val_mean_squared_error: 0.3362\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1832 - mean_squared_error: 0.1832\n",
      "Epoch 00012: val_loss improved from 0.33625 to 0.29755, saving model to model.h5\n",
      "248/248 [==============================] - 0s 204us/sample - loss: 0.2012 - mean_squared_error: 0.2012 - val_loss: 0.2975 - val_mean_squared_error: 0.2975\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1913 - mean_squared_error: 0.1913\n",
      "Epoch 00013: val_loss improved from 0.29755 to 0.26076, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.1755 - mean_squared_error: 0.1755 - val_loss: 0.2608 - val_mean_squared_error: 0.2608\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1700 - mean_squared_error: 0.1700\n",
      "Epoch 00014: val_loss improved from 0.26076 to 0.22698, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.1533 - mean_squared_error: 0.1533 - val_loss: 0.2270 - val_mean_squared_error: 0.2270\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1359 - mean_squared_error: 0.1359\n",
      "Epoch 00015: val_loss improved from 0.22698 to 0.19752, saving model to model.h5\n",
      "248/248 [==============================] - 0s 204us/sample - loss: 0.1372 - mean_squared_error: 0.1372 - val_loss: 0.1975 - val_mean_squared_error: 0.1975\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1263 - mean_squared_error: 0.1263\n",
      "Epoch 00016: val_loss improved from 0.19752 to 0.17161, saving model to model.h5\n",
      "248/248 [==============================] - 0s 188us/sample - loss: 0.1240 - mean_squared_error: 0.1240 - val_loss: 0.1716 - val_mean_squared_error: 0.1716\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1060 - mean_squared_error: 0.1060\n",
      "Epoch 00017: val_loss improved from 0.17161 to 0.14889, saving model to model.h5\n",
      "248/248 [==============================] - 0s 196us/sample - loss: 0.1067 - mean_squared_error: 0.1067 - val_loss: 0.1489 - val_mean_squared_error: 0.1489\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1099 - mean_squared_error: 0.1099\n",
      "Epoch 00018: val_loss improved from 0.14889 to 0.12963, saving model to model.h5\n",
      "248/248 [==============================] - 0s 219us/sample - loss: 0.0939 - mean_squared_error: 0.0939 - val_loss: 0.1296 - val_mean_squared_error: 0.1296\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0898 - mean_squared_error: 0.0898\n",
      "Epoch 00019: val_loss improved from 0.12963 to 0.11308, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0784 - mean_squared_error: 0.0784 - val_loss: 0.1131 - val_mean_squared_error: 0.1131\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0790 - mean_squared_error: 0.0790\n",
      "Epoch 00020: val_loss improved from 0.11308 to 0.09897, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.0700 - mean_squared_error: 0.0700 - val_loss: 0.0990 - val_mean_squared_error: 0.0990\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0547 - mean_squared_error: 0.0547\n",
      "Epoch 00021: val_loss improved from 0.09897 to 0.08720, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.0634 - mean_squared_error: 0.0634 - val_loss: 0.0872 - val_mean_squared_error: 0.0872\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0591 - mean_squared_error: 0.0591\n",
      "Epoch 00022: val_loss improved from 0.08720 to 0.07761, saving model to model.h5\n",
      "248/248 [==============================] - 0s 190us/sample - loss: 0.0602 - mean_squared_error: 0.0602 - val_loss: 0.0776 - val_mean_squared_error: 0.0776\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0507 - mean_squared_error: 0.0507\n",
      "Epoch 00023: val_loss improved from 0.07761 to 0.06959, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.0567 - mean_squared_error: 0.0567 - val_loss: 0.0696 - val_mean_squared_error: 0.0696\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0496 - mean_squared_error: 0.0496\n",
      "Epoch 00024: val_loss improved from 0.06959 to 0.06312, saving model to model.h5\n",
      "248/248 [==============================] - 0s 191us/sample - loss: 0.0540 - mean_squared_error: 0.0540 - val_loss: 0.0631 - val_mean_squared_error: 0.0631\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0605 - mean_squared_error: 0.0605\n",
      "Epoch 00025: val_loss improved from 0.06312 to 0.05837, saving model to model.h5\n",
      "248/248 [==============================] - 0s 188us/sample - loss: 0.0486 - mean_squared_error: 0.0486 - val_loss: 0.0584 - val_mean_squared_error: 0.0584\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0590 - mean_squared_error: 0.0590\n",
      "Epoch 00026: val_loss improved from 0.05837 to 0.05494, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0485 - mean_squared_error: 0.0485 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0367 - mean_squared_error: 0.0367\n",
      "Epoch 00027: val_loss improved from 0.05494 to 0.05266, saving model to model.h5\n",
      "248/248 [==============================] - 0s 192us/sample - loss: 0.0449 - mean_squared_error: 0.0449 - val_loss: 0.0527 - val_mean_squared_error: 0.0527\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0548 - mean_squared_error: 0.0548\n",
      "Epoch 00028: val_loss improved from 0.05266 to 0.05121, saving model to model.h5\n",
      "248/248 [==============================] - 0s 196us/sample - loss: 0.0467 - mean_squared_error: 0.0467 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0587 - mean_squared_error: 0.0587\n",
      "Epoch 00029: val_loss improved from 0.05121 to 0.05053, saving model to model.h5\n",
      "248/248 [==============================] - 0s 183us/sample - loss: 0.0461 - mean_squared_error: 0.0461 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0490 - mean_squared_error: 0.0490\n",
      "Epoch 00030: val_loss improved from 0.05053 to 0.05035, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.0445 - mean_squared_error: 0.0445 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0388 - mean_squared_error: 0.0388\n",
      "Epoch 00031: val_loss did not improve from 0.05035\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0447 - mean_squared_error: 0.0447 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Epoch 00032: val_loss did not improve from 0.05035\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0413 - mean_squared_error: 0.0413 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0462 - mean_squared_error: 0.0462\n",
      "Epoch 00033: val_loss did not improve from 0.05035\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0509 - val_mean_squared_error: 0.0509\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0447 - mean_squared_error: 0.0447\n",
      "Epoch 00034: val_loss did not improve from 0.05035\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0426 - mean_squared_error: 0.0426 - val_loss: 0.0513 - val_mean_squared_error: 0.0513\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0440 - mean_squared_error: 0.0440\n",
      "Epoch 00035: val_loss did not improve from 0.05035\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0403 - mean_squared_error: 0.0403 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0479 - mean_squared_error: 0.0479\n",
      "Epoch 00036: val_loss did not improve from 0.05035\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0405 - mean_squared_error: 0.0405 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0390 - mean_squared_error: 0.0390\n",
      "Epoch 00037: val_loss did not improve from 0.05035\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0393 - mean_squared_error: 0.0393 - val_loss: 0.0511 - val_mean_squared_error: 0.0511\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0471 - mean_squared_error: 0.0471\n",
      "Epoch 00038: val_loss did not improve from 0.05035\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0508 - val_mean_squared_error: 0.0508\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0494 - mean_squared_error: 0.0494\n",
      "Epoch 00039: val_loss did not improve from 0.05035\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0386 - mean_squared_error: 0.0386 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0429 - mean_squared_error: 0.0429\n",
      "Epoch 00040: val_loss improved from 0.05035 to 0.04980, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0531 - mean_squared_error: 0.0531\n",
      "Epoch 00041: val_loss improved from 0.04980 to 0.04924, saving model to model.h5\n",
      "248/248 [==============================] - 0s 188us/sample - loss: 0.0425 - mean_squared_error: 0.0425 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0378 - mean_squared_error: 0.0378\n",
      "Epoch 00042: val_loss improved from 0.04924 to 0.04877, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.0389 - mean_squared_error: 0.0389 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0415 - mean_squared_error: 0.0415\n",
      "Epoch 00043: val_loss improved from 0.04877 to 0.04844, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0398 - mean_squared_error: 0.0398 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0420 - mean_squared_error: 0.0420\n",
      "Epoch 00044: val_loss improved from 0.04844 to 0.04824, saving model to model.h5\n",
      "248/248 [==============================] - 0s 196us/sample - loss: 0.0385 - mean_squared_error: 0.0385 - val_loss: 0.0482 - val_mean_squared_error: 0.0482\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0485 - mean_squared_error: 0.0485\n",
      "Epoch 00045: val_loss improved from 0.04824 to 0.04812, saving model to model.h5\n",
      "248/248 [==============================] - 0s 183us/sample - loss: 0.0429 - mean_squared_error: 0.0429 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0510 - mean_squared_error: 0.0510\n",
      "Epoch 00046: val_loss improved from 0.04812 to 0.04800, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0378 - mean_squared_error: 0.0378 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0461 - mean_squared_error: 0.0461\n",
      "Epoch 00047: val_loss improved from 0.04800 to 0.04791, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0382 - mean_squared_error: 0.0382 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0384 - mean_squared_error: 0.0384\n",
      "Epoch 00048: val_loss improved from 0.04791 to 0.04785, saving model to model.h5\n",
      "248/248 [==============================] - 0s 185us/sample - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0477 - mean_squared_error: 0.0477\n",
      "Epoch 00049: val_loss did not improve from 0.04785\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0383 - mean_squared_error: 0.0383 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0368 - mean_squared_error: 0.0368\n",
      "Epoch 00050: val_loss did not improve from 0.04785\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0385 - mean_squared_error: 0.0385 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0490 - mean_squared_error: 0.0490\n",
      "Epoch 00051: val_loss did not improve from 0.04785\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0366 - mean_squared_error: 0.0366 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00052: val_loss did not improve from 0.04785\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00053: val_loss improved from 0.04785 to 0.04763, saving model to model.h5\n",
      "248/248 [==============================] - 0s 194us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00054: val_loss improved from 0.04763 to 0.04759, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Epoch 00055: val_loss did not improve from 0.04759\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0386 - mean_squared_error: 0.0386\n",
      "Epoch 00056: val_loss improved from 0.04759 to 0.04749, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.0475 - val_mean_squared_error: 0.0475\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00057: val_loss improved from 0.04749 to 0.04719, saving model to model.h5\n",
      "248/248 [==============================] - 0s 188us/sample - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00058: val_loss improved from 0.04719 to 0.04684, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.0386 - mean_squared_error: 0.0386 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0524 - mean_squared_error: 0.0524\n",
      "Epoch 00059: val_loss improved from 0.04684 to 0.04677, saving model to model.h5\n",
      "248/248 [==============================] - 0s 192us/sample - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00060: val_loss did not improve from 0.04677\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00061: val_loss did not improve from 0.04677\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Epoch 00062: val_loss did not improve from 0.04677\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 00063: val_loss did not improve from 0.04677\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0328 - mean_squared_error: 0.0328 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00064: val_loss improved from 0.04677 to 0.04661, saving model to model.h5\n",
      "248/248 [==============================] - 0s 187us/sample - loss: 0.0379 - mean_squared_error: 0.0379 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0431 - mean_squared_error: 0.0431\n",
      "Epoch 00065: val_loss improved from 0.04661 to 0.04627, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0387 - mean_squared_error: 0.0387\n",
      "Epoch 00066: val_loss improved from 0.04627 to 0.04570, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00067: val_loss improved from 0.04570 to 0.04534, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00068: val_loss improved from 0.04534 to 0.04503, saving model to model.h5\n",
      "248/248 [==============================] - 0s 196us/sample - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00069: val_loss improved from 0.04503 to 0.04486, saving model to model.h5\n",
      "248/248 [==============================] - 0s 209us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0438 - mean_squared_error: 0.0438\n",
      "Epoch 00070: val_loss did not improve from 0.04486\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0416 - mean_squared_error: 0.0416\n",
      "Epoch 00071: val_loss improved from 0.04486 to 0.04479, saving model to model.h5\n",
      "248/248 [==============================] - 0s 204us/sample - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00072: val_loss improved from 0.04479 to 0.04452, saving model to model.h5\n",
      "248/248 [==============================] - 0s 218us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00073: val_loss improved from 0.04452 to 0.04392, saving model to model.h5\n",
      "248/248 [==============================] - 0s 224us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Epoch 00074: val_loss improved from 0.04392 to 0.04333, saving model to model.h5\n",
      "248/248 [==============================] - 0s 206us/sample - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Epoch 00075: val_loss improved from 0.04333 to 0.04286, saving model to model.h5\n",
      "248/248 [==============================] - 0s 190us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0345 - mean_squared_error: 0.0345\n",
      "Epoch 00076: val_loss did not improve from 0.04286\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00077: val_loss improved from 0.04286 to 0.04274, saving model to model.h5\n",
      "248/248 [==============================] - 0s 207us/sample - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0326 - mean_squared_error: 0.0326\n",
      "Epoch 00078: val_loss did not improve from 0.04274\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0432 - mean_squared_error: 0.0432\n",
      "Epoch 00079: val_loss did not improve from 0.04274\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0171 - mean_squared_error: 0.0171\n",
      "Epoch 00080: val_loss did not improve from 0.04274\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00081: val_loss did not improve from 0.04274\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00082: val_loss did not improve from 0.04274\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0369 - mean_squared_error: 0.0369\n",
      "Epoch 00083: val_loss did not improve from 0.04274\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00084: val_loss did not improve from 0.04274\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00085: val_loss did not improve from 0.04274\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00086: val_loss did not improve from 0.04274\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00087: val_loss improved from 0.04274 to 0.04260, saving model to model.h5\n",
      "248/248 [==============================] - 0s 216us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00088: val_loss improved from 0.04260 to 0.04220, saving model to model.h5\n",
      "248/248 [==============================] - 0s 196us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00089: val_loss improved from 0.04220 to 0.04176, saving model to model.h5\n",
      "248/248 [==============================] - 0s 207us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00090: val_loss did not improve from 0.04176\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00091: val_loss did not improve from 0.04176\n",
      "248/248 [==============================] - 0s 67us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00092: val_loss did not improve from 0.04176\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Epoch 00093: val_loss did not improve from 0.04176\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00094: val_loss did not improve from 0.04176\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00095: val_loss did not improve from 0.04176\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0402 - mean_squared_error: 0.0402\n",
      "Epoch 00096: val_loss did not improve from 0.04176\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0414 - mean_squared_error: 0.0414\n",
      "Epoch 00097: val_loss did not improve from 0.04176\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00098: val_loss improved from 0.04176 to 0.04140, saving model to model.h5\n",
      "248/248 [==============================] - 0s 204us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0231 - mean_squared_error: 0.0231\n",
      "Epoch 00099: val_loss improved from 0.04140 to 0.04091, saving model to model.h5\n",
      "248/248 [==============================] - 0s 204us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Epoch 00100: val_loss did not improve from 0.04091\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0170 - mean_squared_error: 0.0170\n",
      "Epoch 00101: val_loss improved from 0.04091 to 0.04079, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00102: val_loss improved from 0.04079 to 0.04063, saving model to model.h5\n",
      "248/248 [==============================] - 0s 191us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00103: val_loss did not improve from 0.04063\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00104: val_loss improved from 0.04063 to 0.04052, saving model to model.h5\n",
      "248/248 [==============================] - 0s 207us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0388 - mean_squared_error: 0.0388\n",
      "Epoch 00105: val_loss improved from 0.04052 to 0.04029, saving model to model.h5\n",
      "248/248 [==============================] - 0s 199us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00106: val_loss improved from 0.04029 to 0.03981, saving model to model.h5\n",
      "248/248 [==============================] - 0s 187us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00107: val_loss improved from 0.03981 to 0.03957, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00108: val_loss improved from 0.03957 to 0.03945, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00109: val_loss improved from 0.03945 to 0.03907, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00110: val_loss improved from 0.03907 to 0.03849, saving model to model.h5\n",
      "248/248 [==============================] - 0s 188us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 00111: val_loss improved from 0.03849 to 0.03798, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00112: val_loss improved from 0.03798 to 0.03755, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00113: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Epoch 00114: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0430 - mean_squared_error: 0.0430\n",
      "Epoch 00115: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0308 - mean_squared_error: 0.0308 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00116: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00117: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00118: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00119: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0396 - mean_squared_error: 0.0396\n",
      "Epoch 00120: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 63us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0423 - mean_squared_error: 0.0423\n",
      "Epoch 00121: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00122: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00123: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00124: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00125: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00126: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00127: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00128: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00129: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 130/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00130: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0383 - val_mean_squared_error: 0.0383\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00131: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 132/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00132: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 133/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0348 - mean_squared_error: 0.0348\n",
      "Epoch 00133: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00134: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 135/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00135: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 136/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00136: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 137/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00137: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 138/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00138: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 139/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00139: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 140/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00140: val_loss did not improve from 0.03755\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 141/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0173 - mean_squared_error: 0.0173\n",
      "Epoch 00141: val_loss improved from 0.03755 to 0.03708, saving model to model.h5\n",
      "248/248 [==============================] - 0s 204us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 142/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00142: val_loss did not improve from 0.03708\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 143/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00143: val_loss did not improve from 0.03708\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 144/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00144: val_loss did not improve from 0.03708\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 145/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Epoch 00145: val_loss did not improve from 0.03708\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 146/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00146: val_loss did not improve from 0.03708\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 147/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00147: val_loss did not improve from 0.03708\n",
      "248/248 [==============================] - 0s 92us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 148/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00148: val_loss did not improve from 0.03708\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 149/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00149: val_loss did not improve from 0.03708\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 150/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0307 - mean_squared_error: 0.0307\n",
      "Epoch 00150: val_loss improved from 0.03708 to 0.03669, saving model to model.h5\n",
      "248/248 [==============================] - 0s 217us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0367 - val_mean_squared_error: 0.0367\n",
      "Epoch 151/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00151: val_loss improved from 0.03669 to 0.03621, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0362 - val_mean_squared_error: 0.0362\n",
      "Epoch 152/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00152: val_loss did not improve from 0.03621\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n",
      "Epoch 153/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00153: val_loss did not improve from 0.03621\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 154/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00154: val_loss did not improve from 0.03621\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 155/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Epoch 00155: val_loss did not improve from 0.03621\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 156/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0118 - mean_squared_error: 0.0118\n",
      "Epoch 00156: val_loss did not improve from 0.03621\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 157/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00157: val_loss did not improve from 0.03621\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 158/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00158: val_loss did not improve from 0.03621\n",
      "248/248 [==============================] - 0s 67us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 159/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00159: val_loss did not improve from 0.03621\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00160: val_loss did not improve from 0.03621\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 161/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00161: val_loss did not improve from 0.03621\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 162/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00162: val_loss did not improve from 0.03621\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 163/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0125 - mean_squared_error: 0.0125\n",
      "Epoch 00163: val_loss did not improve from 0.03621\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 164/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00164: val_loss did not improve from 0.03621\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 165/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00165: val_loss did not improve from 0.03621\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 166/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00166: val_loss did not improve from 0.03621\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0366 - val_mean_squared_error: 0.0366\n",
      "Epoch 167/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00167: val_loss did not improve from 0.03621\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n",
      "Epoch 168/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00168: val_loss improved from 0.03621 to 0.03525, saving model to model.h5\n",
      "248/248 [==============================] - 0s 208us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0353 - val_mean_squared_error: 0.0353\n",
      "Epoch 169/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00169: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0353 - val_mean_squared_error: 0.0353\n",
      "Epoch 170/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00170: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0357 - val_mean_squared_error: 0.0357\n",
      "Epoch 171/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0406 - mean_squared_error: 0.0406\n",
      "Epoch 00171: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0357 - val_mean_squared_error: 0.0357\n",
      "Epoch 172/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00172: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0360 - val_mean_squared_error: 0.0360\n",
      "Epoch 173/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00173: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 174/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0173 - mean_squared_error: 0.0173\n",
      "Epoch 00174: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 175/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00175: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 176/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00176: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 177/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0177 - mean_squared_error: 0.0177\n",
      "Epoch 00177: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 178/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00178: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0363 - val_mean_squared_error: 0.0363\n",
      "Epoch 179/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00179: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0362 - val_mean_squared_error: 0.0362\n",
      "Epoch 180/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00180: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0359 - val_mean_squared_error: 0.0359\n",
      "Epoch 181/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00181: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0363 - val_mean_squared_error: 0.0363\n",
      "Epoch 182/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00182: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 89us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0363 - val_mean_squared_error: 0.0363\n",
      "Epoch 183/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00183: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 184/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00184: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 185/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0149 - mean_squared_error: 0.0149\n",
      "Epoch 00185: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 00186: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 187/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00187: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 188/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00188: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 189/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0379 - mean_squared_error: 0.0379\n",
      "Epoch 00189: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 190/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0181 - mean_squared_error: 0.0181\n",
      "Epoch 00190: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 191/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00191: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 192/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00192: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 193/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00193: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 194/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 00194: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 195/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Epoch 00195: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 196/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00196: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 197/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0157 - mean_squared_error: 0.0157\n",
      "Epoch 00197: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 198/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00198: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 199/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00199: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 200/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0126 - mean_squared_error: 0.0126\n",
      "Epoch 00200: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 201/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00201: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 202/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Epoch 00202: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 203/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00203: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 204/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00204: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 205/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00205: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 206/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00206: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 207/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00207: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 208/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.0172\n",
      "Epoch 00208: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 209/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00209: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 210/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00210: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 211/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00211: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 212/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00212: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 213/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0150 - mean_squared_error: 0.0150\n",
      "Epoch 00213: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 214/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00214: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 215/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0107 - mean_squared_error: 0.0107\n",
      "Epoch 00215: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 216/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00216: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 217/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0171 - mean_squared_error: 0.0171\n",
      "Epoch 00217: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 218/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00218: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 219/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00219: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 220/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0149 - mean_squared_error: 0.0149\n",
      "Epoch 00220: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 221/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0159 - mean_squared_error: 0.0159\n",
      "Epoch 00221: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 222/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Epoch 00222: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 223/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00223: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 224/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00224: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 225/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0130 - mean_squared_error: 0.0130\n",
      "Epoch 00225: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 226/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00226: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 227/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00227: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 228/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00228: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 229/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00229: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 230/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00230: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 231/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00231: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 232/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 00232: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 233/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0188 - mean_squared_error: 0.0188\n",
      "Epoch 00233: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 234/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00234: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 235/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00235: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 236/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00236: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 237/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0155 - mean_squared_error: 0.0155\n",
      "Epoch 00237: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 238/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00238: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 239/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00239: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 240/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00240: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 241/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0156 - mean_squared_error: 0.0156\n",
      "Epoch 00241: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 242/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00242: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 243/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00243: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 244/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00244: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 245/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0154 - mean_squared_error: 0.0154\n",
      "Epoch 00245: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 246/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00246: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 247/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00247: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 248/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0151 - mean_squared_error: 0.0151\n",
      "Epoch 00248: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 249/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0158 - mean_squared_error: 0.0158\n",
      "Epoch 00249: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 250/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00250: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 251/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00251: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 252/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0173 - mean_squared_error: 0.0173\n",
      "Epoch 00252: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 253/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00253: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 254/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00254: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 255/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 00255: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 256/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0093 - mean_squared_error: 0.0093\n",
      "Epoch 00256: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 257/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00257: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 258/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00258: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 259/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0352 - mean_squared_error: 0.0352\n",
      "Epoch 00259: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 260/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0160 - mean_squared_error: 0.0160\n",
      "Epoch 00260: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 261/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0132 - mean_squared_error: 0.0132\n",
      "Epoch 00261: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 262/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00262: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 263/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00263: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 264/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0177 - mean_squared_error: 0.0177\n",
      "Epoch 00264: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 265/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00265: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 266/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0089 - mean_squared_error: 0.0089\n",
      "Epoch 00266: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 267/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00267: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 268/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0146 - mean_squared_error: 0.0146\n",
      "Epoch 00268: val_loss did not improve from 0.03525\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Running trial 5\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 1s - loss: 1.1025 - mean_squared_error: 1.1025\n",
      "Epoch 00001: val_loss improved from inf to 0.74158, saving model to model.h5\n",
      "248/248 [==============================] - 5s 18ms/sample - loss: 1.0884 - mean_squared_error: 1.0884 - val_loss: 0.7416 - val_mean_squared_error: 0.7416\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.0488 - mean_squared_error: 1.0488\n",
      "Epoch 00002: val_loss improved from 0.74158 to 0.71639, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 1.0137 - mean_squared_error: 1.0137 - val_loss: 0.7164 - val_mean_squared_error: 0.7164\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8421 - mean_squared_error: 0.8421\n",
      "Epoch 00003: val_loss improved from 0.71639 to 0.69119, saving model to model.h5\n",
      "248/248 [==============================] - 0s 203us/sample - loss: 0.9354 - mean_squared_error: 0.9354 - val_loss: 0.6912 - val_mean_squared_error: 0.6912\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8730 - mean_squared_error: 0.8730\n",
      "Epoch 00004: val_loss improved from 0.69119 to 0.66586, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.8561 - mean_squared_error: 0.8561 - val_loss: 0.6659 - val_mean_squared_error: 0.6659\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7844 - mean_squared_error: 0.7844\n",
      "Epoch 00005: val_loss improved from 0.66586 to 0.64018, saving model to model.h5\n",
      "248/248 [==============================] - 0s 222us/sample - loss: 0.7808 - mean_squared_error: 0.7808 - val_loss: 0.6402 - val_mean_squared_error: 0.6402\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8068 - mean_squared_error: 0.8068\n",
      "Epoch 00006: val_loss improved from 0.64018 to 0.61335, saving model to model.h5\n",
      "248/248 [==============================] - 0s 212us/sample - loss: 0.6936 - mean_squared_error: 0.6936 - val_loss: 0.6134 - val_mean_squared_error: 0.6134\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6569 - mean_squared_error: 0.6569\n",
      "Epoch 00007: val_loss improved from 0.61335 to 0.58461, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 0.5944 - mean_squared_error: 0.5944 - val_loss: 0.5846 - val_mean_squared_error: 0.5846\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4434 - mean_squared_error: 0.4434\n",
      "Epoch 00008: val_loss improved from 0.58461 to 0.55387, saving model to model.h5\n",
      "248/248 [==============================] - 0s 207us/sample - loss: 0.5122 - mean_squared_error: 0.5122 - val_loss: 0.5539 - val_mean_squared_error: 0.5539\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5143 - mean_squared_error: 0.5143\n",
      "Epoch 00009: val_loss improved from 0.55387 to 0.52120, saving model to model.h5\n",
      "248/248 [==============================] - 0s 207us/sample - loss: 0.4265 - mean_squared_error: 0.4265 - val_loss: 0.5212 - val_mean_squared_error: 0.5212\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3742 - mean_squared_error: 0.3742\n",
      "Epoch 00010: val_loss improved from 0.52120 to 0.48680, saving model to model.h5\n",
      "248/248 [==============================] - 0s 206us/sample - loss: 0.3391 - mean_squared_error: 0.3391 - val_loss: 0.4868 - val_mean_squared_error: 0.4868\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3028 - mean_squared_error: 0.3028\n",
      "Epoch 00011: val_loss improved from 0.48680 to 0.45044, saving model to model.h5\n",
      "248/248 [==============================] - 0s 206us/sample - loss: 0.2718 - mean_squared_error: 0.2718 - val_loss: 0.4504 - val_mean_squared_error: 0.4504\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2535 - mean_squared_error: 0.2535\n",
      "Epoch 00012: val_loss improved from 0.45044 to 0.41203, saving model to model.h5\n",
      "248/248 [==============================] - 0s 209us/sample - loss: 0.2128 - mean_squared_error: 0.2128 - val_loss: 0.4120 - val_mean_squared_error: 0.4120\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1660 - mean_squared_error: 0.1660\n",
      "Epoch 00013: val_loss improved from 0.41203 to 0.37203, saving model to model.h5\n",
      "248/248 [==============================] - 0s 215us/sample - loss: 0.1669 - mean_squared_error: 0.1669 - val_loss: 0.3720 - val_mean_squared_error: 0.3720\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1529 - mean_squared_error: 0.1529\n",
      "Epoch 00014: val_loss improved from 0.37203 to 0.33206, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 0.1482 - mean_squared_error: 0.1482 - val_loss: 0.3321 - val_mean_squared_error: 0.3321\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1159 - mean_squared_error: 0.1159\n",
      "Epoch 00015: val_loss improved from 0.33206 to 0.29277, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.1305 - mean_squared_error: 0.1305 - val_loss: 0.2928 - val_mean_squared_error: 0.2928\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1171 - mean_squared_error: 0.1171\n",
      "Epoch 00016: val_loss improved from 0.29277 to 0.25602, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.1195 - mean_squared_error: 0.1195 - val_loss: 0.2560 - val_mean_squared_error: 0.2560\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1061 - mean_squared_error: 0.1061\n",
      "Epoch 00017: val_loss improved from 0.25602 to 0.22289, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.1072 - mean_squared_error: 0.1072 - val_loss: 0.2229 - val_mean_squared_error: 0.2229\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1201 - mean_squared_error: 0.1201\n",
      "Epoch 00018: val_loss improved from 0.22289 to 0.19412, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.1005 - mean_squared_error: 0.1005 - val_loss: 0.1941 - val_mean_squared_error: 0.1941\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0864 - mean_squared_error: 0.0864\n",
      "Epoch 00019: val_loss improved from 0.19412 to 0.16884, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0815 - mean_squared_error: 0.0815 - val_loss: 0.1688 - val_mean_squared_error: 0.1688\n",
      "Epoch 20/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0712 - mean_squared_error: 0.0712\n",
      "Epoch 00020: val_loss improved from 0.16884 to 0.14689, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 0.0708 - mean_squared_error: 0.0708 - val_loss: 0.1469 - val_mean_squared_error: 0.1469\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0580 - mean_squared_error: 0.0580\n",
      "Epoch 00021: val_loss improved from 0.14689 to 0.12854, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.0611 - mean_squared_error: 0.0611 - val_loss: 0.1285 - val_mean_squared_error: 0.1285\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0515 - mean_squared_error: 0.0515\n",
      "Epoch 00022: val_loss improved from 0.12854 to 0.11314, saving model to model.h5\n",
      "248/248 [==============================] - 0s 192us/sample - loss: 0.0567 - mean_squared_error: 0.0567 - val_loss: 0.1131 - val_mean_squared_error: 0.1131\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0539 - mean_squared_error: 0.0539\n",
      "Epoch 00023: val_loss improved from 0.11314 to 0.10035, saving model to model.h5\n",
      "248/248 [==============================] - 0s 190us/sample - loss: 0.0540 - mean_squared_error: 0.0540 - val_loss: 0.1004 - val_mean_squared_error: 0.1004\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0652 - mean_squared_error: 0.0652\n",
      "Epoch 00024: val_loss improved from 0.10035 to 0.09036, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0504 - mean_squared_error: 0.0504 - val_loss: 0.0904 - val_mean_squared_error: 0.0904\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0498 - mean_squared_error: 0.0498\n",
      "Epoch 00025: val_loss improved from 0.09036 to 0.08246, saving model to model.h5\n",
      "248/248 [==============================] - 0s 196us/sample - loss: 0.0533 - mean_squared_error: 0.0533 - val_loss: 0.0825 - val_mean_squared_error: 0.0825\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0627 - mean_squared_error: 0.0627\n",
      "Epoch 00026: val_loss improved from 0.08246 to 0.07629, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0517 - mean_squared_error: 0.0517 - val_loss: 0.0763 - val_mean_squared_error: 0.0763\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0470 - mean_squared_error: 0.0470\n",
      "Epoch 00027: val_loss improved from 0.07629 to 0.07141, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.0459 - mean_squared_error: 0.0459 - val_loss: 0.0714 - val_mean_squared_error: 0.0714\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0420 - mean_squared_error: 0.0420\n",
      "Epoch 00028: val_loss improved from 0.07141 to 0.06817, saving model to model.h5\n",
      "248/248 [==============================] - 0s 194us/sample - loss: 0.0430 - mean_squared_error: 0.0430 - val_loss: 0.0682 - val_mean_squared_error: 0.0682\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00029: val_loss improved from 0.06817 to 0.06565, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0656 - val_mean_squared_error: 0.0656\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Epoch 00030: val_loss improved from 0.06565 to 0.06398, saving model to model.h5\n",
      "248/248 [==============================] - 0s 212us/sample - loss: 0.0417 - mean_squared_error: 0.0417 - val_loss: 0.0640 - val_mean_squared_error: 0.0640\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0436 - mean_squared_error: 0.0436\n",
      "Epoch 00031: val_loss improved from 0.06398 to 0.06274, saving model to model.h5\n",
      "248/248 [==============================] - 0s 196us/sample - loss: 0.0421 - mean_squared_error: 0.0421 - val_loss: 0.0627 - val_mean_squared_error: 0.0627\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0397 - mean_squared_error: 0.0397\n",
      "Epoch 00032: val_loss improved from 0.06274 to 0.06139, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0614 - val_mean_squared_error: 0.0614\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0419 - mean_squared_error: 0.0419\n",
      "Epoch 00033: val_loss improved from 0.06139 to 0.06062, saving model to model.h5\n",
      "248/248 [==============================] - 0s 190us/sample - loss: 0.0446 - mean_squared_error: 0.0446 - val_loss: 0.0606 - val_mean_squared_error: 0.0606\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0608 - mean_squared_error: 0.0608\n",
      "Epoch 00034: val_loss improved from 0.06062 to 0.06011, saving model to model.h5\n",
      "248/248 [==============================] - 0s 183us/sample - loss: 0.0418 - mean_squared_error: 0.0418 - val_loss: 0.0601 - val_mean_squared_error: 0.0601\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0362 - mean_squared_error: 0.0362\n",
      "Epoch 00035: val_loss improved from 0.06011 to 0.05959, saving model to model.h5\n",
      "248/248 [==============================] - 0s 190us/sample - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0596 - val_mean_squared_error: 0.0596\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Epoch 00036: val_loss improved from 0.05959 to 0.05926, saving model to model.h5\n",
      "248/248 [==============================] - 0s 191us/sample - loss: 0.0391 - mean_squared_error: 0.0391 - val_loss: 0.0593 - val_mean_squared_error: 0.0593\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0410 - mean_squared_error: 0.0410\n",
      "Epoch 00037: val_loss improved from 0.05926 to 0.05883, saving model to model.h5\n",
      "248/248 [==============================] - 0s 194us/sample - loss: 0.0404 - mean_squared_error: 0.0404 - val_loss: 0.0588 - val_mean_squared_error: 0.0588\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00038: val_loss improved from 0.05883 to 0.05873, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0587 - val_mean_squared_error: 0.0587\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 00039: val_loss improved from 0.05873 to 0.05864, saving model to model.h5\n",
      "248/248 [==============================] - 0s 187us/sample - loss: 0.0385 - mean_squared_error: 0.0385 - val_loss: 0.0586 - val_mean_squared_error: 0.0586\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0437 - mean_squared_error: 0.0437\n",
      "Epoch 00040: val_loss improved from 0.05864 to 0.05863, saving model to model.h5\n",
      "248/248 [==============================] - 0s 208us/sample - loss: 0.0367 - mean_squared_error: 0.0367 - val_loss: 0.0586 - val_mean_squared_error: 0.0586\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0367 - mean_squared_error: 0.0367\n",
      "Epoch 00041: val_loss did not improve from 0.05863\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0588 - val_mean_squared_error: 0.0588\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0420 - mean_squared_error: 0.0420\n",
      "Epoch 00042: val_loss improved from 0.05863 to 0.05811, saving model to model.h5\n",
      "248/248 [==============================] - 0s 194us/sample - loss: 0.0382 - mean_squared_error: 0.0382 - val_loss: 0.0581 - val_mean_squared_error: 0.0581\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0418 - mean_squared_error: 0.0418\n",
      "Epoch 00043: val_loss improved from 0.05811 to 0.05751, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.0382 - mean_squared_error: 0.0382 - val_loss: 0.0575 - val_mean_squared_error: 0.0575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0345 - mean_squared_error: 0.0345\n",
      "Epoch 00044: val_loss improved from 0.05751 to 0.05650, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0565 - val_mean_squared_error: 0.0565\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00045: val_loss improved from 0.05650 to 0.05541, saving model to model.h5\n",
      "248/248 [==============================] - 0s 191us/sample - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0554 - val_mean_squared_error: 0.0554\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 00046: val_loss improved from 0.05541 to 0.05461, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0546 - val_mean_squared_error: 0.0546\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00047: val_loss improved from 0.05461 to 0.05374, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0537 - val_mean_squared_error: 0.0537\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0326 - mean_squared_error: 0.0326\n",
      "Epoch 00048: val_loss improved from 0.05374 to 0.05361, saving model to model.h5\n",
      "248/248 [==============================] - 0s 186us/sample - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0536 - val_mean_squared_error: 0.0536\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00049: val_loss improved from 0.05361 to 0.05349, saving model to model.h5\n",
      "248/248 [==============================] - 0s 192us/sample - loss: 0.0374 - mean_squared_error: 0.0374 - val_loss: 0.0535 - val_mean_squared_error: 0.0535\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00050: val_loss improved from 0.05349 to 0.05314, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0531 - val_mean_squared_error: 0.0531\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0378 - mean_squared_error: 0.0378\n",
      "Epoch 00051: val_loss improved from 0.05314 to 0.05265, saving model to model.h5\n",
      "248/248 [==============================] - 0s 196us/sample - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0526 - val_mean_squared_error: 0.0526\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00052: val_loss improved from 0.05265 to 0.05189, saving model to model.h5\n",
      "248/248 [==============================] - 0s 192us/sample - loss: 0.0323 - mean_squared_error: 0.0323 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00053: val_loss improved from 0.05189 to 0.05148, saving model to model.h5\n",
      "248/248 [==============================] - 0s 191us/sample - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0515 - val_mean_squared_error: 0.0515\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0517 - mean_squared_error: 0.0517\n",
      "Epoch 00054: val_loss improved from 0.05148 to 0.05087, saving model to model.h5\n",
      "248/248 [==============================] - 0s 196us/sample - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0509 - val_mean_squared_error: 0.0509\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0379 - mean_squared_error: 0.0379\n",
      "Epoch 00055: val_loss improved from 0.05087 to 0.05033, saving model to model.h5\n",
      "248/248 [==============================] - 0s 219us/sample - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0503 - val_mean_squared_error: 0.0503\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0379 - mean_squared_error: 0.0379\n",
      "Epoch 00056: val_loss improved from 0.05033 to 0.05024, saving model to model.h5\n",
      "248/248 [==============================] - 0s 237us/sample - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0509 - mean_squared_error: 0.0509\n",
      "Epoch 00057: val_loss did not improve from 0.05024\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0367 - mean_squared_error: 0.0367\n",
      "Epoch 00058: val_loss did not improve from 0.05024\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0369 - mean_squared_error: 0.0369\n",
      "Epoch 00059: val_loss did not improve from 0.05024\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0422 - mean_squared_error: 0.0422\n",
      "Epoch 00060: val_loss did not improve from 0.05024\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Epoch 00061: val_loss did not improve from 0.05024\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0398 - mean_squared_error: 0.0398\n",
      "Epoch 00062: val_loss did not improve from 0.05024\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0438 - mean_squared_error: 0.0438\n",
      "Epoch 00063: val_loss improved from 0.05024 to 0.04992, saving model to model.h5\n",
      "248/248 [==============================] - 0s 194us/sample - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Epoch 00064: val_loss improved from 0.04992 to 0.04902, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0607 - mean_squared_error: 0.0607\n",
      "Epoch 00065: val_loss improved from 0.04902 to 0.04843, saving model to model.h5\n",
      "248/248 [==============================] - 0s 207us/sample - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00066: val_loss improved from 0.04843 to 0.04777, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0382 - mean_squared_error: 0.0382\n",
      "Epoch 00067: val_loss improved from 0.04777 to 0.04775, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 68/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0387 - mean_squared_error: 0.0387\n",
      "Epoch 00068: val_loss improved from 0.04775 to 0.04765, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0345 - mean_squared_error: 0.0345 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0339 - mean_squared_error: 0.0339\n",
      "Epoch 00069: val_loss improved from 0.04765 to 0.04704, saving model to model.h5\n",
      "248/248 [==============================] - 0s 194us/sample - loss: 0.0383 - mean_squared_error: 0.0383 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00070: val_loss improved from 0.04704 to 0.04673, saving model to model.h5\n",
      "248/248 [==============================] - 0s 234us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00071: val_loss improved from 0.04673 to 0.04616, saving model to model.h5\n",
      "248/248 [==============================] - 0s 192us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Epoch 00072: val_loss did not improve from 0.04616\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0319 - mean_squared_error: 0.0319 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00073: val_loss did not improve from 0.04616\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0331 - mean_squared_error: 0.0331 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00074: val_loss did not improve from 0.04616\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00075: val_loss did not improve from 0.04616\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00076: val_loss did not improve from 0.04616\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00077: val_loss did not improve from 0.04616\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00078: val_loss did not improve from 0.04616\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00079: val_loss did not improve from 0.04616\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0369 - mean_squared_error: 0.0369\n",
      "Epoch 00080: val_loss did not improve from 0.04616\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0320 - mean_squared_error: 0.0320 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00081: val_loss improved from 0.04616 to 0.04598, saving model to model.h5\n",
      "248/248 [==============================] - 0s 209us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00082: val_loss improved from 0.04598 to 0.04589, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00083: val_loss did not improve from 0.04589\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0348 - mean_squared_error: 0.0348\n",
      "Epoch 00084: val_loss did not improve from 0.04589\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Epoch 00085: val_loss did not improve from 0.04589\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0320 - mean_squared_error: 0.0320 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0259 - mean_squared_error: 0.0259\n",
      "Epoch 00086: val_loss did not improve from 0.04589\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0395 - mean_squared_error: 0.0395\n",
      "Epoch 00087: val_loss did not improve from 0.04589\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 00088: val_loss improved from 0.04589 to 0.04507, saving model to model.h5\n",
      "248/248 [==============================] - 0s 203us/sample - loss: 0.0308 - mean_squared_error: 0.0308 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00089: val_loss improved from 0.04507 to 0.04369, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00090: val_loss improved from 0.04369 to 0.04282, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00091: val_loss improved from 0.04282 to 0.04281, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00092: val_loss did not improve from 0.04281\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 93/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0405 - mean_squared_error: 0.0405\n",
      "Epoch 00093: val_loss did not improve from 0.04281\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00094: val_loss did not improve from 0.04281\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Epoch 00095: val_loss did not improve from 0.04281\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00096: val_loss did not improve from 0.04281\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00097: val_loss did not improve from 0.04281\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00098: val_loss improved from 0.04281 to 0.04250, saving model to model.h5\n",
      "248/248 [==============================] - 0s 185us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00099: val_loss improved from 0.04250 to 0.04222, saving model to model.h5\n",
      "248/248 [==============================] - 0s 194us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00100: val_loss improved from 0.04222 to 0.04221, saving model to model.h5\n",
      "248/248 [==============================] - 0s 192us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00101: val_loss did not improve from 0.04221\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00102: val_loss did not improve from 0.04221\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00103: val_loss did not improve from 0.04221\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00104: val_loss did not improve from 0.04221\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00105: val_loss did not improve from 0.04221\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00106: val_loss improved from 0.04221 to 0.04197, saving model to model.h5\n",
      "248/248 [==============================] - 0s 204us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00107: val_loss improved from 0.04197 to 0.04146, saving model to model.h5\n",
      "248/248 [==============================] - 0s 188us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00108: val_loss improved from 0.04146 to 0.04083, saving model to model.h5\n",
      "248/248 [==============================] - 0s 204us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0401 - mean_squared_error: 0.0401\n",
      "Epoch 00109: val_loss improved from 0.04083 to 0.04042, saving model to model.h5\n",
      "248/248 [==============================] - 0s 212us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00110: val_loss improved from 0.04042 to 0.04019, saving model to model.h5\n",
      "248/248 [==============================] - 0s 261us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00111: val_loss did not improve from 0.04019\n",
      "248/248 [==============================] - 0s 89us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00112: val_loss did not improve from 0.04019\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0401 - mean_squared_error: 0.0401\n",
      "Epoch 00113: val_loss did not improve from 0.04019\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00114: val_loss did not improve from 0.04019\n",
      "248/248 [==============================] - 0s 92us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0414 - mean_squared_error: 0.0414\n",
      "Epoch 00115: val_loss did not improve from 0.04019\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00116: val_loss did not improve from 0.04019\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00117: val_loss did not improve from 0.04019\n",
      "248/248 [==============================] - 0s 89us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 118/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00118: val_loss did not improve from 0.04019\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0341 - mean_squared_error: 0.0341\n",
      "Epoch 00119: val_loss did not improve from 0.04019\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00120: val_loss did not improve from 0.04019\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00121: val_loss did not improve from 0.04019\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00122: val_loss did not improve from 0.04019\n",
      "248/248 [==============================] - 0s 92us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0138 - mean_squared_error: 0.0138\n",
      "Epoch 00123: val_loss did not improve from 0.04019\n",
      "248/248 [==============================] - 0s 107us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00124: val_loss did not improve from 0.04019\n",
      "248/248 [==============================] - 0s 100us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00125: val_loss did not improve from 0.04019\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 00126: val_loss did not improve from 0.04019\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0326 - mean_squared_error: 0.0326\n",
      "Epoch 00127: val_loss did not improve from 0.04019\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00128: val_loss did not improve from 0.04019\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00129: val_loss did not improve from 0.04019\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 130/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00130: val_loss did not improve from 0.04019\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00131: val_loss did not improve from 0.04019\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 132/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00132: val_loss did not improve from 0.04019\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 133/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00133: val_loss improved from 0.04019 to 0.04008, saving model to model.h5\n",
      "248/248 [==============================] - 0s 192us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 134/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0156 - mean_squared_error: 0.0156\n",
      "Epoch 00134: val_loss improved from 0.04008 to 0.03935, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 135/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00135: val_loss improved from 0.03935 to 0.03910, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 136/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00136: val_loss improved from 0.03910 to 0.03894, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 137/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00137: val_loss improved from 0.03894 to 0.03878, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 138/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00138: val_loss improved from 0.03878 to 0.03826, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0383 - val_mean_squared_error: 0.0383\n",
      "Epoch 139/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00139: val_loss did not improve from 0.03826\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 140/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00140: val_loss did not improve from 0.03826\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 141/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Epoch 00141: val_loss did not improve from 0.03826\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 142/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00142: val_loss did not improve from 0.03826\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 143/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0377 - mean_squared_error: 0.0377\n",
      "Epoch 00143: val_loss did not improve from 0.03826\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00144: val_loss did not improve from 0.03826\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 145/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00145: val_loss did not improve from 0.03826\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 146/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0412 - mean_squared_error: 0.0412\n",
      "Epoch 00146: val_loss did not improve from 0.03826\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 147/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00147: val_loss did not improve from 0.03826\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 148/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0161 - mean_squared_error: 0.0161\n",
      "Epoch 00148: val_loss improved from 0.03826 to 0.03805, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 149/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00149: val_loss improved from 0.03805 to 0.03800, saving model to model.h5\n",
      "248/248 [==============================] - 0s 189us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 150/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00150: val_loss did not improve from 0.03800\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 151/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0300 - mean_squared_error: 0.0300\n",
      "Epoch 00151: val_loss did not improve from 0.03800\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 152/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00152: val_loss improved from 0.03800 to 0.03770, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 153/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0399 - mean_squared_error: 0.0399\n",
      "Epoch 00153: val_loss improved from 0.03770 to 0.03750, saving model to model.h5\n",
      "248/248 [==============================] - 0s 188us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 154/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00154: val_loss improved from 0.03750 to 0.03732, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 155/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Epoch 00155: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 156/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Epoch 00156: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 157/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00157: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 158/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00158: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 159/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00159: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 160/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0391 - mean_squared_error: 0.0391\n",
      "Epoch 00160: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 161/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00161: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 162/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00162: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 163/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00163: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 164/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00164: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 165/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0170 - mean_squared_error: 0.0170\n",
      "Epoch 00165: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 166/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00166: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 167/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0150 - mean_squared_error: 0.0150\n",
      "Epoch 00167: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 168/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00168: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 169/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0181 - mean_squared_error: 0.0181\n",
      "Epoch 00169: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00170: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 171/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00171: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 172/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 00172: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 173/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00173: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 174/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00174: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 175/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00175: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 176/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00176: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 177/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00177: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 178/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Epoch 00178: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 101us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 179/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00179: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 82us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 180/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00180: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 92us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 181/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00181: val_loss did not improve from 0.03732\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 182/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00182: val_loss improved from 0.03732 to 0.03729, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 183/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00183: val_loss did not improve from 0.03729\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 184/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00184: val_loss did not improve from 0.03729\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 185/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0159 - mean_squared_error: 0.0159\n",
      "Epoch 00185: val_loss did not improve from 0.03729\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 186/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00186: val_loss did not improve from 0.03729\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 187/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0370 - mean_squared_error: 0.0370\n",
      "Epoch 00187: val_loss did not improve from 0.03729\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 188/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Epoch 00188: val_loss did not improve from 0.03729\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 189/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00189: val_loss did not improve from 0.03729\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 190/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Epoch 00190: val_loss improved from 0.03729 to 0.03716, saving model to model.h5\n",
      "248/248 [==============================] - 0s 187us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 191/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0149 - mean_squared_error: 0.0149\n",
      "Epoch 00191: val_loss improved from 0.03716 to 0.03705, saving model to model.h5\n",
      "248/248 [==============================] - 0s 199us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 192/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00192: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 193/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00193: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 194/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0259 - mean_squared_error: 0.0259\n",
      "Epoch 00194: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 195/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0158 - mean_squared_error: 0.0158\n",
      "Epoch 00195: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00196: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 197/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00197: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 198/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.0185\n",
      "Epoch 00198: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 199/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00199: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 200/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00200: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 201/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 00201: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 202/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00202: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 203/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00203: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 204/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0152 - mean_squared_error: 0.0152\n",
      "Epoch 00204: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 67us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 205/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00205: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 206/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00206: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 207/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0132 - mean_squared_error: 0.0132\n",
      "Epoch 00207: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 208/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0414 - mean_squared_error: 0.0414\n",
      "Epoch 00208: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 209/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00209: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 210/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00210: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 211/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00211: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 212/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00212: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 213/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0184 - mean_squared_error: 0.0184\n",
      "Epoch 00213: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 214/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00214: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 215/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0149 - mean_squared_error: 0.0149\n",
      "Epoch 00215: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 216/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00216: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 217/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00217: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 218/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Epoch 00218: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 219/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00219: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 220/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00220: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 221/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00221: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 222/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0098 - mean_squared_error: 0.0098\n",
      "Epoch 00222: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 223/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00223: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 224/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00224: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 83us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 225/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00225: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 226/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00226: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 227/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00227: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 228/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0420 - mean_squared_error: 0.0420\n",
      "Epoch 00228: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 229/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0167 - mean_squared_error: 0.0167\n",
      "Epoch 00229: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 230/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00230: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 231/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00231: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 232/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00232: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 233/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00233: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 234/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00234: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 235/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00235: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 236/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00236: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 237/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00237: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 238/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00238: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 239/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00239: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 240/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.0200\n",
      "Epoch 00240: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 241/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00241: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 242/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00242: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 243/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00243: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 244/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00244: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 245/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0383 - mean_squared_error: 0.0383\n",
      "Epoch 00245: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 246/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00246: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 247/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00247: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 248/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0405 - mean_squared_error: 0.0405\n",
      "Epoch 00248: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 249/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0178 - mean_squared_error: 0.0178\n",
      "Epoch 00249: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 250/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00250: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 251/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00251: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 252/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00252: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 253/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0294 - mean_squared_error: 0.0294\n",
      "Epoch 00253: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 254/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0145 - mean_squared_error: 0.0145\n",
      "Epoch 00254: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 255/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00255: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 256/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00256: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 257/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00257: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 258/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Epoch 00258: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 259/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00259: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 260/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00260: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 261/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0096 - mean_squared_error: 0.0096\n",
      "Epoch 00261: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 262/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00262: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 263/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00263: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 264/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00264: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 265/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00265: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 266/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00266: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 267/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0174 - mean_squared_error: 0.0174\n",
      "Epoch 00267: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 268/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0122 - mean_squared_error: 0.0122\n",
      "Epoch 00268: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 269/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00269: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 270/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0101 - mean_squared_error: 0.0101\n",
      "Epoch 00270: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 271/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00271: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 272/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00272: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 273/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0343 - mean_squared_error: 0.0343\n",
      "Epoch 00273: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 274/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00274: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 275/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0127 - mean_squared_error: 0.0127\n",
      "Epoch 00275: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 276/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00276: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 277/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0156 - mean_squared_error: 0.0156\n",
      "Epoch 00277: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 93us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 278/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 00278: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 279/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00279: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 280/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0125 - mean_squared_error: 0.0125\n",
      "Epoch 00280: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 281/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00281: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 282/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00282: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0175 - mean_squared_error: 0.0175 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 283/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0231 - mean_squared_error: 0.0231\n",
      "Epoch 00283: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 284/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 00284: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 285/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00285: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 286/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00286: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 287/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00287: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 288/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0159 - mean_squared_error: 0.0159\n",
      "Epoch 00288: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 289/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00289: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 290/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00290: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 291/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0142 - mean_squared_error: 0.0142\n",
      "Epoch 00291: val_loss did not improve from 0.03705\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Running trial 6\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 1s - loss: 1.0825 - mean_squared_error: 1.0825\n",
      "Epoch 00001: val_loss improved from inf to 0.74360, saving model to model.h5\n",
      "248/248 [==============================] - 5s 19ms/sample - loss: 1.1180 - mean_squared_error: 1.1180 - val_loss: 0.7436 - val_mean_squared_error: 0.7436\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.0474 - mean_squared_error: 1.0474\n",
      "Epoch 00002: val_loss improved from 0.74360 to 0.72088, saving model to model.h5\n",
      "248/248 [==============================] - 0s 191us/sample - loss: 1.0437 - mean_squared_error: 1.0437 - val_loss: 0.7209 - val_mean_squared_error: 0.7209\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.0482 - mean_squared_error: 1.0482\n",
      "Epoch 00003: val_loss improved from 0.72088 to 0.69811, saving model to model.h5\n",
      "248/248 [==============================] - 0s 221us/sample - loss: 0.9808 - mean_squared_error: 0.9808 - val_loss: 0.6981 - val_mean_squared_error: 0.6981\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9213 - mean_squared_error: 0.9213\n",
      "Epoch 00004: val_loss improved from 0.69811 to 0.67446, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.9101 - mean_squared_error: 0.9101 - val_loss: 0.6745 - val_mean_squared_error: 0.6745\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9162 - mean_squared_error: 0.9162\n",
      "Epoch 00005: val_loss improved from 0.67446 to 0.64921, saving model to model.h5\n",
      "248/248 [==============================] - 0s 207us/sample - loss: 0.8359 - mean_squared_error: 0.8359 - val_loss: 0.6492 - val_mean_squared_error: 0.6492\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7890 - mean_squared_error: 0.7890\n",
      "Epoch 00006: val_loss improved from 0.64921 to 0.62285, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.7594 - mean_squared_error: 0.7594 - val_loss: 0.6228 - val_mean_squared_error: 0.6228\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7130 - mean_squared_error: 0.7130\n",
      "Epoch 00007: val_loss improved from 0.62285 to 0.59443, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.6886 - mean_squared_error: 0.6886 - val_loss: 0.5944 - val_mean_squared_error: 0.5944\n",
      "Epoch 8/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6908 - mean_squared_error: 0.6908\n",
      "Epoch 00008: val_loss improved from 0.59443 to 0.56392, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 0.6063 - mean_squared_error: 0.6063 - val_loss: 0.5639 - val_mean_squared_error: 0.5639\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6134 - mean_squared_error: 0.6134\n",
      "Epoch 00009: val_loss improved from 0.56392 to 0.53019, saving model to model.h5\n",
      "248/248 [==============================] - 0s 204us/sample - loss: 0.5291 - mean_squared_error: 0.5291 - val_loss: 0.5302 - val_mean_squared_error: 0.5302\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4796 - mean_squared_error: 0.4796\n",
      "Epoch 00010: val_loss improved from 0.53019 to 0.49416, saving model to model.h5\n",
      "248/248 [==============================] - 0s 192us/sample - loss: 0.4592 - mean_squared_error: 0.4592 - val_loss: 0.4942 - val_mean_squared_error: 0.4942\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3879 - mean_squared_error: 0.3879\n",
      "Epoch 00011: val_loss improved from 0.49416 to 0.45476, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.3717 - mean_squared_error: 0.3717 - val_loss: 0.4548 - val_mean_squared_error: 0.4548\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3022 - mean_squared_error: 0.3022\n",
      "Epoch 00012: val_loss improved from 0.45476 to 0.41300, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.3134 - mean_squared_error: 0.3134 - val_loss: 0.4130 - val_mean_squared_error: 0.4130\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2520 - mean_squared_error: 0.2520\n",
      "Epoch 00013: val_loss improved from 0.41300 to 0.36931, saving model to model.h5\n",
      "248/248 [==============================] - 0s 208us/sample - loss: 0.2363 - mean_squared_error: 0.2363 - val_loss: 0.3693 - val_mean_squared_error: 0.3693\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2282 - mean_squared_error: 0.2282\n",
      "Epoch 00014: val_loss improved from 0.36931 to 0.32550, saving model to model.h5\n",
      "248/248 [==============================] - 0s 188us/sample - loss: 0.1972 - mean_squared_error: 0.1972 - val_loss: 0.3255 - val_mean_squared_error: 0.3255\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1684 - mean_squared_error: 0.1684\n",
      "Epoch 00015: val_loss improved from 0.32550 to 0.28370, saving model to model.h5\n",
      "248/248 [==============================] - 0s 185us/sample - loss: 0.1678 - mean_squared_error: 0.1678 - val_loss: 0.2837 - val_mean_squared_error: 0.2837\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1681 - mean_squared_error: 0.1681\n",
      "Epoch 00016: val_loss improved from 0.28370 to 0.24547, saving model to model.h5\n",
      "248/248 [==============================] - 0s 196us/sample - loss: 0.1539 - mean_squared_error: 0.1539 - val_loss: 0.2455 - val_mean_squared_error: 0.2455\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1503 - mean_squared_error: 0.1503\n",
      "Epoch 00017: val_loss improved from 0.24547 to 0.21158, saving model to model.h5\n",
      "248/248 [==============================] - 0s 210us/sample - loss: 0.1313 - mean_squared_error: 0.1313 - val_loss: 0.2116 - val_mean_squared_error: 0.2116\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1165 - mean_squared_error: 0.1165\n",
      "Epoch 00018: val_loss improved from 0.21158 to 0.18266, saving model to model.h5\n",
      "248/248 [==============================] - 0s 194us/sample - loss: 0.1147 - mean_squared_error: 0.1147 - val_loss: 0.1827 - val_mean_squared_error: 0.1827\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1186 - mean_squared_error: 0.1186\n",
      "Epoch 00019: val_loss improved from 0.18266 to 0.15746, saving model to model.h5\n",
      "248/248 [==============================] - 0s 210us/sample - loss: 0.0964 - mean_squared_error: 0.0964 - val_loss: 0.1575 - val_mean_squared_error: 0.1575\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0651 - mean_squared_error: 0.0651\n",
      "Epoch 00020: val_loss improved from 0.15746 to 0.13622, saving model to model.h5\n",
      "248/248 [==============================] - 0s 210us/sample - loss: 0.0773 - mean_squared_error: 0.0773 - val_loss: 0.1362 - val_mean_squared_error: 0.1362\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0731 - mean_squared_error: 0.0731\n",
      "Epoch 00021: val_loss improved from 0.13622 to 0.11787, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0706 - mean_squared_error: 0.0706 - val_loss: 0.1179 - val_mean_squared_error: 0.1179\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0625 - mean_squared_error: 0.0625\n",
      "Epoch 00022: val_loss improved from 0.11787 to 0.10249, saving model to model.h5\n",
      "248/248 [==============================] - 0s 196us/sample - loss: 0.0586 - mean_squared_error: 0.0586 - val_loss: 0.1025 - val_mean_squared_error: 0.1025\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0631 - mean_squared_error: 0.0631\n",
      "Epoch 00023: val_loss improved from 0.10249 to 0.08984, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.0570 - mean_squared_error: 0.0570 - val_loss: 0.0898 - val_mean_squared_error: 0.0898\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0599 - mean_squared_error: 0.0599\n",
      "Epoch 00024: val_loss improved from 0.08984 to 0.07978, saving model to model.h5\n",
      "248/248 [==============================] - 0s 204us/sample - loss: 0.0575 - mean_squared_error: 0.0575 - val_loss: 0.0798 - val_mean_squared_error: 0.0798\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0442 - mean_squared_error: 0.0442\n",
      "Epoch 00025: val_loss improved from 0.07978 to 0.07176, saving model to model.h5\n",
      "248/248 [==============================] - 0s 216us/sample - loss: 0.0504 - mean_squared_error: 0.0504 - val_loss: 0.0718 - val_mean_squared_error: 0.0718\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0601 - mean_squared_error: 0.0601\n",
      "Epoch 00026: val_loss improved from 0.07176 to 0.06562, saving model to model.h5\n",
      "248/248 [==============================] - 0s 196us/sample - loss: 0.0503 - mean_squared_error: 0.0503 - val_loss: 0.0656 - val_mean_squared_error: 0.0656\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0572 - mean_squared_error: 0.0572\n",
      "Epoch 00027: val_loss improved from 0.06562 to 0.06099, saving model to model.h5\n",
      "248/248 [==============================] - 0s 216us/sample - loss: 0.0457 - mean_squared_error: 0.0457 - val_loss: 0.0610 - val_mean_squared_error: 0.0610\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Epoch 00028: val_loss improved from 0.06099 to 0.05773, saving model to model.h5\n",
      "248/248 [==============================] - 0s 214us/sample - loss: 0.0491 - mean_squared_error: 0.0491 - val_loss: 0.0577 - val_mean_squared_error: 0.0577\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0414 - mean_squared_error: 0.0414\n",
      "Epoch 00029: val_loss improved from 0.05773 to 0.05545, saving model to model.h5\n",
      "248/248 [==============================] - 0s 206us/sample - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0555 - val_mean_squared_error: 0.0555\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0467 - mean_squared_error: 0.0467\n",
      "Epoch 00030: val_loss improved from 0.05545 to 0.05408, saving model to model.h5\n",
      "248/248 [==============================] - 0s 212us/sample - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0541 - val_mean_squared_error: 0.0541\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Epoch 00031: val_loss improved from 0.05408 to 0.05323, saving model to model.h5\n",
      "248/248 [==============================] - 0s 203us/sample - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0532 - val_mean_squared_error: 0.0532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00032: val_loss improved from 0.05323 to 0.05263, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0380 - mean_squared_error: 0.0380 - val_loss: 0.0526 - val_mean_squared_error: 0.0526\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0383 - mean_squared_error: 0.0383\n",
      "Epoch 00033: val_loss improved from 0.05263 to 0.05232, saving model to model.h5\n",
      "248/248 [==============================] - 0s 203us/sample - loss: 0.0419 - mean_squared_error: 0.0419 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0479 - mean_squared_error: 0.0479\n",
      "Epoch 00034: val_loss did not improve from 0.05232\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0423 - mean_squared_error: 0.0423 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00035: val_loss did not improve from 0.05232\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0452 - mean_squared_error: 0.0452\n",
      "Epoch 00036: val_loss did not improve from 0.05232\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0393 - mean_squared_error: 0.0393\n",
      "Epoch 00037: val_loss improved from 0.05232 to 0.05224, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.0361 - mean_squared_error: 0.0361 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0511 - mean_squared_error: 0.0511\n",
      "Epoch 00038: val_loss did not improve from 0.05224\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0395 - mean_squared_error: 0.0395 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0448 - mean_squared_error: 0.0448\n",
      "Epoch 00039: val_loss improved from 0.05224 to 0.05222, saving model to model.h5\n",
      "248/248 [==============================] - 0s 208us/sample - loss: 0.0384 - mean_squared_error: 0.0384 - val_loss: 0.0522 - val_mean_squared_error: 0.0522\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00040: val_loss improved from 0.05222 to 0.05160, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0516 - val_mean_squared_error: 0.0516\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0448 - mean_squared_error: 0.0448\n",
      "Epoch 00041: val_loss improved from 0.05160 to 0.05103, saving model to model.h5\n",
      "248/248 [==============================] - 0s 190us/sample - loss: 0.0372 - mean_squared_error: 0.0372 - val_loss: 0.0510 - val_mean_squared_error: 0.0510\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Epoch 00042: val_loss improved from 0.05103 to 0.05070, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 0.0395 - mean_squared_error: 0.0395 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0505 - mean_squared_error: 0.0505\n",
      "Epoch 00043: val_loss improved from 0.05070 to 0.05023, saving model to model.h5\n",
      "248/248 [==============================] - 0s 212us/sample - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Epoch 00044: val_loss improved from 0.05023 to 0.04991, saving model to model.h5\n",
      "248/248 [==============================] - 0s 183us/sample - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00045: val_loss improved from 0.04991 to 0.04973, saving model to model.h5\n",
      "248/248 [==============================] - 0s 216us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0408 - mean_squared_error: 0.0408\n",
      "Epoch 00046: val_loss improved from 0.04973 to 0.04956, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0334 - mean_squared_error: 0.0334\n",
      "Epoch 00047: val_loss did not improve from 0.04956\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0470 - mean_squared_error: 0.0470\n",
      "Epoch 00048: val_loss did not improve from 0.04956\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Epoch 00049: val_loss improved from 0.04956 to 0.04953, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00050: val_loss improved from 0.04953 to 0.04905, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0391 - mean_squared_error: 0.0391 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00051: val_loss improved from 0.04905 to 0.04865, saving model to model.h5\n",
      "248/248 [==============================] - 0s 188us/sample - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0413 - mean_squared_error: 0.0413\n",
      "Epoch 00052: val_loss improved from 0.04865 to 0.04844, saving model to model.h5\n",
      "248/248 [==============================] - 0s 191us/sample - loss: 0.0389 - mean_squared_error: 0.0389 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00053: val_loss did not improve from 0.04844\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00054: val_loss did not improve from 0.04844\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0361 - mean_squared_error: 0.0361 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0410 - mean_squared_error: 0.0410\n",
      "Epoch 00055: val_loss did not improve from 0.04844\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00056: val_loss did not improve from 0.04844\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0422 - mean_squared_error: 0.0422\n",
      "Epoch 00057: val_loss did not improve from 0.04844\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0371 - mean_squared_error: 0.0371 - val_loss: 0.0487 - val_mean_squared_error: 0.0487\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0418 - mean_squared_error: 0.0418\n",
      "Epoch 00058: val_loss did not improve from 0.04844\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00059: val_loss did not improve from 0.04844\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0348 - mean_squared_error: 0.0348\n",
      "Epoch 00060: val_loss did not improve from 0.04844\n",
      "248/248 [==============================] - 0s 65us/sample - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0379 - mean_squared_error: 0.0379\n",
      "Epoch 00061: val_loss did not improve from 0.04844\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00062: val_loss did not improve from 0.04844\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0328 - mean_squared_error: 0.0328 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00063: val_loss did not improve from 0.04844\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Epoch 00064: val_loss did not improve from 0.04844\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Epoch 00065: val_loss did not improve from 0.04844\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00066: val_loss did not improve from 0.04844\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00067: val_loss did not improve from 0.04844\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0378 - mean_squared_error: 0.0378\n",
      "Epoch 00068: val_loss did not improve from 0.04844\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0386 - mean_squared_error: 0.0386\n",
      "Epoch 00069: val_loss improved from 0.04844 to 0.04811, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0330 - mean_squared_error: 0.0330 - val_loss: 0.0481 - val_mean_squared_error: 0.0481\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00070: val_loss improved from 0.04811 to 0.04728, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.0330 - mean_squared_error: 0.0330 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00071: val_loss improved from 0.04728 to 0.04687, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0469 - mean_squared_error: 0.0469\n",
      "Epoch 00072: val_loss improved from 0.04687 to 0.04635, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0380 - mean_squared_error: 0.0380 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00073: val_loss improved from 0.04635 to 0.04574, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0457 - val_mean_squared_error: 0.0457\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00074: val_loss did not improve from 0.04574\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00075: val_loss did not improve from 0.04574\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0413 - mean_squared_error: 0.0413\n",
      "Epoch 00076: val_loss did not improve from 0.04574\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0473 - mean_squared_error: 0.0473\n",
      "Epoch 00077: val_loss did not improve from 0.04574\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00078: val_loss did not improve from 0.04574\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0473 - val_mean_squared_error: 0.0473\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0406 - mean_squared_error: 0.0406\n",
      "Epoch 00079: val_loss did not improve from 0.04574\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00080: val_loss did not improve from 0.04574\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00081: val_loss did not improve from 0.04574\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00082: val_loss did not improve from 0.04574\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00083: val_loss improved from 0.04574 to 0.04515, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0436 - mean_squared_error: 0.0436\n",
      "Epoch 00084: val_loss improved from 0.04515 to 0.04456, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00085: val_loss improved from 0.04456 to 0.04376, saving model to model.h5\n",
      "248/248 [==============================] - 0s 208us/sample - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00086: val_loss improved from 0.04376 to 0.04299, saving model to model.h5\n",
      "248/248 [==============================] - 0s 196us/sample - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00087: val_loss improved from 0.04299 to 0.04296, saving model to model.h5\n",
      "248/248 [==============================] - 0s 209us/sample - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0426 - mean_squared_error: 0.0426\n",
      "Epoch 00088: val_loss improved from 0.04296 to 0.04269, saving model to model.h5\n",
      "248/248 [==============================] - 0s 221us/sample - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0402 - mean_squared_error: 0.0402\n",
      "Epoch 00089: val_loss improved from 0.04269 to 0.04252, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0453 - mean_squared_error: 0.0453\n",
      "Epoch 00090: val_loss improved from 0.04252 to 0.04210, saving model to model.h5\n",
      "248/248 [==============================] - 0s 192us/sample - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00091: val_loss did not improve from 0.04210\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Epoch 00092: val_loss did not improve from 0.04210\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0295 - mean_squared_error: 0.0295\n",
      "Epoch 00093: val_loss did not improve from 0.04210\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00094: val_loss did not improve from 0.04210\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Epoch 00095: val_loss did not improve from 0.04210\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00096: val_loss did not improve from 0.04210\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00097: val_loss did not improve from 0.04210\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00098: val_loss did not improve from 0.04210\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00099: val_loss did not improve from 0.04210\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00100: val_loss did not improve from 0.04210\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00101: val_loss did not improve from 0.04210\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00102: val_loss did not improve from 0.04210\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0369 - mean_squared_error: 0.0369\n",
      "Epoch 00103: val_loss did not improve from 0.04210\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00104: val_loss did not improve from 0.04210\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00105: val_loss did not improve from 0.04210\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Epoch 00106: val_loss did not improve from 0.04210\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0394 - mean_squared_error: 0.0394\n",
      "Epoch 00107: val_loss improved from 0.04210 to 0.04162, saving model to model.h5\n",
      "248/248 [==============================] - 0s 245us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 108/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0377 - mean_squared_error: 0.0377\n",
      "Epoch 00108: val_loss improved from 0.04162 to 0.04089, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.0328 - mean_squared_error: 0.0328 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00109: val_loss improved from 0.04089 to 0.04056, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00110: val_loss did not improve from 0.04056\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0368 - mean_squared_error: 0.0368\n",
      "Epoch 00111: val_loss improved from 0.04056 to 0.04053, saving model to model.h5\n",
      "248/248 [==============================] - 0s 204us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00112: val_loss did not improve from 0.04053\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0372 - mean_squared_error: 0.0372\n",
      "Epoch 00113: val_loss did not improve from 0.04053\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0341 - mean_squared_error: 0.0341\n",
      "Epoch 00114: val_loss did not improve from 0.04053\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00115: val_loss did not improve from 0.04053\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00116: val_loss did not improve from 0.04053\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 00117: val_loss did not improve from 0.04053\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00118: val_loss did not improve from 0.04053\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Epoch 00119: val_loss did not improve from 0.04053\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00120: val_loss did not improve from 0.04053\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00121: val_loss improved from 0.04053 to 0.04051, saving model to model.h5\n",
      "248/248 [==============================] - 0s 204us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0472 - mean_squared_error: 0.0472\n",
      "Epoch 00122: val_loss improved from 0.04051 to 0.03972, saving model to model.h5\n",
      "248/248 [==============================] - 0s 196us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00123: val_loss improved from 0.03972 to 0.03853, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0307 - mean_squared_error: 0.0307\n",
      "Epoch 00124: val_loss improved from 0.03853 to 0.03776, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0295 - mean_squared_error: 0.0295\n",
      "Epoch 00125: val_loss improved from 0.03776 to 0.03750, saving model to model.h5\n",
      "248/248 [==============================] - 0s 214us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00126: val_loss did not improve from 0.03750\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00127: val_loss did not improve from 0.03750\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0173 - mean_squared_error: 0.0173\n",
      "Epoch 00128: val_loss did not improve from 0.03750\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00129: val_loss did not improve from 0.03750\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 130/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00130: val_loss did not improve from 0.03750\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00131: val_loss did not improve from 0.03750\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 132/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00132: val_loss did not improve from 0.03750\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 133/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00133: val_loss did not improve from 0.03750\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 134/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00134: val_loss did not improve from 0.03750\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 135/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00135: val_loss did not improve from 0.03750\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 136/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00136: val_loss did not improve from 0.03750\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 137/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00137: val_loss did not improve from 0.03750\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 138/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00138: val_loss did not improve from 0.03750\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 139/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00139: val_loss did not improve from 0.03750\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 140/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00140: val_loss did not improve from 0.03750\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 141/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0188 - mean_squared_error: 0.0188\n",
      "Epoch 00141: val_loss did not improve from 0.03750\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 142/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00142: val_loss did not improve from 0.03750\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 143/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00143: val_loss did not improve from 0.03750\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 144/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00144: val_loss did not improve from 0.03750\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 145/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00145: val_loss did not improve from 0.03750\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 146/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Epoch 00146: val_loss did not improve from 0.03750\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 147/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0141 - mean_squared_error: 0.0141\n",
      "Epoch 00147: val_loss did not improve from 0.03750\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 148/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0170 - mean_squared_error: 0.0170\n",
      "Epoch 00148: val_loss did not improve from 0.03750\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 149/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00149: val_loss improved from 0.03750 to 0.03730, saving model to model.h5\n",
      "248/248 [==============================] - 0s 239us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 150/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0227 - mean_squared_error: 0.0227\n",
      "Epoch 00150: val_loss improved from 0.03730 to 0.03719, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 151/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00151: val_loss improved from 0.03719 to 0.03718, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 152/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00152: val_loss did not improve from 0.03718\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 153/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00153: val_loss did not improve from 0.03718\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 154/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00154: val_loss did not improve from 0.03718\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 155/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0123 - mean_squared_error: 0.0123\n",
      "Epoch 00155: val_loss did not improve from 0.03718\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 156/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00156: val_loss did not improve from 0.03718\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 157/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00157: val_loss did not improve from 0.03718\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 158/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0181 - mean_squared_error: 0.0181\n",
      "Epoch 00158: val_loss did not improve from 0.03718\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00159: val_loss did not improve from 0.03718\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 160/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00160: val_loss did not improve from 0.03718\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0383 - val_mean_squared_error: 0.0383\n",
      "Epoch 161/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00161: val_loss did not improve from 0.03718\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 162/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00162: val_loss did not improve from 0.03718\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 163/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00163: val_loss did not improve from 0.03718\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 164/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0379 - mean_squared_error: 0.0379\n",
      "Epoch 00164: val_loss did not improve from 0.03718\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 165/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Epoch 00165: val_loss did not improve from 0.03718\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 166/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0383 - mean_squared_error: 0.0383\n",
      "Epoch 00166: val_loss did not improve from 0.03718\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 167/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00167: val_loss did not improve from 0.03718\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 168/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00168: val_loss improved from 0.03718 to 0.03711, saving model to model.h5\n",
      "248/248 [==============================] - 0s 210us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 169/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00169: val_loss improved from 0.03711 to 0.03688, saving model to model.h5\n",
      "248/248 [==============================] - 0s 225us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 170/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00170: val_loss improved from 0.03688 to 0.03683, saving model to model.h5\n",
      "248/248 [==============================] - 0s 217us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 171/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00171: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 172/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00172: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 97us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 173/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00173: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 174/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00174: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 175/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00175: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 176/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00176: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 177/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0101 - mean_squared_error: 0.0101\n",
      "Epoch 00177: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 178/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Epoch 00178: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 179/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00179: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 180/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0152 - mean_squared_error: 0.0152\n",
      "Epoch 00180: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 181/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00181: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 182/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00182: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 183/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00183: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 184/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00184: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00185: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 186/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0392 - mean_squared_error: 0.0392\n",
      "Epoch 00186: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 187/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00187: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 188/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00188: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 189/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0151 - mean_squared_error: 0.0151\n",
      "Epoch 00189: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 190/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00190: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 191/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00191: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 192/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00192: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 193/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00193: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 194/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00194: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 195/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.0185\n",
      "Epoch 00195: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 196/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00196: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 197/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00197: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 198/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0177 - mean_squared_error: 0.0177\n",
      "Epoch 00198: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 199/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00199: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 200/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00200: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 201/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00201: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 202/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00202: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 203/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00203: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 204/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00204: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 205/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 00205: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 206/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00206: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 207/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00207: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 208/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0139 - mean_squared_error: 0.0139\n",
      "Epoch 00208: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 209/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00209: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 210/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00210: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 211/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.0172\n",
      "Epoch 00211: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 212/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00212: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 67us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 213/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00213: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 214/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00214: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 66us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 215/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00215: val_loss did not improve from 0.03683\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 216/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00216: val_loss improved from 0.03683 to 0.03666, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0367 - val_mean_squared_error: 0.0367\n",
      "Epoch 217/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Epoch 00217: val_loss improved from 0.03666 to 0.03634, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0363 - val_mean_squared_error: 0.0363\n",
      "Epoch 218/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00218: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n",
      "Epoch 219/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00219: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0363 - val_mean_squared_error: 0.0363\n",
      "Epoch 220/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00220: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 221/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00221: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 222/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00222: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 223/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00223: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 224/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00224: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 225/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00225: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 226/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00226: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 227/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00227: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 228/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0160 - mean_squared_error: 0.0160\n",
      "Epoch 00228: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 229/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00229: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 230/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0184 - mean_squared_error: 0.0184\n",
      "Epoch 00230: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 231/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 00231: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 232/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0151 - mean_squared_error: 0.0151\n",
      "Epoch 00232: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 233/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Epoch 00233: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 234/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00234: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 235/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0147 - mean_squared_error: 0.0147\n",
      "Epoch 00235: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 236/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00236: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00237: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 238/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00238: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 239/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0140 - mean_squared_error: 0.0140\n",
      "Epoch 00239: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 240/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00240: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 241/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Epoch 00241: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 242/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00242: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 243/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0148 - mean_squared_error: 0.0148\n",
      "Epoch 00243: val_loss did not improve from 0.03634\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 244/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00244: val_loss improved from 0.03634 to 0.03583, saving model to model.h5\n",
      "248/248 [==============================] - 0s 222us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0358 - val_mean_squared_error: 0.0358\n",
      "Epoch 245/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00245: val_loss improved from 0.03583 to 0.03547, saving model to model.h5\n",
      "248/248 [==============================] - 0s 190us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0355 - val_mean_squared_error: 0.0355\n",
      "Epoch 246/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00246: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0355 - val_mean_squared_error: 0.0355\n",
      "Epoch 247/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00247: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0360 - val_mean_squared_error: 0.0360\n",
      "Epoch 248/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00248: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 249/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0259 - mean_squared_error: 0.0259\n",
      "Epoch 00249: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 250/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0120 - mean_squared_error: 0.0120\n",
      "Epoch 00250: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 251/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00251: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 252/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00252: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 253/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00253: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 67us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 254/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00254: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 255/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00255: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 256/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00256: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 257/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0145 - mean_squared_error: 0.0145\n",
      "Epoch 00257: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 258/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00258: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 259/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00259: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 260/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00260: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 261/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00261: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0362 - val_mean_squared_error: 0.0362\n",
      "Epoch 262/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00262: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0360 - val_mean_squared_error: 0.0360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 263/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00263: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0363 - val_mean_squared_error: 0.0363\n",
      "Epoch 264/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0139 - mean_squared_error: 0.0139\n",
      "Epoch 00264: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 265/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00265: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 266/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00266: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0365 - val_mean_squared_error: 0.0365\n",
      "Epoch 267/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00267: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 268/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0115 - mean_squared_error: 0.0115\n",
      "Epoch 00268: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 269/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00269: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 270/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00270: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 271/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00271: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 272/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00272: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 273/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00273: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 274/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0131 - mean_squared_error: 0.0131\n",
      "Epoch 00274: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 89us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 275/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Epoch 00275: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 276/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0151 - mean_squared_error: 0.0151\n",
      "Epoch 00276: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 277/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00277: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 278/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 00278: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 279/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00279: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 280/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00280: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 281/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00281: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 282/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00282: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 283/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00283: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0363 - val_mean_squared_error: 0.0363\n",
      "Epoch 284/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00284: val_loss did not improve from 0.03547\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0356 - val_mean_squared_error: 0.0356\n",
      "Epoch 285/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00285: val_loss improved from 0.03547 to 0.03543, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0354 - val_mean_squared_error: 0.0354\n",
      "Epoch 286/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00286: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0362 - val_mean_squared_error: 0.0362\n",
      "Epoch 287/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0140 - mean_squared_error: 0.0140\n",
      "Epoch 00287: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 288/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00288: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00289: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 290/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0110 - mean_squared_error: 0.0110\n",
      "Epoch 00290: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 291/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0176 - mean_squared_error: 0.0176\n",
      "Epoch 00291: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 292/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0341 - mean_squared_error: 0.0341\n",
      "Epoch 00292: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 293/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0231 - mean_squared_error: 0.0231\n",
      "Epoch 00293: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 294/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 00294: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 295/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00295: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 296/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00296: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 297/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00297: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 298/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00298: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 299/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0160 - mean_squared_error: 0.0160\n",
      "Epoch 00299: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0383 - val_mean_squared_error: 0.0383\n",
      "Epoch 300/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0127 - mean_squared_error: 0.0127\n",
      "Epoch 00300: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 301/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00301: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 302/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0146 - mean_squared_error: 0.0146\n",
      "Epoch 00302: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 303/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0130 - mean_squared_error: 0.0130\n",
      "Epoch 00303: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 304/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00304: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 305/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00305: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 306/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00306: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 307/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00307: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 308/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0079 - mean_squared_error: 0.0079\n",
      "Epoch 00308: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 309/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00309: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 310/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00310: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 311/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00311: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 312/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.0172\n",
      "Epoch 00312: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 313/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00313: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 314/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00314: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 93us/sample - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 315/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0187 - mean_squared_error: 0.0187\n",
      "Epoch 00315: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 316/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.0172\n",
      "Epoch 00316: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 317/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0161 - mean_squared_error: 0.0161\n",
      "Epoch 00317: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 318/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00318: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 319/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 00319: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 320/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00320: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 321/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0148 - mean_squared_error: 0.0148\n",
      "Epoch 00321: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 322/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00322: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 323/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00323: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 324/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00324: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 325/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0116 - mean_squared_error: 0.0116\n",
      "Epoch 00325: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 326/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00326: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 327/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Epoch 00327: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 328/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00328: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 329/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00329: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 330/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0158 - mean_squared_error: 0.0158\n",
      "Epoch 00330: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 331/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0227 - mean_squared_error: 0.0227\n",
      "Epoch 00331: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 332/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00332: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 333/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00333: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 334/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 00334: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 335/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00335: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 336/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00336: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 337/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00337: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 338/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Epoch 00338: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 339/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00339: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 340/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.0172\n",
      "Epoch 00340: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 341/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00341: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 342/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0184 - mean_squared_error: 0.0184\n",
      "Epoch 00342: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 343/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0155 - mean_squared_error: 0.0155\n",
      "Epoch 00343: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 344/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00344: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 345/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00345: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 346/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 00346: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 347/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00347: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 348/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0117 - mean_squared_error: 0.0117\n",
      "Epoch 00348: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 349/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00349: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 350/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00350: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 351/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00351: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 352/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0157 - mean_squared_error: 0.0157\n",
      "Epoch 00352: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 353/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0184 - mean_squared_error: 0.0184\n",
      "Epoch 00353: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 354/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00354: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 355/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00355: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 356/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00356: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 357/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00357: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 358/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0187 - mean_squared_error: 0.0187\n",
      "Epoch 00358: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 359/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00359: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 360/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0174 - mean_squared_error: 0.0174\n",
      "Epoch 00360: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 361/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00361: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 362/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0145 - mean_squared_error: 0.0145\n",
      "Epoch 00362: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 363/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00363: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 364/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00364: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 365/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00365: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 366/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0122 - mean_squared_error: 0.0122\n",
      "Epoch 00366: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 367/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00367: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 368/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00368: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 369/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.0172\n",
      "Epoch 00369: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 370/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00370: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 371/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0176 - mean_squared_error: 0.0176\n",
      "Epoch 00371: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 372/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0231 - mean_squared_error: 0.0231\n",
      "Epoch 00372: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 373/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00373: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 374/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0153 - mean_squared_error: 0.0153\n",
      "Epoch 00374: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 375/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00375: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 376/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0151 - mean_squared_error: 0.0151\n",
      "Epoch 00376: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 377/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00377: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 378/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00378: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 379/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00379: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 380/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00380: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 381/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00381: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 382/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00382: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 383/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0127 - mean_squared_error: 0.0127\n",
      "Epoch 00383: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 384/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0119 - mean_squared_error: 0.0119\n",
      "Epoch 00384: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 385/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0160 - mean_squared_error: 0.0160\n",
      "Epoch 00385: val_loss did not improve from 0.03543\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Running trial 7\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 1s - loss: 1.1448 - mean_squared_error: 1.1448\n",
      "Epoch 00001: val_loss improved from inf to 0.68117, saving model to model.h5\n",
      "248/248 [==============================] - 5s 19ms/sample - loss: 1.0429 - mean_squared_error: 1.0429 - val_loss: 0.6812 - val_mean_squared_error: 0.6812\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9375 - mean_squared_error: 0.9375\n",
      "Epoch 00002: val_loss improved from 0.68117 to 0.65485, saving model to model.h5\n",
      "248/248 [==============================] - 0s 206us/sample - loss: 0.9713 - mean_squared_error: 0.9713 - val_loss: 0.6548 - val_mean_squared_error: 0.6548\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9930 - mean_squared_error: 0.9930\n",
      "Epoch 00003: val_loss improved from 0.65485 to 0.63001, saving model to model.h5\n",
      "248/248 [==============================] - 0s 206us/sample - loss: 0.9041 - mean_squared_error: 0.9041 - val_loss: 0.6300 - val_mean_squared_error: 0.6300\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8789 - mean_squared_error: 0.8789\n",
      "Epoch 00004: val_loss improved from 0.63001 to 0.60349, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.8394 - mean_squared_error: 0.8394 - val_loss: 0.6035 - val_mean_squared_error: 0.6035\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7976 - mean_squared_error: 0.7976\n",
      "Epoch 00005: val_loss improved from 0.60349 to 0.57657, saving model to model.h5\n",
      "248/248 [==============================] - 0s 210us/sample - loss: 0.7633 - mean_squared_error: 0.7633 - val_loss: 0.5766 - val_mean_squared_error: 0.5766\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6692 - mean_squared_error: 0.6692\n",
      "Epoch 00006: val_loss improved from 0.57657 to 0.54822, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.6927 - mean_squared_error: 0.6927 - val_loss: 0.5482 - val_mean_squared_error: 0.5482\n",
      "Epoch 7/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6203 - mean_squared_error: 0.6203\n",
      "Epoch 00007: val_loss improved from 0.54822 to 0.51812, saving model to model.h5\n",
      "248/248 [==============================] - 0s 210us/sample - loss: 0.6036 - mean_squared_error: 0.6036 - val_loss: 0.5181 - val_mean_squared_error: 0.5181\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5656 - mean_squared_error: 0.5656\n",
      "Epoch 00008: val_loss improved from 0.51812 to 0.48486, saving model to model.h5\n",
      "248/248 [==============================] - 0s 215us/sample - loss: 0.5376 - mean_squared_error: 0.5376 - val_loss: 0.4849 - val_mean_squared_error: 0.4849\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4994 - mean_squared_error: 0.4994\n",
      "Epoch 00009: val_loss improved from 0.48486 to 0.44962, saving model to model.h5\n",
      "248/248 [==============================] - 0s 215us/sample - loss: 0.4571 - mean_squared_error: 0.4571 - val_loss: 0.4496 - val_mean_squared_error: 0.4496\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4538 - mean_squared_error: 0.4538\n",
      "Epoch 00010: val_loss improved from 0.44962 to 0.41114, saving model to model.h5\n",
      "248/248 [==============================] - 0s 210us/sample - loss: 0.3837 - mean_squared_error: 0.3837 - val_loss: 0.4111 - val_mean_squared_error: 0.4111\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3144 - mean_squared_error: 0.3144\n",
      "Epoch 00011: val_loss improved from 0.41114 to 0.37116, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.3112 - mean_squared_error: 0.3112 - val_loss: 0.3712 - val_mean_squared_error: 0.3712\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2619 - mean_squared_error: 0.2619\n",
      "Epoch 00012: val_loss improved from 0.37116 to 0.32959, saving model to model.h5\n",
      "248/248 [==============================] - 0s 210us/sample - loss: 0.2608 - mean_squared_error: 0.2608 - val_loss: 0.3296 - val_mean_squared_error: 0.3296\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1960 - mean_squared_error: 0.1960\n",
      "Epoch 00013: val_loss improved from 0.32959 to 0.28861, saving model to model.h5\n",
      "248/248 [==============================] - 0s 219us/sample - loss: 0.2107 - mean_squared_error: 0.2107 - val_loss: 0.2886 - val_mean_squared_error: 0.2886\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1716 - mean_squared_error: 0.1716\n",
      "Epoch 00014: val_loss improved from 0.28861 to 0.24930, saving model to model.h5\n",
      "248/248 [==============================] - 0s 209us/sample - loss: 0.1817 - mean_squared_error: 0.1817 - val_loss: 0.2493 - val_mean_squared_error: 0.2493\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1411 - mean_squared_error: 0.1411\n",
      "Epoch 00015: val_loss improved from 0.24930 to 0.21390, saving model to model.h5\n",
      "248/248 [==============================] - 0s 209us/sample - loss: 0.1629 - mean_squared_error: 0.1629 - val_loss: 0.2139 - val_mean_squared_error: 0.2139\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1535 - mean_squared_error: 0.1535\n",
      "Epoch 00016: val_loss improved from 0.21390 to 0.18305, saving model to model.h5\n",
      "248/248 [==============================] - 0s 222us/sample - loss: 0.1412 - mean_squared_error: 0.1412 - val_loss: 0.1830 - val_mean_squared_error: 0.1830\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1144 - mean_squared_error: 0.1144\n",
      "Epoch 00017: val_loss improved from 0.18305 to 0.15681, saving model to model.h5\n",
      "248/248 [==============================] - 0s 247us/sample - loss: 0.1186 - mean_squared_error: 0.1186 - val_loss: 0.1568 - val_mean_squared_error: 0.1568\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1007 - mean_squared_error: 0.1007\n",
      "Epoch 00018: val_loss improved from 0.15681 to 0.13414, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.1018 - mean_squared_error: 0.1018 - val_loss: 0.1341 - val_mean_squared_error: 0.1341\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0868 - mean_squared_error: 0.0868\n",
      "Epoch 00019: val_loss improved from 0.13414 to 0.11556, saving model to model.h5\n",
      "248/248 [==============================] - 0s 207us/sample - loss: 0.0828 - mean_squared_error: 0.0828 - val_loss: 0.1156 - val_mean_squared_error: 0.1156\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0794 - mean_squared_error: 0.0794\n",
      "Epoch 00020: val_loss improved from 0.11556 to 0.10033, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.0709 - mean_squared_error: 0.0709 - val_loss: 0.1003 - val_mean_squared_error: 0.1003\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0525 - mean_squared_error: 0.0525\n",
      "Epoch 00021: val_loss improved from 0.10033 to 0.08763, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.0613 - mean_squared_error: 0.0613 - val_loss: 0.0876 - val_mean_squared_error: 0.0876\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0578 - mean_squared_error: 0.0578\n",
      "Epoch 00022: val_loss improved from 0.08763 to 0.07744, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.0561 - mean_squared_error: 0.0561 - val_loss: 0.0774 - val_mean_squared_error: 0.0774\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0459 - mean_squared_error: 0.0459\n",
      "Epoch 00023: val_loss improved from 0.07744 to 0.06939, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0501 - mean_squared_error: 0.0501 - val_loss: 0.0694 - val_mean_squared_error: 0.0694\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0496 - mean_squared_error: 0.0496\n",
      "Epoch 00024: val_loss improved from 0.06939 to 0.06327, saving model to model.h5\n",
      "248/248 [==============================] - 0s 212us/sample - loss: 0.0491 - mean_squared_error: 0.0491 - val_loss: 0.0633 - val_mean_squared_error: 0.0633\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0527 - mean_squared_error: 0.0527\n",
      "Epoch 00025: val_loss improved from 0.06327 to 0.05834, saving model to model.h5\n",
      "248/248 [==============================] - 0s 199us/sample - loss: 0.0529 - mean_squared_error: 0.0529 - val_loss: 0.0583 - val_mean_squared_error: 0.0583\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0766 - mean_squared_error: 0.0766\n",
      "Epoch 00026: val_loss improved from 0.05834 to 0.05486, saving model to model.h5\n",
      "248/248 [==============================] - 0s 196us/sample - loss: 0.0504 - mean_squared_error: 0.0504 - val_loss: 0.0549 - val_mean_squared_error: 0.0549\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0602 - mean_squared_error: 0.0602\n",
      "Epoch 00027: val_loss improved from 0.05486 to 0.05264, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0509 - mean_squared_error: 0.0509 - val_loss: 0.0526 - val_mean_squared_error: 0.0526\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0473 - mean_squared_error: 0.0473\n",
      "Epoch 00028: val_loss improved from 0.05264 to 0.05118, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0431 - mean_squared_error: 0.0431 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0370 - mean_squared_error: 0.0370\n",
      "Epoch 00029: val_loss improved from 0.05118 to 0.05016, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0502 - val_mean_squared_error: 0.0502\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0553 - mean_squared_error: 0.0553\n",
      "Epoch 00030: val_loss improved from 0.05016 to 0.04960, saving model to model.h5\n",
      "248/248 [==============================] - 0s 194us/sample - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0496 - val_mean_squared_error: 0.0496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0423 - mean_squared_error: 0.0423\n",
      "Epoch 00031: val_loss improved from 0.04960 to 0.04904, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0468 - mean_squared_error: 0.0468 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0386 - mean_squared_error: 0.0386\n",
      "Epoch 00032: val_loss improved from 0.04904 to 0.04846, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0453 - mean_squared_error: 0.0453\n",
      "Epoch 00033: val_loss improved from 0.04846 to 0.04827, saving model to model.h5\n",
      "248/248 [==============================] - 0s 203us/sample - loss: 0.0431 - mean_squared_error: 0.0431 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0513 - mean_squared_error: 0.0513\n",
      "Epoch 00034: val_loss improved from 0.04827 to 0.04798, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.0447 - mean_squared_error: 0.0447 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0429 - mean_squared_error: 0.0429\n",
      "Epoch 00035: val_loss improved from 0.04798 to 0.04767, saving model to model.h5\n",
      "248/248 [==============================] - 0s 203us/sample - loss: 0.0398 - mean_squared_error: 0.0398 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0379 - mean_squared_error: 0.0379\n",
      "Epoch 00036: val_loss improved from 0.04767 to 0.04724, saving model to model.h5\n",
      "248/248 [==============================] - 0s 225us/sample - loss: 0.0427 - mean_squared_error: 0.0427 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0377 - mean_squared_error: 0.0377\n",
      "Epoch 00037: val_loss improved from 0.04724 to 0.04698, saving model to model.h5\n",
      "248/248 [==============================] - 0s 194us/sample - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Epoch 00038: val_loss improved from 0.04698 to 0.04695, saving model to model.h5\n",
      "248/248 [==============================] - 0s 209us/sample - loss: 0.0371 - mean_squared_error: 0.0371 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0346 - mean_squared_error: 0.0346\n",
      "Epoch 00039: val_loss improved from 0.04695 to 0.04663, saving model to model.h5\n",
      "248/248 [==============================] - 0s 204us/sample - loss: 0.0363 - mean_squared_error: 0.0363 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00040: val_loss improved from 0.04663 to 0.04620, saving model to model.h5\n",
      "248/248 [==============================] - 0s 212us/sample - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0451 - mean_squared_error: 0.0451\n",
      "Epoch 00041: val_loss improved from 0.04620 to 0.04601, saving model to model.h5\n",
      "248/248 [==============================] - 0s 212us/sample - loss: 0.0386 - mean_squared_error: 0.0386 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 00042: val_loss improved from 0.04601 to 0.04575, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0353 - mean_squared_error: 0.0353\n",
      "Epoch 00043: val_loss improved from 0.04575 to 0.04539, saving model to model.h5\n",
      "248/248 [==============================] - 0s 204us/sample - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00044: val_loss improved from 0.04539 to 0.04514, saving model to model.h5\n",
      "248/248 [==============================] - 0s 207us/sample - loss: 0.0319 - mean_squared_error: 0.0319 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00045: val_loss improved from 0.04514 to 0.04488, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.0365 - mean_squared_error: 0.0365 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0424 - mean_squared_error: 0.0424\n",
      "Epoch 00046: val_loss improved from 0.04488 to 0.04486, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0449 - mean_squared_error: 0.0449\n",
      "Epoch 00047: val_loss improved from 0.04486 to 0.04484, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0387 - mean_squared_error: 0.0387 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0346 - mean_squared_error: 0.0346\n",
      "Epoch 00048: val_loss did not improve from 0.04484\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0374 - mean_squared_error: 0.0374 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0390 - mean_squared_error: 0.0390\n",
      "Epoch 00049: val_loss did not improve from 0.04484\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0379 - mean_squared_error: 0.0379 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00050: val_loss did not improve from 0.04484\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0407 - mean_squared_error: 0.0407\n",
      "Epoch 00051: val_loss did not improve from 0.04484\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00052: val_loss did not improve from 0.04484\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0463 - val_mean_squared_error: 0.0463\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0508 - mean_squared_error: 0.0508\n",
      "Epoch 00053: val_loss did not improve from 0.04484\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0456 - mean_squared_error: 0.0456\n",
      "Epoch 00054: val_loss did not improve from 0.04484\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0227 - mean_squared_error: 0.0227\n",
      "Epoch 00055: val_loss did not improve from 0.04484\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0366 - mean_squared_error: 0.0366 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0377 - mean_squared_error: 0.0377\n",
      "Epoch 00056: val_loss did not improve from 0.04484\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0370 - mean_squared_error: 0.0370 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0477 - mean_squared_error: 0.0477\n",
      "Epoch 00057: val_loss did not improve from 0.04484\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00058: val_loss did not improve from 0.04484\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00059: val_loss did not improve from 0.04484\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Epoch 00060: val_loss did not improve from 0.04484\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 00061: val_loss did not improve from 0.04484\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0407 - mean_squared_error: 0.0407\n",
      "Epoch 00062: val_loss did not improve from 0.04484\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0397 - mean_squared_error: 0.0397\n",
      "Epoch 00063: val_loss did not improve from 0.04484\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00064: val_loss improved from 0.04484 to 0.04460, saving model to model.h5\n",
      "248/248 [==============================] - 0s 217us/sample - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0339 - mean_squared_error: 0.0339\n",
      "Epoch 00065: val_loss improved from 0.04460 to 0.04452, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0341 - mean_squared_error: 0.0341\n",
      "Epoch 00066: val_loss improved from 0.04452 to 0.04436, saving model to model.h5\n",
      "248/248 [==============================] - 0s 199us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0379 - mean_squared_error: 0.0379\n",
      "Epoch 00067: val_loss did not improve from 0.04436\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00068: val_loss did not improve from 0.04436\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0469 - mean_squared_error: 0.0469\n",
      "Epoch 00069: val_loss did not improve from 0.04436\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00070: val_loss did not improve from 0.04436\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00071: val_loss did not improve from 0.04436\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0323 - mean_squared_error: 0.0323 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00072: val_loss did not improve from 0.04436\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0440 - mean_squared_error: 0.0440\n",
      "Epoch 00073: val_loss did not improve from 0.04436\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00074: val_loss did not improve from 0.04436\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Epoch 00075: val_loss did not improve from 0.04436\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00076: val_loss did not improve from 0.04436\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0330 - mean_squared_error: 0.0330 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0371 - mean_squared_error: 0.0371\n",
      "Epoch 00077: val_loss improved from 0.04436 to 0.04407, saving model to model.h5\n",
      "248/248 [==============================] - 0s 219us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00078: val_loss improved from 0.04407 to 0.04342, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Epoch 00079: val_loss improved from 0.04342 to 0.04331, saving model to model.h5\n",
      "248/248 [==============================] - 0s 225us/sample - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00080: val_loss did not improve from 0.04331\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00081: val_loss did not improve from 0.04331\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0339 - mean_squared_error: 0.0339\n",
      "Epoch 00082: val_loss did not improve from 0.04331\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00083: val_loss did not improve from 0.04331\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00084: val_loss did not improve from 0.04331\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00085: val_loss did not improve from 0.04331\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0333 - mean_squared_error: 0.0333 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0307 - mean_squared_error: 0.0307\n",
      "Epoch 00086: val_loss did not improve from 0.04331\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00087: val_loss did not improve from 0.04331\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00088: val_loss did not improve from 0.04331\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0331 - mean_squared_error: 0.0331 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00089: val_loss improved from 0.04331 to 0.04310, saving model to model.h5\n",
      "248/248 [==============================] - 0s 266us/sample - loss: 0.0308 - mean_squared_error: 0.0308 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00090: val_loss improved from 0.04310 to 0.04302, saving model to model.h5\n",
      "248/248 [==============================] - 0s 190us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0418 - mean_squared_error: 0.0418\n",
      "Epoch 00091: val_loss improved from 0.04302 to 0.04297, saving model to model.h5\n",
      "248/248 [==============================] - 0s 188us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Epoch 00092: val_loss improved from 0.04297 to 0.04296, saving model to model.h5\n",
      "248/248 [==============================] - 0s 228us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00093: val_loss improved from 0.04296 to 0.04240, saving model to model.h5\n",
      "248/248 [==============================] - 0s 199us/sample - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00094: val_loss improved from 0.04240 to 0.04207, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00095: val_loss improved from 0.04207 to 0.04183, saving model to model.h5\n",
      "248/248 [==============================] - 0s 199us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0362 - mean_squared_error: 0.0362\n",
      "Epoch 00096: val_loss improved from 0.04183 to 0.04155, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0376 - mean_squared_error: 0.0376\n",
      "Epoch 00097: val_loss improved from 0.04155 to 0.04142, saving model to model.h5\n",
      "248/248 [==============================] - 0s 192us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00098: val_loss improved from 0.04142 to 0.04094, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00099: val_loss improved from 0.04094 to 0.04074, saving model to model.h5\n",
      "248/248 [==============================] - 0s 209us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Epoch 00100: val_loss did not improve from 0.04074\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0368 - mean_squared_error: 0.0368\n",
      "Epoch 00101: val_loss did not improve from 0.04074\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00102: val_loss did not improve from 0.04074\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00103: val_loss did not improve from 0.04074\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00104: val_loss did not improve from 0.04074\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00105: val_loss did not improve from 0.04074\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Epoch 00106: val_loss did not improve from 0.04074\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00107: val_loss did not improve from 0.04074\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00108: val_loss did not improve from 0.04074\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00109: val_loss improved from 0.04074 to 0.04032, saving model to model.h5\n",
      "248/248 [==============================] - 0s 217us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00110: val_loss improved from 0.04032 to 0.04003, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00111: val_loss improved from 0.04003 to 0.03990, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00112: val_loss did not improve from 0.03990\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00113: val_loss did not improve from 0.03990\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00114: val_loss did not improve from 0.03990\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0322 - mean_squared_error: 0.0322 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00115: val_loss did not improve from 0.03990\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0411 - mean_squared_error: 0.0411\n",
      "Epoch 00116: val_loss did not improve from 0.03990\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00117: val_loss did not improve from 0.03990\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00118: val_loss did not improve from 0.03990\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0344 - mean_squared_error: 0.0344\n",
      "Epoch 00119: val_loss did not improve from 0.03990\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00120: val_loss did not improve from 0.03990\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00121: val_loss did not improve from 0.03990\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00122: val_loss improved from 0.03990 to 0.03988, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0337 - mean_squared_error: 0.0337\n",
      "Epoch 00123: val_loss improved from 0.03988 to 0.03946, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00124: val_loss improved from 0.03946 to 0.03918, saving model to model.h5\n",
      "248/248 [==============================] - 0s 196us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00125: val_loss improved from 0.03918 to 0.03913, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00126: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00127: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0403 - mean_squared_error: 0.0403\n",
      "Epoch 00128: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Epoch 00129: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 130/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Epoch 00130: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00131: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 132/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00132: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 133/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0308 - mean_squared_error: 0.0308\n",
      "Epoch 00133: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 134/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00134: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 135/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00135: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 136/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0361 - mean_squared_error: 0.0361\n",
      "Epoch 00136: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 137/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00137: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 138/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00138: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 139/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00139: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 140/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00140: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 141/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00141: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 142/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00142: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 143/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00143: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 144/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00144: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 145/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00145: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 146/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00146: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 147/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00147: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 148/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00148: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 89us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 149/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00149: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 92us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 150/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00150: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 105us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 151/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00151: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 97us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 152/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00152: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 97us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 153/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00153: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 101us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 154/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00154: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 101us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 155/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00155: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 156/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Epoch 00156: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 157/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00157: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 158/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0372 - mean_squared_error: 0.0372\n",
      "Epoch 00158: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 159/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00159: val_loss did not improve from 0.03913\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 160/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00160: val_loss improved from 0.03913 to 0.03906, saving model to model.h5\n",
      "248/248 [==============================] - 0s 221us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 161/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00161: val_loss improved from 0.03906 to 0.03831, saving model to model.h5\n",
      "248/248 [==============================] - 0s 253us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0383 - val_mean_squared_error: 0.0383\n",
      "Epoch 162/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Epoch 00162: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 163/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00163: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 164/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00164: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 165/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0259 - mean_squared_error: 0.0259\n",
      "Epoch 00165: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 166/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00166: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 167/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00167: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 168/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00168: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 169/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00169: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 170/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00170: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 171/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00171: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 172/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00172: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 173/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00173: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 174/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0397 - mean_squared_error: 0.0397\n",
      "Epoch 00174: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 175/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0366 - mean_squared_error: 0.0366\n",
      "Epoch 00175: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 176/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0177 - mean_squared_error: 0.0177\n",
      "Epoch 00176: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 177/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00177: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 178/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00178: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 179/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00179: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 180/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00180: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 181/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0341 - mean_squared_error: 0.0341\n",
      "Epoch 00181: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 182/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00182: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 92us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 183/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0385 - mean_squared_error: 0.0385\n",
      "Epoch 00183: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00184: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 185/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00185: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 186/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0154 - mean_squared_error: 0.0154\n",
      "Epoch 00186: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 187/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00187: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 188/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00188: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 189/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00189: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 190/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0259 - mean_squared_error: 0.0259\n",
      "Epoch 00190: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 92us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 191/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00191: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 101us/sample - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 192/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0177 - mean_squared_error: 0.0177\n",
      "Epoch 00192: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 193/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00193: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 97us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 194/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00194: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 96us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 195/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0153 - mean_squared_error: 0.0153\n",
      "Epoch 00195: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 86us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 196/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.0292\n",
      "Epoch 00196: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 197/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00197: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 198/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00198: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 199/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00199: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 200/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00200: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 201/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00201: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 202/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00202: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 203/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0152 - mean_squared_error: 0.0152\n",
      "Epoch 00203: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 204/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00204: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 205/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0177 - mean_squared_error: 0.0177\n",
      "Epoch 00205: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 87us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 206/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00206: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 207/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00207: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 89us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 208/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00208: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 209/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00209: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 210/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00210: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 89us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 211/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0326 - mean_squared_error: 0.0326\n",
      "Epoch 00211: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 212/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00212: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 213/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00213: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 214/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00214: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 215/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00215: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 216/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00216: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 217/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0392 - mean_squared_error: 0.0392\n",
      "Epoch 00217: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 218/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00218: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 219/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00219: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 220/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0108 - mean_squared_error: 0.0108\n",
      "Epoch 00220: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 221/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0155 - mean_squared_error: 0.0155\n",
      "Epoch 00221: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 222/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0334 - mean_squared_error: 0.0334\n",
      "Epoch 00222: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 223/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Epoch 00223: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 224/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00224: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 225/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00225: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 226/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0171 - mean_squared_error: 0.0171\n",
      "Epoch 00226: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 227/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0110 - mean_squared_error: 0.0110\n",
      "Epoch 00227: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 228/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0170 - mean_squared_error: 0.0170\n",
      "Epoch 00228: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 229/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00229: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 230/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00230: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 231/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 00231: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 232/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0188 - mean_squared_error: 0.0188\n",
      "Epoch 00232: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 233/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00233: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 234/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00234: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 235/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00235: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 236/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00236: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 237/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00237: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 87us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 238/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00238: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 96us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 239/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00239: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 240/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00240: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 241/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00241: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 242/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00242: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 243/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00243: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 83us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 244/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00244: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 245/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00245: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 246/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0149 - mean_squared_error: 0.0149\n",
      "Epoch 00246: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 247/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00247: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 248/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00248: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 249/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00249: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 250/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00250: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 251/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00251: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 96us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 252/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00252: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 253/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00253: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 254/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00254: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 255/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00255: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 256/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.0185\n",
      "Epoch 00256: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 89us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 257/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00257: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 258/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00258: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 259/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00259: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 260/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00260: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 261/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0113 - mean_squared_error: 0.0113\n",
      "Epoch 00261: val_loss did not improve from 0.03831\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Running trial 8\n",
      "(310, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 1s - loss: 1.0850 - mean_squared_error: 1.0850\n",
      "Epoch 00001: val_loss improved from inf to 0.73644, saving model to model.h5\n",
      "248/248 [==============================] - 5s 20ms/sample - loss: 1.1174 - mean_squared_error: 1.1174 - val_loss: 0.7364 - val_mean_squared_error: 0.7364\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.0053 - mean_squared_error: 1.0053\n",
      "Epoch 00002: val_loss improved from 0.73644 to 0.71643, saving model to model.h5\n",
      "248/248 [==============================] - 0s 216us/sample - loss: 1.0443 - mean_squared_error: 1.0443 - val_loss: 0.7164 - val_mean_squared_error: 0.7164\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7941 - mean_squared_error: 0.7941\n",
      "Epoch 00003: val_loss improved from 0.71643 to 0.69719, saving model to model.h5\n",
      "248/248 [==============================] - 0s 221us/sample - loss: 0.9809 - mean_squared_error: 0.9809 - val_loss: 0.6972 - val_mean_squared_error: 0.6972\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.0213 - mean_squared_error: 1.0213\n",
      "Epoch 00004: val_loss improved from 0.69719 to 0.67660, saving model to model.h5\n",
      "248/248 [==============================] - 0s 229us/sample - loss: 0.9173 - mean_squared_error: 0.9173 - val_loss: 0.6766 - val_mean_squared_error: 0.6766\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9532 - mean_squared_error: 0.9532\n",
      "Epoch 00005: val_loss improved from 0.67660 to 0.65501, saving model to model.h5\n",
      "248/248 [==============================] - 0s 233us/sample - loss: 0.8539 - mean_squared_error: 0.8539 - val_loss: 0.6550 - val_mean_squared_error: 0.6550\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8247 - mean_squared_error: 0.8247\n",
      "Epoch 00006: val_loss improved from 0.65501 to 0.63160, saving model to model.h5\n",
      "248/248 [==============================] - 0s 229us/sample - loss: 0.7800 - mean_squared_error: 0.7800 - val_loss: 0.6316 - val_mean_squared_error: 0.6316\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6996 - mean_squared_error: 0.6996\n",
      "Epoch 00007: val_loss improved from 0.63160 to 0.60553, saving model to model.h5\n",
      "248/248 [==============================] - 0s 210us/sample - loss: 0.7054 - mean_squared_error: 0.7054 - val_loss: 0.6055 - val_mean_squared_error: 0.6055\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6521 - mean_squared_error: 0.6521\n",
      "Epoch 00008: val_loss improved from 0.60553 to 0.57653, saving model to model.h5\n",
      "248/248 [==============================] - 0s 220us/sample - loss: 0.6236 - mean_squared_error: 0.6236 - val_loss: 0.5765 - val_mean_squared_error: 0.5765\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5262 - mean_squared_error: 0.5262\n",
      "Epoch 00009: val_loss improved from 0.57653 to 0.54437, saving model to model.h5\n",
      "248/248 [==============================] - 0s 246us/sample - loss: 0.5477 - mean_squared_error: 0.5477 - val_loss: 0.5444 - val_mean_squared_error: 0.5444\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4658 - mean_squared_error: 0.4658\n",
      "Epoch 00010: val_loss improved from 0.54437 to 0.50905, saving model to model.h5\n",
      "248/248 [==============================] - 0s 257us/sample - loss: 0.4535 - mean_squared_error: 0.4535 - val_loss: 0.5091 - val_mean_squared_error: 0.5091\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4627 - mean_squared_error: 0.4627\n",
      "Epoch 00011: val_loss improved from 0.50905 to 0.46984, saving model to model.h5\n",
      "248/248 [==============================] - 0s 241us/sample - loss: 0.3799 - mean_squared_error: 0.3799 - val_loss: 0.4698 - val_mean_squared_error: 0.4698\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2716 - mean_squared_error: 0.2716\n",
      "Epoch 00012: val_loss improved from 0.46984 to 0.42794, saving model to model.h5\n",
      "248/248 [==============================] - 0s 261us/sample - loss: 0.2994 - mean_squared_error: 0.2994 - val_loss: 0.4279 - val_mean_squared_error: 0.4279\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2700 - mean_squared_error: 0.2700\n",
      "Epoch 00013: val_loss improved from 0.42794 to 0.38387, saving model to model.h5\n",
      "248/248 [==============================] - 0s 239us/sample - loss: 0.2378 - mean_squared_error: 0.2378 - val_loss: 0.3839 - val_mean_squared_error: 0.3839\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1856 - mean_squared_error: 0.1856\n",
      "Epoch 00014: val_loss improved from 0.38387 to 0.33957, saving model to model.h5\n",
      "248/248 [==============================] - 0s 220us/sample - loss: 0.1834 - mean_squared_error: 0.1834 - val_loss: 0.3396 - val_mean_squared_error: 0.3396\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1548 - mean_squared_error: 0.1548\n",
      "Epoch 00015: val_loss improved from 0.33957 to 0.29687, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.1503 - mean_squared_error: 0.1503 - val_loss: 0.2969 - val_mean_squared_error: 0.2969\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1428 - mean_squared_error: 0.1428\n",
      "Epoch 00016: val_loss improved from 0.29687 to 0.25820, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.1347 - mean_squared_error: 0.1347 - val_loss: 0.2582 - val_mean_squared_error: 0.2582\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1246 - mean_squared_error: 0.1246\n",
      "Epoch 00017: val_loss improved from 0.25820 to 0.22437, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.1229 - mean_squared_error: 0.1229 - val_loss: 0.2244 - val_mean_squared_error: 0.2244\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1223 - mean_squared_error: 0.1223\n",
      "Epoch 00018: val_loss improved from 0.22437 to 0.19518, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.1144 - mean_squared_error: 0.1144 - val_loss: 0.1952 - val_mean_squared_error: 0.1952\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1072 - mean_squared_error: 0.1072\n",
      "Epoch 00019: val_loss improved from 0.19518 to 0.17019, saving model to model.h5\n",
      "248/248 [==============================] - 0s 208us/sample - loss: 0.1058 - mean_squared_error: 0.1058 - val_loss: 0.1702 - val_mean_squared_error: 0.1702\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0763 - mean_squared_error: 0.0763\n",
      "Epoch 00020: val_loss improved from 0.17019 to 0.14906, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.0830 - mean_squared_error: 0.0830 - val_loss: 0.1491 - val_mean_squared_error: 0.1491\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0828 - mean_squared_error: 0.0828\n",
      "Epoch 00021: val_loss improved from 0.14906 to 0.13053, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.0733 - mean_squared_error: 0.0733 - val_loss: 0.1305 - val_mean_squared_error: 0.1305\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0586 - mean_squared_error: 0.0586\n",
      "Epoch 00022: val_loss improved from 0.13053 to 0.11455, saving model to model.h5\n",
      "248/248 [==============================] - 0s 194us/sample - loss: 0.0660 - mean_squared_error: 0.0660 - val_loss: 0.1145 - val_mean_squared_error: 0.1145\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0482 - mean_squared_error: 0.0482\n",
      "Epoch 00023: val_loss improved from 0.11455 to 0.10039, saving model to model.h5\n",
      "248/248 [==============================] - 0s 204us/sample - loss: 0.0594 - mean_squared_error: 0.0594 - val_loss: 0.1004 - val_mean_squared_error: 0.1004\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0632 - mean_squared_error: 0.0632\n",
      "Epoch 00024: val_loss improved from 0.10039 to 0.08845, saving model to model.h5\n",
      "248/248 [==============================] - 0s 219us/sample - loss: 0.0575 - mean_squared_error: 0.0575 - val_loss: 0.0884 - val_mean_squared_error: 0.0884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0542 - mean_squared_error: 0.0542\n",
      "Epoch 00025: val_loss improved from 0.08845 to 0.07859, saving model to model.h5\n",
      "248/248 [==============================] - 0s 215us/sample - loss: 0.0540 - mean_squared_error: 0.0540 - val_loss: 0.0786 - val_mean_squared_error: 0.0786\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0575 - mean_squared_error: 0.0575\n",
      "Epoch 00026: val_loss improved from 0.07859 to 0.07026, saving model to model.h5\n",
      "248/248 [==============================] - 0s 225us/sample - loss: 0.0556 - mean_squared_error: 0.0556 - val_loss: 0.0703 - val_mean_squared_error: 0.0703\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0440 - mean_squared_error: 0.0440\n",
      "Epoch 00027: val_loss improved from 0.07026 to 0.06372, saving model to model.h5\n",
      "248/248 [==============================] - 0s 217us/sample - loss: 0.0474 - mean_squared_error: 0.0474 - val_loss: 0.0637 - val_mean_squared_error: 0.0637\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0457 - mean_squared_error: 0.0457\n",
      "Epoch 00028: val_loss improved from 0.06372 to 0.05881, saving model to model.h5\n",
      "248/248 [==============================] - 0s 212us/sample - loss: 0.0459 - mean_squared_error: 0.0459 - val_loss: 0.0588 - val_mean_squared_error: 0.0588\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0406 - mean_squared_error: 0.0406\n",
      "Epoch 00029: val_loss improved from 0.05881 to 0.05506, saving model to model.h5\n",
      "248/248 [==============================] - 0s 220us/sample - loss: 0.0449 - mean_squared_error: 0.0449 - val_loss: 0.0551 - val_mean_squared_error: 0.0551\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0442 - mean_squared_error: 0.0442\n",
      "Epoch 00030: val_loss improved from 0.05506 to 0.05232, saving model to model.h5\n",
      "248/248 [==============================] - 0s 211us/sample - loss: 0.0447 - mean_squared_error: 0.0447 - val_loss: 0.0523 - val_mean_squared_error: 0.0523\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0444 - mean_squared_error: 0.0444\n",
      "Epoch 00031: val_loss improved from 0.05232 to 0.05062, saving model to model.h5\n",
      "248/248 [==============================] - 0s 214us/sample - loss: 0.0465 - mean_squared_error: 0.0465 - val_loss: 0.0506 - val_mean_squared_error: 0.0506\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0501 - mean_squared_error: 0.0501\n",
      "Epoch 00032: val_loss improved from 0.05062 to 0.04977, saving model to model.h5\n",
      "248/248 [==============================] - 0s 207us/sample - loss: 0.0428 - mean_squared_error: 0.0428 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0394 - mean_squared_error: 0.0394\n",
      "Epoch 00033: val_loss improved from 0.04977 to 0.04932, saving model to model.h5\n",
      "248/248 [==============================] - 0s 221us/sample - loss: 0.0427 - mean_squared_error: 0.0427 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00034: val_loss improved from 0.04932 to 0.04921, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.0454 - mean_squared_error: 0.0454 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00035: val_loss did not improve from 0.04921\n",
      "248/248 [==============================] - 0s 92us/sample - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0386 - mean_squared_error: 0.0386\n",
      "Epoch 00036: val_loss did not improve from 0.04921\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0439 - mean_squared_error: 0.0439 - val_loss: 0.0499 - val_mean_squared_error: 0.0499\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00037: val_loss did not improve from 0.04921\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0406 - mean_squared_error: 0.0406 - val_loss: 0.0505 - val_mean_squared_error: 0.0505\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0503 - mean_squared_error: 0.0503\n",
      "Epoch 00038: val_loss did not improve from 0.04921\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0480 - mean_squared_error: 0.0480 - val_loss: 0.0512 - val_mean_squared_error: 0.0512\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Epoch 00039: val_loss did not improve from 0.04921\n",
      "248/248 [==============================] - 0s 86us/sample - loss: 0.0385 - mean_squared_error: 0.0385 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0409 - mean_squared_error: 0.0409\n",
      "Epoch 00040: val_loss did not improve from 0.04921\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0437 - mean_squared_error: 0.0437 - val_loss: 0.0524 - val_mean_squared_error: 0.0524\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0398 - mean_squared_error: 0.0398\n",
      "Epoch 00041: val_loss did not improve from 0.04921\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0449 - mean_squared_error: 0.0449\n",
      "Epoch 00042: val_loss did not improve from 0.04921\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0400 - mean_squared_error: 0.0400 - val_loss: 0.0532 - val_mean_squared_error: 0.0532\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0468 - mean_squared_error: 0.0468\n",
      "Epoch 00043: val_loss did not improve from 0.04921\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0412 - mean_squared_error: 0.0412 - val_loss: 0.0530 - val_mean_squared_error: 0.0530\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0435 - mean_squared_error: 0.0435\n",
      "Epoch 00044: val_loss did not improve from 0.04921\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0414 - mean_squared_error: 0.0414 - val_loss: 0.0529 - val_mean_squared_error: 0.0529\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0456 - mean_squared_error: 0.0456\n",
      "Epoch 00045: val_loss did not improve from 0.04921\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0526 - val_mean_squared_error: 0.0526\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0370 - mean_squared_error: 0.0370\n",
      "Epoch 00046: val_loss did not improve from 0.04921\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0411 - mean_squared_error: 0.0411 - val_loss: 0.0521 - val_mean_squared_error: 0.0521\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0483 - mean_squared_error: 0.0483\n",
      "Epoch 00047: val_loss did not improve from 0.04921\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.0520 - val_mean_squared_error: 0.0520\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0370 - mean_squared_error: 0.0370\n",
      "Epoch 00048: val_loss did not improve from 0.04921\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0392 - mean_squared_error: 0.0392 - val_loss: 0.0518 - val_mean_squared_error: 0.0518\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0420 - mean_squared_error: 0.0420\n",
      "Epoch 00049: val_loss did not improve from 0.04921\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0399 - mean_squared_error: 0.0399 - val_loss: 0.0516 - val_mean_squared_error: 0.0516\n",
      "Epoch 50/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0442 - mean_squared_error: 0.0442\n",
      "Epoch 00050: val_loss did not improve from 0.04921\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0415 - mean_squared_error: 0.0415 - val_loss: 0.0514 - val_mean_squared_error: 0.0514\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00051: val_loss did not improve from 0.04921\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0394 - mean_squared_error: 0.0394 - val_loss: 0.0511 - val_mean_squared_error: 0.0511\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0440 - mean_squared_error: 0.0440\n",
      "Epoch 00052: val_loss did not improve from 0.04921\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0391 - mean_squared_error: 0.0391 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0334 - mean_squared_error: 0.0334\n",
      "Epoch 00053: val_loss did not improve from 0.04921\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0392 - mean_squared_error: 0.0392 - val_loss: 0.0501 - val_mean_squared_error: 0.0501\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0339 - mean_squared_error: 0.0339\n",
      "Epoch 00054: val_loss did not improve from 0.04921\n",
      "248/248 [==============================] - 0s 97us/sample - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00055: val_loss did not improve from 0.04921\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0478 - mean_squared_error: 0.0478\n",
      "Epoch 00056: val_loss did not improve from 0.04921\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0380 - mean_squared_error: 0.0380 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00057: val_loss improved from 0.04921 to 0.04919, saving model to model.h5\n",
      "248/248 [==============================] - 0s 346us/sample - loss: 0.0367 - mean_squared_error: 0.0367 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Epoch 00058: val_loss did not improve from 0.04919\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0348 - mean_squared_error: 0.0348\n",
      "Epoch 00059: val_loss did not improve from 0.04919\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.0493 - val_mean_squared_error: 0.0493\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0508 - mean_squared_error: 0.0508\n",
      "Epoch 00060: val_loss did not improve from 0.04919\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0418 - mean_squared_error: 0.0418\n",
      "Epoch 00061: val_loss did not improve from 0.04919\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00062: val_loss did not improve from 0.04919\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0308 - mean_squared_error: 0.0308 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0368 - mean_squared_error: 0.0368\n",
      "Epoch 00063: val_loss did not improve from 0.04919\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0448 - mean_squared_error: 0.0448\n",
      "Epoch 00064: val_loss did not improve from 0.04919\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0497 - val_mean_squared_error: 0.0497\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0278 - mean_squared_error: 0.0278\n",
      "Epoch 00065: val_loss did not improve from 0.04919\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0386 - mean_squared_error: 0.0386 - val_loss: 0.0494 - val_mean_squared_error: 0.0494\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0415 - mean_squared_error: 0.0415\n",
      "Epoch 00066: val_loss did not improve from 0.04919\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0393 - mean_squared_error: 0.0393 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Epoch 00067: val_loss did not improve from 0.04919\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00068: val_loss improved from 0.04919 to 0.04903, saving model to model.h5\n",
      "248/248 [==============================] - 0s 207us/sample - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0490 - val_mean_squared_error: 0.0490\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00069: val_loss improved from 0.04903 to 0.04881, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Epoch 00070: val_loss improved from 0.04881 to 0.04859, saving model to model.h5\n",
      "248/248 [==============================] - 0s 221us/sample - loss: 0.0372 - mean_squared_error: 0.0372 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00071: val_loss improved from 0.04859 to 0.04827, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0397 - mean_squared_error: 0.0397\n",
      "Epoch 00072: val_loss improved from 0.04827 to 0.04817, saving model to model.h5\n",
      "248/248 [==============================] - 0s 217us/sample - loss: 0.0390 - mean_squared_error: 0.0390 - val_loss: 0.0482 - val_mean_squared_error: 0.0482\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0440 - mean_squared_error: 0.0440\n",
      "Epoch 00073: val_loss improved from 0.04817 to 0.04804, saving model to model.h5\n",
      "248/248 [==============================] - 0s 209us/sample - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 00074: val_loss improved from 0.04804 to 0.04765, saving model to model.h5\n",
      "248/248 [==============================] - 0s 229us/sample - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0344 - mean_squared_error: 0.0344\n",
      "Epoch 00075: val_loss improved from 0.04765 to 0.04724, saving model to model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 217us/sample - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00076: val_loss improved from 0.04724 to 0.04676, saving model to model.h5\n",
      "248/248 [==============================] - 0s 221us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0413 - mean_squared_error: 0.0413\n",
      "Epoch 00077: val_loss improved from 0.04676 to 0.04647, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00078: val_loss improved from 0.04647 to 0.04606, saving model to model.h5\n",
      "248/248 [==============================] - 0s 225us/sample - loss: 0.0334 - mean_squared_error: 0.0334 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0294 - mean_squared_error: 0.0294\n",
      "Epoch 00079: val_loss improved from 0.04606 to 0.04530, saving model to model.h5\n",
      "248/248 [==============================] - 0s 209us/sample - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00080: val_loss improved from 0.04530 to 0.04505, saving model to model.h5\n",
      "248/248 [==============================] - 0s 221us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0458 - mean_squared_error: 0.0458\n",
      "Epoch 00081: val_loss did not improve from 0.04505\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00082: val_loss did not improve from 0.04505\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00083: val_loss did not improve from 0.04505\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00084: val_loss did not improve from 0.04505\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00085: val_loss did not improve from 0.04505\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00086: val_loss did not improve from 0.04505\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0323 - mean_squared_error: 0.0323 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0295 - mean_squared_error: 0.0295\n",
      "Epoch 00087: val_loss did not improve from 0.04505\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0295 - mean_squared_error: 0.0295\n",
      "Epoch 00088: val_loss did not improve from 0.04505\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0358 - mean_squared_error: 0.0358\n",
      "Epoch 00089: val_loss did not improve from 0.04505\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 00090: val_loss did not improve from 0.04505\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00091: val_loss improved from 0.04505 to 0.04503, saving model to model.h5\n",
      "248/248 [==============================] - 0s 219us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Epoch 00092: val_loss did not improve from 0.04503\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Epoch 00093: val_loss did not improve from 0.04503\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0421 - mean_squared_error: 0.0421\n",
      "Epoch 00094: val_loss did not improve from 0.04503\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Epoch 00095: val_loss did not improve from 0.04503\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00096: val_loss did not improve from 0.04503\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00097: val_loss did not improve from 0.04503\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00098: val_loss did not improve from 0.04503\n",
      "248/248 [==============================] - 0s 83us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Epoch 00099: val_loss did not improve from 0.04503\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0311 - mean_squared_error: 0.0311 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0448 - mean_squared_error: 0.0448\n",
      "Epoch 00100: val_loss did not improve from 0.04503\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 101/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00101: val_loss did not improve from 0.04503\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00102: val_loss did not improve from 0.04503\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00103: val_loss improved from 0.04503 to 0.04457, saving model to model.h5\n",
      "248/248 [==============================] - 0s 246us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0365 - mean_squared_error: 0.0365\n",
      "Epoch 00104: val_loss improved from 0.04457 to 0.04429, saving model to model.h5\n",
      "248/248 [==============================] - 0s 237us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "Epoch 00105: val_loss improved from 0.04429 to 0.04402, saving model to model.h5\n",
      "248/248 [==============================] - 0s 228us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0333 - mean_squared_error: 0.0333\n",
      "Epoch 00106: val_loss improved from 0.04402 to 0.04398, saving model to model.h5\n",
      "248/248 [==============================] - 0s 249us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Epoch 00107: val_loss did not improve from 0.04398\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00108: val_loss did not improve from 0.04398\n",
      "248/248 [==============================] - 0s 92us/sample - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00109: val_loss did not improve from 0.04398\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00110: val_loss did not improve from 0.04398\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00111: val_loss did not improve from 0.04398\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00112: val_loss improved from 0.04398 to 0.04339, saving model to model.h5\n",
      "248/248 [==============================] - 0s 232us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00113: val_loss improved from 0.04339 to 0.04261, saving model to model.h5\n",
      "248/248 [==============================] - 0s 237us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0469 - mean_squared_error: 0.0469\n",
      "Epoch 00114: val_loss improved from 0.04261 to 0.04239, saving model to model.h5\n",
      "248/248 [==============================] - 0s 225us/sample - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00115: val_loss improved from 0.04239 to 0.04225, saving model to model.h5\n",
      "248/248 [==============================] - 0s 220us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00116: val_loss improved from 0.04225 to 0.04199, saving model to model.h5\n",
      "248/248 [==============================] - 0s 237us/sample - loss: 0.0307 - mean_squared_error: 0.0307 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0294 - mean_squared_error: 0.0294\n",
      "Epoch 00117: val_loss did not improve from 0.04199\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0174 - mean_squared_error: 0.0174\n",
      "Epoch 00118: val_loss improved from 0.04199 to 0.04198, saving model to model.h5\n",
      "248/248 [==============================] - 0s 257us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00119: val_loss did not improve from 0.04198\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00120: val_loss did not improve from 0.04198\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00121: val_loss improved from 0.04198 to 0.04191, saving model to model.h5\n",
      "248/248 [==============================] - 0s 245us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00122: val_loss did not improve from 0.04191\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Epoch 00123: val_loss improved from 0.04191 to 0.04180, saving model to model.h5\n",
      "248/248 [==============================] - 0s 237us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00124: val_loss did not improve from 0.04180\n",
      "248/248 [==============================] - 0s 92us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0153 - mean_squared_error: 0.0153\n",
      "Epoch 00125: val_loss did not improve from 0.04180\n",
      "248/248 [==============================] - 0s 92us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00126: val_loss did not improve from 0.04180\n",
      "248/248 [==============================] - 0s 89us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0174 - mean_squared_error: 0.0174\n",
      "Epoch 00127: val_loss did not improve from 0.04180\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00128: val_loss did not improve from 0.04180\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00129: val_loss did not improve from 0.04180\n",
      "248/248 [==============================] - 0s 93us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 130/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00130: val_loss did not improve from 0.04180\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00131: val_loss did not improve from 0.04180\n",
      "248/248 [==============================] - 0s 129us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 132/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Epoch 00132: val_loss did not improve from 0.04180\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 133/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00133: val_loss did not improve from 0.04180\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 134/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00134: val_loss did not improve from 0.04180\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 135/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00135: val_loss did not improve from 0.04180\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 136/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00136: val_loss did not improve from 0.04180\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 137/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.0241\n",
      "Epoch 00137: val_loss did not improve from 0.04180\n",
      "248/248 [==============================] - 0s 101us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 138/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00138: val_loss improved from 0.04180 to 0.04162, saving model to model.h5\n",
      "248/248 [==============================] - 0s 195us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 139/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0374 - mean_squared_error: 0.0374\n",
      "Epoch 00139: val_loss improved from 0.04162 to 0.04132, saving model to model.h5\n",
      "248/248 [==============================] - 0s 187us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 140/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0177 - mean_squared_error: 0.0177\n",
      "Epoch 00140: val_loss did not improve from 0.04132\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 141/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00141: val_loss did not improve from 0.04132\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 142/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00142: val_loss improved from 0.04132 to 0.04098, saving model to model.h5\n",
      "248/248 [==============================] - 0s 204us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 143/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00143: val_loss improved from 0.04098 to 0.04052, saving model to model.h5\n",
      "248/248 [==============================] - 0s 199us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 144/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 00144: val_loss improved from 0.04052 to 0.04038, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 145/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0393 - mean_squared_error: 0.0393\n",
      "Epoch 00145: val_loss improved from 0.04038 to 0.04008, saving model to model.h5\n",
      "248/248 [==============================] - 0s 197us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 146/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00146: val_loss did not improve from 0.04008\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 147/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00147: val_loss did not improve from 0.04008\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0291 - mean_squared_error: 0.0291 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 148/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0334 - mean_squared_error: 0.0334\n",
      "Epoch 00148: val_loss did not improve from 0.04008\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 149/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.0185\n",
      "Epoch 00149: val_loss did not improve from 0.04008\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 150/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0257 - mean_squared_error: 0.0257\n",
      "Epoch 00150: val_loss did not improve from 0.04008\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 151/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.0185\n",
      "Epoch 00151: val_loss did not improve from 0.04008\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00152: val_loss did not improve from 0.04008\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 153/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00153: val_loss did not improve from 0.04008\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 154/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00154: val_loss did not improve from 0.04008\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 155/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00155: val_loss did not improve from 0.04008\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 156/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Epoch 00156: val_loss did not improve from 0.04008\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 157/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Epoch 00157: val_loss did not improve from 0.04008\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 158/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00158: val_loss did not improve from 0.04008\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 159/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00159: val_loss did not improve from 0.04008\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 160/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00160: val_loss improved from 0.04008 to 0.03979, saving model to model.h5\n",
      "248/248 [==============================] - 0s 216us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 161/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00161: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 162/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00162: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 163/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00163: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 164/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Epoch 00164: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 165/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0363 - mean_squared_error: 0.0363\n",
      "Epoch 00165: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 166/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0390 - mean_squared_error: 0.0390\n",
      "Epoch 00166: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 167/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00167: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 168/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00168: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 169/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0121 - mean_squared_error: 0.0121\n",
      "Epoch 00169: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 170/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00170: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 171/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00171: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 172/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00172: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 173/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0170 - mean_squared_error: 0.0170\n",
      "Epoch 00173: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 174/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00174: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 175/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00175: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 176/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0134 - mean_squared_error: 0.0134\n",
      "Epoch 00176: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 177/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00177: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00178: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 179/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 00179: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 180/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00180: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 181/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00181: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 182/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0173 - mean_squared_error: 0.0173\n",
      "Epoch 00182: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 183/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00183: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 184/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Epoch 00184: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 185/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00185: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 186/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00186: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 187/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00187: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 188/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00188: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 189/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0187 - mean_squared_error: 0.0187\n",
      "Epoch 00189: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 190/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00190: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 191/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00191: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 192/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00192: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 193/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00193: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 194/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00194: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 195/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00195: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 196/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0333 - mean_squared_error: 0.0333\n",
      "Epoch 00196: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 197/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00197: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 198/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Epoch 00198: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 199/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00199: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 200/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00200: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 201/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00201: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 67us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 202/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00202: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 203/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00203: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 204/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00204: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 205/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00205: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 206/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00206: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 207/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0393 - mean_squared_error: 0.0393\n",
      "Epoch 00207: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 208/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00208: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 209/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00209: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 210/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.0316\n",
      "Epoch 00210: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 64us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 211/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00211: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 212/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0357 - mean_squared_error: 0.0357\n",
      "Epoch 00212: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 213/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.0165\n",
      "Epoch 00213: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0440 - val_mean_squared_error: 0.0440\n",
      "Epoch 214/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00214: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 215/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0130 - mean_squared_error: 0.0130\n",
      "Epoch 00215: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 216/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00216: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 217/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0147 - mean_squared_error: 0.0147\n",
      "Epoch 00217: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 218/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00218: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 219/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00219: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 220/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00220: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 221/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0132 - mean_squared_error: 0.0132\n",
      "Epoch 00221: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 222/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0369 - mean_squared_error: 0.0369\n",
      "Epoch 00222: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 223/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.0172\n",
      "Epoch 00223: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 224/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00224: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 225/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00225: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 226/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00226: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 89us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 227/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00227: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 228/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00228: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 229/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00229: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 230/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0345 - mean_squared_error: 0.0345\n",
      "Epoch 00230: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 231/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0259 - mean_squared_error: 0.0259\n",
      "Epoch 00231: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 232/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00232: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 101us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 233/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.0197\n",
      "Epoch 00233: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 234/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00234: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 235/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0167 - mean_squared_error: 0.0167\n",
      "Epoch 00235: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 236/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00236: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 237/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206\n",
      "Epoch 00237: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 238/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00238: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 239/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00239: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 240/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.0183\n",
      "Epoch 00240: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 241/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00241: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 242/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00242: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 243/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00243: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 244/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00244: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 245/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00245: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 246/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00246: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 247/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0150 - mean_squared_error: 0.0150\n",
      "Epoch 00247: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 248/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.0172\n",
      "Epoch 00248: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 249/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00249: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 250/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0158 - mean_squared_error: 0.0158\n",
      "Epoch 00250: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 251/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00251: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 252/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00252: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 253/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0176 - mean_squared_error: 0.0176\n",
      "Epoch 00253: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 109us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 254/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00254: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 101us/sample - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 255/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0143 - mean_squared_error: 0.0143\n",
      "Epoch 00255: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 97us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 256/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00256: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 109us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 257/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00257: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 97us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 258/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0142 - mean_squared_error: 0.0142\n",
      "Epoch 00258: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 259/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00259: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 260/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00260: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Running trial 9\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 1s - loss: 1.0342 - mean_squared_error: 1.0342\n",
      "Epoch 00001: val_loss improved from inf to 0.74071, saving model to model.h5\n",
      "248/248 [==============================] - 5s 21ms/sample - loss: 1.1063 - mean_squared_error: 1.1063 - val_loss: 0.7407 - val_mean_squared_error: 0.7407\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.3279 - mean_squared_error: 1.3279\n",
      "Epoch 00002: val_loss improved from 0.74071 to 0.71530, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 1.0294 - mean_squared_error: 1.0294 - val_loss: 0.7153 - val_mean_squared_error: 0.7153\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.0043 - mean_squared_error: 1.0043\n",
      "Epoch 00003: val_loss improved from 0.71530 to 0.68986, saving model to model.h5\n",
      "248/248 [==============================] - 0s 229us/sample - loss: 0.9541 - mean_squared_error: 0.9541 - val_loss: 0.6899 - val_mean_squared_error: 0.6899\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9655 - mean_squared_error: 0.9655\n",
      "Epoch 00004: val_loss improved from 0.68986 to 0.66312, saving model to model.h5\n",
      "248/248 [==============================] - 0s 214us/sample - loss: 0.8735 - mean_squared_error: 0.8735 - val_loss: 0.6631 - val_mean_squared_error: 0.6631\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7307 - mean_squared_error: 0.7307\n",
      "Epoch 00005: val_loss improved from 0.66312 to 0.63551, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 0.8017 - mean_squared_error: 0.8017 - val_loss: 0.6355 - val_mean_squared_error: 0.6355\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7630 - mean_squared_error: 0.7630\n",
      "Epoch 00006: val_loss improved from 0.63551 to 0.60574, saving model to model.h5\n",
      "248/248 [==============================] - 0s 209us/sample - loss: 0.7123 - mean_squared_error: 0.7123 - val_loss: 0.6057 - val_mean_squared_error: 0.6057\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6323 - mean_squared_error: 0.6323\n",
      "Epoch 00007: val_loss improved from 0.60574 to 0.57388, saving model to model.h5\n",
      "248/248 [==============================] - 0s 225us/sample - loss: 0.6399 - mean_squared_error: 0.6399 - val_loss: 0.5739 - val_mean_squared_error: 0.5739\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.5773 - mean_squared_error: 0.5773\n",
      "Epoch 00008: val_loss improved from 0.57388 to 0.53907, saving model to model.h5\n",
      "248/248 [==============================] - 0s 220us/sample - loss: 0.5436 - mean_squared_error: 0.5436 - val_loss: 0.5391 - val_mean_squared_error: 0.5391\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4334 - mean_squared_error: 0.4334\n",
      "Epoch 00009: val_loss improved from 0.53907 to 0.49923, saving model to model.h5\n",
      "248/248 [==============================] - 0s 219us/sample - loss: 0.4409 - mean_squared_error: 0.4409 - val_loss: 0.4992 - val_mean_squared_error: 0.4992\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4283 - mean_squared_error: 0.4283\n",
      "Epoch 00010: val_loss improved from 0.49923 to 0.45613, saving model to model.h5\n",
      "248/248 [==============================] - 0s 246us/sample - loss: 0.3675 - mean_squared_error: 0.3675 - val_loss: 0.4561 - val_mean_squared_error: 0.4561\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2806 - mean_squared_error: 0.2806\n",
      "Epoch 00011: val_loss improved from 0.45613 to 0.40965, saving model to model.h5\n",
      "248/248 [==============================] - 0s 217us/sample - loss: 0.2741 - mean_squared_error: 0.2741 - val_loss: 0.4097 - val_mean_squared_error: 0.4097\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2196 - mean_squared_error: 0.2196\n",
      "Epoch 00012: val_loss improved from 0.40965 to 0.36031, saving model to model.h5\n",
      "248/248 [==============================] - 0s 219us/sample - loss: 0.2066 - mean_squared_error: 0.2066 - val_loss: 0.3603 - val_mean_squared_error: 0.3603\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1556 - mean_squared_error: 0.1556\n",
      "Epoch 00013: val_loss improved from 0.36031 to 0.31115, saving model to model.h5\n",
      "248/248 [==============================] - 0s 233us/sample - loss: 0.1604 - mean_squared_error: 0.1604 - val_loss: 0.3112 - val_mean_squared_error: 0.3112\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1445 - mean_squared_error: 0.1445\n",
      "Epoch 00014: val_loss improved from 0.31115 to 0.26386, saving model to model.h5\n",
      "248/248 [==============================] - 0s 219us/sample - loss: 0.1327 - mean_squared_error: 0.1327 - val_loss: 0.2639 - val_mean_squared_error: 0.2639\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1041 - mean_squared_error: 0.1041\n",
      "Epoch 00015: val_loss improved from 0.26386 to 0.22143, saving model to model.h5\n",
      "248/248 [==============================] - 0s 209us/sample - loss: 0.1103 - mean_squared_error: 0.1103 - val_loss: 0.2214 - val_mean_squared_error: 0.2214\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0980 - mean_squared_error: 0.0980\n",
      "Epoch 00016: val_loss improved from 0.22143 to 0.18440, saving model to model.h5\n",
      "248/248 [==============================] - 0s 201us/sample - loss: 0.0978 - mean_squared_error: 0.0978 - val_loss: 0.1844 - val_mean_squared_error: 0.1844\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1007 - mean_squared_error: 0.1007\n",
      "Epoch 00017: val_loss improved from 0.18440 to 0.15323, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.0881 - mean_squared_error: 0.0881 - val_loss: 0.1532 - val_mean_squared_error: 0.1532\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0943 - mean_squared_error: 0.0943\n",
      "Epoch 00018: val_loss improved from 0.15323 to 0.12793, saving model to model.h5\n",
      "248/248 [==============================] - 0s 214us/sample - loss: 0.0847 - mean_squared_error: 0.0847 - val_loss: 0.1279 - val_mean_squared_error: 0.1279\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0802 - mean_squared_error: 0.0802\n",
      "Epoch 00019: val_loss improved from 0.12793 to 0.10702, saving model to model.h5\n",
      "248/248 [==============================] - 0s 245us/sample - loss: 0.0646 - mean_squared_error: 0.0646 - val_loss: 0.1070 - val_mean_squared_error: 0.1070\n",
      "Epoch 20/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0600 - mean_squared_error: 0.0600\n",
      "Epoch 00020: val_loss improved from 0.10702 to 0.08975, saving model to model.h5\n",
      "248/248 [==============================] - 0s 211us/sample - loss: 0.0589 - mean_squared_error: 0.0589 - val_loss: 0.0898 - val_mean_squared_error: 0.0898\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0634 - mean_squared_error: 0.0634\n",
      "Epoch 00021: val_loss improved from 0.08975 to 0.07614, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 0.0539 - mean_squared_error: 0.0539 - val_loss: 0.0761 - val_mean_squared_error: 0.0761\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0481 - mean_squared_error: 0.0481\n",
      "Epoch 00022: val_loss improved from 0.07614 to 0.06566, saving model to model.h5\n",
      "248/248 [==============================] - 0s 203us/sample - loss: 0.0473 - mean_squared_error: 0.0473 - val_loss: 0.0657 - val_mean_squared_error: 0.0657\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0558 - mean_squared_error: 0.0558\n",
      "Epoch 00023: val_loss improved from 0.06566 to 0.05778, saving model to model.h5\n",
      "248/248 [==============================] - 0s 203us/sample - loss: 0.0453 - mean_squared_error: 0.0453 - val_loss: 0.0578 - val_mean_squared_error: 0.0578\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0455 - mean_squared_error: 0.0455\n",
      "Epoch 00024: val_loss improved from 0.05778 to 0.05192, saving model to model.h5\n",
      "248/248 [==============================] - 0s 215us/sample - loss: 0.0428 - mean_squared_error: 0.0428 - val_loss: 0.0519 - val_mean_squared_error: 0.0519\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0471 - mean_squared_error: 0.0471\n",
      "Epoch 00025: val_loss improved from 0.05192 to 0.04789, saving model to model.h5\n",
      "248/248 [==============================] - 0s 196us/sample - loss: 0.0442 - mean_squared_error: 0.0442 - val_loss: 0.0479 - val_mean_squared_error: 0.0479\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0384 - mean_squared_error: 0.0384\n",
      "Epoch 00026: val_loss improved from 0.04789 to 0.04531, saving model to model.h5\n",
      "248/248 [==============================] - 0s 193us/sample - loss: 0.0386 - mean_squared_error: 0.0386 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0502 - mean_squared_error: 0.0502\n",
      "Epoch 00027: val_loss improved from 0.04531 to 0.04380, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.0449 - mean_squared_error: 0.0449 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0481 - mean_squared_error: 0.0481\n",
      "Epoch 00028: val_loss improved from 0.04380 to 0.04314, saving model to model.h5\n",
      "248/248 [==============================] - 0s 198us/sample - loss: 0.0410 - mean_squared_error: 0.0410 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0415 - mean_squared_error: 0.0415\n",
      "Epoch 00029: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0402 - mean_squared_error: 0.0402 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Epoch 00030: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 82us/sample - loss: 0.0407 - mean_squared_error: 0.0407 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Epoch 00031: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0385 - mean_squared_error: 0.0385 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0372 - mean_squared_error: 0.0372\n",
      "Epoch 00032: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0418 - mean_squared_error: 0.0418 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0488 - mean_squared_error: 0.0488\n",
      "Epoch 00033: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Epoch 00034: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00035: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0372 - mean_squared_error: 0.0372 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0427 - mean_squared_error: 0.0427\n",
      "Epoch 00036: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0384 - mean_squared_error: 0.0384 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0416 - mean_squared_error: 0.0416\n",
      "Epoch 00037: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0492 - val_mean_squared_error: 0.0492\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0362 - mean_squared_error: 0.0362\n",
      "Epoch 00038: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0363 - mean_squared_error: 0.0363 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0427 - mean_squared_error: 0.0427\n",
      "Epoch 00039: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0383 - mean_squared_error: 0.0383\n",
      "Epoch 00040: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0376 - mean_squared_error: 0.0376 - val_loss: 0.0482 - val_mean_squared_error: 0.0482\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0423 - mean_squared_error: 0.0423\n",
      "Epoch 00041: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Epoch 00042: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0360 - mean_squared_error: 0.0360 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00043: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0357 - mean_squared_error: 0.0357 - val_loss: 0.0470 - val_mean_squared_error: 0.0470\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0367 - mean_squared_error: 0.0367\n",
      "Epoch 00044: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 45/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0404 - mean_squared_error: 0.0404\n",
      "Epoch 00045: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0466 - mean_squared_error: 0.0466\n",
      "Epoch 00046: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00047: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0331 - mean_squared_error: 0.0331 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00048: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0460 - val_mean_squared_error: 0.0460\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0415 - mean_squared_error: 0.0415\n",
      "Epoch 00049: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00050: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0336 - mean_squared_error: 0.0336 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 00051: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0453 - val_mean_squared_error: 0.0453\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0376 - mean_squared_error: 0.0376\n",
      "Epoch 00052: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00053: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00054: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 89us/sample - loss: 0.0351 - mean_squared_error: 0.0351 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0372 - mean_squared_error: 0.0372\n",
      "Epoch 00055: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 89us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0176 - mean_squared_error: 0.0176\n",
      "Epoch 00056: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0328 - mean_squared_error: 0.0328 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0423 - mean_squared_error: 0.0423\n",
      "Epoch 00057: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0369 - mean_squared_error: 0.0369\n",
      "Epoch 00058: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 89us/sample - loss: 0.0348 - mean_squared_error: 0.0348 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0399 - mean_squared_error: 0.0399\n",
      "Epoch 00059: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 97us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00060: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00061: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00062: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00063: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 89us/sample - loss: 0.0320 - mean_squared_error: 0.0320 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00064: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.0293\n",
      "Epoch 00065: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0333 - mean_squared_error: 0.0333 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00066: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Epoch 00067: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0566 - mean_squared_error: 0.0566\n",
      "Epoch 00068: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0359 - mean_squared_error: 0.0359 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0270 - mean_squared_error: 0.0270\n",
      "Epoch 00069: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00070: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 71/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0403 - mean_squared_error: 0.0403\n",
      "Epoch 00071: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00072: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0328 - mean_squared_error: 0.0328 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00073: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Epoch 00074: val_loss did not improve from 0.04314\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0308 - mean_squared_error: 0.0308 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00075: val_loss improved from 0.04314 to 0.04286, saving model to model.h5\n",
      "248/248 [==============================] - 0s 221us/sample - loss: 0.0335 - mean_squared_error: 0.0335 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Epoch 00076: val_loss improved from 0.04286 to 0.04252, saving model to model.h5\n",
      "248/248 [==============================] - 0s 221us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Epoch 00077: val_loss improved from 0.04252 to 0.04243, saving model to model.h5\n",
      "248/248 [==============================] - 0s 229us/sample - loss: 0.0332 - mean_squared_error: 0.0332 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0366 - mean_squared_error: 0.0366\n",
      "Epoch 00078: val_loss did not improve from 0.04243\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00079: val_loss did not improve from 0.04243\n",
      "248/248 [==============================] - 0s 96us/sample - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00080: val_loss did not improve from 0.04243\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0408 - mean_squared_error: 0.0408\n",
      "Epoch 00081: val_loss did not improve from 0.04243\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0373 - mean_squared_error: 0.0373\n",
      "Epoch 00082: val_loss did not improve from 0.04243\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0331 - mean_squared_error: 0.0331 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00083: val_loss did not improve from 0.04243\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00084: val_loss did not improve from 0.04243\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 00085: val_loss did not improve from 0.04243\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00086: val_loss did not improve from 0.04243\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00087: val_loss improved from 0.04243 to 0.04219, saving model to model.h5\n",
      "248/248 [==============================] - 0s 233us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00088: val_loss improved from 0.04219 to 0.04176, saving model to model.h5\n",
      "248/248 [==============================] - 0s 221us/sample - loss: 0.0310 - mean_squared_error: 0.0310 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0367 - mean_squared_error: 0.0367\n",
      "Epoch 00089: val_loss improved from 0.04176 to 0.04139, saving model to model.h5\n",
      "248/248 [==============================] - 0s 217us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.0202\n",
      "Epoch 00090: val_loss improved from 0.04139 to 0.04090, saving model to model.h5\n",
      "248/248 [==============================] - 0s 229us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00091: val_loss did not improve from 0.04090\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0440 - mean_squared_error: 0.0440\n",
      "Epoch 00092: val_loss did not improve from 0.04090\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0252 - mean_squared_error: 0.0252\n",
      "Epoch 00093: val_loss did not improve from 0.04090\n",
      "248/248 [==============================] - 0s 97us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00094: val_loss did not improve from 0.04090\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00095: val_loss did not improve from 0.04090\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00096: val_loss did not improve from 0.04090\n",
      "248/248 [==============================] - 0s 92us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 00097: val_loss did not improve from 0.04090\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281\n",
      "Epoch 00098: val_loss did not improve from 0.04090\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00099: val_loss did not improve from 0.04090\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00100: val_loss did not improve from 0.04090\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Epoch 00101: val_loss did not improve from 0.04090\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0302 - mean_squared_error: 0.0302 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00102: val_loss did not improve from 0.04090\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 00103: val_loss improved from 0.04090 to 0.04023, saving model to model.h5\n",
      "248/248 [==============================] - 0s 210us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00104: val_loss improved from 0.04023 to 0.03950, saving model to model.h5\n",
      "248/248 [==============================] - 0s 219us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.0194\n",
      "Epoch 00105: val_loss improved from 0.03950 to 0.03914, saving model to model.h5\n",
      "248/248 [==============================] - 0s 236us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00106: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 93us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Epoch 00107: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0312 - mean_squared_error: 0.0312 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00108: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00109: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00110: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00111: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0174 - mean_squared_error: 0.0174\n",
      "Epoch 00112: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00113: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00114: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00115: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00116: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Epoch 00117: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00118: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.0205\n",
      "Epoch 00119: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0174 - mean_squared_error: 0.0174\n",
      "Epoch 00120: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00121: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00122: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 00123: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00124: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00125: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00126: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00127: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00128: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00129: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 130/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00130: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00131: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 132/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 00132: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0277 - mean_squared_error: 0.0277 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 133/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00133: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 134/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00134: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 135/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00135: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 136/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0345 - mean_squared_error: 0.0345\n",
      "Epoch 00136: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 137/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.0172\n",
      "Epoch 00137: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 138/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0188 - mean_squared_error: 0.0188\n",
      "Epoch 00138: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 139/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00139: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 140/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00140: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 141/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00141: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 142/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00142: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 143/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00143: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 144/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00144: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 145/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00145: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 146/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0376 - mean_squared_error: 0.0376\n",
      "Epoch 00146: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 147/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00147: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 148/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0423 - mean_squared_error: 0.0423\n",
      "Epoch 00148: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0308 - mean_squared_error: 0.0308 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 149/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0155 - mean_squared_error: 0.0155\n",
      "Epoch 00149: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 150/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00150: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0436 - val_mean_squared_error: 0.0436\n",
      "Epoch 151/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00151: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 152/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0413 - mean_squared_error: 0.0413\n",
      "Epoch 00152: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 68us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 153/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00153: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 154/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00154: val_loss did not improve from 0.03914\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 155/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.0243\n",
      "Epoch 00155: val_loss improved from 0.03914 to 0.03876, saving model to model.h5\n",
      "248/248 [==============================] - 0s 214us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 156/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00156: val_loss did not improve from 0.03876\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 157/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00157: val_loss did not improve from 0.03876\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 158/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00158: val_loss did not improve from 0.03876\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 159/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00159: val_loss did not improve from 0.03876\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 160/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00160: val_loss did not improve from 0.03876\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 161/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0326 - mean_squared_error: 0.0326\n",
      "Epoch 00161: val_loss did not improve from 0.03876\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 162/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0144 - mean_squared_error: 0.0144\n",
      "Epoch 00162: val_loss improved from 0.03876 to 0.03871, saving model to model.h5\n",
      "248/248 [==============================] - 0s 215us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 163/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 00163: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 164/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00164: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 82us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 165/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00165: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 166/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00166: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 93us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 167/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Epoch 00167: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 168/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00168: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 69us/sample - loss: 0.0279 - mean_squared_error: 0.0279 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 169/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00169: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 97us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 170/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00170: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 171/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00171: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 172/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0187 - mean_squared_error: 0.0187\n",
      "Epoch 00172: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 173/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00173: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 174/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00174: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0282 - mean_squared_error: 0.0282\n",
      "Epoch 00175: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0411 - val_mean_squared_error: 0.0411\n",
      "Epoch 176/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0437 - mean_squared_error: 0.0437\n",
      "Epoch 00176: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 177/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00177: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 178/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00178: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 179/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00179: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 180/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 00180: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 181/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00181: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 92us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 182/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0142 - mean_squared_error: 0.0142\n",
      "Epoch 00182: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 83us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 183/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00183: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 184/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00184: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 185/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00185: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 186/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00186: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0434 - val_mean_squared_error: 0.0434\n",
      "Epoch 187/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00187: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0438 - val_mean_squared_error: 0.0438\n",
      "Epoch 188/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0324 - mean_squared_error: 0.0324\n",
      "Epoch 00188: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 189/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0144 - mean_squared_error: 0.0144\n",
      "Epoch 00189: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 190/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00190: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 191/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0167 - mean_squared_error: 0.0167\n",
      "Epoch 00191: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 192/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00192: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 193/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0157 - mean_squared_error: 0.0157\n",
      "Epoch 00193: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0439 - val_mean_squared_error: 0.0439\n",
      "Epoch 194/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00194: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 195/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 00195: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 196/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00196: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 82us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 197/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00197: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 198/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00198: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 82us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 199/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.0234\n",
      "Epoch 00199: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 200/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.0219\n",
      "Epoch 00200: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 201/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00201: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 202/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00202: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0421 - val_mean_squared_error: 0.0421\n",
      "Epoch 203/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00203: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 204/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0176 - mean_squared_error: 0.0176\n",
      "Epoch 00204: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 92us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 205/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0169 - mean_squared_error: 0.0169\n",
      "Epoch 00205: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0418 - val_mean_squared_error: 0.0418\n",
      "Epoch 206/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.0182\n",
      "Epoch 00206: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 93us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0428 - val_mean_squared_error: 0.0428\n",
      "Epoch 207/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00207: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 208/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00208: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0458 - val_mean_squared_error: 0.0458\n",
      "Epoch 209/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.0195\n",
      "Epoch 00209: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0447 - val_mean_squared_error: 0.0447\n",
      "Epoch 210/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00210: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0426 - val_mean_squared_error: 0.0426\n",
      "Epoch 211/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0346 - mean_squared_error: 0.0346\n",
      "Epoch 00211: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 212/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0121 - mean_squared_error: 0.0121\n",
      "Epoch 00212: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 213/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 00213: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 113us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 214/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0291 - mean_squared_error: 0.0291\n",
      "Epoch 00214: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 215/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0167 - mean_squared_error: 0.0167\n",
      "Epoch 00215: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 216/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00216: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 86us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 217/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.0217\n",
      "Epoch 00217: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 218/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0174 - mean_squared_error: 0.0174\n",
      "Epoch 00218: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 219/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0150 - mean_squared_error: 0.0150\n",
      "Epoch 00219: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 92us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 220/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00220: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 221/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0145 - mean_squared_error: 0.0145\n",
      "Epoch 00221: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0441 - val_mean_squared_error: 0.0441\n",
      "Epoch 222/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0296 - mean_squared_error: 0.0296\n",
      "Epoch 00222: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 223/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0113 - mean_squared_error: 0.0113\n",
      "Epoch 00223: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 224/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0173 - mean_squared_error: 0.0173\n",
      "Epoch 00224: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 225/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00225: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0464 - val_mean_squared_error: 0.0464\n",
      "Epoch 226/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00226: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 227/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0417 - mean_squared_error: 0.0417\n",
      "Epoch 00227: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 228/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00228: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 229/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00229: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 230/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00230: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 231/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00231: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 232/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0181 - mean_squared_error: 0.0181\n",
      "Epoch 00232: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0442 - val_mean_squared_error: 0.0442\n",
      "Epoch 233/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.0242\n",
      "Epoch 00233: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0466 - val_mean_squared_error: 0.0466\n",
      "Epoch 234/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00234: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 235/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00235: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0472 - val_mean_squared_error: 0.0472\n",
      "Epoch 236/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0177 - mean_squared_error: 0.0177\n",
      "Epoch 00236: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 237/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0259 - mean_squared_error: 0.0259\n",
      "Epoch 00237: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 96us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0484 - val_mean_squared_error: 0.0484\n",
      "Epoch 238/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.0332\n",
      "Epoch 00238: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 239/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00239: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 240/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00240: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 241/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00241: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 242/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0279 - mean_squared_error: 0.0279\n",
      "Epoch 00242: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0212 - mean_squared_error: 0.0212 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 243/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00243: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 244/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0321 - mean_squared_error: 0.0321\n",
      "Epoch 00244: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0477 - val_mean_squared_error: 0.0477\n",
      "Epoch 245/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0347 - mean_squared_error: 0.0347\n",
      "Epoch 00245: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
      "Epoch 246/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00246: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 92us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 247/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.0233\n",
      "Epoch 00247: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 248/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00248: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 249/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 00249: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0446 - val_mean_squared_error: 0.0446\n",
      "Epoch 250/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00250: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 251/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0125 - mean_squared_error: 0.0125\n",
      "Epoch 00251: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 252/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00252: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0491 - val_mean_squared_error: 0.0491\n",
      "Epoch 253/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.0228\n",
      "Epoch 00253: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0498 - val_mean_squared_error: 0.0498\n",
      "Epoch 254/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0156 - mean_squared_error: 0.0156\n",
      "Epoch 00254: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0485 - val_mean_squared_error: 0.0485\n",
      "Epoch 255/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00255: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 256/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0173 - mean_squared_error: 0.0173\n",
      "Epoch 00256: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 257/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00257: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0465 - val_mean_squared_error: 0.0465\n",
      "Epoch 258/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.0288\n",
      "Epoch 00258: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 259/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00259: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0469 - val_mean_squared_error: 0.0469\n",
      "Epoch 260/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00260: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0478 - val_mean_squared_error: 0.0478\n",
      "Epoch 261/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00261: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 262/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.0299\n",
      "Epoch 00262: val_loss did not improve from 0.03871\n",
      "248/248 [==============================] - 0s 92us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0488 - val_mean_squared_error: 0.0488\n",
      "Running trial 10\n",
      "(310, 6)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 1s - loss: 1.4320 - mean_squared_error: 1.4320\n",
      "Epoch 00001: val_loss improved from inf to 0.72578, saving model to model.h5\n",
      "248/248 [==============================] - 5s 21ms/sample - loss: 1.1111 - mean_squared_error: 1.1111 - val_loss: 0.7258 - val_mean_squared_error: 0.7258\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8606 - mean_squared_error: 0.8606\n",
      "Epoch 00002: val_loss improved from 0.72578 to 0.70050, saving model to model.h5\n",
      "248/248 [==============================] - 0s 233us/sample - loss: 1.0366 - mean_squared_error: 1.0366 - val_loss: 0.7005 - val_mean_squared_error: 0.7005\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 1.0346 - mean_squared_error: 1.0346\n",
      "Epoch 00003: val_loss improved from 0.70050 to 0.67526, saving model to model.h5\n",
      "248/248 [==============================] - 0s 200us/sample - loss: 0.9727 - mean_squared_error: 0.9727 - val_loss: 0.6753 - val_mean_squared_error: 0.6753\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.9735 - mean_squared_error: 0.9735\n",
      "Epoch 00004: val_loss improved from 0.67526 to 0.64922, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.9057 - mean_squared_error: 0.9057 - val_loss: 0.6492 - val_mean_squared_error: 0.6492\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.8183 - mean_squared_error: 0.8183\n",
      "Epoch 00005: val_loss improved from 0.64922 to 0.62083, saving model to model.h5\n",
      "248/248 [==============================] - 0s 219us/sample - loss: 0.8348 - mean_squared_error: 0.8348 - val_loss: 0.6208 - val_mean_squared_error: 0.6208\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.7855 - mean_squared_error: 0.7855\n",
      "Epoch 00006: val_loss improved from 0.62083 to 0.58997, saving model to model.h5\n",
      "248/248 [==============================] - 0s 253us/sample - loss: 0.7536 - mean_squared_error: 0.7536 - val_loss: 0.5900 - val_mean_squared_error: 0.5900\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6055 - mean_squared_error: 0.6055\n",
      "Epoch 00007: val_loss improved from 0.58997 to 0.55598, saving model to model.h5\n",
      "248/248 [==============================] - 0s 258us/sample - loss: 0.6684 - mean_squared_error: 0.6684 - val_loss: 0.5560 - val_mean_squared_error: 0.5560\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.6835 - mean_squared_error: 0.6835\n",
      "Epoch 00008: val_loss improved from 0.55598 to 0.51942, saving model to model.h5\n",
      "248/248 [==============================] - 0s 251us/sample - loss: 0.5881 - mean_squared_error: 0.5881 - val_loss: 0.5194 - val_mean_squared_error: 0.5194\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4702 - mean_squared_error: 0.4702\n",
      "Epoch 00009: val_loss improved from 0.51942 to 0.47953, saving model to model.h5\n",
      "248/248 [==============================] - 0s 253us/sample - loss: 0.4907 - mean_squared_error: 0.4907 - val_loss: 0.4795 - val_mean_squared_error: 0.4795\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.4931 - mean_squared_error: 0.4931\n",
      "Epoch 00010: val_loss improved from 0.47953 to 0.43843, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.4275 - mean_squared_error: 0.4275 - val_loss: 0.4384 - val_mean_squared_error: 0.4384\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.3495 - mean_squared_error: 0.3495\n",
      "Epoch 00011: val_loss improved from 0.43843 to 0.39411, saving model to model.h5\n",
      "248/248 [==============================] - 0s 202us/sample - loss: 0.3282 - mean_squared_error: 0.3282 - val_loss: 0.3941 - val_mean_squared_error: 0.3941\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2924 - mean_squared_error: 0.2924\n",
      "Epoch 00012: val_loss improved from 0.39411 to 0.34843, saving model to model.h5\n",
      "248/248 [==============================] - 0s 230us/sample - loss: 0.2610 - mean_squared_error: 0.2610 - val_loss: 0.3484 - val_mean_squared_error: 0.3484\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.2165 - mean_squared_error: 0.2165\n",
      "Epoch 00013: val_loss improved from 0.34843 to 0.30343, saving model to model.h5\n",
      "248/248 [==============================] - 0s 273us/sample - loss: 0.2122 - mean_squared_error: 0.2122 - val_loss: 0.3034 - val_mean_squared_error: 0.3034\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1755 - mean_squared_error: 0.1755\n",
      "Epoch 00014: val_loss improved from 0.30343 to 0.26103, saving model to model.h5\n",
      "248/248 [==============================] - 0s 213us/sample - loss: 0.1664 - mean_squared_error: 0.1664 - val_loss: 0.2610 - val_mean_squared_error: 0.2610\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1455 - mean_squared_error: 0.1455\n",
      "Epoch 00015: val_loss improved from 0.26103 to 0.22280, saving model to model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 213us/sample - loss: 0.1505 - mean_squared_error: 0.1505 - val_loss: 0.2228 - val_mean_squared_error: 0.2228\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1025 - mean_squared_error: 0.1025\n",
      "Epoch 00016: val_loss improved from 0.22280 to 0.18944, saving model to model.h5\n",
      "248/248 [==============================] - 0s 418us/sample - loss: 0.1340 - mean_squared_error: 0.1340 - val_loss: 0.1894 - val_mean_squared_error: 0.1894\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1120 - mean_squared_error: 0.1120\n",
      "Epoch 00017: val_loss improved from 0.18944 to 0.16040, saving model to model.h5\n",
      "248/248 [==============================] - 0s 287us/sample - loss: 0.1141 - mean_squared_error: 0.1141 - val_loss: 0.1604 - val_mean_squared_error: 0.1604\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.1189 - mean_squared_error: 0.1189\n",
      "Epoch 00018: val_loss improved from 0.16040 to 0.13629, saving model to model.h5\n",
      "248/248 [==============================] - 0s 294us/sample - loss: 0.1016 - mean_squared_error: 0.1016 - val_loss: 0.1363 - val_mean_squared_error: 0.1363\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0838 - mean_squared_error: 0.0838\n",
      "Epoch 00019: val_loss improved from 0.13629 to 0.11628, saving model to model.h5\n",
      "248/248 [==============================] - 0s 297us/sample - loss: 0.0905 - mean_squared_error: 0.0905 - val_loss: 0.1163 - val_mean_squared_error: 0.1163\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0724 - mean_squared_error: 0.0724\n",
      "Epoch 00020: val_loss improved from 0.11628 to 0.09934, saving model to model.h5\n",
      "248/248 [==============================] - 0s 261us/sample - loss: 0.0683 - mean_squared_error: 0.0683 - val_loss: 0.0993 - val_mean_squared_error: 0.0993\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0624 - mean_squared_error: 0.0624\n",
      "Epoch 00021: val_loss improved from 0.09934 to 0.08567, saving model to model.h5\n",
      "248/248 [==============================] - 0s 386us/sample - loss: 0.0579 - mean_squared_error: 0.0579 - val_loss: 0.0857 - val_mean_squared_error: 0.0857\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0572 - mean_squared_error: 0.0572\n",
      "Epoch 00022: val_loss improved from 0.08567 to 0.07477, saving model to model.h5\n",
      "248/248 [==============================] - 0s 338us/sample - loss: 0.0593 - mean_squared_error: 0.0593 - val_loss: 0.0748 - val_mean_squared_error: 0.0748\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0508 - mean_squared_error: 0.0508\n",
      "Epoch 00023: val_loss improved from 0.07477 to 0.06598, saving model to model.h5\n",
      "248/248 [==============================] - 0s 334us/sample - loss: 0.0510 - mean_squared_error: 0.0510 - val_loss: 0.0660 - val_mean_squared_error: 0.0660\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0422 - mean_squared_error: 0.0422\n",
      "Epoch 00024: val_loss improved from 0.06598 to 0.05926, saving model to model.h5\n",
      "248/248 [==============================] - 0s 315us/sample - loss: 0.0443 - mean_squared_error: 0.0443 - val_loss: 0.0593 - val_mean_squared_error: 0.0593\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0531 - mean_squared_error: 0.0531\n",
      "Epoch 00025: val_loss improved from 0.05926 to 0.05430, saving model to model.h5\n",
      "248/248 [==============================] - 0s 330us/sample - loss: 0.0468 - mean_squared_error: 0.0468 - val_loss: 0.0543 - val_mean_squared_error: 0.0543\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0402 - mean_squared_error: 0.0402\n",
      "Epoch 00026: val_loss improved from 0.05430 to 0.05044, saving model to model.h5\n",
      "248/248 [==============================] - 0s 265us/sample - loss: 0.0448 - mean_squared_error: 0.0448 - val_loss: 0.0504 - val_mean_squared_error: 0.0504\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0564 - mean_squared_error: 0.0564\n",
      "Epoch 00027: val_loss improved from 0.05044 to 0.04761, saving model to model.h5\n",
      "248/248 [==============================] - 0s 269us/sample - loss: 0.0448 - mean_squared_error: 0.0448 - val_loss: 0.0476 - val_mean_squared_error: 0.0476\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0566 - mean_squared_error: 0.0566\n",
      "Epoch 00028: val_loss improved from 0.04761 to 0.04560, saving model to model.h5\n",
      "248/248 [==============================] - 0s 318us/sample - loss: 0.0428 - mean_squared_error: 0.0428 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0403 - mean_squared_error: 0.0403\n",
      "Epoch 00029: val_loss improved from 0.04560 to 0.04435, saving model to model.h5\n",
      "248/248 [==============================] - 0s 290us/sample - loss: 0.0441 - mean_squared_error: 0.0441 - val_loss: 0.0444 - val_mean_squared_error: 0.0444\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0358 - mean_squared_error: 0.0358\n",
      "Epoch 00030: val_loss improved from 0.04435 to 0.04366, saving model to model.h5\n",
      "248/248 [==============================] - 0s 253us/sample - loss: 0.0430 - mean_squared_error: 0.0430 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0413 - mean_squared_error: 0.0413\n",
      "Epoch 00031: val_loss improved from 0.04366 to 0.04328, saving model to model.h5\n",
      "248/248 [==============================] - 0s 245us/sample - loss: 0.0387 - mean_squared_error: 0.0387 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0532 - mean_squared_error: 0.0532\n",
      "Epoch 00032: val_loss improved from 0.04328 to 0.04316, saving model to model.h5\n",
      "248/248 [==============================] - 0s 241us/sample - loss: 0.0381 - mean_squared_error: 0.0381 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00033: val_loss improved from 0.04316 to 0.04307, saving model to model.h5\n",
      "248/248 [==============================] - 0s 230us/sample - loss: 0.0374 - mean_squared_error: 0.0374 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0404 - mean_squared_error: 0.0404\n",
      "Epoch 00034: val_loss improved from 0.04307 to 0.04307, saving model to model.h5\n",
      "248/248 [==============================] - 0s 253us/sample - loss: 0.0377 - mean_squared_error: 0.0377 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00035: val_loss did not improve from 0.04307\n",
      "248/248 [==============================] - 0s 137us/sample - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0313 - mean_squared_error: 0.0313\n",
      "Epoch 00036: val_loss did not improve from 0.04307\n",
      "248/248 [==============================] - 0s 97us/sample - loss: 0.0391 - mean_squared_error: 0.0391 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0476 - mean_squared_error: 0.0476\n",
      "Epoch 00037: val_loss did not improve from 0.04307\n",
      "248/248 [==============================] - 0s 101us/sample - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0432 - val_mean_squared_error: 0.0432\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0362 - mean_squared_error: 0.0362\n",
      "Epoch 00038: val_loss improved from 0.04307 to 0.04293, saving model to model.h5\n",
      "248/248 [==============================] - 0s 334us/sample - loss: 0.0367 - mean_squared_error: 0.0367 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0446 - mean_squared_error: 0.0446\n",
      "Epoch 00039: val_loss improved from 0.04293 to 0.04274, saving model to model.h5\n",
      "248/248 [==============================] - 0s 261us/sample - loss: 0.0388 - mean_squared_error: 0.0388 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0330 - mean_squared_error: 0.0330\n",
      "Epoch 00040: val_loss improved from 0.04274 to 0.04241, saving model to model.h5\n",
      "248/248 [==============================] - 0s 229us/sample - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00041: val_loss improved from 0.04241 to 0.04201, saving model to model.h5\n",
      "248/248 [==============================] - 0s 229us/sample - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0285 - mean_squared_error: 0.0285\n",
      "Epoch 00042: val_loss improved from 0.04201 to 0.04167, saving model to model.h5\n",
      "248/248 [==============================] - 0s 229us/sample - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0404 - mean_squared_error: 0.0404\n",
      "Epoch 00043: val_loss improved from 0.04167 to 0.04115, saving model to model.h5\n",
      "248/248 [==============================] - 0s 323us/sample - loss: 0.0349 - mean_squared_error: 0.0349 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0464 - mean_squared_error: 0.0464\n",
      "Epoch 00044: val_loss improved from 0.04115 to 0.04068, saving model to model.h5\n",
      "248/248 [==============================] - 0s 229us/sample - loss: 0.0341 - mean_squared_error: 0.0341 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0435 - mean_squared_error: 0.0435\n",
      "Epoch 00045: val_loss improved from 0.04068 to 0.04028, saving model to model.h5\n",
      "248/248 [==============================] - 0s 199us/sample - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0265 - mean_squared_error: 0.0265\n",
      "Epoch 00046: val_loss improved from 0.04028 to 0.04006, saving model to model.h5\n",
      "248/248 [==============================] - 0s 204us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00047: val_loss improved from 0.04006 to 0.03988, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.0365 - mean_squared_error: 0.0365 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0498 - mean_squared_error: 0.0498\n",
      "Epoch 00048: val_loss did not improve from 0.03988\n",
      "248/248 [==============================] - 0s 97us/sample - loss: 0.0365 - mean_squared_error: 0.0365 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00049: val_loss did not improve from 0.03988\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0353 - mean_squared_error: 0.0353 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0334 - mean_squared_error: 0.0334\n",
      "Epoch 00050: val_loss did not improve from 0.03988\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0400 - val_mean_squared_error: 0.0400\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0365 - mean_squared_error: 0.0365\n",
      "Epoch 00051: val_loss did not improve from 0.03988\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0298 - mean_squared_error: 0.0298\n",
      "Epoch 00052: val_loss improved from 0.03988 to 0.03979, saving model to model.h5\n",
      "248/248 [==============================] - 0s 287us/sample - loss: 0.0330 - mean_squared_error: 0.0330 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0479 - mean_squared_error: 0.0479\n",
      "Epoch 00053: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.0364\n",
      "Epoch 00054: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0369 - mean_squared_error: 0.0369 - val_loss: 0.0404 - val_mean_squared_error: 0.0404\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0356 - mean_squared_error: 0.0356\n",
      "Epoch 00055: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0365 - mean_squared_error: 0.0365 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00056: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 57/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0445 - mean_squared_error: 0.0445\n",
      "Epoch 00057: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0377 - mean_squared_error: 0.0377\n",
      "Epoch 00058: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0347 - mean_squared_error: 0.0347 - val_loss: 0.0405 - val_mean_squared_error: 0.0405\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0407 - mean_squared_error: 0.0407\n",
      "Epoch 00059: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0338 - mean_squared_error: 0.0338 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "Epoch 00060: val_loss did not improve from 0.03979\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0366 - mean_squared_error: 0.0366\n",
      "Epoch 00061: val_loss improved from 0.03979 to 0.03958, saving model to model.h5\n",
      "248/248 [==============================] - 0s 224us/sample - loss: 0.0333 - mean_squared_error: 0.0333 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0377 - mean_squared_error: 0.0377\n",
      "Epoch 00062: val_loss improved from 0.03958 to 0.03930, saving model to model.h5\n",
      "248/248 [==============================] - 0s 226us/sample - loss: 0.0364 - mean_squared_error: 0.0364 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0432 - mean_squared_error: 0.0432\n",
      "Epoch 00063: val_loss did not improve from 0.03930\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0433 - mean_squared_error: 0.0433\n",
      "Epoch 00064: val_loss improved from 0.03930 to 0.03911, saving model to model.h5\n",
      "248/248 [==============================] - 0s 231us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.0213\n",
      "Epoch 00065: val_loss improved from 0.03911 to 0.03896, saving model to model.h5\n",
      "248/248 [==============================] - 0s 211us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0338 - mean_squared_error: 0.0338\n",
      "Epoch 00066: val_loss improved from 0.03896 to 0.03877, saving model to model.h5\n",
      "248/248 [==============================] - 0s 211us/sample - loss: 0.0316 - mean_squared_error: 0.0316 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0360 - mean_squared_error: 0.0360\n",
      "Epoch 00067: val_loss did not improve from 0.03877\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00068: val_loss did not improve from 0.03877\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0391 - val_mean_squared_error: 0.0391\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Epoch 00069: val_loss did not improve from 0.03877\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0306 - mean_squared_error: 0.0306 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00070: val_loss did not improve from 0.03877\n",
      "248/248 [==============================] - 0s 90us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0348 - mean_squared_error: 0.0348\n",
      "Epoch 00071: val_loss did not improve from 0.03877\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0317 - mean_squared_error: 0.0317 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.0301\n",
      "Epoch 00072: val_loss did not improve from 0.03877\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0355 - mean_squared_error: 0.0355\n",
      "Epoch 00073: val_loss did not improve from 0.03877\n",
      "248/248 [==============================] - 0s 96us/sample - loss: 0.0319 - mean_squared_error: 0.0319 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00074: val_loss did not improve from 0.03877\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0390 - val_mean_squared_error: 0.0390\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00075: val_loss improved from 0.03877 to 0.03856, saving model to model.h5\n",
      "248/248 [==============================] - 0s 232us/sample - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00076: val_loss improved from 0.03856 to 0.03819, saving model to model.h5\n",
      "248/248 [==============================] - 0s 208us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0374 - mean_squared_error: 0.0374\n",
      "Epoch 00077: val_loss improved from 0.03819 to 0.03807, saving model to model.h5\n",
      "248/248 [==============================] - 0s 224us/sample - loss: 0.0327 - mean_squared_error: 0.0327 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0365 - mean_squared_error: 0.0365\n",
      "Epoch 00078: val_loss improved from 0.03807 to 0.03793, saving model to model.h5\n",
      "248/248 [==============================] - 0s 228us/sample - loss: 0.0337 - mean_squared_error: 0.0337 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.0248\n",
      "Epoch 00079: val_loss improved from 0.03793 to 0.03776, saving model to model.h5\n",
      "248/248 [==============================] - 0s 210us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00080: val_loss did not improve from 0.03776\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Epoch 00081: val_loss did not improve from 0.03776\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00082: val_loss did not improve from 0.03776\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00083: val_loss did not improve from 0.03776\n",
      "248/248 [==============================] - 0s 86us/sample - loss: 0.0315 - mean_squared_error: 0.0315 - val_loss: 0.0389 - val_mean_squared_error: 0.0389\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0369 - mean_squared_error: 0.0369\n",
      "Epoch 00084: val_loss did not improve from 0.03776\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 85/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00085: val_loss did not improve from 0.03776\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0285 - mean_squared_error: 0.0285 - val_loss: 0.0394 - val_mean_squared_error: 0.0394\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00086: val_loss did not improve from 0.03776\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 00087: val_loss did not improve from 0.03776\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0371 - mean_squared_error: 0.0371\n",
      "Epoch 00088: val_loss did not improve from 0.03776\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0399 - val_mean_squared_error: 0.0399\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00089: val_loss did not improve from 0.03776\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0305 - mean_squared_error: 0.0305 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.0283\n",
      "Epoch 00090: val_loss did not improve from 0.03776\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0392 - val_mean_squared_error: 0.0392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0325 - mean_squared_error: 0.0325\n",
      "Epoch 00091: val_loss did not improve from 0.03776\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0325 - mean_squared_error: 0.0325 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0412 - mean_squared_error: 0.0412\n",
      "Epoch 00092: val_loss did not improve from 0.03776\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0303 - mean_squared_error: 0.0303 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00093: val_loss improved from 0.03776 to 0.03776, saving model to model.h5\n",
      "248/248 [==============================] - 0s 217us/sample - loss: 0.0313 - mean_squared_error: 0.0313 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00094: val_loss improved from 0.03776 to 0.03742, saving model to model.h5\n",
      "248/248 [==============================] - 0s 219us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00095: val_loss improved from 0.03742 to 0.03737, saving model to model.h5\n",
      "248/248 [==============================] - 0s 214us/sample - loss: 0.0300 - mean_squared_error: 0.0300 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0275 - mean_squared_error: 0.0275\n",
      "Epoch 00096: val_loss did not improve from 0.03737\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0394 - mean_squared_error: 0.0394\n",
      "Epoch 00097: val_loss did not improve from 0.03737\n",
      "248/248 [==============================] - 0s 82us/sample - loss: 0.0282 - mean_squared_error: 0.0282 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.0262\n",
      "Epoch 00098: val_loss did not improve from 0.03737\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0276 - mean_squared_error: 0.0276 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00099: val_loss did not improve from 0.03737\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Epoch 00100: val_loss did not improve from 0.03737\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0324 - mean_squared_error: 0.0324 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 101/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0380 - mean_squared_error: 0.0380\n",
      "Epoch 00101: val_loss did not improve from 0.03737\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 102/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 00102: val_loss improved from 0.03737 to 0.03721, saving model to model.h5\n",
      "248/248 [==============================] - 0s 246us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 103/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00103: val_loss improved from 0.03721 to 0.03699, saving model to model.h5\n",
      "248/248 [==============================] - 0s 229us/sample - loss: 0.0293 - mean_squared_error: 0.0293 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 104/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 00104: val_loss improved from 0.03699 to 0.03685, saving model to model.h5\n",
      "248/248 [==============================] - 0s 253us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 105/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0342 - mean_squared_error: 0.0342\n",
      "Epoch 00105: val_loss did not improve from 0.03685\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0253 - mean_squared_error: 0.0253 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 106/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0369 - mean_squared_error: 0.0369\n",
      "Epoch 00106: val_loss did not improve from 0.03685\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0270 - mean_squared_error: 0.0270 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 107/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0388 - mean_squared_error: 0.0388\n",
      "Epoch 00107: val_loss did not improve from 0.03685\n",
      "248/248 [==============================] - 0s 82us/sample - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 108/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00108: val_loss did not improve from 0.03685\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0272 - mean_squared_error: 0.0272 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 109/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00109: val_loss did not improve from 0.03685\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 110/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 00110: val_loss did not improve from 0.03685\n",
      "248/248 [==============================] - 0s 97us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 111/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0350 - mean_squared_error: 0.0350\n",
      "Epoch 00111: val_loss did not improve from 0.03685\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0280 - mean_squared_error: 0.0280 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 112/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0386 - mean_squared_error: 0.0386\n",
      "Epoch 00112: val_loss did not improve from 0.03685\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0296 - mean_squared_error: 0.0296 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 113/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00113: val_loss did not improve from 0.03685\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0309 - mean_squared_error: 0.0309 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 114/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00114: val_loss did not improve from 0.03685\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0275 - mean_squared_error: 0.0275 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 115/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.0258\n",
      "Epoch 00115: val_loss did not improve from 0.03685\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 116/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0334 - mean_squared_error: 0.0334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00116: val_loss did not improve from 0.03685\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 117/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00117: val_loss did not improve from 0.03685\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 118/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0231 - mean_squared_error: 0.0231\n",
      "Epoch 00118: val_loss did not improve from 0.03685\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 119/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0409 - mean_squared_error: 0.0409\n",
      "Epoch 00119: val_loss did not improve from 0.03685\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 120/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00120: val_loss did not improve from 0.03685\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0281 - mean_squared_error: 0.0281 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 121/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0154 - mean_squared_error: 0.0154\n",
      "Epoch 00121: val_loss did not improve from 0.03685\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 122/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0156 - mean_squared_error: 0.0156\n",
      "Epoch 00122: val_loss did not improve from 0.03685\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 123/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00123: val_loss improved from 0.03685 to 0.03664, saving model to model.h5\n",
      "248/248 [==============================] - 0s 208us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0366 - val_mean_squared_error: 0.0366\n",
      "Epoch 124/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 00124: val_loss improved from 0.03664 to 0.03642, saving model to model.h5\n",
      "248/248 [==============================] - 0s 217us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n",
      "Epoch 125/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00125: val_loss did not improve from 0.03642\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0366 - val_mean_squared_error: 0.0366\n",
      "Epoch 126/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00126: val_loss did not improve from 0.03642\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 127/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
      "Epoch 00127: val_loss did not improve from 0.03642\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0278 - mean_squared_error: 0.0278 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 128/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0411 - mean_squared_error: 0.0411\n",
      "Epoch 00128: val_loss did not improve from 0.03642\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0292 - mean_squared_error: 0.0292 - val_loss: 0.0367 - val_mean_squared_error: 0.0367\n",
      "Epoch 129/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Epoch 00129: val_loss did not improve from 0.03642\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0289 - mean_squared_error: 0.0289 - val_loss: 0.0365 - val_mean_squared_error: 0.0365\n",
      "Epoch 130/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00130: val_loss improved from 0.03642 to 0.03627, saving model to model.h5\n",
      "248/248 [==============================] - 0s 209us/sample - loss: 0.0283 - mean_squared_error: 0.0283 - val_loss: 0.0363 - val_mean_squared_error: 0.0363\n",
      "Epoch 131/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00131: val_loss did not improve from 0.03627\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0363 - val_mean_squared_error: 0.0363\n",
      "Epoch 132/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0314 - mean_squared_error: 0.0314\n",
      "Epoch 00132: val_loss did not improve from 0.03627\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0366 - val_mean_squared_error: 0.0366\n",
      "Epoch 133/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0335 - mean_squared_error: 0.0335\n",
      "Epoch 00133: val_loss did not improve from 0.03627\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0288 - mean_squared_error: 0.0288 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 134/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00134: val_loss did not improve from 0.03627\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 135/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0106 - mean_squared_error: 0.0106\n",
      "Epoch 00135: val_loss did not improve from 0.03627\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 136/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.0226\n",
      "Epoch 00136: val_loss did not improve from 0.03627\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 137/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0307 - mean_squared_error: 0.0307\n",
      "Epoch 00137: val_loss did not improve from 0.03627\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 138/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0304 - mean_squared_error: 0.0304\n",
      "Epoch 00138: val_loss did not improve from 0.03627\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 139/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00139: val_loss did not improve from 0.03627\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 140/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00140: val_loss did not improve from 0.03627\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 141/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Epoch 00141: val_loss did not improve from 0.03627\n",
      "248/248 [==============================] - 0s 93us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0383 - val_mean_squared_error: 0.0383\n",
      "Epoch 142/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 00142: val_loss did not improve from 0.03627\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 143/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 00143: val_loss did not improve from 0.03627\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 144/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0104 - mean_squared_error: 0.0104\n",
      "Epoch 00144: val_loss did not improve from 0.03627\n",
      "248/248 [==============================] - 0s 101us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 145/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00145: val_loss did not improve from 0.03627\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 146/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0272 - mean_squared_error: 0.0272\n",
      "Epoch 00146: val_loss did not improve from 0.03627\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 147/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00147: val_loss did not improve from 0.03627\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 148/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0290 - mean_squared_error: 0.0290\n",
      "Epoch 00148: val_loss did not improve from 0.03627\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 149/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.0199\n",
      "Epoch 00149: val_loss did not improve from 0.03627\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
      "Epoch 150/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 00150: val_loss did not improve from 0.03627\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0383 - val_mean_squared_error: 0.0383\n",
      "Epoch 151/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 00151: val_loss did not improve from 0.03627\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 152/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0323 - mean_squared_error: 0.0323\n",
      "Epoch 00152: val_loss did not improve from 0.03627\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 153/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00153: val_loss did not improve from 0.03627\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 154/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 00154: val_loss improved from 0.03627 to 0.03625, saving model to model.h5\n",
      "248/248 [==============================] - 0s 205us/sample - loss: 0.0269 - mean_squared_error: 0.0269 - val_loss: 0.0363 - val_mean_squared_error: 0.0363\n",
      "Epoch 155/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00155: val_loss improved from 0.03625 to 0.03585, saving model to model.h5\n",
      "248/248 [==============================] - 0s 214us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0358 - val_mean_squared_error: 0.0358\n",
      "Epoch 156/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0160 - mean_squared_error: 0.0160\n",
      "Epoch 00156: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0364 - val_mean_squared_error: 0.0364\n",
      "Epoch 157/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 00157: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0267 - mean_squared_error: 0.0267 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 158/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00158: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 100us/sample - loss: 0.0228 - mean_squared_error: 0.0228 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 159/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Epoch 00159: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 97us/sample - loss: 0.0268 - mean_squared_error: 0.0268 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 160/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 00160: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0245 - mean_squared_error: 0.0245 - val_loss: 0.0380 - val_mean_squared_error: 0.0380\n",
      "Epoch 161/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00161: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 87us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 162/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0309 - mean_squared_error: 0.0309\n",
      "Epoch 00162: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0373 - val_mean_squared_error: 0.0373\n",
      "Epoch 163/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.0246\n",
      "Epoch 00163: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0239 - mean_squared_error: 0.0239 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 164/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0261 - mean_squared_error: 0.0261\n",
      "Epoch 00164: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0365 - val_mean_squared_error: 0.0365\n",
      "Epoch 165/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00165: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0365 - val_mean_squared_error: 0.0365\n",
      "Epoch 166/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 00166: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 167/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00167: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.0297\n",
      "Epoch 00168: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 169/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0328 - mean_squared_error: 0.0328\n",
      "Epoch 00169: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 170/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.0306\n",
      "Epoch 00170: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0260 - mean_squared_error: 0.0260 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 171/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0101 - mean_squared_error: 0.0101\n",
      "Epoch 00171: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0388 - val_mean_squared_error: 0.0388\n",
      "Epoch 172/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0274 - mean_squared_error: 0.0274\n",
      "Epoch 00172: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0375 - val_mean_squared_error: 0.0375\n",
      "Epoch 173/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00173: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 174/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00174: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0370 - val_mean_squared_error: 0.0370\n",
      "Epoch 175/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0351 - mean_squared_error: 0.0351\n",
      "Epoch 00175: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 75us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 176/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0319 - mean_squared_error: 0.0319\n",
      "Epoch 00176: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 177/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0181 - mean_squared_error: 0.0181\n",
      "Epoch 00177: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 178/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0521 - mean_squared_error: 0.0521\n",
      "Epoch 00178: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 179/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0157 - mean_squared_error: 0.0157\n",
      "Epoch 00179: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0265 - mean_squared_error: 0.0265 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 180/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 00180: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 181/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0135 - mean_squared_error: 0.0135\n",
      "Epoch 00181: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0214 - mean_squared_error: 0.0214 - val_loss: 0.0365 - val_mean_squared_error: 0.0365\n",
      "Epoch 182/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.0163\n",
      "Epoch 00182: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0368 - val_mean_squared_error: 0.0368\n",
      "Epoch 183/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.0255\n",
      "Epoch 00183: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0379 - val_mean_squared_error: 0.0379\n",
      "Epoch 184/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 00184: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 185/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.0193\n",
      "Epoch 00185: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0383 - val_mean_squared_error: 0.0383\n",
      "Epoch 186/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 00186: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0378 - val_mean_squared_error: 0.0378\n",
      "Epoch 187/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 00187: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 188/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.0208\n",
      "Epoch 00188: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0369 - val_mean_squared_error: 0.0369\n",
      "Epoch 189/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0118 - mean_squared_error: 0.0118\n",
      "Epoch 00189: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 0.0376 - val_mean_squared_error: 0.0376\n",
      "Epoch 190/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Epoch 00190: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 97us/sample - loss: 0.0261 - mean_squared_error: 0.0261 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 191/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00191: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0383 - val_mean_squared_error: 0.0383\n",
      "Epoch 192/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00192: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0251 - mean_squared_error: 0.0251 - val_loss: 0.0383 - val_mean_squared_error: 0.0383\n",
      "Epoch 193/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00193: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0381 - val_mean_squared_error: 0.0381\n",
      "Epoch 194/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.0266\n",
      "Epoch 00194: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0250 - mean_squared_error: 0.0250 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 195/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 00195: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 196/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0223 - mean_squared_error: 0.0223\n",
      "Epoch 00196: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0384 - val_mean_squared_error: 0.0384\n",
      "Epoch 197/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "Epoch 00197: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 198/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0136 - mean_squared_error: 0.0136\n",
      "Epoch 00198: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0254 - mean_squared_error: 0.0254 - val_loss: 0.0377 - val_mean_squared_error: 0.0377\n",
      "Epoch 199/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0173 - mean_squared_error: 0.0173\n",
      "Epoch 00199: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0374 - val_mean_squared_error: 0.0374\n",
      "Epoch 200/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 00200: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 201/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 00201: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0371 - val_mean_squared_error: 0.0371\n",
      "Epoch 202/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.0212\n",
      "Epoch 00202: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0246 - mean_squared_error: 0.0246 - val_loss: 0.0372 - val_mean_squared_error: 0.0372\n",
      "Epoch 203/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 00203: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 70us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0382 - val_mean_squared_error: 0.0382\n",
      "Epoch 204/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.0207\n",
      "Epoch 00204: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0393 - val_mean_squared_error: 0.0393\n",
      "Epoch 205/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0184 - mean_squared_error: 0.0184\n",
      "Epoch 00205: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0395 - val_mean_squared_error: 0.0395\n",
      "Epoch 206/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0251 - mean_squared_error: 0.0251\n",
      "Epoch 00206: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 207/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.0229\n",
      "Epoch 00207: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0398 - val_mean_squared_error: 0.0398\n",
      "Epoch 208/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 00208: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0264 - mean_squared_error: 0.0264 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 209/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00209: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0218 - mean_squared_error: 0.0218 - val_loss: 0.0397 - val_mean_squared_error: 0.0397\n",
      "Epoch 210/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0152 - mean_squared_error: 0.0152\n",
      "Epoch 00210: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0396 - val_mean_squared_error: 0.0396\n",
      "Epoch 211/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.0222\n",
      "Epoch 00211: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 212/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.0271\n",
      "Epoch 00212: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 213/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.0232\n",
      "Epoch 00213: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 82us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 214/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00214: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0241 - mean_squared_error: 0.0241 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 215/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 00215: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 92us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 216/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0138 - mean_squared_error: 0.0138\n",
      "Epoch 00216: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 217/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00217: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0414 - val_mean_squared_error: 0.0414\n",
      "Epoch 218/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0231 - mean_squared_error: 0.0231\n",
      "Epoch 00218: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 219/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.0247\n",
      "Epoch 00219: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 220/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Epoch 00220: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 221/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.0239\n",
      "Epoch 00221: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 222/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 00222: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 73us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0406 - val_mean_squared_error: 0.0406\n",
      "Epoch 223/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0135 - mean_squared_error: 0.0135\n",
      "Epoch 00223: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 224/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 00224: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0423 - val_mean_squared_error: 0.0423\n",
      "Epoch 225/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0327 - mean_squared_error: 0.0327\n",
      "Epoch 00225: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 71us/sample - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0427 - val_mean_squared_error: 0.0427\n",
      "Epoch 226/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00226: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0223 - mean_squared_error: 0.0223 - val_loss: 0.0422 - val_mean_squared_error: 0.0422\n",
      "Epoch 227/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 00227: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 87us/sample - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0412 - val_mean_squared_error: 0.0412\n",
      "Epoch 228/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 00228: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 229/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0220 - mean_squared_error: 0.0220\n",
      "Epoch 00229: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0403 - val_mean_squared_error: 0.0403\n",
      "Epoch 230/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0260 - mean_squared_error: 0.0260\n",
      "Epoch 00230: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 81us/sample - loss: 0.0219 - mean_squared_error: 0.0219 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 231/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.0218\n",
      "Epoch 00231: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 78us/sample - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0424 - val_mean_squared_error: 0.0424\n",
      "Epoch 232/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 00232: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 233/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0269 - mean_squared_error: 0.0269\n",
      "Epoch 00233: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 234/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0245 - mean_squared_error: 0.0245\n",
      "Epoch 00234: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0429 - val_mean_squared_error: 0.0429\n",
      "Epoch 235/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0311 - mean_squared_error: 0.0311\n",
      "Epoch 00235: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 74us/sample - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0420 - val_mean_squared_error: 0.0420\n",
      "Epoch 236/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00236: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0409 - val_mean_squared_error: 0.0409\n",
      "Epoch 237/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.0210\n",
      "Epoch 00237: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
      "Epoch 238/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.0244\n",
      "Epoch 00238: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 239/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00239: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.0413 - val_mean_squared_error: 0.0413\n",
      "Epoch 240/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0111 - mean_squared_error: 0.0111\n",
      "Epoch 00240: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 79us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0425 - val_mean_squared_error: 0.0425\n",
      "Epoch 241/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0176 - mean_squared_error: 0.0176\n",
      "Epoch 00241: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 72us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0443 - val_mean_squared_error: 0.0443\n",
      "Epoch 242/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0120 - mean_squared_error: 0.0120\n",
      "Epoch 00242: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0450 - val_mean_squared_error: 0.0450\n",
      "Epoch 243/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0273 - mean_squared_error: 0.0273\n",
      "Epoch 00243: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0452 - val_mean_squared_error: 0.0452\n",
      "Epoch 244/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0295 - mean_squared_error: 0.0295\n",
      "Epoch 00244: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 77us/sample - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0445 - val_mean_squared_error: 0.0445\n",
      "Epoch 245/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 00245: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 76us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0430 - val_mean_squared_error: 0.0430\n",
      "Epoch 246/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0317 - mean_squared_error: 0.0317\n",
      "Epoch 00246: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0240 - mean_squared_error: 0.0240 - val_loss: 0.0417 - val_mean_squared_error: 0.0417\n",
      "Epoch 247/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.0236\n",
      "Epoch 00247: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 97us/sample - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0410 - val_mean_squared_error: 0.0410\n",
      "Epoch 248/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.0198\n",
      "Epoch 00248: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 85us/sample - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
      "Epoch 249/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 00249: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 95us/sample - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0416 - val_mean_squared_error: 0.0416\n",
      "Epoch 250/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0093 - mean_squared_error: 0.0093\n",
      "Epoch 00250: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0435 - val_mean_squared_error: 0.0435\n",
      "Epoch 251/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Epoch 00251: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 88us/sample - loss: 0.0215 - mean_squared_error: 0.0215 - val_loss: 0.0454 - val_mean_squared_error: 0.0454\n",
      "Epoch 252/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0305 - mean_squared_error: 0.0305\n",
      "Epoch 00252: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0243 - mean_squared_error: 0.0243 - val_loss: 0.0456 - val_mean_squared_error: 0.0456\n",
      "Epoch 253/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.0214\n",
      "Epoch 00253: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 84us/sample - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0448 - val_mean_squared_error: 0.0448\n",
      "Epoch 254/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 00254: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 87us/sample - loss: 0.0225 - mean_squared_error: 0.0225 - val_loss: 0.0451 - val_mean_squared_error: 0.0451\n",
      "Epoch 255/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 00255: val_loss did not improve from 0.03585\n",
      "248/248 [==============================] - 0s 80us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0449 - val_mean_squared_error: 0.0449\n"
     ]
    }
   ],
   "source": [
    "test_pred_lstm_gru  = []\n",
    "\n",
    "\n",
    "for fold in range(trials):\n",
    "    print(f'Running trial {fold+1}')\n",
    "    print(X_train.shape)\n",
    "    \n",
    "    X_t_reshaped   = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_val_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "\n",
    "    check = ModelCheckpoint('model.h5', verbose=1, save_best_only=True)\n",
    "\n",
    "    early = EarlyStopping(patience=100)\n",
    "\n",
    "    optimizer = Adam(lr=0.00001)\n",
    "    model = Sequential()\n",
    "    model.add(GRU(50,return_sequences= True, input_shape=(X_t_reshaped.shape[1], X_t_reshaped.shape[2])))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(50,return_sequences= False))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "# validation_split = 0.2\n",
    "    model.compile(loss='mse', optimizer='Adam', metrics=['mse'])\n",
    "\n",
    "    \n",
    "\n",
    "    history = model.fit(X_t_reshaped, \n",
    "                    y_train, \n",
    "                    validation_split = 0.2,\n",
    "                    epochs=1000, \n",
    "                    batch_size=96, \n",
    "                    verbose=1, callbacks=[check, early])\n",
    "    X_val_reshaped = X_val_reshaped.reshape(X_test.shape[0], 6)\n",
    "\n",
    "    #running function\n",
    "    test_forecast = X_test\n",
    "    y_lstm_gru = forecast(model, test_forecast, gwl, steps_ahead)\n",
    "    y_lstm_gru = np.array(y_lstm_gru)\n",
    "    \n",
    "    #metrics for test\n",
    "    test_pred_lstm_gru.append(y_lstm_gru)\n",
    "    mse_lstm_gru = mean_squared_error(y_test, y_lstm_gru)\n",
    "    r2_lstm_gru = r2_score(y_test, y_lstm_gru)\n",
    "    lstm_gru_ave_r2.append(r2_lstm_gru)\n",
    "    lstm_gru_ave_mse.append(mse_lstm_gru)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f0cc08e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7871948994664498,\n",
       " 0.8090611698998873,\n",
       " 0.8211632786525459,\n",
       " 0.8149046903706346,\n",
       " 0.805680859400021,\n",
       " 0.8373312803618187,\n",
       " 0.8233821621132185,\n",
       " 0.7637981195607422,\n",
       " 0.7932581666983907,\n",
       " 0.8124262760369946]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_gru_r2_new = ave(lstm_gru_ave_r2)\n",
    "lstm_gru_mse_new = ave(lstm_gru_ave_mse)\n",
    "lstm_gru_ave_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ebdfad",
   "metadata": {},
   "source": [
    "## DBN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dbdbe52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310, 16) (310, 1)\n",
      "(61, 16) (61, 1)\n"
     ]
    }
   ],
   "source": [
    "Y = Y.reshape(-1,1)\n",
    "tss = TimeSeriesSplit(n_splits = 5)\n",
    "for train_index, test_index in tss.split(X):\n",
    "    X_train, X_test = X[train_index, :], X[test_index,:]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "\n",
    "dummyXtrain = pd.DataFrame(X_train)\n",
    "dummyXtrain.columns = ['a','b','c','d','e','f', 'g']\n",
    "X_train = np.array(dummyXtrain.drop(['g'], axis = 1).reset_index(drop=True))\n",
    "\n",
    "dummyXtest = pd.DataFrame(X_test)\n",
    "dummyXtest.columns = ['a','b','c','d','e','f', 'g']\n",
    "X_test = np.array(dummyXtest.drop(['g'], axis = 1).reset_index(drop=True))\n",
    "\n",
    "X_train = regress.transform(X_train)\n",
    "X_test = regress.transform(X_test)\n",
    "\n",
    "print (X_train.shape, y_train.shape)\n",
    "print (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8273f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_hybrid(model, test_forecast, gwl, steps_ahead):\n",
    "    y_pred = []\n",
    "    current_output = gwl\n",
    "    for step in range(steps_ahead):\n",
    "        #print(test_forecast[step])\n",
    "        input_test = np.concatenate([test_forecast[step][:-1], [current_output]])\n",
    "        #input_test = [test_forecast[0][0][:-1], gwl]\n",
    "        #print(input_test)\n",
    "        pred = model.predict(input_test.reshape(1,1,X_test.shape[1]))\n",
    "        y_pred.append(pred[0][0])\n",
    "        #print(pred)\n",
    "        current_output = pred[0][0]\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b823b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running trial 1\n",
      "(310, 16)\n",
      "Train on 248 samples, validate on 62 samples\n",
      "Epoch 1/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00001: val_loss did not improve from inf\n",
      "248/248 [==============================] - 2s 7ms/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 2/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00002: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 56us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 3/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00003: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 69us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 4/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00004: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 60us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 5/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00005: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 64us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 6/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00006: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 64us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 7/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00007: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 72us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 8/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00008: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 76us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 9/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00009: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 66us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 10/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00010: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 64us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 11/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00011: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 75us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 12/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00012: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 65us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 13/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00013: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 62us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 14/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00014: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 68us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 15/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00015: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 64us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 16/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00016: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 56us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 17/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00017: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 62us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 18/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00018: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 60us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 19/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00019: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 56us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 20/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00020: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 60us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 21/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00021: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 56us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 22/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00022: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 61us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 23/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00023: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 60us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 24/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00024: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 56us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 25/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00025: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 53us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 26/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00026: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 69us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 27/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00027: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 60us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 28/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00028: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 61us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00029: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 64us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 30/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00030: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 69us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 31/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00031: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 64us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 32/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00032: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 69us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 33/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00033: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 64us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 34/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00034: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 60us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 35/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00035: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 64us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 36/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00036: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 60us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 37/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00037: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 68us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 38/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00038: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 58us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 39/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00039: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 61us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 40/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00040: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 56us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 41/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00041: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 64us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 42/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00042: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 64us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 43/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00043: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 64us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 44/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00044: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 60us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 45/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00045: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 64us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 46/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00046: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 64us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 47/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00047: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 60us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 48/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00048: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 84us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 49/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00049: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 64us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 50/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00050: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 68us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 51/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00051: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 63us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 52/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00052: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 76us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 53/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00053: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 68us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 54/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00054: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 64us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 55/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00055: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 73us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 56/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00056: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 68us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 57/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00057: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 72us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 58/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00058: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 72us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 59/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00059: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 72us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 60/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00060: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 72us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 61/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00061: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 76us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 62/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00062: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 68us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 63/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00063: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 68us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 64/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00064: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 84us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 65/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00065: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 75us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 66/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00066: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 68us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 67/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00067: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 84us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 68/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00068: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 72us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 69/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00069: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 68us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 70/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00070: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 68us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 71/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00071: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 73us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 72/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00072: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 68us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 73/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00073: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 64us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 74/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00074: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 76us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 75/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00075: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 80us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 76/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00076: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 77us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 77/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00077: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 68us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 78/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00078: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 73us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 79/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00079: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 64us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 80/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00080: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 66us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 81/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00081: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 68us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 82/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00082: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 76us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 83/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00083: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 72us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 84/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00084: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 72us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 85/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00085: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 90us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 86/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00086: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 80us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 87/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00087: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 72us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 88/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00088: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 72us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 89/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00089: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 70us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 90/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00090: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 64us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 91/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00091: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 72us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 92/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00092: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 68us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 93/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00093: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 64us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 94/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00094: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 68us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 95/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00095: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 72us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 96/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00096: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 64us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 97/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00097: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 68us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 98/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00098: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 68us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 99/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00099: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 68us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 100/1000\n",
      " 96/248 [==========>...................] - ETA: 0s - loss: nan - mean_squared_error: nan\n",
      "Epoch 00100: val_loss did not improve from inf\n",
      "248/248 [==============================] - 0s 64us/sample - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-23-fc72b44e8e56>\", line 45, in <module>\n",
      "    mse_dbn_lstm = mean_squared_error(y_test, dbn_lstm_y)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 336, in mean_squared_error\n",
      "    y_true, y_pred, multioutput)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 90, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 664, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 106, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ValueError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-23-fc72b44e8e56>\", line 45, in <module>\n",
      "    mse_dbn_lstm = mean_squared_error(y_test, dbn_lstm_y)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 336, in mean_squared_error\n",
      "    y_true, y_pred, multioutput)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 90, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 664, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 106, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ValueError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3360, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1193, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-23-fc72b44e8e56>\", line 45, in <module>\n",
      "    mse_dbn_lstm = mean_squared_error(y_test, dbn_lstm_y)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 336, in mean_squared_error\n",
      "    y_true, y_pred, multioutput)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 90, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 664, in check_array\n",
      "    allow_nan=force_all_finite == 'allow-nan')\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 106, in _assert_all_finite\n",
      "    msg_dtype if msg_dtype is not None else X.dtype)\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ValueError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3360, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1193, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2895, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3072, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3282, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1211, in structured_traceback\n",
      "    chained_exceptions_tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n"
     ]
    }
   ],
   "source": [
    "# rounding off\n",
    "Round = 3      \n",
    "\n",
    "test_pred_dbn_lstm  = [] \n",
    "\n",
    "for fold in range(trials):\n",
    "    print(f'Running trial {fold+1}')\n",
    "    print(X_train.shape)\n",
    "    \n",
    "    X_t_reshaped   = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_val_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "    \n",
    "    steps_ahead = X_test.shape[0]\n",
    "\n",
    "    check = ModelCheckpoint('model.h5', verbose=1, save_best_only=True)\n",
    "\n",
    "    early = EarlyStopping(patience=100)\n",
    "\n",
    "    optimizer = Adam(lr=0.01)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(X_t_reshaped.shape[1], X_t_reshaped.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(loss='mse', optimizer='Adam', metrics=['mse'])\n",
    "\n",
    "    \n",
    "\n",
    "    history = model.fit(X_t_reshaped, \n",
    "                    y_train, \n",
    "                    validation_split = 0.2,\n",
    "                    epochs=1000, \n",
    "                    batch_size=96, \n",
    "                    verbose=1, callbacks=[check, early])\n",
    "    X_val_reshaped = X_val_reshaped.reshape(X_test.shape[0], X_test.shape[1])\n",
    "\n",
    "    #running function\n",
    "    test_forecast = X_test\n",
    "    dbn_lstm_y = forecast_hybrid(model, test_forecast, gwl, steps_ahead)\n",
    "    dbn_lstm_y = np.array(dbn_lstm_y)\n",
    "    \n",
    "    #metrics for test\n",
    "    test_pred_dbn_lstm.append(dbn_lstm_y)\n",
    "    mse_dbn_lstm = mean_squared_error(y_test, dbn_lstm_y)\n",
    "    r2_dbn_lstm = r2_score(y_test, dbn_lstm_y)\n",
    "    dbn_lstm_ave_r2.append(r2_dbn_lstm)\n",
    "    dbn_lstm_ave_mse.append(mse_dbn_lstm)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991e7dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-23-05f085e4e823>\", line 1, in <module>\n",
      "    dbn_lstm_r2_new = ave(dbn_lstm_ave_r2)\n",
      "  File \"<ipython-input-5-0299d8b8e94b>\", line 2, in ave\n",
      "    return sum(lis)/ len(lis)\n",
      "ZeroDivisionError: division by zero\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ZeroDivisionError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "ERROR! Session/line number was not unique in database. History logging moved to new session 1923\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-23-05f085e4e823>\", line 1, in <module>\n",
      "    dbn_lstm_r2_new = ave(dbn_lstm_ave_r2)\n",
      "  File \"<ipython-input-5-0299d8b8e94b>\", line 2, in ave\n",
      "    return sum(lis)/ len(lis)\n",
      "ZeroDivisionError: division by zero\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ZeroDivisionError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3360, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1193, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-23-05f085e4e823>\", line 1, in <module>\n",
      "    dbn_lstm_r2_new = ave(dbn_lstm_ave_r2)\n",
      "  File \"<ipython-input-5-0299d8b8e94b>\", line 2, in ave\n",
      "    return sum(lis)/ len(lis)\n",
      "ZeroDivisionError: division by zero\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ZeroDivisionError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3360, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1193, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2895, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3072, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3282, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1211, in structured_traceback\n",
      "    chained_exceptions_tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n"
     ]
    }
   ],
   "source": [
    "dbn_lstm_r2_new = ave(dbn_lstm_ave_r2)\n",
    "dbn_lstm_mse_new = ave(dbn_lstm_ave_mse)\n",
    "dbn_lstm_ave_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "704dcff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 1924\n"
     ]
    }
   ],
   "source": [
    "def fix(year):\n",
    "    s = year.split('-')\n",
    "    new_year = s[0]\n",
    "    return new_year\n",
    "\n",
    "# dummyXtest['g']\n",
    "df_train = dummyXtrain['g'].apply(lambda x : fix(x))\n",
    "year_train = np.array(df_train.tolist())\n",
    "\n",
    "df_test = dummyXtest['g'].apply(lambda x : fix(x))\n",
    "year_test = np.array(df_test.tolist())\n",
    "len(year_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6237526d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_test_gwls</th>\n",
       "      <th>LSTM_gwls</th>\n",
       "      <th>GRU_gwls</th>\n",
       "      <th>FFNN_gwls</th>\n",
       "      <th>LSTM_GRU_gwls</th>\n",
       "      <th>DBN_LSTM_gwls</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>24.368131</td>\n",
       "      <td>19.702671</td>\n",
       "      <td>19.660814</td>\n",
       "      <td>19.684308</td>\n",
       "      <td>19.729442</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>24.400000</td>\n",
       "      <td>19.680039</td>\n",
       "      <td>19.628282</td>\n",
       "      <td>19.671406</td>\n",
       "      <td>19.701585</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>19.682169</td>\n",
       "      <td>19.630466</td>\n",
       "      <td>19.686855</td>\n",
       "      <td>19.697535</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>22.970000</td>\n",
       "      <td>19.637164</td>\n",
       "      <td>19.599673</td>\n",
       "      <td>19.642054</td>\n",
       "      <td>19.648734</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>22.955770</td>\n",
       "      <td>19.525364</td>\n",
       "      <td>19.515665</td>\n",
       "      <td>19.518522</td>\n",
       "      <td>19.529649</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual_test_gwls  LSTM_gwls   GRU_gwls  FFNN_gwls  LSTM_GRU_gwls  \\\n",
       "y                                                                        \n",
       "1999         24.368131  19.702671  19.660814  19.684308      19.729442   \n",
       "2000         24.400000  19.680039  19.628282  19.671406      19.701585   \n",
       "2000         24.000000  19.682169  19.630466  19.686855      19.697535   \n",
       "2000         22.970000  19.637164  19.599673  19.642054      19.648734   \n",
       "2000         22.955770  19.525364  19.515665  19.518522      19.529649   \n",
       "\n",
       "      DBN_LSTM_gwls  \n",
       "y                    \n",
       "1999            NaN  \n",
       "2000            NaN  \n",
       "2000            NaN  \n",
       "2000            NaN  \n",
       "2000            NaN  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "import statistics\n",
    "\n",
    "def extract(lst):\n",
    "    return [[el] for el in lst]\n",
    "\n",
    "df_fake = df.copy()\n",
    "n = len(test_pred_dbn_lstm[0])\n",
    "\n",
    "pred_lstm = [0] * n\n",
    "pred_gru = [0] * n\n",
    "pred_ffnn = [0] * n\n",
    "pred_lstm_gru = [0] * n\n",
    "pred_dbn_lstm = [0] * n\n",
    "\n",
    "\n",
    "for i in range(len(test_pred_dbn_lstm)):\n",
    "    y_lstm = test_pred_lstm[i].tolist()\n",
    "    y_gru = test_pred_gru[i].tolist()\n",
    "    y_ffnn = test_pred_ffnn[i].tolist()\n",
    "    y_lstm_gru = test_pred_lstm_gru[i].tolist()\n",
    "    y_dbn_lstm = test_pred_dbn_lstm[i].tolist()\n",
    "    \n",
    "    pred_lstm = list( map(add, pred_lstm, y_lstm))\n",
    "    pred_lstm_gru = list( map(add, pred_lstm_gru, y_lstm_gru))\n",
    "    pred_dbn_lstm = list( map(add, pred_dbn_lstm, y_dbn_lstm))\n",
    "    pred_gru = list( map(add, pred_gru, y_gru))\n",
    "    pred_ffnn = list( map(add, pred_ffnn, y_ffnn))\n",
    "\n",
    "    \n",
    "y_pred_LSTM = []\n",
    "y_pred_LSTM_GRU = []\n",
    "y_pred_DBN_LSTM = []\n",
    "y_pred_GRU = []\n",
    "y_pred_FFNN = []\n",
    "\n",
    "for i in range(len(pred_lstm)):\n",
    "    ay_lstm = float(pred_lstm[i]/len(test_pred_lstm))\n",
    "    ay_lstm_gru = float(pred_lstm_gru[i]/len(test_pred_lstm_gru))\n",
    "    ay_dbn_lstm = float(pred_dbn_lstm[i]/len(test_pred_dbn_lstm))\n",
    "    ay_gru = float(pred_gru[i]/len(test_pred_gru))\n",
    "    ay_ffnn = float(pred_ffnn[i]/len(test_pred_ffnn))\n",
    "    \n",
    "    \n",
    "    y_pred_LSTM.append(ay_lstm)\n",
    "    y_pred_LSTM_GRU.append(ay_lstm_gru)\n",
    "    y_pred_DBN_LSTM.append(ay_dbn_lstm)\n",
    "    y_pred_GRU.append(ay_gru)\n",
    "    y_pred_FFNN.append(ay_ffnn)\n",
    "    \n",
    "\n",
    "y_pred_LSTM = extract(y_pred_LSTM)\n",
    "y_pred_LSTM_GRU = extract(y_pred_LSTM_GRU)\n",
    "y_pred_DBN_LSTM = extract(y_pred_DBN_LSTM)\n",
    "y_pred_FFNN = extract(y_pred_FFNN)\n",
    "y_pred_GRU = extract(y_pred_GRU)\n",
    "\n",
    "# Y_test = extract(Y_test)\n",
    "# Y_train = extract(Y_train)\n",
    "\n",
    "\n",
    "y_pred_LSTM = scalery.inverse_transform(y_pred_LSTM)*-1\n",
    "y_pred_LSTM_GRU = scalery.inverse_transform(y_pred_LSTM_GRU)*-1\n",
    "y_pred_DBN_LSTM = scalery.inverse_transform(y_pred_DBN_LSTM)*-1\n",
    "y_pred_GRU = scalery.inverse_transform(y_pred_GRU)*-1\n",
    "y_pred_FFNN = scalery.inverse_transform(y_pred_FFNN)*-1\n",
    "\n",
    "\n",
    "# Y_test\n",
    "# error = history.history['val_loss']\n",
    "# error = scalery.inverse_transform(error)\n",
    "\n",
    "Y_test_sc = scalery.inverse_transform(Y_test)*-1\n",
    "\n",
    "Y_train_sc = scalery.inverse_transform(Y_train)*-1\n",
    "\n",
    "# #LSTM\n",
    "pred_LSTM = pd.DataFrame(y_pred_LSTM)\n",
    "pred_LSTM['y'] = pd.DataFrame(year_test)\n",
    "pred_LSTM.columns = ['LSTM_gwls', 'y']\n",
    "pred_LSTM = pred_LSTM.set_index('y')\n",
    "\n",
    "# #LSTM_GRU\n",
    "pred_LSTM_GRU = pd.DataFrame(y_pred_LSTM_GRU)\n",
    "pred_LSTM_GRU['y'] = pd.DataFrame(year_test)\n",
    "pred_LSTM_GRU.columns = ['LSTM_GRU_gwls', 'y']\n",
    "pred_LSTM_GRU = pred_LSTM_GRU.set_index('y')\n",
    "\n",
    "#DBN-LSTM\n",
    "y_pred_DBN_LSTM = pd.DataFrame(y_pred_DBN_LSTM)\n",
    "y_pred_DBN_LSTM['y'] = pd.DataFrame(year_test)\n",
    "y_pred_DBN_LSTM.columns = ['DBN_LSTM_gwls', 'y']\n",
    "y_pred_DBN_LSTM = y_pred_DBN_LSTM.set_index('y')\n",
    "\n",
    "#FFNN\n",
    "pred_FFNN = pd.DataFrame(y_pred_FFNN)\n",
    "pred_FFNN['y'] = pd.DataFrame(year_test)\n",
    "pred_FFNN.columns = ['FFNN_gwls', 'y']\n",
    "pred_FFNN = pred_FFNN.set_index('y')\n",
    "\n",
    "#GRU\n",
    "pred_GRU = pd.DataFrame(y_pred_GRU)\n",
    "pred_GRU['y'] = pd.DataFrame(year_test)\n",
    "pred_GRU.columns = ['GRU_gwls', 'y']\n",
    "pred_GRU = pred_GRU.set_index('y')\n",
    "\n",
    "\n",
    "real_output = pd.DataFrame(Y_test_sc)\n",
    "real_output['y'] = pd.DataFrame(year_test)\n",
    "real_output.columns = ['actual_test_gwls', 'y']\n",
    "real_output = real_output.set_index('y')\n",
    "\n",
    "train_output = pd.DataFrame(Y_train_sc)\n",
    "train_output['y'] = pd.DataFrame(year_train)\n",
    "train_output.columns = ['actual_gwls', 'y']\n",
    "train_output = train_output.set_index('y')\n",
    "\n",
    "pre_df = pd.concat([real_output, pred_LSTM,pred_GRU,pred_FFNN , pred_LSTM_GRU, y_pred_DBN_LSTM], axis = 1)\n",
    "\n",
    "# the_df = pd.concat([train_output,pre_df], axis = 0)\n",
    "\n",
    "mess_LSTM = pd.DataFrame(test_pred_lstm).transpose()\n",
    "mess_LSTM = scalery.inverse_transform(mess_LSTM)*-1\n",
    "mess_LSTM = pd.DataFrame(mess_LSTM)\n",
    "mess_LSTM['sem'] = mess_LSTM.std(axis = 1, skipna = True)\n",
    "\n",
    "mess_GRU = pd.DataFrame(test_pred_gru).transpose()\n",
    "mess_GRU = scalery.inverse_transform(mess_GRU)*-1\n",
    "mess_GRU = pd.DataFrame(mess_GRU)\n",
    "mess_GRU['sem'] = mess_GRU.std(axis = 1, skipna = True)\n",
    "\n",
    "mess_FFNN = pd.DataFrame(test_pred_ffnn).transpose()\n",
    "mess_FFNN = scalery.inverse_transform(mess_FFNN)*-1\n",
    "mess_FFNN = pd.DataFrame(mess_FFNN)\n",
    "mess_FFNN['sem'] = mess_FFNN.std(axis = 1, skipna = True)\n",
    "\n",
    "mess_LSTM_GRU = pd.DataFrame(test_pred_lstm_gru).transpose()\n",
    "mess_LSTM_GRU = scalery.inverse_transform(mess_LSTM_GRU)*-1\n",
    "mess_LSTM_GRU = pd.DataFrame(mess_LSTM_GRU)\n",
    "mess_LSTM_GRU['sem'] = mess_LSTM_GRU.std(axis = 1, skipna = True)\n",
    "\n",
    "mess_DBN_LSTM = pd.DataFrame(test_pred_dbn_lstm).transpose()\n",
    "mess_DBN_LSTM = scalery.inverse_transform(mess_DBN_LSTM)*-1\n",
    "mess_DBN_LSTM = pd.DataFrame(mess_DBN_LSTM)\n",
    "mess_DBN_LSTM['sem'] = mess_DBN_LSTM.std(axis = 1, skipna = True)\n",
    "pre_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0242eba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_gwls</th>\n",
       "      <th>actual_test_gwls</th>\n",
       "      <th>LSTM_gwls</th>\n",
       "      <th>GRU_gwls</th>\n",
       "      <th>FFNN_gwls</th>\n",
       "      <th>LSTM_GRU_gwls</th>\n",
       "      <th>DBN_LSTM_gwls</th>\n",
       "      <th>lstm_low_err</th>\n",
       "      <th>lstm_up_err</th>\n",
       "      <th>gru_low_err</th>\n",
       "      <th>gru_up_err</th>\n",
       "      <th>ffnn_low_err</th>\n",
       "      <th>ffnn_up_err</th>\n",
       "      <th>lstm_gru_low_err</th>\n",
       "      <th>lstm_gru_up_err</th>\n",
       "      <th>dbn_lstm_low_err</th>\n",
       "      <th>dbn_lstm_up_err</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>17.1750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>16.5675</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>16.4000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>16.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>16.6200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>NaN</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>18.850189</td>\n",
       "      <td>18.984790</td>\n",
       "      <td>18.850591</td>\n",
       "      <td>18.793064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.703040</td>\n",
       "      <td>18.997339</td>\n",
       "      <td>17.961721</td>\n",
       "      <td>20.007859</td>\n",
       "      <td>18.414813</td>\n",
       "      <td>19.286369</td>\n",
       "      <td>18.599106</td>\n",
       "      <td>18.987022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>NaN</td>\n",
       "      <td>14.920000</td>\n",
       "      <td>18.859911</td>\n",
       "      <td>19.001212</td>\n",
       "      <td>18.866323</td>\n",
       "      <td>18.805942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.708102</td>\n",
       "      <td>19.011721</td>\n",
       "      <td>17.985333</td>\n",
       "      <td>20.017091</td>\n",
       "      <td>18.437365</td>\n",
       "      <td>19.295281</td>\n",
       "      <td>18.596020</td>\n",
       "      <td>19.015863</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>NaN</td>\n",
       "      <td>14.940000</td>\n",
       "      <td>18.871488</td>\n",
       "      <td>19.018899</td>\n",
       "      <td>18.882531</td>\n",
       "      <td>18.816745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.701732</td>\n",
       "      <td>19.041245</td>\n",
       "      <td>18.023343</td>\n",
       "      <td>20.014454</td>\n",
       "      <td>18.452723</td>\n",
       "      <td>19.312338</td>\n",
       "      <td>18.596940</td>\n",
       "      <td>19.036550</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>NaN</td>\n",
       "      <td>14.919672</td>\n",
       "      <td>18.893435</td>\n",
       "      <td>19.043615</td>\n",
       "      <td>18.907283</td>\n",
       "      <td>18.831164</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.686765</td>\n",
       "      <td>19.100105</td>\n",
       "      <td>18.118852</td>\n",
       "      <td>19.968378</td>\n",
       "      <td>18.478809</td>\n",
       "      <td>19.335757</td>\n",
       "      <td>18.594285</td>\n",
       "      <td>19.068043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>NaN</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>18.907826</td>\n",
       "      <td>19.061204</td>\n",
       "      <td>18.922910</td>\n",
       "      <td>18.842530</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.672797</td>\n",
       "      <td>19.142855</td>\n",
       "      <td>18.180400</td>\n",
       "      <td>19.942008</td>\n",
       "      <td>18.487188</td>\n",
       "      <td>19.358632</td>\n",
       "      <td>18.595668</td>\n",
       "      <td>19.089392</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>371 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual_gwls  actual_test_gwls  LSTM_gwls   GRU_gwls  FFNN_gwls  \\\n",
       "y                                                                      \n",
       "1974      17.1750               NaN        NaN        NaN        NaN   \n",
       "1974      16.5675               NaN        NaN        NaN        NaN   \n",
       "1974      16.4000               NaN        NaN        NaN        NaN   \n",
       "1974      16.5000               NaN        NaN        NaN        NaN   \n",
       "1974      16.6200               NaN        NaN        NaN        NaN   \n",
       "...           ...               ...        ...        ...        ...   \n",
       "2004          NaN         14.900000  18.850189  18.984790  18.850591   \n",
       "2004          NaN         14.920000  18.859911  19.001212  18.866323   \n",
       "2004          NaN         14.940000  18.871488  19.018899  18.882531   \n",
       "2004          NaN         14.919672  18.893435  19.043615  18.907283   \n",
       "2004          NaN         14.900000  18.907826  19.061204  18.922910   \n",
       "\n",
       "      LSTM_GRU_gwls  DBN_LSTM_gwls  lstm_low_err  lstm_up_err  gru_low_err  \\\n",
       "y                                                                            \n",
       "1974            NaN            NaN           NaN          NaN          NaN   \n",
       "1974            NaN            NaN           NaN          NaN          NaN   \n",
       "1974            NaN            NaN           NaN          NaN          NaN   \n",
       "1974            NaN            NaN           NaN          NaN          NaN   \n",
       "1974            NaN            NaN           NaN          NaN          NaN   \n",
       "...             ...            ...           ...          ...          ...   \n",
       "2004      18.793064            NaN     18.703040    18.997339    17.961721   \n",
       "2004      18.805942            NaN     18.708102    19.011721    17.985333   \n",
       "2004      18.816745            NaN     18.701732    19.041245    18.023343   \n",
       "2004      18.831164            NaN     18.686765    19.100105    18.118852   \n",
       "2004      18.842530            NaN     18.672797    19.142855    18.180400   \n",
       "\n",
       "      gru_up_err  ffnn_low_err  ffnn_up_err  lstm_gru_low_err  \\\n",
       "y                                                               \n",
       "1974         NaN           NaN          NaN               NaN   \n",
       "1974         NaN           NaN          NaN               NaN   \n",
       "1974         NaN           NaN          NaN               NaN   \n",
       "1974         NaN           NaN          NaN               NaN   \n",
       "1974         NaN           NaN          NaN               NaN   \n",
       "...          ...           ...          ...               ...   \n",
       "2004   20.007859     18.414813    19.286369         18.599106   \n",
       "2004   20.017091     18.437365    19.295281         18.596020   \n",
       "2004   20.014454     18.452723    19.312338         18.596940   \n",
       "2004   19.968378     18.478809    19.335757         18.594285   \n",
       "2004   19.942008     18.487188    19.358632         18.595668   \n",
       "\n",
       "      lstm_gru_up_err  dbn_lstm_low_err  dbn_lstm_up_err  \n",
       "y                                                         \n",
       "1974              NaN               NaN              NaN  \n",
       "1974              NaN               NaN              NaN  \n",
       "1974              NaN               NaN              NaN  \n",
       "1974              NaN               NaN              NaN  \n",
       "1974              NaN               NaN              NaN  \n",
       "...               ...               ...              ...  \n",
       "2004        18.987022               NaN              NaN  \n",
       "2004        19.015863               NaN              NaN  \n",
       "2004        19.036550               NaN              NaN  \n",
       "2004        19.068043               NaN              NaN  \n",
       "2004        19.089392               NaN              NaN  \n",
       "\n",
       "[371 rows x 17 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# #LSTM\n",
    "E_low_L = pd.DataFrame(pre_df[\"LSTM_gwls\"].values - mess_LSTM['sem'].values).set_index(year_test)\n",
    "E_low_L.columns = ['lstm_low_err']\n",
    "\n",
    "E_up_L = pd.DataFrame(pre_df[\"LSTM_gwls\"].values + mess_LSTM['sem'].values).set_index(year_test)\n",
    "E_up_L.columns = ['lstm_up_err']\n",
    "\n",
    "# #GRU\n",
    "E_low_G = pd.DataFrame(pre_df[\"GRU_gwls\"].values - mess_GRU['sem'].values).set_index(year_test)\n",
    "E_low_G.columns = ['gru_low_err']\n",
    "\n",
    "E_up_G = pd.DataFrame(pre_df[\"GRU_gwls\"].values + mess_GRU['sem'].values).set_index(year_test)\n",
    "E_up_G.columns = ['gru_up_err']\n",
    "\n",
    "# #GRU\n",
    "E_low_F = pd.DataFrame(pre_df[\"FFNN_gwls\"].values - mess_FFNN['sem'].values).set_index(year_test)\n",
    "E_low_F.columns = ['ffnn_low_err']\n",
    "\n",
    "E_up_F = pd.DataFrame(pre_df[\"FFNN_gwls\"].values + mess_FFNN['sem'].values).set_index(year_test)\n",
    "E_up_F.columns = ['ffnn_up_err']\n",
    "\n",
    "# #LSTM-GRU\n",
    "E_low_LG = pd.DataFrame(pre_df[\"LSTM_GRU_gwls\"].values - mess_LSTM_GRU['sem'].values).set_index(year_test)\n",
    "E_low_LG.columns = ['lstm_gru_low_err']\n",
    "\n",
    "E_up_LG = pd.DataFrame(pre_df[\"LSTM_GRU_gwls\"].values + mess_LSTM_GRU['sem'].values).set_index(year_test)\n",
    "E_up_LG.columns = ['lstm_gru_up_err']\n",
    "\n",
    "#DBN-LSTM\n",
    "E_low_DL = pd.DataFrame(pre_df[\"DBN_LSTM_gwls\"].values - mess_DBN_LSTM['sem'].values).set_index(year_test)\n",
    "E_low_DL.columns = ['dbn_lstm_low_err']\n",
    "\n",
    "E_up_DL = pd.DataFrame(pre_df[\"DBN_LSTM_gwls\"].values + mess_DBN_LSTM['sem'].values).set_index(year_test)\n",
    "E_up_DL.columns = ['dbn_lstm_up_err']\n",
    "\n",
    "pre_df_full = pd.concat([pre_df, E_low_L, E_up_L,E_low_G,E_up_G,E_low_F,E_up_F, \n",
    "                    E_low_LG, E_up_LG, E_low_DL, E_up_DL], axis = 1)\n",
    "\n",
    "the_df = pd.concat([train_output,pre_df_full], axis = 0)\n",
    "the_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c9c03b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LSTM_gwls</th>\n",
       "      <th>GRU_gwls</th>\n",
       "      <th>FFNN_gwls</th>\n",
       "      <th>LSTM_GRU_gwls</th>\n",
       "      <th>DBN_LSTM_gwls</th>\n",
       "      <th>lstm_low_err</th>\n",
       "      <th>lstm_up_err</th>\n",
       "      <th>gru_low_err</th>\n",
       "      <th>gru_up_err</th>\n",
       "      <th>ffnn_low_err</th>\n",
       "      <th>ffnn_up_err</th>\n",
       "      <th>lstm_gru_low_err</th>\n",
       "      <th>lstm_gru_up_err</th>\n",
       "      <th>dbn_lstm_low_err</th>\n",
       "      <th>dbn_lstm_up_err</th>\n",
       "      <th>Actual_gwls</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.567500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>18.850189</td>\n",
       "      <td>18.984790</td>\n",
       "      <td>18.850591</td>\n",
       "      <td>18.793064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.703040</td>\n",
       "      <td>18.997339</td>\n",
       "      <td>17.961721</td>\n",
       "      <td>20.007859</td>\n",
       "      <td>18.414813</td>\n",
       "      <td>19.286369</td>\n",
       "      <td>18.599106</td>\n",
       "      <td>18.987022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>18.859911</td>\n",
       "      <td>19.001212</td>\n",
       "      <td>18.866323</td>\n",
       "      <td>18.805942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.708102</td>\n",
       "      <td>19.011721</td>\n",
       "      <td>17.985333</td>\n",
       "      <td>20.017091</td>\n",
       "      <td>18.437365</td>\n",
       "      <td>19.295281</td>\n",
       "      <td>18.596020</td>\n",
       "      <td>19.015863</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>18.871488</td>\n",
       "      <td>19.018899</td>\n",
       "      <td>18.882531</td>\n",
       "      <td>18.816745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.701732</td>\n",
       "      <td>19.041245</td>\n",
       "      <td>18.023343</td>\n",
       "      <td>20.014454</td>\n",
       "      <td>18.452723</td>\n",
       "      <td>19.312338</td>\n",
       "      <td>18.596940</td>\n",
       "      <td>19.036550</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>18.893435</td>\n",
       "      <td>19.043615</td>\n",
       "      <td>18.907283</td>\n",
       "      <td>18.831164</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.686765</td>\n",
       "      <td>19.100105</td>\n",
       "      <td>18.118852</td>\n",
       "      <td>19.968378</td>\n",
       "      <td>18.478809</td>\n",
       "      <td>19.335757</td>\n",
       "      <td>18.594285</td>\n",
       "      <td>19.068043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.919672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>18.907826</td>\n",
       "      <td>19.061204</td>\n",
       "      <td>18.922910</td>\n",
       "      <td>18.842530</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.672797</td>\n",
       "      <td>19.142855</td>\n",
       "      <td>18.180400</td>\n",
       "      <td>19.942008</td>\n",
       "      <td>18.487188</td>\n",
       "      <td>19.358632</td>\n",
       "      <td>18.595668</td>\n",
       "      <td>19.089392</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>371 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LSTM_gwls   GRU_gwls  FFNN_gwls  LSTM_GRU_gwls  DBN_LSTM_gwls  \\\n",
       "y                                                                     \n",
       "1974        NaN        NaN        NaN            NaN            NaN   \n",
       "1974        NaN        NaN        NaN            NaN            NaN   \n",
       "1974        NaN        NaN        NaN            NaN            NaN   \n",
       "1974        NaN        NaN        NaN            NaN            NaN   \n",
       "1974        NaN        NaN        NaN            NaN            NaN   \n",
       "...         ...        ...        ...            ...            ...   \n",
       "2004  18.850189  18.984790  18.850591      18.793064            NaN   \n",
       "2004  18.859911  19.001212  18.866323      18.805942            NaN   \n",
       "2004  18.871488  19.018899  18.882531      18.816745            NaN   \n",
       "2004  18.893435  19.043615  18.907283      18.831164            NaN   \n",
       "2004  18.907826  19.061204  18.922910      18.842530            NaN   \n",
       "\n",
       "      lstm_low_err  lstm_up_err  gru_low_err  gru_up_err  ffnn_low_err  \\\n",
       "y                                                                        \n",
       "1974           NaN          NaN          NaN         NaN           NaN   \n",
       "1974           NaN          NaN          NaN         NaN           NaN   \n",
       "1974           NaN          NaN          NaN         NaN           NaN   \n",
       "1974           NaN          NaN          NaN         NaN           NaN   \n",
       "1974           NaN          NaN          NaN         NaN           NaN   \n",
       "...            ...          ...          ...         ...           ...   \n",
       "2004     18.703040    18.997339    17.961721   20.007859     18.414813   \n",
       "2004     18.708102    19.011721    17.985333   20.017091     18.437365   \n",
       "2004     18.701732    19.041245    18.023343   20.014454     18.452723   \n",
       "2004     18.686765    19.100105    18.118852   19.968378     18.478809   \n",
       "2004     18.672797    19.142855    18.180400   19.942008     18.487188   \n",
       "\n",
       "      ffnn_up_err  lstm_gru_low_err  lstm_gru_up_err  dbn_lstm_low_err  \\\n",
       "y                                                                        \n",
       "1974          NaN               NaN              NaN               NaN   \n",
       "1974          NaN               NaN              NaN               NaN   \n",
       "1974          NaN               NaN              NaN               NaN   \n",
       "1974          NaN               NaN              NaN               NaN   \n",
       "1974          NaN               NaN              NaN               NaN   \n",
       "...           ...               ...              ...               ...   \n",
       "2004    19.286369         18.599106        18.987022               NaN   \n",
       "2004    19.295281         18.596020        19.015863               NaN   \n",
       "2004    19.312338         18.596940        19.036550               NaN   \n",
       "2004    19.335757         18.594285        19.068043               NaN   \n",
       "2004    19.358632         18.595668        19.089392               NaN   \n",
       "\n",
       "      dbn_lstm_up_err  Actual_gwls  \n",
       "y                                   \n",
       "1974              NaN    17.175000  \n",
       "1974              NaN    16.567500  \n",
       "1974              NaN    16.400000  \n",
       "1974              NaN    16.500000  \n",
       "1974              NaN    16.620000  \n",
       "...               ...          ...  \n",
       "2004              NaN    14.900000  \n",
       "2004              NaN    14.920000  \n",
       "2004              NaN    14.940000  \n",
       "2004              NaN    14.919672  \n",
       "2004              NaN    14.900000  \n",
       "\n",
       "[371 rows x 16 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mg = the_df['actual_gwls'].fillna(the_df['actual_test_gwls'])\n",
    "the_df = the_df.drop([\"actual_gwls\"], axis = 1)\n",
    "the_df = the_df.drop([\"actual_test_gwls\"], axis = 1)\n",
    "\n",
    "\n",
    "the_df[\"Actual_gwls\"] = mg\n",
    "the_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0718a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "the_df['Actual_gwls'].plot(figsize=(20,10), fontsize=30,color='black', linewidth=3)\n",
    "# the_df['actual_test_gwls'].plot(fontsize=15, color='black', linewidth=2)\n",
    "\n",
    "the_df['LSTM_gwls'].plot(color='#E52B50', linewidth=3)\n",
    "x = np.arange(len(the_df.index))\n",
    "plt.fill_between(x, the_df['lstm_low_err'], the_df['lstm_up_err'], \n",
    "             color = '#9F2B68', alpha = 0.5)\n",
    "\n",
    "the_df['GRU_gwls'].plot(color='#333399', linewidth=3)\n",
    "x = np.arange(len(the_df.index))\n",
    "plt.fill_between(x, the_df['gru_low_err'], the_df['gru_up_err'], \n",
    "             color = '#A2A2D0', alpha = 0.5)\n",
    "\n",
    "the_df['FFNN_gwls'].plot(color='#665D1E', linewidth=3)\n",
    "x = np.arange(len(the_df.index))\n",
    "plt.fill_between(x, the_df['ffnn_low_err'], the_df['ffnn_up_err'], \n",
    "             color = '#CD9575', alpha = 0.5)\n",
    "\n",
    "the_df['LSTM_GRU_gwls'].plot(color='#0048BA', linewidth=3)\n",
    "# x = np.arange(len(the_df.index))\n",
    "plt.fill_between(x, the_df['lstm_gru_low_err'], the_df['lstm_gru_up_err'], \n",
    "             color = '#72A0C1', alpha = 0.5)\n",
    "\n",
    "\n",
    "the_df['DBN_LSTM_gwls'].plot(color='green', linewidth=3)\n",
    "# x = np.arange(len(the_df.index))\n",
    "plt.fill_between(x, the_df['dbn_lstm_low_err'], the_df['dbn_lstm_up_err'], \n",
    "             color = '#B0BF1A', alpha = 0.5)\n",
    "\n",
    "plt.xlabel(\"Year\",fontsize=30)\n",
    "plt.ylabel(\"Groundwater level (mbgl)\",fontsize=30)\n",
    "# plt.xticks(the_df.index[0])\n",
    "plt.title(bbh,fontsize=30)\n",
    "plt.legend(loc='best', prop={'size': 20})\n",
    "plt.savefig(figname, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30e9c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D4N0053\n",
      "\n",
      "\n",
      "DNB r2:  0.3217002189225071\n",
      "LSTM r2:  0.8070387563603821\n",
      "GRU r2:  0.7380001784529395\n",
      "FFNN r2:  0.8049576939792218\n",
      "LSTM-GRU r2:  0.8068200902560705\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-28-050527ee8e87>\", line 8, in <module>\n",
      "    print(\"DBN-LSTM r2: \",dbn_lstm_r2_new)\n",
      "NameError: name 'dbn_lstm_r2_new' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-28-050527ee8e87>\", line 8, in <module>\n",
      "    print(\"DBN-LSTM r2: \",dbn_lstm_r2_new)\n",
      "NameError: name 'dbn_lstm_r2_new' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3360, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1193, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-28-050527ee8e87>\", line 8, in <module>\n",
      "    print(\"DBN-LSTM r2: \",dbn_lstm_r2_new)\n",
      "NameError: name 'dbn_lstm_r2_new' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3360, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1193, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2895, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3072, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3282, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1211, in structured_traceback\n",
      "    chained_exceptions_tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\b14ck\\anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n"
     ]
    }
   ],
   "source": [
    "print(borehole)\n",
    "print(\"\\n\")\n",
    "print(\"DNB r2: \",dbn_r2_new)\n",
    "print(\"LSTM r2: \",lstm_r2_new)\n",
    "print(\"GRU r2: \",gru_r2_new)\n",
    "print(\"FFNN r2: \",ffnn_r2_new)\n",
    "print(\"LSTM-GRU r2: \",lstm_gru_r2_new)\n",
    "print(\"DBN-LSTM r2: \",dbn_lstm_r2_new)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"DNB mse: \",dbn_mse_new)\n",
    "print(\"LSTM mse: \",lstm_mse_new)\n",
    "print(\"GRU mse: \",gru_mse_new)\n",
    "print(\"FFNN mse: \",ffnn_mse_new)\n",
    "print(\"LSTM-GRU mse: \",lstm_gru_mse_new)\n",
    "print(\"DBN-LSTM mse: \",dbn_lstm_mse_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f4282",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
